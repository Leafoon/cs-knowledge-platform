# Hugging Face Transformers å®Œæ•´å­¦ä¹ å¤§çº²

> **Version**: Based on Transformers v4.40+ (2026å¹´1æœˆ)  
> **Target Audience**: AI ç ”ç©¶å‘˜ã€æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆã€ç ”ç©¶ç”Ÿ  
> **Prerequisite**: Python åŸºç¡€ã€PyTorch åŸºç¡€ã€æ·±åº¦å­¦ä¹ åŸºæœ¬æ¦‚å¿µ

---

## ğŸ“š **è¯¾ç¨‹ç»“æ„æ¦‚è§ˆ**

```
Part I: åŸºç¡€å…¥é—¨ (Chapters 0-3)
Part II: æ¨¡å‹è®­ç»ƒä¸å¾®è°ƒ (Chapters 4-7)
Part III: å‚æ•°é«˜æ•ˆå¾®è°ƒ (Chapters 8-10)
Part IV: é‡åŒ–ä¸ä½ç²¾åº¦ (Chapters 11-13)
Part V: åˆ†å¸ƒå¼è®­ç»ƒ (Chapters 14-16)
Part VI: æ¨ç†ä¼˜åŒ– (Chapters 17-19)
Part VII: ç”Ÿäº§éƒ¨ç½² (Chapters 20-22)
Part VIII: åº•å±‚æœºåˆ¶ä¸è‡ªå®šä¹‰ (Chapters 23-25)
Part IX: é«˜çº§ä¸»é¢˜ä¸ç”Ÿæ€ (Chapters 26-28)
```

---

## Part I: åŸºç¡€å…¥é—¨ (Foundation)

### **Chapter 0: Transformers ç”Ÿæ€ç³»ç»Ÿæ¦‚è§ˆ**
- 0.1 ä»€ä¹ˆæ˜¯ Hugging Face Transformersï¼Ÿ
  - 0.1.1 è®¾è®¡å“²å­¦ï¼šç»Ÿä¸€çš„ API æ¥å£
  - 0.1.2 ä¸å…¶ä»–æ¡†æ¶å¯¹æ¯”ï¼ˆFairseqã€AllenNLPã€PaddleNLPï¼‰
  - 0.1.3 ç”Ÿæ€ç»„ä»¶å…¨æ™¯å›¾ï¼ˆDatasetsã€Tokenizersã€Accelerateã€PEFTï¼‰
- 0.2 ç¯å¢ƒå‡†å¤‡ä¸å®‰è£…
  - 0.2.1 å®‰è£…ç­–ç•¥ï¼ˆpip vs condaï¼ŒCPU vs GPUï¼‰
  - 0.2.2 ç‰ˆæœ¬å…¼å®¹æ€§çŸ©é˜µï¼ˆPyTorchã€CUDAã€transformersï¼‰
  - 0.2.3 éªŒè¯å®‰è£…ï¼šå¿«é€Ÿæµ‹è¯•è„šæœ¬
- 0.3 Hugging Face Hub å…¥é—¨
  - 0.3.1 æ¨¡å‹ä»“åº“ç»“æ„ï¼ˆconfig.jsonã€pytorch_model.binã€tokenizer æ–‡ä»¶ï¼‰
  - 0.3.2 è®¿é—®ä»¤ç‰Œï¼ˆAccess Tokenï¼‰ä¸ç§æœ‰æ¨¡å‹
  - 0.3.3 æœ¬åœ°ç¼“å­˜æœºåˆ¶ï¼ˆ~/.cache/huggingfaceï¼‰
- 0.4 ç¬¬ä¸€ä¸ªç¤ºä¾‹ï¼šæƒ…æ„Ÿåˆ†æ Pipeline
  - 0.4.1 é›¶ä»£ç ä½“éªŒï¼špipeline() ä¸€è¡Œè°ƒç”¨
  - 0.4.2 è¾“å‡ºè§£æï¼šlogitsã€labelsã€scores
  - 0.4.3 æ”¯æŒçš„ä»»åŠ¡ç±»å‹å…¨åˆ—è¡¨

### **Chapter 1: Pipeline å¿«é€Ÿä¸Šæ‰‹**
- 1.1 Pipeline æ¶æ„è§£æ
  - 1.1.1 ä¸‰é˜¶æ®µæµæ°´çº¿ï¼ˆTokenization â†’ Model â†’ Post-processingï¼‰
  - 1.1.2 è‡ªåŠ¨ä»»åŠ¡æ¨æ–­æœºåˆ¶
  - 1.1.3 è®¾å¤‡ç®¡ç†ï¼ˆCPUã€GPUã€å¤š GPUï¼‰
- 1.2 æ–‡æœ¬åˆ†ç±» Pipeline
  - 1.2.1 æƒ…æ„Ÿåˆ†æï¼ˆsentiment-analysisï¼‰
  - 1.2.2 é›¶æ ·æœ¬åˆ†ç±»ï¼ˆzero-shot-classificationï¼‰
  - 1.2.3 è‡ªå®šä¹‰æ ‡ç­¾æ˜ å°„
- 1.3 æ–‡æœ¬ç”Ÿæˆ Pipeline
  - 1.3.1 åŸºç¡€æ–‡æœ¬ç”Ÿæˆï¼ˆtext-generationï¼‰
  - 1.3.2 ç”Ÿæˆå‚æ•°è¯¦è§£ï¼ˆmax_lengthã€temperatureã€top_kã€top_pã€num_beamsï¼‰
  - 1.3.3 æ‰¹é‡ç”Ÿæˆä¸æµå¼è¾“å‡º
- 1.4 é—®ç­”ä¸æŠ½å– Pipeline
  - 1.4.1 æŠ½å–å¼é—®ç­”ï¼ˆquestion-answeringï¼‰
  - 1.4.2 è¡¨æ ¼é—®ç­”ï¼ˆtable-question-answeringï¼‰
  - 1.4.3 æ–‡æ¡£é—®ç­”ï¼ˆdocument-question-answeringï¼‰
- 1.5 å…¶ä»–å¸¸ç”¨ Pipeline
  - 1.5.1 å‘½åå®ä½“è¯†åˆ«ï¼ˆner / token-classificationï¼‰
  - 1.5.2 æ‘˜è¦ç”Ÿæˆï¼ˆsummarizationï¼‰
  - 1.5.3 ç¿»è¯‘ï¼ˆtranslationï¼‰
  - 1.5.4 å¡«ç©ºï¼ˆfill-maskï¼‰
  - 1.5.5 ç‰¹å¾æå–ï¼ˆfeature-extractionï¼‰
- 1.6 Pipeline çš„é™åˆ¶ä¸ä½•æ—¶ä¸ç”¨
  - 1.6.1 æ€§èƒ½ç“¶é¢ˆåˆ†æ
  - 1.6.2 æ‰¹å¤„ç†çš„å¿…è¦æ€§
  - 1.6.3 è½¬å‘åº•å±‚ API çš„æ—¶æœº

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `PipelineFlowVisualizer` - Pipeline ä¸‰é˜¶æ®µæµç¨‹å¯è§†åŒ–
- `TaskGallery` - æ‰€æœ‰ä»»åŠ¡ç±»å‹äº¤äº’å¼æ¼”ç¤º

---

### **Chapter 2: Tokenization æ·±åº¦å‰–æ**
- 2.1 Tokenizer æ ¸å¿ƒæ¦‚å¿µ
  - 2.1.1 ä»æ–‡æœ¬åˆ° ID çš„æ˜ å°„è¿‡ç¨‹
  - 2.1.2 è¯æ±‡è¡¨ï¼ˆVocabularyï¼‰ä¸ç‰¹æ®Šæ ‡è®°ï¼ˆ[CLS], [SEP], [PAD], [MASK]ï¼‰
  - 2.1.3 ç¼–ç ï¼ˆencodeï¼‰ä¸è§£ç ï¼ˆdecodeï¼‰
- 2.2 Tokenization ç®—æ³•å®¶æ—
  - 2.2.1 WordPieceï¼ˆBERTã€DistilBERTï¼‰
  - 2.2.2 Byte-Pair Encoding / BPEï¼ˆGPT-2ã€RoBERTaï¼‰
  - 2.2.3 Unigramï¼ˆXLNetã€ALBERTï¼‰
  - 2.2.4 SentencePieceï¼ˆT5ã€ALBERTã€XLM-RoBERTaï¼‰
  - 2.2.5 ç®—æ³•å¯¹æ¯”ä¸é€‰æ‹©æŒ‡å—
- 2.3 AutoTokenizer ä½¿ç”¨è¯¦è§£
  - 2.3.1 from_pretrained() å‚æ•°è¯¦è§£
  - 2.3.2 æ‰¹é‡ç¼–ç ï¼ˆbatch_encode_plusï¼‰
  - 2.3.3 æˆªæ–­ï¼ˆtruncationï¼‰ä¸å¡«å……ï¼ˆpaddingï¼‰ç­–ç•¥
  - 2.3.4 è¿”å›å¼ é‡æ ¼å¼ï¼ˆreturn_tensors='pt' vs 'tf' vs 'np'ï¼‰
- 2.4 é«˜çº§ Tokenization æŠ€å·§
  - 2.4.1 åŠ¨æ€ paddingï¼ˆDataCollatorï¼‰
  - 2.4.2 å¤„ç†é•¿æ–‡æœ¬ï¼ˆstrideã€max_lengthã€overflowï¼‰
  - 2.4.3 Fast Tokenizer çš„ä¼˜åŠ¿ï¼ˆRust å®ç°ã€offset mappingï¼‰
  - 2.4.4 è‡ªå®šä¹‰è¯æ±‡è¡¨ä¸è®­ç»ƒ tokenizer
- 2.5 ç‰¹æ®Šåœºæ™¯å¤„ç†
  - 2.5.1 å¤šè¯­è¨€ tokenizationï¼ˆXLM-RoBERTaï¼‰
  - 2.5.2 å¯¹è¯å†å²ç¼–ç ï¼ˆchat templatesï¼‰
  - 2.5.3 ç»“æ„åŒ–è¾“å…¥ï¼ˆJSONã€è¡¨æ ¼ï¼‰
  - 2.5.4 ä»£ç  tokenizationï¼ˆCodeBERTã€CodeGenï¼‰
- 2.6 å¸¸è§é™·é˜±ä¸è°ƒè¯•
  - 2.6.1 Token ID ä¸ Position ID çš„åŒºåˆ«
  - 2.6.2 Attention Mask çš„ä½œç”¨
  - 2.6.3 ä¸ºä»€ä¹ˆæœ‰æ—¶éœ€è¦ token_type_idsï¼Ÿ
  - 2.6.4 Tokenizer ç‰ˆæœ¬ä¸åŒ¹é…é—®é¢˜

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `TokenizationVisualizer` - å®æ—¶å±•ç¤ºæ–‡æœ¬ â†’ subword â†’ ID è¿‡ç¨‹
- `TokenAlgorithmComparison` - WordPiece vs BPE vs Unigram å¯¹æ¯”
- `AttentionMaskBuilder` - Attention Mask ä¸ Padding å¯è§†åŒ–

---

### **Chapter 3: æ¨¡å‹æ¶æ„ä¸ Auto ç±»**
- 3.1 Transformer æ¨¡å‹å®¶æ—æ¦‚è§ˆ
  - 3.1.1 Encoder-onlyï¼ˆBERTã€RoBERTaã€ELECTRAï¼‰
  - 3.1.2 Decoder-onlyï¼ˆGPT ç³»åˆ—ã€LLaMAã€Mistralï¼‰
  - 3.1.3 Encoder-Decoderï¼ˆT5ã€BARTã€mT5ï¼‰
  - 3.1.4 æ¶æ„é€‰æ‹©æŒ‡å—ï¼ˆä»»åŠ¡ â†’ æ¶æ„æ˜ å°„ï¼‰
- 3.2 Auto ç±»ä½“ç³»
  - 3.2.1 AutoConfigï¼šé…ç½®è‡ªåŠ¨åŠ è½½
  - 3.2.2 AutoTokenizerï¼štokenizer è‡ªåŠ¨åŒ¹é…
  - 3.2.3 AutoModelï¼šé€šç”¨æ¨¡å‹åŠ è½½
  - 3.2.4 AutoModelForXXXï¼šä»»åŠ¡ä¸“ç”¨æ¨¡å‹å¤´
- 3.3 æ¨¡å‹åŠ è½½è¯¦è§£
  - 3.3.1 from_pretrained() å‚æ•°å…¨è§£æ
  - 3.3.2 æœ¬åœ°åŠ è½½ vs Hub åŠ è½½
  - 3.3.3 æƒé‡æ–‡ä»¶æ ¼å¼ï¼ˆsafetensors vs bin vs h5ï¼‰
  - 3.3.4 åˆ†ç‰‡åŠ è½½å¤§æ¨¡å‹ï¼ˆsharded checkpointsï¼‰
- 3.4 æ¨¡å‹é…ç½®ï¼ˆConfigï¼‰
  - 3.4.1 config.json ç»“æ„è¯¦è§£
  - 3.4.2 ä¿®æ”¹æ¨¡å‹é…ç½®ï¼ˆnum_labelsã€hidden_size ç­‰ï¼‰
  - 3.4.3 è‡ªå®šä¹‰é…ç½®ç±»
- 3.5 æ¨¡å‹è¾“å‡ºç»“æ„
  - 3.5.1 ModelOutput åŸºç±»
  - 3.5.2 logitsã€hidden_statesã€attentions çš„å«ä¹‰
  - 3.5.3 output_hidden_states ä¸ output_attentions å‚æ•°
- 3.6 é¢„è®­ç»ƒæƒé‡çš„è¿ç§»å­¦ä¹ 
  - 3.6.1 å¤´éƒ¨æ›¿æ¢ï¼ˆå¿½ç•¥æƒé‡è­¦å‘Šï¼‰
  - 3.6.2 éƒ¨åˆ†æƒé‡åˆå§‹åŒ–
  - 3.6.3 è·¨æ¨¡å‹æƒé‡è¿ç§»

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `ArchitectureExplorer` - å¯äº¤äº’çš„æ¨¡å‹æ¶æ„å›¾ï¼ˆBERT vs GPT vs T5ï¼‰
- `ConfigEditor` - å®æ—¶ä¿®æ”¹ config å¹¶æŸ¥çœ‹å½±å“
- `ModelOutputInspector` - æ¢ç´¢æ¨¡å‹è¾“å‡ºçš„æ¯ä¸ªå­—æ®µ

---

## Part II: æ¨¡å‹è®­ç»ƒä¸å¾®è°ƒ (Training & Fine-tuning)

### **Chapter 4: Datasets åº“ä¸æ•°æ®é¢„å¤„ç†**
- 4.1 Datasets åº“åŸºç¡€
  - 4.1.1 ä¸ºä»€ä¹ˆéœ€è¦ Datasetsï¼Ÿï¼ˆå†…å­˜æ˜ å°„ã€Arrow åç«¯ï¼‰
  - 4.1.2 åŠ è½½æ•°æ®é›†ï¼ˆload_datasetï¼‰
  - 4.1.3 Hub æ•°æ®é›†æµè§ˆï¼ˆdatasets-serverï¼‰
- 4.2 æ•°æ®é›†æ“ä½œ
  - 4.2.1 map()ï¼šæ‰¹é‡è½¬æ¢
  - 4.2.2 filter()ï¼šæ¡ä»¶ç­›é€‰
  - 4.2.3 select()ã€shuffle()ã€train_test_split()
  - 4.2.4 æ•°æ®é›†æ‹¼æ¥ä¸äº¤ç»‡ï¼ˆconcatenateã€interleaveï¼‰
- 4.3 Tokenization é›†æˆ
  - 4.3.1 ä½¿ç”¨ map() æ‰¹é‡ tokenize
  - 4.3.2 remove_columns() æ¸…ç†åŸå§‹å­—æ®µ
  - 4.3.3 set_format()ï¼šPyTorch/TensorFlow æ ¼å¼
- 4.4 DataCollator å®¶æ—
  - 4.4.1 DataCollatorWithPaddingï¼šåŠ¨æ€ padding
  - 4.4.2 DataCollatorForLanguageModelingï¼šMLM æ©ç 
  - 4.4.3 DataCollatorForSeq2Seqï¼šEncoder-Decoder ä¸“ç”¨
  - 4.4.4 è‡ªå®šä¹‰ DataCollator
- 4.5 æµå¼æ•°æ®é›†ï¼ˆStreamingï¼‰
  - 4.5.1 ä½•æ—¶ä½¿ç”¨æµå¼æ¨¡å¼
  - 4.5.2 IterableDataset vs Dataset
  - 4.5.3 æµå¼æ•°æ®çš„ shuffle ä¸ç¼“å†²
- 4.6 è‡ªå®šä¹‰æ•°æ®é›†
  - 4.6.1 ä» CSV/JSON åŠ è½½
  - 4.6.2 ä» Python å­—å…¸åˆ›å»º
  - 4.6.3 ä¸Šä¼ è‡ªå®šä¹‰æ•°æ®é›†åˆ° Hub

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `DatasetPipeline` - æ•°æ®é¢„å¤„ç†æµç¨‹å¯è§†åŒ–ï¼ˆåŸå§‹æ–‡æœ¬ â†’ tokenized â†’ batchedï¼‰
- `DataCollatorDemo` - åŠ¨æ€ padding è¿‡ç¨‹æ¼”ç¤º

---

### **Chapter 5: Trainer API å®Œæ•´æŒ‡å—**
- 5.1 Trainer æ ¸å¿ƒè®¾è®¡
  - 5.1.1 ä¸ºä»€ä¹ˆéœ€è¦ Trainerï¼Ÿï¼ˆvs æ‰‹å†™è®­ç»ƒå¾ªç¯ï¼‰
  - 5.1.2 Trainer å†…éƒ¨æµç¨‹æ¦‚è§ˆ
  - 5.1.3 ä¸ PyTorch Lightningã€Keras çš„å¯¹æ¯”
- 5.2 TrainingArguments è¯¦è§£
  - 5.2.1 è¾“å‡ºä¸æ—¥å¿—ï¼ˆoutput_dirã€logging_dirã€logging_stepsï¼‰
  - 5.2.2 è®­ç»ƒè¶…å‚æ•°ï¼ˆlearning_rateã€num_train_epochsã€per_device_train_batch_sizeï¼‰
  - 5.2.3 ä¼˜åŒ–å™¨é€‰æ‹©ï¼ˆoptim="adamw_torch" vs "adafactor"ï¼‰
  - 5.2.4 å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆlr_scheduler_typeï¼‰
  - 5.2.5 æ¢¯åº¦ç›¸å…³ï¼ˆgradient_accumulation_stepsã€max_grad_normï¼‰
  - 5.2.6 è¯„ä¼°ä¸ä¿å­˜ï¼ˆevaluation_strategyã€save_strategyã€load_best_model_at_endï¼‰
  - 5.2.7 æ··åˆç²¾åº¦ï¼ˆfp16ã€bf16ã€tf32ï¼‰
- 5.3 ç¬¬ä¸€ä¸ªå®Œæ•´è®­ç»ƒç¤ºä¾‹
  - 5.3.1 æƒ…æ„Ÿåˆ†æå¾®è°ƒï¼ˆBERT on IMDBï¼‰
  - 5.3.2 è®¡ç®—æŒ‡æ ‡ï¼ˆaccuracyã€F1ï¼‰
  - 5.3.3 Trainer åˆå§‹åŒ–ä¸è®­ç»ƒ
  - 5.3.4 é¢„æµ‹ä¸è¯„ä¼°
- 5.4 å›è°ƒå‡½æ•°ï¼ˆCallbacksï¼‰
  - 5.4.1 å†…ç½®å›è°ƒï¼ˆEarlyStoppingCallbackã€TensorBoardCallbackï¼‰
  - 5.4.2 è‡ªå®šä¹‰ Callback
  - 5.4.3 è®­ç»ƒè¿‡ç¨‹ç›‘æ§ä¸å¹²é¢„
- 5.5 å¤š GPU è®­ç»ƒåŸºç¡€
  - 5.5.1 DataParallelï¼ˆä¸æ¨èï¼‰
  - 5.5.2 DistributedDataParallelï¼ˆæ¨èï¼‰
  - 5.5.3 å¯åŠ¨å‘½ä»¤ï¼ˆtorchrunã€accelerate launchï¼‰
- 5.6 Trainer çš„é«˜çº§ç‰¹æ€§
  - 5.6.1 è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰
  - 5.6.2 æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆgradient_checkpointingï¼‰
  - 5.6.3 æ¢¯åº¦ç´¯ç§¯
  - 5.6.4 è¶…å‚æ•°æœç´¢ï¼ˆOptunaã€Ray Tuneï¼‰

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `TrainingLoopVisualizer` - Trainer å†…éƒ¨å¾ªç¯å¯è§†åŒ–
- `TrainingMetricsPlot` - å®æ—¶è®­ç»ƒæ›²çº¿ç»˜åˆ¶
- `GradientAccumulationDemo` - æ¢¯åº¦ç´¯ç§¯åŸç†æ¼”ç¤º

---

### **Chapter 6: åºåˆ—åˆ°åºåˆ—ä»»åŠ¡å¾®è°ƒ**
- 6.1 Seq2Seq æ¨¡å‹æ¦‚è§ˆ
  - 6.1.1 T5ã€BARTã€mBARTã€Pegasus
  - 6.1.2 Encoder-Decoder æ¶æ„è¯¦è§£
  - 6.1.3 ä½•æ—¶ä½¿ç”¨ Seq2Seq æ¨¡å‹
- 6.2 æ–‡æœ¬æ‘˜è¦å¾®è°ƒ
  - 6.2.1 æ•°æ®é›†é€‰æ‹©ï¼ˆCNN/DailyMailã€XSumï¼‰
  - 6.2.2 æ‘˜è¦è´¨é‡è¯„ä¼°ï¼ˆROUGEã€BERTScoreï¼‰
  - 6.2.3 å®Œæ•´è®­ç»ƒä»£ç 
- 6.3 æœºå™¨ç¿»è¯‘å¾®è°ƒ
  - 6.3.1 æ•°æ®é›†ï¼ˆWMTã€OPUSï¼‰
  - 6.3.2 BLEU è¯„åˆ†
  - 6.3.3 å¤šè¯­è¨€æ¨¡å‹ï¼ˆmT5ã€mBARTï¼‰
- 6.4 ç”Ÿæˆä»»åŠ¡çš„ç‰¹æ®Šè€ƒè™‘
  - 6.4.1 label_smoothing
  - 6.4.2 length_penalty
  - 6.4.3 early_stopping ç­–ç•¥
- 6.5 Seq2SeqTrainer ä¸“ç”¨åŠŸèƒ½
  - 6.5.1 predict_with_generate
  - 6.5.2 generation_max_length
  - 6.5.3 generation_num_beams

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `Seq2SeqArchitecture` - Encoder-Decoder æ³¨æ„åŠ›æµå¯è§†åŒ–
- `BeamSearchVisualizer` - Beam Search ç”Ÿæˆè¿‡ç¨‹åŠ¨ç”»

---

### **Chapter 7: æ–‡æœ¬ç”Ÿæˆæ·±åº¦æ¢ç´¢**
- 7.1 ç”Ÿæˆå¼æ¨¡å‹åŸºç¡€
  - 7.1.1 è‡ªå›å½’ç”ŸæˆåŸç†
  - 7.1.2 Causal Language Modeling
  - 7.1.3 GPT ç³»åˆ—æ¨¡å‹
- 7.2 generate() æ–¹æ³•è¯¦è§£
  - 7.2.1 æ ¸å¿ƒå‚æ•°ä¸€è§ˆ
  - 7.2.2 åœæ­¢æ¡ä»¶ï¼ˆmax_lengthã€max_new_tokensã€eos_token_idï¼‰
  - 7.2.3 è¾“å‡ºæ§åˆ¶ï¼ˆnum_return_sequencesã€return_dict_in_generateï¼‰
- 7.3 è§£ç ç­–ç•¥ï¼ˆDecoding Strategiesï¼‰
  - 7.3.1 Greedy Searchï¼ˆè´ªå©ªæœç´¢ï¼‰
  - 7.3.2 Beam Searchï¼ˆæŸæœç´¢ï¼‰
  - 7.3.3 Samplingï¼ˆé‡‡æ ·ï¼‰
    - 7.3.3.1 Top-K Sampling
    - 7.3.3.2 Top-P / Nucleus Sampling
    - 7.3.3.3 Temperature Scaling
  - 7.3.4 Contrastive Search
  - 7.3.5 è§£ç ç­–ç•¥å¯¹æ¯”å®éªŒ
- 7.4 ç”Ÿæˆè´¨é‡æ§åˆ¶
  - 7.4.1 é‡å¤æƒ©ç½šï¼ˆrepetition_penaltyï¼‰
  - 7.4.2 é•¿åº¦æƒ©ç½šï¼ˆlength_penaltyï¼‰
  - 7.4.3 No Repeat N-gram
  - 7.4.4 Bad Words è¿‡æ»¤
- 7.5 Constrained Generation
  - 7.5.1 å‰ç¼€çº¦æŸï¼ˆprefixï¼‰
  - 7.5.2 å¼ºåˆ¶è¯è¯­ï¼ˆforce_words_idsï¼‰
  - 7.5.3 LogitsProcessor è‡ªå®šä¹‰
- 7.6 æµå¼ç”Ÿæˆï¼ˆStreamingï¼‰
  - 7.6.1 TextIteratorStreamer
  - 7.6.2 å®æ—¶è¾“å‡ºå®ç°
  - 7.6.3 Web åº”ç”¨é›†æˆ
- 7.7 Chat æ¨¡æ¿ä¸å¯¹è¯ç”Ÿæˆ
  - 7.7.1 apply_chat_template()
  - 7.7.2 å¤šè½®å¯¹è¯å†å²ç®¡ç†
  - 7.7.3 ChatMLã€Alpacaã€Vicuna æ ¼å¼

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `GenerationStrategyComparator` - å„ç§è§£ç ç­–ç•¥å®æ—¶å¯¹æ¯”
- `TemperatureSlider` - Temperature å‚æ•°å¯è§†åŒ–å½±å“
- `KVCacheVisualizer` - KV Cache åŠ¨æ€ç®¡ç†è¿‡ç¨‹
- `ChatTemplateBuilder` - Chat æ¨¡æ¿å¯è§†åŒ–ç¼–è¾‘å™¨

---

## Part III: å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT)

### **Chapter 8: PEFT åº“å…¥é—¨**
- 8.1 ä¸ºä»€ä¹ˆéœ€è¦ PEFTï¼Ÿ
  - 8.1.1 å…¨å‚æ•°å¾®è°ƒçš„å›°å¢ƒï¼ˆæ˜¾å­˜ã€æ—¶é—´ã€å­˜å‚¨ï¼‰
  - 8.1.2 å‚æ•°é«˜æ•ˆæ–¹æ³•çš„ç†è®ºåŸºç¡€
  - 8.1.3 æ€§èƒ½å¯¹æ¯”ï¼šå‡†ç¡®ç‡ vs å‚æ•°é‡
- 8.2 PEFT åº“æ¶æ„
  - 8.2.1 å®‰è£…ä¸ç‰ˆæœ¬å…¼å®¹
  - 8.2.2 æ”¯æŒçš„æ–¹æ³•ä¸€è§ˆï¼ˆLoRAã€Prefix Tuningã€P-Tuningã€Adapter ç­‰ï¼‰
  - 8.2.3 ä¸ Transformers Trainer æ— ç¼é›†æˆ
- 8.3 PEFT åŸºæœ¬å·¥ä½œæµ
  - 8.3.1 åŠ è½½åŸºç¡€æ¨¡å‹
  - 8.3.2 é…ç½® PEFT æ–¹æ³•
  - 8.3.3 åº”ç”¨ get_peft_model()
  - 8.3.4 è®­ç»ƒä¸ä¿å­˜
  - 8.3.5 åŠ è½½ PEFT æƒé‡æ¨ç†
- 8.4 å¯è®­ç»ƒå‚æ•°å¯¹æ¯”
  - 8.4.1 print_trainable_parameters()
  - 8.4.2 å‚æ•°é‡å¯¹æ¯”å®éªŒï¼ˆFull Fine-tuning vs PEFTï¼‰

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `PEFTMethodsGallery` - PEFT å„æ–¹æ³•å¯è§†åŒ–å¯¹æ¯”
- `ParameterCountComparison` - å‚æ•°é‡æŸ±çŠ¶å›¾å¯¹æ¯”

---

### **Chapter 9: LoRA è¯¦è§£**
- 9.1 LoRA åŸç†æ·±åº¦å‰–æ
  - 9.1.1 ä½ç§©åˆ†è§£æ•°å­¦åŸºç¡€ï¼ˆ$W = W_0 + BA$ï¼‰
  - 9.1.2 ä¸ºä»€ä¹ˆä½ç§©é€‚é…æœ‰æ•ˆï¼Ÿ
  - 9.1.3 ä¸ Adapterã€Prefix Tuning çš„åŒºåˆ«
- 9.2 LoraConfig å‚æ•°è¯¦è§£
  - 9.2.1 rï¼ˆç§©ï¼‰ï¼šæ€§èƒ½ä¸æ•ˆç‡çš„æƒè¡¡
  - 9.2.2 lora_alphaï¼šç¼©æ”¾å› å­
  - 9.2.3 lora_dropoutï¼šæ­£åˆ™åŒ–
  - 9.2.4 target_modulesï¼šåº”ç”¨åˆ°å“ªäº›å±‚ï¼ˆq_projã€v_projã€å…¨è¿æ¥å±‚ï¼‰
  - 9.2.5 biasï¼šåç½®é¡¹å¤„ç†
  - 9.2.6 task_typeï¼šä»»åŠ¡ç±»å‹æ ‡è¯†
- 9.3 å®Œæ•´ LoRA å¾®è°ƒç¤ºä¾‹
  - 9.3.1 LLaMA-2 æŒ‡ä»¤å¾®è°ƒ
  - 9.3.2 Alpaca æ•°æ®é›†å‡†å¤‡
  - 9.3.3 è®­ç»ƒä¸éªŒè¯
  - 9.3.4 åˆå¹¶æƒé‡ï¼ˆmerge_and_unloadï¼‰
- 9.4 LoRA é«˜çº§æŠ€å·§
  - 9.4.1 å¤š LoRA é€‚é…å™¨ï¼ˆMulti-Adapterï¼‰
  - 9.4.2 åŠ¨æ€åˆ‡æ¢é€‚é…å™¨
  - 9.4.3 LoRA æƒé‡åˆå¹¶ç­–ç•¥
  - 9.4.4 Rank-Stabilized LoRAï¼ˆrsLoRAï¼‰
- 9.5 æ€§èƒ½åˆ†æ
  - 9.5.1 æ˜¾å­˜å ç”¨å¯¹æ¯”
  - 9.5.2 è®­ç»ƒé€Ÿåº¦å¯¹æ¯”
  - 9.5.3 ä¸åŒ rank çš„å‡†ç¡®ç‡æ›²çº¿

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `LoRAMatrixInjection` - LoRA çŸ©é˜µæ³¨å…¥è¿‡ç¨‹åŠ¨ç”»
- `LoRARankExplorer` - å¯è°ƒèŠ‚ rank å¹¶å®æ—¶æŸ¥çœ‹å‚æ•°é‡å˜åŒ–
- `LoRAWeightMerge` - æƒé‡åˆå¹¶å‰åå¯¹æ¯”

---

### **Chapter 10: QLoRA ä¸é‡åŒ–å¾®è°ƒ**
- 10.1 QLoRA çªç ´æ€§åˆ›æ–°
  - 10.1.1 QLoRA è®ºæ–‡æ ¸å¿ƒæ€æƒ³
  - 10.1.2 4-bit NormalFloatï¼ˆNF4ï¼‰æ•°æ®ç±»å‹
  - 10.1.3 åŒé‡é‡åŒ–ï¼ˆDouble Quantizationï¼‰
  - 10.1.4 Paged Optimizers
- 10.2 BitsAndBytesConfig è¯¦è§£
  - 10.2.1 load_in_4bit vs load_in_8bit
  - 10.2.2 bnb_4bit_compute_dtypeï¼ˆæ¨è bfloat16ï¼‰
  - 10.2.3 bnb_4bit_use_double_quant
  - 10.2.4 bnb_4bit_quant_typeï¼ˆfp4 vs nf4ï¼‰
- 10.3 QLoRA å®Œæ•´å®æˆ˜
  - 10.3.1 ç¯å¢ƒå‡†å¤‡ï¼ˆbitsandbytes å®‰è£…ï¼‰
  - 10.3.2 åŠ è½½é‡åŒ–æ¨¡å‹
  - 10.3.3 åº”ç”¨ LoRA åˆ°é‡åŒ–æ¨¡å‹
  - 10.3.4 è®­ç»ƒä¸æ¨ç†
- 10.4 æ˜¾å­˜ä¼˜åŒ–æé™
  - 10.4.1 70B æ¨¡å‹åœ¨å•å¡ 24GB æ˜¾å¡å¾®è°ƒ
  - 10.4.2 æ˜¾å­˜åˆ†æå·¥å…·ï¼ˆnvidia-smiã€torch.cuda.memory_summaryï¼‰
  - 10.4.3 ä¸å…¨ç²¾åº¦å¾®è°ƒå¯¹æ¯”
- 10.5 é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰
  - 10.5.1 QAT vs Post-Training Quantization
  - 10.5.2 QAT è®­ç»ƒæµç¨‹
- 10.6 å…¶ä»– PEFT æ–¹æ³•
  - 10.6.1 Prefix Tuning
  - 10.6.2 P-Tuning v2
  - 10.6.3 Prompt Tuning
  - 10.6.4 Adapter Layers
  - 10.6.5 (IA)Â³ - Infused Adapter

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `QuantizationVisualizer` - é‡åŒ–å‰åæƒé‡åˆ†å¸ƒå¯¹æ¯”ï¼ˆç›´æ–¹å›¾ï¼‰
- `QLoRAMemoryBreakdown` - QLoRA æ˜¾å­˜å ç”¨åˆ†è§£å›¾
- `NF4EncodingDemo` - NF4 ç¼–ç è¿‡ç¨‹æ¼”ç¤º

---

## Part IV: é‡åŒ–ä¸ä½ç²¾åº¦ (Quantization & Low-Precision)

### **Chapter 11: æ··åˆç²¾åº¦è®­ç»ƒ**
- 11.1 æµ®ç‚¹æ•°åŸºç¡€
  - 11.1.1 FP32ã€FP16ã€BF16 æ ¼å¼å¯¹æ¯”
  - 11.1.2 åŠ¨æ€èŒƒå›´ä¸ç²¾åº¦æƒè¡¡
  - 11.1.3 ä¸ºä»€ä¹ˆ BF16 æ›´é€‚åˆæ·±åº¦å­¦ä¹ ï¼Ÿ
- 11.2 è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰
  - 11.2.1 torch.cuda.amp åŸç†
  - 11.2.2 GradScaler æ¢¯åº¦ç¼©æ”¾
  - 11.2.3 Trainer ä¸­å¯ç”¨ fp16/bf16
- 11.3 TrainingArguments æ··åˆç²¾åº¦å‚æ•°
  - 11.3.1 fp16=True
  - 11.3.2 bf16=Trueï¼ˆéœ€è¦ Ampere æ¶æ„ï¼‰
  - 11.3.3 tf32=Trueï¼ˆA100 ä¼˜åŒ–ï¼‰
  - 11.3.4 fp16_opt_levelï¼ˆApexï¼‰
- 11.4 æ··åˆç²¾åº¦æœ€ä½³å®è·µ
  - 11.4.1 loss scaling ç­–ç•¥
  - 11.4.2 é¿å…æ•°å€¼æº¢å‡º
  - 11.4.3 ä½•æ—¶ä¸ä½¿ç”¨æ··åˆç²¾åº¦
- 11.5 æ€§èƒ½åŸºå‡†æµ‹è¯•
  - 11.5.1 è®­ç»ƒé€Ÿåº¦æå‡ï¼ˆ1.5x-3xï¼‰
  - 11.5.2 æ˜¾å­˜èŠ‚çœï¼ˆ~50%ï¼‰
  - 11.5.3 å‡†ç¡®ç‡å½±å“åˆ†æ

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `FloatFormatComparison` - FP32/FP16/BF16 æ ¼å¼å¯è§†åŒ–å¯¹æ¯”
- `AMPWorkflow` - AMP è®­ç»ƒæµç¨‹åŠ¨ç”»
- `GradScalerVisualizer` - æ¢¯åº¦ç¼©æ”¾è¿‡ç¨‹æ¼”ç¤º

---

### **Chapter 12: Post-Training Quantization (PTQ)**
- 12.1 PTQ åŸºç¡€æ¦‚å¿µ
  - 12.1.1 è®­ç»ƒåé‡åŒ– vs é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
  - 12.1.2 é™æ€é‡åŒ– vs åŠ¨æ€é‡åŒ–
  - 12.1.3 é‡åŒ–ç²’åº¦ï¼ˆPer-Tensor vs Per-Channelï¼‰
- 12.2 GPTQ é‡åŒ–
  - 12.2.1 GPTQ ç®—æ³•åŸç†ï¼ˆOptimal Brain Quantizationï¼‰
  - 12.2.2 å®‰è£… auto-gptq
  - 12.2.3 é‡åŒ–æ¨¡å‹åŠ è½½ï¼ˆGPTQConfigï¼‰
  - 12.2.4 é‡åŒ–æ¨¡å‹æ¨ç†
  - 12.2.5 ä¸ bitsandbytes å¯¹æ¯”
- 12.3 AWQ é‡åŒ–
  - 12.3.1 Activation-aware Weight Quantization
  - 12.3.2 å®‰è£… autoawq
  - 12.3.3 AWQ é‡åŒ–æµç¨‹
  - 12.3.4 æ¨ç†åŠ é€Ÿæ•ˆæœ
- 12.4 å…¶ä»–é‡åŒ–æ–¹æ³•
  - 12.4.1 GGUF/GGMLï¼ˆllama.cpp ç”Ÿæ€ï¼‰
  - 12.4.2 HQQï¼ˆHalf-Quadratic Quantizationï¼‰
  - 12.4.3 EETQï¼ˆEfficient Exact Token Quantizationï¼‰
  - 12.4.4 SmoothQuant
- 12.5 é‡åŒ–è¯„ä¼°
  - 12.5.1 å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰å¯¹æ¯”
  - 12.5.2 ä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®ç‡
  - 12.5.3 æ¨ç†ååé‡
  - 12.5.4 æ¨¡å‹å¤§å°å‹ç¼©æ¯”

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `QuantizationMethodComparison` - GPTQ vs AWQ vs bitsandbytes å¯¹æ¯”è¡¨
- `PerplexityChart` - é‡åŒ–å‰åå›°æƒ‘åº¦å¯¹æ¯”
- `WeightDistributionShift` - é‡åŒ–å¯¼è‡´çš„æƒé‡åˆ†å¸ƒå˜åŒ–

---

### **Chapter 13: Gradient Checkpointing ä¸å†…å­˜ä¼˜åŒ–**
- 13.1 æ¢¯åº¦æ£€æŸ¥ç‚¹åŸç†
  - 13.1.1 è®¡ç®—æ¢å†…å­˜ï¼ˆRecomputationï¼‰
  - 13.1.2 é€‚ç”¨åœºæ™¯ï¼ˆå¤§æ¨¡å‹ã€é•¿åºåˆ—ï¼‰
  - 13.1.3 æ€§èƒ½ trade-off
- 13.2 å¯ç”¨ Gradient Checkpointing
  - 13.2.1 model.gradient_checkpointing_enable()
  - 13.2.2 TrainingArguments.gradient_checkpointing
  - 13.2.3 use_reentrant å‚æ•°
- 13.3 å…¶ä»–å†…å­˜ä¼˜åŒ–æŠ€å·§
  - 13.3.1 æ¢¯åº¦ç´¯ç§¯ï¼ˆgradient_accumulation_stepsï¼‰
  - 13.3.2 flash attentionï¼ˆuse_flash_attention_2ï¼‰
  - 13.3.3 CPU Offload
  - 13.3.4 è™šæ‹Ÿæ˜¾å­˜ï¼ˆvram sharingï¼‰
- 13.4 å†…å­˜åˆ†æå·¥å…·
  - 13.4.1 torch.cuda.memory_summary()
  - 13.4.2 torch.profiler
  - 13.4.3 nvidia-smi æŒç»­ç›‘æ§
- 13.5 æé™æ˜¾å­˜ä¼˜åŒ–ç»„åˆ
  - 13.5.1 QLoRA + Gradient Checkpointing + Flash Attention
  - 13.5.2 ZeRO-Offload
  - 13.5.3 å®æˆ˜ï¼šå•å¡ 24GB è®­ç»ƒ 70B æ¨¡å‹

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `GradientCheckpointingVisualizer` - å‰å‘/åå‘ä¼ æ’­å†…å­˜å ç”¨å¯¹æ¯”
- `MemoryBreakdownChart` - æ˜¾å­˜å ç”¨åˆ†è§£ï¼ˆæ¨¡å‹æƒé‡ã€ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¿€æ´»å€¼ã€æ¢¯åº¦ï¼‰
- `OptimizationCombinator` - å¯é€‰æ‹©å¤šç§ä¼˜åŒ–æŠ€æœ¯å¹¶æŸ¥çœ‹æ˜¾å­˜å½±å“

---

## Part V: åˆ†å¸ƒå¼è®­ç»ƒ (Distributed Training)

### **Chapter 14: Accelerate åº“å®Œå…¨æŒ‡å—**
- 14.1 Accelerate è®¾è®¡å“²å­¦
  - 14.1.1 ç»Ÿä¸€çš„åˆ†å¸ƒå¼è®­ç»ƒæ¥å£
  - 14.1.2 ä¸ Trainer çš„å…³ç³»
  - 14.1.3 æ”¯æŒçš„åç«¯ï¼ˆDDPã€FSDPã€DeepSpeedã€TPUï¼‰
- 14.2 Accelerate åŸºç¡€å·¥ä½œæµ
  - 14.2.1 accelerate config é…ç½®å‘å¯¼
  - 14.2.2 Accelerator ç±»æ ¸å¿ƒ API
  - 14.2.3 ä»£ç ä¿®æ”¹æœ€å°åŒ–ï¼ˆ3 è¡Œæ”¹åŠ¨ï¼‰
  - 14.2.4 accelerate launch å¯åŠ¨è„šæœ¬
- 14.3 ä»å•å¡åˆ°å¤šå¡
  - 14.3.1 å• GPU è®­ç»ƒ
  - 14.3.2 å¤š GPU å•æœºï¼ˆDDPï¼‰
  - 14.3.3 å¤šæœºå¤šå¡é›†ç¾¤
  - 14.3.4 æ··åˆç²¾åº¦é›†æˆ
- 14.4 Accelerator é«˜çº§åŠŸèƒ½
  - 14.4.1 æ¢¯åº¦ç´¯ç§¯
  - 14.4.2 Checkpoint ä¿å­˜ä¸æ¢å¤
  - 14.4.3 Logging ä¸åŒæ­¥
  - 14.4.4 ä¸»è¿›ç¨‹æ§åˆ¶ï¼ˆmain_process_firstï¼‰
- 14.5 ä¸ Trainer é›†æˆ
  - 14.5.1 Trainer è‡ªåŠ¨æ£€æµ‹ Accelerate é…ç½®
  - 14.5.2 è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ vs Trainer
- 14.6 è°ƒè¯•æŠ€å·§
  - 14.6.1 ACCELERATE_DEBUG_MODE
  - 14.6.2 gather() ä¸ reduce() æ“ä½œ
  - 14.6.3 æ­»é”æ’æŸ¥

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `AccelerateWorkflow` - Accelerate ä»£ç è½¬æ¢å‰åå¯¹æ¯”
- `DistributedCommunication` - å¤š GPU é€šä¿¡æ¨¡å¼å¯è§†åŒ–ï¼ˆall-reduceã€broadcastï¼‰

---

### **Chapter 15: FSDP (Fully Sharded Data Parallel)**
- 15.1 FSDP åŸç†æ·±åº¦è§£æ
  - 15.1.1 ZeRO ä¼˜åŒ–å™¨çš„ä¸‰ä¸ªé˜¶æ®µ
  - 15.1.2 PyTorch FSDP vs DeepSpeed ZeRO
  - 15.1.3 åˆ†ç‰‡ç­–ç•¥ï¼ˆFULL_SHARDã€SHARD_GRAD_OPã€NO_SHARDï¼‰
- 15.2 FSDP é…ç½®
  - 15.2.1 fsdp_config.yaml æ–‡ä»¶ç¼–å†™
  - 15.2.2 TrainingArguments.fsdp å‚æ•°
  - 15.2.3 sharding_strategy é€‰æ‹©
  - 15.2.4 cpu_offload é…ç½®
- 15.3 FSDP è®­ç»ƒå®æˆ˜
  - 15.3.1 å¯åŠ¨å‘½ä»¤ï¼ˆtorchrun vs accelerate launchï¼‰
  - 15.3.2 æ¨¡å‹åŒ…è£…ï¼ˆauto_wrap_policyï¼‰
  - 15.3.3 æ··åˆç²¾åº¦ä¸ FSDP
  - 15.3.4 Checkpoint ä¿å­˜ç­–ç•¥
- 15.4 FSDP æœ€ä½³å®è·µ
  - 15.4.1 å±‚çº§åŒ…è£…ï¼ˆtransformer_layer_cls_to_wrapï¼‰
  - 15.4.2 æ¿€æ´»æ£€æŸ¥ç‚¹é›†æˆ
  - 15.4.3 é€šä¿¡ä¼˜åŒ–ï¼ˆbackward_prefetchï¼‰
- 15.5 æ€§èƒ½åˆ†æ
  - 15.5.1 æ‰©å±•æ€§æµ‹è¯•ï¼ˆ1/2/4/8 GPUï¼‰
  - 15.5.2 é€šä¿¡å¼€é”€åˆ†æ
  - 15.5.3 ä¸ DDP å¯¹æ¯”

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `FSDPShardingVisualizer` - FSDP å‚æ•°åˆ†ç‰‡è¿‡ç¨‹åŠ¨ç”»
- `ZeROStagesComparison` - ZeRO-1/2/3 å†…å­˜å ç”¨å¯¹æ¯”
- `AllGatherReduceScatter` - all-gather ä¸ reduce-scatter é€šä¿¡åŠ¨ç”»

---

### **Chapter 16: DeepSpeed é›†æˆ**
- 16.1 DeepSpeed æ¦‚è§ˆ
  - 16.1.1 ZeRO ä¼˜åŒ–å™¨ï¼ˆZeRO-1/2/3ï¼‰
  - 16.1.2 ä¸ FSDP çš„å·®å¼‚
  - 16.1.3 ä½•æ—¶é€‰æ‹© DeepSpeed
- 16.2 DeepSpeed é…ç½®æ–‡ä»¶
  - 16.2.1 ds_config.json ç»“æ„è¯¦è§£
  - 16.2.2 ZeRO Stage é€‰æ‹©ï¼ˆ0/1/2/3ï¼‰
  - 16.2.3 Offload é…ç½®ï¼ˆCPU/NVMeï¼‰
  - 16.2.4 æ··åˆç²¾åº¦é…ç½®
- 16.3 Trainer + DeepSpeed
  - 16.3.1 TrainingArguments.deepspeed å‚æ•°
  - 16.3.2 å¯åŠ¨è®­ç»ƒï¼ˆdeepspeed launcherï¼‰
  - 16.3.3 Checkpoint è½¬æ¢
- 16.4 ZeRO-Offload ä¸ ZeRO-Infinity
  - 16.4.1 CPU Offload ç­–ç•¥
  - 16.4.2 NVMe Offloadï¼ˆè¶…å¤§æ¨¡å‹ï¼‰
  - 16.4.3 æ€§èƒ½ trade-off
- 16.5 DeepSpeed æ¨ç†
  - 16.5.1 ZeRO-Inference
  - 16.5.2 Kernel èåˆåŠ é€Ÿ
  - 16.5.3 å¼ é‡å¹¶è¡Œ
- 16.6 é«˜çº§ç‰¹æ€§
  - 16.6.1 Pipeline Parallelism
  - 16.6.2 3D Parallelismï¼ˆæ•°æ® + å¼ é‡ + æµæ°´çº¿ï¼‰
  - 16.6.3 Curriculum Learning

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `DeepSpeedZeROVisualizer` - ZeRO-3 å†…å­˜åˆ†ç‰‡ä¸ Offload æµç¨‹
- `3DParallelismDiagram` - æ•°æ®å¹¶è¡Œ + å¼ é‡å¹¶è¡Œ + æµæ°´çº¿å¹¶è¡Œæ¶æ„å›¾
- `OffloadTimeline` - CPU/NVMe Offload æ—¶é—´çº¿åˆ†æ

---

## Part VI: æ¨ç†ä¼˜åŒ– (Inference Optimization)

### **Chapter 17: é«˜æ•ˆæ¨ç†åŸºç¡€**
- 17.1 æ¨ç†æ€§èƒ½æŒ‡æ ‡
  - 17.1.1 å»¶è¿Ÿï¼ˆLatencyï¼‰vs ååé‡ï¼ˆThroughputï¼‰
  - 17.1.2 Time to First Token (TTFT)
  - 17.1.3 Tokens per Second (TPS)
  - 17.1.4 æ‰¹å¤„ç†æ•ˆç‡
- 17.2 BetterTransformer
  - 17.2.1 FastPath æ‰§è¡Œè·¯å¾„
  - 17.2.2 å¯ç”¨æ–¹å¼ï¼ˆmodel.to_bettertransformer()ï¼‰
  - 17.2.3 æ”¯æŒçš„æ¨¡å‹æ¶æ„
  - 17.2.4 æ€§èƒ½æå‡ï¼ˆ1.2x-2xï¼‰
- 17.3 Flash Attention 2
  - 17.3.1 IO-Aware æ³¨æ„åŠ›ç®—æ³•
  - 17.3.2 å®‰è£… flash-attn
  - 17.3.3 use_flash_attention_2=True
  - 17.3.4 æ˜¾å­˜èŠ‚çœä¸é€Ÿåº¦æå‡
- 17.4 torch.compile (PyTorch 2.0+)
  - 17.4.1 TorchDynamo + TorchInductor
  - 17.4.2 ç¼–è¯‘æ¨¡å¼ï¼ˆdefaultã€reduce-overheadã€max-autotuneï¼‰
  - 17.4.3 æ¨¡å‹ç¼–è¯‘ï¼ˆtorch.compile(model)ï¼‰
  - 17.4.4 é¦–æ¬¡è¿è¡Œå¼€é”€ï¼ˆwarm-upï¼‰
- 17.5 é™æ€ KV Cache
  - 17.5.1 åŠ¨æ€ vs é™æ€ Cache
  - 17.5.2 generation_config.cache_implementation="static"
  - 17.5.3 æ€§èƒ½å¯¹æ¯”
- 17.6 æ‰¹å¤„ç†ä¼˜åŒ–
  - 17.6.1 åŠ¨æ€ Batching
  - 17.6.2 Padding ç­–ç•¥
  - 17.6.3 Continuous Batchingï¼ˆvLLM å¼•å…¥ï¼‰

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `AttentionIOAnalysis` - Flash Attention IO ä¼˜åŒ–å¯è§†åŒ–
- `CompilationSpeedup` - torch.compile ç¼–è¯‘å‰åé€Ÿåº¦å¯¹æ¯”
- `KVCacheComparison` - åŠ¨æ€ vs é™æ€ KV Cache å†…å­˜å ç”¨

---

### **Chapter 18: vLLM ä¸ TGI**
- 18.1 vLLM æ·±åº¦å‰–æ
  - 18.1.1 PagedAttention åŸç†
  - 18.1.2 Continuous Batching
  - 18.1.3 ä¸ Hugging Face çš„äº’æ“ä½œæ€§
- 18.2 vLLM ä½¿ç”¨æŒ‡å—
  - 18.2.1 å®‰è£… vllm
  - 18.2.2 ç¦»çº¿æ¨ç†ï¼ˆLLM ç±»ï¼‰
  - 18.2.3 åœ¨çº¿æœåŠ¡ï¼ˆOpenAI-compatible APIï¼‰
  - 18.2.4 æ€§èƒ½è°ƒä¼˜å‚æ•°ï¼ˆtensor_parallel_sizeã€gpu_memory_utilizationï¼‰
- 18.3 Text Generation Inference (TGI)
  - 18.3.1 TGI æ¶æ„è®¾è®¡
  - 18.3.2 Docker éƒ¨ç½²
  - 18.3.3 æ”¯æŒçš„ä¼˜åŒ–æŠ€æœ¯ï¼ˆFlash Attentionã€Paged Attentionï¼‰
  - 18.3.4 Streaming ç”Ÿæˆ
- 18.4 TGI é«˜çº§ç‰¹æ€§
  - 18.4.1 å¼ é‡å¹¶è¡Œï¼ˆtensor_parallelï¼‰
  - 18.4.2 é‡åŒ–æ¨ç†ï¼ˆbitsandbytesã€GPTQï¼‰
  - 18.4.3 Safetensors å¿«é€ŸåŠ è½½
  - 18.4.4 Messages APIï¼ˆChat æ¨¡æ¿ï¼‰
- 18.5 æ€§èƒ½å¯¹æ¯”
  - 18.5.1 vLLM vs TGI vs Transformers
  - 18.5.2 ååé‡åŸºå‡†æµ‹è¯•
  - 18.5.3 å»¶è¿Ÿå¯¹æ¯”

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `PagedAttentionVisualizer` - PagedAttention å†…å­˜åˆ†é…åŠ¨ç”»
- `ContinuousBatchingDemo` - Continuous Batching vs Static Batching
- `InferenceFrameworkComparison` - vLLM vs TGI vs åŸç”Ÿ Transformers æ€§èƒ½å¯¹æ¯”è¡¨

---

### **Chapter 19: Speculative Decoding ä¸å…¶ä»–å‰æ²¿æŠ€æœ¯**
- 19.1 Speculative Decoding åŸç†
  - 19.1.1 å¤§æ¨¡å‹ + å°æ¨¡å‹ååŒ
  - 19.1.2 æ¨æµ‹ â†’ éªŒè¯æµç¨‹
  - 19.1.3 ç†è®ºåŠ é€Ÿä¸Šé™
- 19.2 Transformers ä¸­çš„å®ç°
  - 19.2.1 assisted_generation
  - 19.2.2 draft_model é…ç½®
  - 19.2.3 å®æµ‹åŠ é€Ÿæ•ˆæœ
- 19.3 å…¶ä»–æ¨ç†ä¼˜åŒ–æŠ€æœ¯
  - 19.3.1 Multi-Query Attention (MQA)
  - 19.3.2 Grouped-Query Attention (GQA)
  - 19.3.3 Sliding Window Attention
  - 19.3.4 KV Cache å‹ç¼©ï¼ˆH2Oã€StreamingLLMï¼‰
- 19.4 æ¨¡å‹å‹ç¼©æŠ€æœ¯
  - 19.4.1 çŸ¥è¯†è’¸é¦ï¼ˆDistilBERTã€TinyBERTï¼‰
  - 19.4.2 å‰ªæï¼ˆPruningï¼‰
  - 19.4.3 æƒé‡å…±äº«
- 19.5 æ¨ç†ç¡¬ä»¶åŠ é€Ÿ
  - 19.5.1 TensorRT-LLM
  - 19.5.2 ONNX Runtime
  - 19.5.3 OpenVINO
  - 19.5.4 Apple Neural Engine (CoreML)

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `SpeculativeDecodingFlow` - æ¨æµ‹è§£ç æµç¨‹åŠ¨ç”»
- `MQAvsGQA` - MQAã€GQAã€MHA æ¶æ„å¯¹æ¯”
- `KVCacheCompression` - KV Cache å‹ç¼©ç­–ç•¥å¯è§†åŒ–

---

## Part VII: ç”Ÿäº§éƒ¨ç½² (Production Deployment)

### **Chapter 20: æ¨¡å‹å¯¼å‡ºä¸è½¬æ¢**
- 20.1 æ¨¡å‹åºåˆ—åŒ–æ ¼å¼
  - 20.1.1 PyTorch (.binã€.ptã€.pth)
  - 20.1.2 Safetensorsï¼ˆæ¨èï¼‰
  - 20.1.3 æ ¼å¼è½¬æ¢å·¥å…·
- 20.2 ONNX å¯¼å‡º
  - 20.2.1 ONNX æ ‡å‡†æ¦‚è¿°
  - 20.2.2 ä½¿ç”¨ optimum å¯¼å‡º
  - 20.2.3 ONNX Runtime æ¨ç†
  - 20.2.4 é‡åŒ– ONNX æ¨¡å‹
- 20.3 TorchScript å¯¼å‡º
  - 20.3.1 torch.jit.trace vs torch.jit.script
  - 20.3.2 ç”Ÿæˆä»»åŠ¡çš„ç‰¹æ®Šå¤„ç†
  - 20.3.3 TorchScript æ¨¡å‹ä¼˜åŒ–
- 20.4 å…¶ä»–å¯¼å‡ºæ ¼å¼
  - 20.4.1 CoreMLï¼ˆiOS éƒ¨ç½²ï¼‰
  - 20.4.2 TensorFlow Liteï¼ˆç§»åŠ¨ç«¯ï¼‰
  - 20.4.3 TensorRTï¼ˆNVIDIA GPUï¼‰
  - 20.4.4 ExecuTorchï¼ˆè¾¹ç¼˜è®¾å¤‡ï¼‰
- 20.5 æ¨¡å‹ä¼˜åŒ–
  - 20.5.1 ONNX Simplifier
  - 20.5.2 å›¾ä¼˜åŒ–ï¼ˆOperator Fusionï¼‰
  - 20.5.3 å¸¸é‡æŠ˜å 

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `ModelExportPipeline` - æ¨¡å‹å¯¼å‡ºæµç¨‹å›¾
- `FormatComparison` - å„æ ¼å¼æ–‡ä»¶å¤§å°ã€åŠ è½½é€Ÿåº¦å¯¹æ¯”

---

### **Chapter 21: Optimum åº“è¯¦è§£**
- 21.1 Optimum ç”Ÿæ€æ¦‚è§ˆ
  - 21.1.1 ç¡¬ä»¶åŠ é€Ÿå™¨é€‚é…å±‚
  - 21.1.2 æ”¯æŒçš„åç«¯ï¼ˆONNXã€Intelã€Habanaã€AMDã€AWSï¼‰
  - 21.1.3 ä¸ Transformers çš„é›†æˆ
- 21.2 ONNX Runtime åŠ é€Ÿ
  - 21.2.1 ORTModelForXXX ç±»
  - 21.2.2 é‡åŒ–ä¼˜åŒ–ï¼ˆåŠ¨æ€é‡åŒ–ã€é™æ€é‡åŒ–ï¼‰
  - 21.2.3 å›¾ä¼˜åŒ–çº§åˆ«
  - 21.2.4 æ€§èƒ½å¯¹æ¯”
- 21.3 Intel ä¼˜åŒ–ï¼ˆOptimum-Intelï¼‰
  - 21.3.1 Intel Neural Compressor
  - 21.3.2 OpenVINO é›†æˆ
  - 21.3.3 CPU æ¨ç†åŠ é€Ÿ
- 21.4 å…¶ä»–åç«¯
  - 21.4.1 Habana Gaudiï¼ˆOptimum-Habanaï¼‰
  - 21.4.2 AWS Inferentiaï¼ˆOptimum-Neuronï¼‰
  - 21.4.3 AMDï¼ˆOptimum-AMDï¼‰
- 21.5 Optimum + PEFT
  - 21.5.1 å¯¼å‡º LoRA é€‚é…å™¨
  - 21.5.2 é‡åŒ– + PEFT è”åˆä¼˜åŒ–

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `OptimumBackendSelector` - ç¡¬ä»¶é€‰æ‹©å™¨ä¸æ¨èåç«¯
- `QuantizationBenchmark` - Optimum é‡åŒ–å‰åæ€§èƒ½å¯¹æ¯”

---

### **Chapter 22: API æœåŠ¡ä¸ Docker éƒ¨ç½²**
- 22.1 FastAPI æœåŠ¡å°è£…
  - 22.1.1 åŸºç¡€ API è®¾è®¡
  - 22.1.2 å¼‚æ­¥æ¨ç†ï¼ˆasync/awaitï¼‰
  - 22.1.3 è¯·æ±‚é˜Ÿåˆ—ç®¡ç†
  - 22.1.4 è´Ÿè½½å‡è¡¡
- 22.2 Docker å®¹å™¨åŒ–
  - 22.2.1 Dockerfile æœ€ä½³å®è·µ
  - 22.2.2 å¤šé˜¶æ®µæ„å»º
  - 22.2.3 CUDA é•œåƒé€‰æ‹©
  - 22.2.4 æ¨¡å‹ç¼“å­˜ä¼˜åŒ–
- 22.3 Kubernetes éƒ¨ç½²
  - 22.3.1 Deployment YAML é…ç½®
  - 22.3.2 GPU èµ„æºè¯·æ±‚
  - 22.3.3 è‡ªåŠ¨æ‰©ç¼©å®¹ï¼ˆHPAï¼‰
  - 22.3.4 æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
- 22.4 ç›‘æ§ä¸æ—¥å¿—
  - 22.4.1 Prometheus æŒ‡æ ‡æš´éœ²
  - 22.4.2 Grafana å¯è§†åŒ–
  - 22.4.3 æ—¥å¿—èšåˆï¼ˆELKï¼‰
  - 22.4.4 é“¾è·¯è¿½è¸ªï¼ˆJaegerï¼‰
- 22.5 å®‰å…¨æ€§è€ƒè™‘
  - 22.5.1 è¾“å…¥éªŒè¯ä¸è¿‡æ»¤
  - 22.5.2 Rate Limiting
  - 22.5.3 è®¤è¯ä¸æˆæƒï¼ˆJWTï¼‰
  - 22.5.4 æ¨¡å‹æ°´å°

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `DeploymentArchitecture` - ç”Ÿäº§éƒ¨ç½²æ¶æ„å›¾ï¼ˆLoad Balancer â†’ API Server â†’ Modelï¼‰
- `K8sResourceVisualizer` - Kubernetes èµ„æºé…ç½®å¯è§†åŒ–

---

## Part VIII: åº•å±‚æœºåˆ¶ä¸è‡ªå®šä¹‰ (Internals & Customization)

### **Chapter 23: Attention æœºåˆ¶æ·±åº¦è§£æ**
- 23.1 Self-Attention æ•°å­¦æ¨å¯¼
  - 23.1.1 $Qã€Kã€V$ çŸ©é˜µè®¡ç®—
  - 23.1.2 ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆScaled Dot-Productï¼‰
  - 23.1.3 Softmax å½’ä¸€åŒ–
  - 23.1.4 å®Œæ•´å…¬å¼ï¼š$\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$
- 23.2 Multi-Head Attention
  - 23.2.1 å¤šå¤´å¹¶è¡Œè®¡ç®—
  - 23.2.2 æ‹¼æ¥ä¸çº¿æ€§å˜æ¢
  - 23.2.3 ä¸ºä»€ä¹ˆå¤šå¤´æœ‰æ•ˆï¼Ÿ
- 23.3 Attention Mask è¯¦è§£
  - 23.3.1 Padding Maskï¼ˆencoderï¼‰
  - 23.3.2 Causal Maskï¼ˆdecoderï¼Œä¸‹ä¸‰è§’çŸ©é˜µï¼‰
  - 23.3.3 ç»„åˆ Maskï¼ˆPadding + Causalï¼‰
  - 23.3.4 ä»£ç å®ç°ä¸å¯è§†åŒ–
- 23.4 Position Encoding
  - 23.4.1 ç»å¯¹ä½ç½®ç¼–ç ï¼ˆSinusoidalã€Learnedï¼‰
  - 23.4.2 ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆT5ã€DeBERTaï¼‰
  - 23.4.3 æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼ŒLLaMAï¼‰
  - 23.4.4 ALiBiï¼ˆPress et al.ï¼‰
- 23.5 KV Cache åº•å±‚å®ç°
  - 23.5.1 Past Key Values ç»“æ„
  - 23.5.2 åŠ¨æ€å¢é•¿ç­–ç•¥
  - 23.5.3 å†…å­˜ç®¡ç†ï¼ˆPagedAttentionï¼‰
- 23.6 Cross-Attentionï¼ˆEncoder-Decoderï¼‰
  - 23.6.1 Query æ¥è‡ª Decoderï¼ŒKV æ¥è‡ª Encoder
  - 23.6.2 å®ç°ç»†èŠ‚

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `AttentionWeightHeatmap` - æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾ï¼ˆå®æ—¶è®¡ç®—ï¼‰
- `MaskBuilder` - äº¤äº’å¼ Mask æ„å»ºå™¨ï¼ˆæ‹–æ‹½ç”Ÿæˆ Padding/Causal Maskï¼‰
- `PositionEncodingVisualizer` - å„ç§ä½ç½®ç¼–ç å¯è§†åŒ–å¯¹æ¯”
- `KVCacheDynamics` - KV Cache é€ token å¢é•¿åŠ¨ç”»

---

### **Chapter 24: è‡ªå®šä¹‰æ¨¡å‹å¼€å‘**
- 24.1 PreTrainedModel åŸºç±»
  - 24.1.1 å¿…é¡»å®ç°çš„æ–¹æ³•
  - 24.1.2 é…ç½®ç±»ï¼ˆPretrainedConfigï¼‰
  - 24.1.3 æƒé‡åˆå§‹åŒ–ï¼ˆ_init_weightsï¼‰
- 24.2 ä»é›¶å®ç°ä¸€ä¸ª BERT
  - 24.2.1 Embedding Layer
  - 24.2.2 Transformer Encoder Layer
  - 24.2.3 Pooler ä¸ Classification Head
  - 24.2.4 å®Œæ•´ä»£ç å®ç°
- 24.3 æ·»åŠ æ–°çš„æ¨¡å‹æ¶æ„
  - 24.3.1 æ³¨å†Œæ¨¡å‹ï¼ˆAutoModelï¼‰
  - 24.3.2 é…ç½® mapping
  - 24.3.3 ä¸Šä¼ åˆ° Hub
- 24.4 è‡ªå®šä¹‰ Attention
  - 24.4.1 å®ç° Sparse Attention
  - 24.4.2 Local Attention Window
  - 24.4.3 ä¸æ ‡å‡† Attention å¯¹æ¯”
- 24.5 è‡ªå®šä¹‰ Tokenizer
  - 24.5.1 Tokenizer åŸºç±»
  - 24.5.2 è®­ç»ƒæ–° tokenizerï¼ˆtrain_new_from_iteratorï¼‰
  - 24.5.3 æ·»åŠ ç‰¹æ®Š token
  - 24.5.4 ä¿å­˜ä¸åŠ è½½

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `ModelBuilderTool` - å¯è§†åŒ–æ¨¡å‹æ­å»ºå·¥å…·ï¼ˆæ‹–æ‹½ç»„ä»¶ï¼‰
- `CustomAttentionComparator` - è‡ªå®šä¹‰æ³¨æ„åŠ›æ¨¡å¼å¯¹æ¯”

---

### **Chapter 25: è‡ªå®šä¹‰ Trainer ä¸è®­ç»ƒå¾ªç¯**
- 25.1 Trainer å†…éƒ¨æœºåˆ¶
  - 25.1.1 è®­ç»ƒå¾ªç¯æºç èµ°è¯»
  - 25.1.2 é’©å­å‡½æ•°ï¼ˆHooksï¼‰ä½ç½®
  - 25.1.3 è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
- 25.2 ç»§æ‰¿ Trainer ç±»
  - 25.2.1 é‡å†™ compute_loss()
  - 25.2.2 é‡å†™ training_step()
  - 25.2.3 é‡å†™ evaluation_loop()
  - 25.2.4 ç¤ºä¾‹ï¼šå¯¹æ¯”å­¦ä¹  Trainer
- 25.3 è‡ªå®šä¹‰ Callback
  - 25.3.1 TrainerCallback åŸºç±»
  - 25.3.2 äº‹ä»¶è§¦å‘ç‚¹ï¼ˆon_epoch_endã€on_train_begin ç­‰ï¼‰
  - 25.3.3 ç¤ºä¾‹ï¼šè‡ªå®šä¹‰å­¦ä¹ ç‡é¢„çƒ­
- 25.4 å®Œå…¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯
  - 25.4.1 ä½¿ç”¨ Accelerate æ›¿ä»£ Trainer
  - 25.4.2 æ‰‹åŠ¨å®ç°æ¢¯åº¦ç´¯ç§¯
  - 25.4.3 æ··åˆç²¾åº¦é›†æˆ
  - 25.4.4 åˆ†å¸ƒå¼è®­ç»ƒé€‚é…
- 25.5 é«˜çº§æŸå¤±å‡½æ•°
  - 25.5.1 Focal Loss
  - 25.5.2 Contrastive Loss
  - 25.5.3 KL Divergenceï¼ˆçŸ¥è¯†è’¸é¦ï¼‰
  - 25.5.4 å¤šä»»åŠ¡å­¦ä¹ æŸå¤±ç»„åˆ

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `TrainerHookFlow` - Trainer æ‰§è¡Œæµç¨‹ä¸é’©å­ä½ç½®å¯è§†åŒ–
- `LossFunctionExplorer` - å„ç§æŸå¤±å‡½æ•°æ›²çº¿å¯¹æ¯”

---

## Part IX: é«˜çº§ä¸»é¢˜ä¸ç”Ÿæ€é›†æˆ (Advanced Topics & Ecosystem)

### **Chapter 26: å¤šæ¨¡æ€æ¨¡å‹ï¼ˆVision-Languageï¼‰**
- 26.1 å¤šæ¨¡æ€æ¶æ„æ¦‚è§ˆ
  - 26.1.1 CLIPï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰
  - 26.1.2 BLIP / BLIP-2ï¼ˆè§†è§‰é—®ç­”ï¼‰
  - 26.1.3 LLaVAï¼ˆå¤§è¯­è¨€æ¨¡å‹ + è§†è§‰ï¼‰
  - 26.1.4 Flamingo / IDEFICS
- 26.2 å›¾åƒç¼–ç å™¨
  - 26.2.1 Vision Transformer (ViT)
  - 26.2.2 CLIP Vision Encoder
  - 26.2.3 ç‰¹å¾æå–ä¸å¯¹é½
- 26.3 è§†è§‰é—®ç­”å¾®è°ƒ
  - 26.3.1 æ•°æ®é›†ï¼ˆVQAv2ã€GQAï¼‰
  - 26.3.2 Processorï¼ˆå›¾åƒ + æ–‡æœ¬é¢„å¤„ç†ï¼‰
  - 26.3.3 è®­ç»ƒä¸è¯„ä¼°
- 26.4 å›¾åƒç”Ÿæˆï¼ˆDiffusionï¼‰
  - 26.4.1 Stable Diffusion ä¸ Transformers
  - 26.4.2 Text-to-Image Pipeline
  - 26.4.3 ControlNet é›†æˆ
- 26.5 éŸ³é¢‘æ¨¡å‹
  - 26.5.1 Whisperï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰
  - 26.5.2 Wav2Vec2ï¼ˆè‡ªç›‘ç£å­¦ä¹ ï¼‰
  - 26.5.3 éŸ³é¢‘åˆ†ç±»ä¸è½¬å½•

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `MultimodalArchitecture` - CLIP/LLaVA æ¶æ„å›¾
- `VisionEncoderVisualizer` - ViT Patch Embedding å¯è§†åŒ–

---

### **Chapter 27: å¼ºåŒ–å­¦ä¹ ä¸ RLHF**
- 27.1 RLHF åŸºç¡€æ¦‚å¿µ
  - 27.1.1 äººç±»åé¦ˆçš„é‡è¦æ€§
  - 27.1.2 ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼ˆSFT â†’ RM â†’ PPOï¼‰
  - 27.1.3 InstructGPT è®ºæ–‡è§£è¯»
- 27.2 TRL åº“ï¼ˆTransformer Reinforcement Learningï¼‰
  - 27.2.1 å®‰è£…ä¸é…ç½®
  - 27.2.2 SFTTrainerï¼ˆç›‘ç£å¾®è°ƒï¼‰
  - 27.2.3 RewardTrainerï¼ˆå¥–åŠ±æ¨¡å‹ï¼‰
  - 27.2.4 PPOTrainerï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰
- 27.3 DPOï¼ˆDirect Preference Optimizationï¼‰
  - 27.3.1 DPO åŸç†ï¼ˆæ— éœ€å¥–åŠ±æ¨¡å‹ï¼‰
  - 27.3.2 DPOTrainer ä½¿ç”¨
  - 27.3.3 ä¸ PPO å¯¹æ¯”
- 27.4 å…¶ä»–å¯¹é½æ–¹æ³•
  - 27.4.1 Constitutional AI
  - 27.4.2 RLAIFï¼ˆAI Feedbackï¼‰
  - 27.4.3 çº¢è“å¯¹æŠ—
- 27.5 å®æˆ˜ï¼šæŒ‡ä»¤å¾®è°ƒ LLaMA
  - 27.5.1 æ•°æ®é›†å‡†å¤‡ï¼ˆAlpacaã€Dollyï¼‰
  - 27.5.2 SFT è®­ç»ƒ
  - 27.5.3 å¥–åŠ±æ¨¡å‹è®­ç»ƒ
  - 27.5.4 PPO å¾®è°ƒ

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `RLHFPipeline` - RLHF ä¸‰é˜¶æ®µæµç¨‹å¯è§†åŒ–
- `DPOvsRLHF` - DPO ä¸ RLHF è®­ç»ƒæ›²çº¿å¯¹æ¯”

---

### **Chapter 28: å‰æ²¿ç ”ç©¶ä¸æœªæ¥æ–¹å‘**
- 28.1 é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡
  - 28.1.1 ä½ç½®æ’å€¼ï¼ˆPosition Interpolationï¼‰
  - 28.1.2 ALiBiã€RoPE æ‰©å±•
  - 28.1.3 Sparse Attentionï¼ˆLongformerã€BigBirdï¼‰
  - 28.1.4 Retrieval-Augmented Generation (RAG)
- 28.2 é«˜æ•ˆæ¶æ„
  - 28.2.1 Mixture of Experts (MoE)
  - 28.2.2 State Space Modelsï¼ˆMambaã€S4ï¼‰
  - 28.2.3 RetNetï¼ˆRetentive Networksï¼‰
  - 28.2.4 RWKVï¼ˆRNN-like Transformerï¼‰
- 28.3 æ¨¡å‹åˆå¹¶ä¸ç»„åˆ
  - 28.3.1 Model Mergingï¼ˆSLERPã€TIESï¼‰
  - 28.3.2 LoRA é€‚é…å™¨ç»„åˆ
  - 28.3.3 Ensemble æ–¹æ³•
- 28.4 å¯è§£é‡Šæ€§ä¸åˆ†æ
  - 28.4.1 Attention å¯è§†åŒ–ï¼ˆBertVizï¼‰
  - 28.4.2 æ¢é’ˆåˆ†ç±»ï¼ˆProbingï¼‰
  - 28.4.3 æ¿€æ´»å€¼åˆ†æ
  - 28.4.4 å› æœå¹²é¢„å®éªŒ
- 28.5 å®‰å…¨æ€§ä¸å¯¹é½
  - 28.5.1 å¯¹æŠ—æ”»å‡»ä¸é˜²å¾¡
  - 28.5.2 æœ‰å®³å†…å®¹æ£€æµ‹
  - 28.5.3 åè§è¯„ä¼°ï¼ˆBiasï¼‰
  - 28.5.4 å¯æ§ç”Ÿæˆ
- 28.6 æœªæ¥å±•æœ›
  - 28.6.1 å¤šæ¨¡æ€å¤§ä¸€ç»Ÿæ¨¡å‹
  - 28.6.2 ç«¯åˆ°ç«¯è¯­éŸ³å¯¹è¯
  - 28.6.3 ä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelsï¼‰
  - 28.6.4 AGI è·¯å¾„æ¢è®¨

**äº¤äº’å¼ç»„ä»¶**ï¼š
- `LongContextStrategies` - é•¿ä¸Šä¸‹æ–‡å¤„ç†ç­–ç•¥å¯¹æ¯”
- `MoERouting` - MoE è·¯ç”±å¯è§†åŒ–
- `AttentionPatternAnalyzer` - æ³¨æ„åŠ›æ¨¡å¼åˆ†æå·¥å…·

---

## ğŸ“– **é™„å½• (Appendices)**

### **Appendix A: å¸¸è§é”™è¯¯ä¸è°ƒè¯•**
- A.1 CUDA Out of Memory
- A.2 Tokenizer ä¸åŒ¹é…
- A.3 æƒé‡åŠ è½½è­¦å‘Š
- A.4 åˆ†å¸ƒå¼è®­ç»ƒå¡æ­»
- A.5 ç”Ÿæˆè´¨é‡å·®

### **Appendix B: æ€§èƒ½åŸºå‡†æµ‹è¯•**
- B.1 å¸¸è§æ¨¡å‹æ¨ç†é€Ÿåº¦å¯¹æ¯”
- B.2 è®­ç»ƒååé‡å¯¹æ¯”
- B.3 æ˜¾å­˜å ç”¨å¯¹æ¯”è¡¨
- B.4 é‡åŒ–æ–¹æ³•å¯¹æ¯”çŸ©é˜µ

### **Appendix C: èµ„æºæ¸…å•**
- C.1 å®˜æ–¹æ–‡æ¡£ä¸æ•™ç¨‹
- C.2 é‡è¦è®ºæ–‡åˆ—è¡¨
- C.3 æ¨èå¼€æºé¡¹ç›®
- C.4 ç¤¾åŒºèµ„æºï¼ˆDiscordã€è®ºå›ï¼‰

### **Appendix D: API é€ŸæŸ¥è¡¨**
- D.1 AutoModelForXXX ç±»åˆ—è¡¨
- D.2 TrainingArguments å‚æ•°é€ŸæŸ¥
- D.3 Generation Config å‚æ•°
- D.4 PEFT é…ç½®å‚æ•°

---

## ğŸ¯ **å­¦ä¹ è·¯å¾„å»ºè®®**

### **åˆå­¦è€…è·¯å¾„ï¼ˆ2-4 å‘¨ï¼‰**
```
Chapter 0 â†’ Chapter 1 â†’ Chapter 2 â†’ Chapter 3 â†’ Chapter 4 â†’ Chapter 5
```

### **å·¥ç¨‹å¸ˆè·¯å¾„ï¼ˆ1-2 æœˆï¼‰**
```
åŸºç¡€ (0-5) â†’ PEFT (8-10) â†’ åˆ†å¸ƒå¼ (14-16) â†’ æ¨ç† (17-19) â†’ éƒ¨ç½² (20-22)
```

### **ç ”ç©¶å‘˜è·¯å¾„ï¼ˆ2-3 æœˆï¼‰**
```
å…¨éƒ¨ç« èŠ‚ + é‡ç‚¹ï¼šåº•å±‚æœºåˆ¶ (23-25) + é«˜çº§ä¸»é¢˜ (26-28)
```

---

## ğŸ“Š **é…å¥—äº¤äº’å¼ç»„ä»¶æ¸…å•ï¼ˆ70+ ä¸ªï¼‰**

æ¯ç« å»ºè®®çš„å¯è§†åŒ–ç»„ä»¶å·²åœ¨ç« èŠ‚å†…æ ‡æ³¨ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
- Pipeline æµç¨‹å¯è§†åŒ–
- Tokenization è¿‡ç¨‹æ¼”ç¤º
- Attention æƒé‡çƒ­åŠ›å›¾
- LoRA çŸ©é˜µæ³¨å…¥åŠ¨ç”»
- é‡åŒ–å‰åå¯¹æ¯”å›¾
- FSDP åˆ†ç‰‡è¿‡ç¨‹
- KV Cache åŠ¨æ€ç®¡ç†
- ç”Ÿæˆç­–ç•¥å¯¹æ¯”å™¨
- éƒ¨ç½²æ¶æ„å›¾
- è®­ç»ƒæ›²çº¿ç»˜åˆ¶å™¨
- ç­‰ç­‰...

---

**æ€»è®¡**ï¼š28 ä¸ªä¸»ç« èŠ‚ï¼Œ90+ å°èŠ‚ï¼Œ200+ å…·ä½“çŸ¥è¯†ç‚¹ï¼Œ70+ äº¤äº’å¼ç»„ä»¶

**é¢„è®¡å†…å®¹é‡**ï¼šçº¦ **150,000-200,000 å­—**ï¼ŒåŒ…å« **500+ ä»£ç ç¤ºä¾‹**

---

**ä¸‹ä¸€æ­¥**ï¼š
1. è¯·æ‚¨ review æ­¤å¤§çº²ï¼Œæå‡ºä¿®æ”¹æ„è§
2. ç¡®è®¤åï¼Œæˆ‘å°†æŒ‰ç« èŠ‚é¡ºåºé€ä¸€è¯¦ç»†å±•å¼€å†…å®¹
3. åŒæ—¶è§„åˆ’éœ€è¦å¼€å‘çš„äº¤äº’å¼å¯è§†åŒ–ç»„ä»¶

**æ‚¨å¯¹è¿™ä¸ªå¤§çº²æœ‰ä»€ä¹ˆæ„è§æˆ–éœ€è¦è°ƒæ•´çš„åœ°æ–¹å—ï¼Ÿ**
