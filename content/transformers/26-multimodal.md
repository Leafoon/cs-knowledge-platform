---
title: "Chapter 26. å¤šæ¨¡æ€æ¨¡å‹ï¼ˆVision-Language Modelsï¼‰"
description: "å­¦ä¹  CLIPã€BLIPã€LLaVA ç­‰è§†è§‰-è¯­è¨€æ¨¡å‹ã€ViT å›¾åƒç¼–ç å™¨ã€Whisper è¯­éŸ³è¯†åˆ«"
updated: "2026-01-22"
---

å‰é¢çš„ç« èŠ‚ä¸»è¦èšç„¦äºçº¯æ–‡æœ¬æ¨¡å‹ã€‚æœ¬ç« å°†æ¢ç´¢**å¤šæ¨¡æ€ï¼ˆMultimodalï¼‰**é¢†åŸŸï¼Œå­¦ä¹ å¦‚ä½•ä½¿ç”¨ Hugging Face Transformers å¤„ç†**å›¾åƒ+æ–‡æœ¬ã€éŸ³é¢‘+æ–‡æœ¬**ç­‰è·¨æ¨¡æ€ä»»åŠ¡ã€‚æˆ‘ä»¬å°†æ·±å…¥ç ”ç©¶ CLIPã€BLIPã€LLaVA ç­‰è§†è§‰-è¯­è¨€æ¨¡å‹ã€ViTï¼ˆVision Transformerï¼‰å›¾åƒç¼–ç å™¨ã€Stable Diffusion æ–‡æœ¬ç”Ÿæˆå›¾åƒã€Whisper è¯­éŸ³è¯†åˆ«ç­‰å‰æ²¿æŠ€æœ¯ã€‚

---

## 26.1 å¤šæ¨¡æ€æ¶æ„æ¦‚è§ˆ

å¤šæ¨¡æ€æ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†**ä¸åŒæ¨¡æ€**ï¼ˆvisionã€textã€audioï¼‰çš„æ•°æ®æ˜ å°„åˆ°**å…±äº«çš„è¡¨ç¤ºç©ºé—´**ï¼Œä»è€Œå®ç°è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚

### 26.1.1 CLIPï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰

**CLIP**ï¼ˆContrastive Language-Image Pre-trainingï¼ŒOpenAI 2021ï¼‰é€šè¿‡å¯¹æ¯”å­¦ä¹ åœ¨å¤§è§„æ¨¡å›¾åƒ-æ–‡æœ¬å¯¹ä¸Šè®­ç»ƒï¼Œå­¦ä¹ ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€è¡¨ç¤ºã€‚

<div data-component="MultimodalArchitecture"></div>

**æ ¸å¿ƒè®¾è®¡**ï¼š
- **åŒå¡”æ¶æ„**ï¼ˆTwo-Towerï¼‰ï¼š
  - **Image Encoder**ï¼šVision Transformerï¼ˆViTï¼‰æˆ– ResNet
  - **Text Encoder**ï¼šTransformerï¼ˆç±»ä¼¼ BERTï¼‰
- **å¯¹æ¯”æŸå¤±**ï¼ˆInfoNCEï¼‰ï¼š
  $$
  \mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp(\text{sim}(I_i, T_i) / \tau)}{\sum_{j=1}^{N} \exp(\text{sim}(I_i, T_j) / \tau)}
  $$
  - æ­£æ ·æœ¬å¯¹ï¼šåŒ¹é…çš„å›¾åƒ-æ–‡æœ¬ $(I_i, T_i)$
  - è´Ÿæ ·æœ¬å¯¹ï¼šbatch å†…å…¶ä»–å›¾åƒ-æ–‡æœ¬
  - æ¸©åº¦å‚æ•° $\tau$ æ§åˆ¶åˆ†å¸ƒå¹³æ»‘åº¦

**Hugging Face ä½¿ç”¨**ï¼š
```python
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import requests

# åŠ è½½æ¨¡å‹
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# å‡†å¤‡æ•°æ®
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
texts = ["a photo of a cat", "a photo of a dog", "a photo of a car"]

# é¢„å¤„ç†
inputs = processor(text=texts, images=image, return_tensors="pt", padding=True)

# å‰å‘ä¼ æ’­
outputs = model(**inputs)

# è®¡ç®—ç›¸ä¼¼åº¦
logits_per_image = outputs.logits_per_image  # (1, 3)
probs = logits_per_image.softmax(dim=1)  # (1, 3)

print("Label probabilities:")
for text, prob in zip(texts, probs[0]):
    print(f"{text}: {prob.item():.4f}")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
Label probabilities:
a photo of a cat: 0.9921
a photo of a dog: 0.0065
a photo of a car: 0.0014
```

**åº”ç”¨åœºæ™¯**ï¼š
- **Zero-shot å›¾åƒåˆ†ç±»**ï¼šæ— éœ€è®­ç»ƒå³å¯åˆ†ç±»
- **å›¾åƒæ£€ç´¢**ï¼šæ ¹æ®æ–‡æœ¬æè¿°æœç´¢å›¾åƒ
- **æ–‡æœ¬æ£€ç´¢**ï¼šæ ¹æ®å›¾åƒæŸ¥æ‰¾ç›¸å…³æ–‡æœ¬

### 26.1.2 BLIP / BLIP-2ï¼ˆè§†è§‰é—®ç­”ï¼‰

**BLIP**ï¼ˆBootstrapping Language-Image Pre-trainingï¼ŒSalesforce 2022ï¼‰å¼•å…¥äº†**å¤šä»»åŠ¡ç»Ÿä¸€æ¡†æ¶**ï¼Œæ”¯æŒå›¾åƒæè¿°ã€VQAã€æ£€ç´¢ç­‰ä»»åŠ¡ã€‚

**æ¶æ„åˆ›æ–°**ï¼š
1. **ç¼–ç å™¨-è§£ç å™¨æ¶æ„**ï¼ˆEncoder-Decoderï¼‰ï¼š
   - **Image Encoder**ï¼šViT
   - **Text Encoder**ï¼šBERT-like
   - **Text Decoder**ï¼šGPT-likeï¼ˆç”¨äºç”Ÿæˆï¼‰
2. **ä¸‰ç§è®­ç»ƒç›®æ ‡**ï¼š
   - **ITC**ï¼ˆImage-Text Contrastiveï¼‰ï¼šå¯¹æ¯”å­¦ä¹ 
   - **ITM**ï¼ˆImage-Text Matchingï¼‰ï¼šäºŒåˆ†ç±»ï¼ˆåŒ¹é…/ä¸åŒ¹é…ï¼‰
   - **LM**ï¼ˆLanguage Modelingï¼‰ï¼šå›¾åƒæ¡ä»¶ä¸‹çš„æ–‡æœ¬ç”Ÿæˆ

**BLIP-2 æ”¹è¿›**ï¼š
- **Q-Former**ï¼ˆQuery Transformerï¼‰ï¼šè½»é‡çº§æ¨¡å—æ¡¥æ¥å†»ç»“çš„å›¾åƒç¼–ç å™¨å’Œ LLM
- **ä¸¤é˜¶æ®µè®­ç»ƒ**ï¼š
  1. è¡¨ç¤ºå­¦ä¹ ï¼ˆä»å†»ç»“çš„å›¾åƒç¼–ç å™¨å­¦ä¹ ï¼‰
  2. ç”Ÿæˆå­¦ä¹ ï¼ˆä¸å†»ç»“çš„ LLM å¯¹é½ï¼‰

**ä½¿ç”¨ç¤ºä¾‹ï¼ˆå›¾åƒæè¿°ï¼‰**ï¼š
```python
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

# åŠ è½½æ¨¡å‹
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# åŠ è½½å›¾åƒ
image = Image.open("beach.jpg")

# æ— æ¡ä»¶ç”Ÿæˆï¼ˆè‡ªåŠ¨æè¿°ï¼‰
inputs = processor(image, return_tensors="pt")
outputs = model.generate(**inputs)
caption = processor.decode(outputs[0], skip_special_tokens=True)
print(f"Caption: {caption}")

# æœ‰æ¡ä»¶ç”Ÿæˆï¼ˆé—®ç­”ï¼‰
question = "What is on the beach?"
inputs = processor(image, question, return_tensors="pt")
outputs = model.generate(**inputs)
answer = processor.decode(outputs[0], skip_special_tokens=True)
print(f"Answer: {answer}")
```

### 26.1.3 LLaVAï¼ˆå¤§è¯­è¨€æ¨¡å‹ + è§†è§‰ï¼‰

**LLaVA**ï¼ˆLarge Language and Vision Assistantï¼Œ2023ï¼‰å°†é¢„è®­ç»ƒçš„ **ViT** å’Œ **LLaMA/Vicuna** é€šè¿‡ç®€å•çš„**çº¿æ€§æŠ•å½±å±‚**è¿æ¥ã€‚

**æ¶æ„**ï¼š
```
Image â†’ ViT â†’ Linear Projection â†’ LLM (LLaMA/Vicuna) â†’ Text
```

**è®­ç»ƒæµç¨‹**ï¼š
1. **é¢„è®­ç»ƒé˜¶æ®µ**ï¼šåªè®­ç»ƒæŠ•å½±å±‚ï¼ˆå†»ç»“ ViT å’Œ LLMï¼‰
   - æ•°æ®ï¼šå›¾åƒ-æè¿°å¯¹ï¼ˆCC3M ç­‰ï¼‰
   - ç›®æ ‡ï¼šå¯¹é½è§†è§‰å’Œè¯­è¨€ç‰¹å¾
2. **æŒ‡ä»¤å¾®è°ƒé˜¶æ®µ**ï¼šè®­ç»ƒæŠ•å½±å±‚ + LLM
   - æ•°æ®ï¼šå¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®ï¼ˆGPT-4 ç”Ÿæˆï¼‰
   - ç›®æ ‡ï¼šæå‡å¯¹è¯èƒ½åŠ›

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from transformers import LlavaForConditionalGeneration, AutoProcessor
from PIL import Image

# åŠ è½½æ¨¡å‹
model = LlavaForConditionalGeneration.from_pretrained("llava-hf/llava-1.5-7b-hf")
processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")

# å‡†å¤‡å¯¹è¯
image = Image.open("example.jpg")
conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "What is shown in this image?"}
        ]
    }
]

# åº”ç”¨èŠå¤©æ¨¡æ¿
prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
inputs = processor(images=image, text=prompt, return_tensors="pt")

# ç”Ÿæˆå›å¤
outputs = model.generate(**inputs, max_new_tokens=200)
response = processor.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### 26.1.4 Flamingo / IDEFICS

**Flamingo**ï¼ˆDeepMind 2022ï¼‰æ”¯æŒ**äº¤é”™çš„å›¾åƒ-æ–‡æœ¬è¾“å…¥**ï¼ˆinterleavedï¼‰ï¼Œé€‚åˆå¤šè½®å¯¹è¯ã€‚

**IDEFICS**ï¼ˆHuggingFace å¼€æºå¤ç°ç‰ˆï¼‰ï¼š
- åŸºäº Flamingo æ¶æ„
- æ”¯æŒå¤šå›¾åƒè¾“å…¥
- å¼€æ”¾æƒé‡ï¼ˆ80B å‚æ•°ç‰ˆæœ¬ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from transformers import IdeficsForVisionText2Text, AutoProcessor

model = IdeficsForVisionText2Text.from_pretrained("HuggingFaceM4/idefics-9b")
processor = AutoProcessor.from_pretrained("HuggingFaceM4/idefics-9b")

# å¤šå›¾åƒè¾“å…¥
prompts = [
    "User: What is in this image?",
    "<image>",
    "Assistant:",
    "User: And what about this one?",
    "<image>",
    "Assistant:"
]

images = [Image.open("img1.jpg"), Image.open("img2.jpg")]
inputs = processor(prompts, images=images, return_tensors="pt")

outputs = model.generate(**inputs, max_new_tokens=100)
print(processor.decode(outputs[0], skip_special_tokens=True))
```

---

## 26.2 å›¾åƒç¼–ç å™¨

### 26.2.1 Vision Transformer (ViT)

**ViT**ï¼ˆGoogle 2020ï¼‰å°† Transformer æ¶æ„åº”ç”¨äºå›¾åƒï¼Œå®Œå…¨æŠ›å¼ƒå·ç§¯æ“ä½œã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
1. **Patch Embedding**ï¼šå°†å›¾åƒåˆ‡åˆ†ä¸ºå›ºå®šå¤§å°çš„ patchï¼ˆå¦‚ 16Ã—16ï¼‰
2. **çº¿æ€§æŠ•å½±**ï¼šæ¯ä¸ª patch å±•å¹³å¹¶é€šè¿‡çº¿æ€§å±‚æ˜ å°„åˆ°åµŒå…¥ç»´åº¦
3. **ä½ç½®ç¼–ç **ï¼šæ·»åŠ å¯å­¦ä¹ çš„ä½ç½®åµŒå…¥
4. **Transformer Encoder**ï¼šæ ‡å‡† Self-Attention + FFN
5. **åˆ†ç±»å¤´**ï¼š[CLS] token è¾“å‡ºç”¨äºåˆ†ç±»

<div data-component="VisionEncoderVisualizer"></div>

**æ•°å­¦è¡¨ç¤º**ï¼š
1. **Patch Embedding**ï¼š
   - è¾“å…¥å›¾åƒï¼š$\mathbf{x} \in \mathbb{R}^{H \times W \times C}$
   - Patch å¤§å°ï¼š$P \times P$
   - Patch æ•°é‡ï¼š$N = \frac{HW}{P^2}$
   - å±•å¹³ï¼š$\mathbf{x}_p \in \mathbb{R}^{N \times (P^2 \cdot C)}$
   - æŠ•å½±ï¼š$\mathbf{z}_0 = [\mathbf{x}_{\text{class}}; \mathbf{x}_p^1 \mathbf{E}; \dots; \mathbf{x}_p^N \mathbf{E}] + \mathbf{E}_{\text{pos}}$
   - å…¶ä¸­ $\mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}$

2. **Transformer Encoder**ï¼š
   $$
   \begin{aligned}
   \mathbf{z}'_\ell &= \text{MSA}(\text{LN}(\mathbf{z}_{\ell-1})) + \mathbf{z}_{\ell-1} \\
   \mathbf{z}_\ell &= \text{MLP}(\text{LN}(\mathbf{z}'_\ell)) + \mathbf{z}'_\ell
   \end{aligned}
   $$

3. **åˆ†ç±»å¤´**ï¼š
   $$
   \mathbf{y} = \text{LN}(\mathbf{z}_L^0)
   $$

**ä»£ç å®ç°**ï¼š
```python
from transformers import ViTImageProcessor, ViTForImageClassification
from PIL import Image

# åŠ è½½æ¨¡å‹
processor = ViTImageProcessor.from_pretrained("google/vit-base-patch16-224")
model = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224")

# é¢„å¤„ç†
image = Image.open("cat.jpg")
inputs = processor(images=image, return_tensors="pt")

# å‰å‘ä¼ æ’­
outputs = model(**inputs)
logits = outputs.logits

# é¢„æµ‹
predicted_class_idx = logits.argmax(-1).item()
print(f"Predicted class: {model.config.id2label[predicted_class_idx]}")
```

**æ¶æ„å˜ç§**ï¼š
- **ViT-B/16**ï¼šBase æ¨¡å‹ï¼Œ16Ã—16 patchï¼ˆ86M å‚æ•°ï¼‰
- **ViT-L/16**ï¼šLarge æ¨¡å‹ï¼ˆ307M å‚æ•°ï¼‰
- **ViT-H/14**ï¼šHuge æ¨¡å‹ï¼Œ14Ã—14 patchï¼ˆ632M å‚æ•°ï¼‰
- **DeiT**ï¼ˆData-efficient ViTï¼‰ï¼šè’¸é¦è®­ç»ƒï¼Œé€‚åˆå°æ•°æ®é›†

### 26.2.2 CLIP Vision Encoder

CLIP ä½¿ç”¨ ViT ä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œä½†æœ‰ä»¥ä¸‹æ”¹è¿›ï¼š
- **å…¨å±€å¹³å‡æ± åŒ–**ï¼šä¸ä½¿ç”¨ [CLS] tokenï¼Œè€Œæ˜¯å¯¹æ‰€æœ‰ patch å–å¹³å‡
- **å¯¹æ¯”å­¦ä¹ ç›®æ ‡**ï¼šä¸æ–‡æœ¬ç¼–ç å™¨è”åˆè®­ç»ƒ
- **æ›´å¤§çš„è®­ç»ƒæ•°æ®**ï¼š4äº¿å›¾åƒ-æ–‡æœ¬å¯¹

**æå–ç‰¹å¾**ï¼š
```python
from transformers import CLIPModel, CLIPProcessor

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

image = Image.open("example.jpg")
inputs = processor(images=image, return_tensors="pt")

# æå–å›¾åƒç‰¹å¾
image_features = model.get_image_features(**inputs)  # (1, 512)
print(f"Image feature shape: {image_features.shape}")

# L2 å½’ä¸€åŒ–
image_features = image_features / image_features.norm(dim=-1, keepdim=True)
```

### 26.2.3 ç‰¹å¾æå–ä¸å¯¹é½

**ç‰¹å¾å¯¹é½ç›®æ ‡**ï¼š
- å°†å›¾åƒç‰¹å¾ $\mathbf{v} \in \mathbb{R}^{d_v}$ æ˜ å°„åˆ°è¯­è¨€ç©ºé—´ $\mathbb{R}^{d_t}$
- å¸¸ç”¨æ–¹æ³•ï¼š
  1. **çº¿æ€§æŠ•å½±**ï¼š$\mathbf{v}' = \mathbf{W}_v \mathbf{v}$
  2. **MLP**ï¼š$\mathbf{v}' = \text{MLP}(\mathbf{v})$
  3. **Q-Former**ï¼ˆBLIP-2ï¼‰ï¼šTransformer æ¨¡å—

**è‡ªå®šä¹‰ç‰¹å¾æå–å™¨**ï¼š
```python
import torch
import torch.nn as nn

class VisionLanguageProjector(nn.Module):
    def __init__(self, vision_dim=768, text_dim=768, hidden_dim=1024):
        super().__init__()
        self.projection = nn.Sequential(
            nn.Linear(vision_dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, text_dim)
        )
    
    def forward(self, vision_features):
        """
        Args:
            vision_features: (batch, vision_dim)
        Returns:
            aligned_features: (batch, text_dim)
        """
        return self.projection(vision_features)

# ä½¿ç”¨
projector = VisionLanguageProjector(vision_dim=512, text_dim=768)
vision_feat = torch.randn(4, 512)
aligned_feat = projector(vision_feat)  # (4, 768)
```

---

## 26.3 è§†è§‰é—®ç­”å¾®è°ƒ

### 26.3.1 æ•°æ®é›†ï¼ˆVQAv2ã€GQAï¼‰

**VQAv2**ï¼ˆVisual Question Answering v2ï¼‰ï¼š
- å›¾åƒï¼šCOCO æ•°æ®é›†
- é—®é¢˜ï¼šæ¯å¼ å›¾åƒ 3 ä¸ªé—®é¢˜
- ç­”æ¡ˆï¼šæ¯ä¸ªé—®é¢˜ 10 ä¸ªäººå·¥æ ‡æ³¨ç­”æ¡ˆ
- æ€»è®¡ï¼š~1M é—®ç­”å¯¹

**GQA**ï¼ˆVisual Reasoningï¼‰ï¼š
- å¼ºè°ƒæ¨ç†èƒ½åŠ›ï¼ˆspatialã€logicalï¼‰
- ç»“æ„åŒ–åœºæ™¯å›¾ï¼ˆScene Graphï¼‰
- 22M é—®ç­”å¯¹

**æ•°æ®åŠ è½½**ï¼š
```python
from datasets import load_dataset

# åŠ è½½ VQAv2
dataset = load_dataset("HuggingFaceM4/VQAv2", split="train")

# æŸ¥çœ‹æ ·ä¾‹
sample = dataset[0]
print(f"Image: {sample['image']}")
print(f"Question: {sample['question']}")
print(f"Answers: {sample['answers']}")  # å¤šä¸ªç­”æ¡ˆ
```

### 26.3.2 Processorï¼ˆå›¾åƒ + æ–‡æœ¬é¢„å¤„ç†ï¼‰

**Processor** ç»Ÿä¸€å¤„ç†å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼š

```python
from transformers import BlipProcessor

processor = BlipProcessor.from_pretrained("Salesforce/blip-vqa-base")

# å¤„ç†å•ä¸ªæ ·æœ¬
image = Image.open("image.jpg")
question = "What is in the image?"
inputs = processor(images=image, text=question, return_tensors="pt")

# inputs åŒ…å«ï¼š
# - pixel_values: (1, 3, 384, 384)
# - input_ids: (1, seq_len)
# - attention_mask: (1, seq_len)
```

**æ‰¹é‡å¤„ç†**ï¼š
```python
def preprocess_function(examples):
    """
    æ‰¹é‡é¢„å¤„ç†å‡½æ•°
    """
    images = [img.convert("RGB") for img in examples["image"]]
    questions = examples["question"]
    
    # å¤„ç†è¾“å…¥
    inputs = processor(
        images=images,
        text=questions,
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )
    
    # å¤„ç†ç­”æ¡ˆï¼ˆå–æœ€å¸¸è§ç­”æ¡ˆï¼‰
    answers = [ans[0] for ans in examples["answers"]]
    targets = processor.tokenizer(
        answers,
        padding="max_length",
        truncation=True,
        max_length=32,
        return_tensors="pt"
    )
    
    inputs["labels"] = targets["input_ids"]
    return inputs

# åº”ç”¨åˆ°æ•°æ®é›†
tokenized_dataset = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset.column_names
)
```

### 26.3.3 è®­ç»ƒä¸è¯„ä¼°

**å®Œæ•´è®­ç»ƒæµç¨‹**ï¼š
```python
from transformers import BlipForQuestionAnswering, Trainer, TrainingArguments

# 1. åŠ è½½æ¨¡å‹
model = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base")

# 2. è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./blip-vqa-finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    learning_rate=5e-5,
    warmup_steps=500,
    logging_steps=100,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=500,
    load_best_model_at_end=True,
    fp16=True  # æ··åˆç²¾åº¦
)

# 3. è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    
    # è§£ç é¢„æµ‹å’Œæ ‡ç­¾
    pred_tokens = predictions.argmax(-1)
    pred_str = processor.batch_decode(pred_tokens, skip_special_tokens=True)
    label_str = processor.batch_decode(labels, skip_special_tokens=True)
    
    # è®¡ç®—å‡†ç¡®ç‡ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
    exact_match = sum(p.strip().lower() == l.strip().lower() 
                     for p, l in zip(pred_str, label_str)) / len(pred_str)
    
    return {"exact_match": exact_match}

# 4. åˆ›å»º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    compute_metrics=compute_metrics
)

# 5. è®­ç»ƒ
trainer.train()

# 6. æ¨ç†
def answer_question(image_path, question):
    image = Image.open(image_path)
    inputs = processor(image, question, return_tensors="pt")
    
    outputs = model.generate(**inputs, max_length=50)
    answer = processor.decode(outputs[0], skip_special_tokens=True)
    
    return answer

# æµ‹è¯•
answer = answer_question("test.jpg", "How many people are in the image?")
print(f"Answer: {answer}")
```

---

## 26.4 å›¾åƒç”Ÿæˆï¼ˆDiffusionï¼‰

### 26.4.1 Stable Diffusion ä¸ Transformers

**Stable Diffusion** ä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelï¼‰ä»æ–‡æœ¬ç”Ÿæˆå›¾åƒï¼ŒHugging Face æä¾›äº†å®Œæ•´çš„ Pipelineã€‚

**æ¶æ„ç»„ä»¶**ï¼š
1. **Text Encoder**ï¼šCLIP Text Encoderï¼ˆå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¡ä»¶ï¼‰
2. **UNet**ï¼šå»å™ªç½‘ç»œï¼ˆæ ¸å¿ƒï¼‰
3. **VAE**ï¼ˆVariational AutoEncoderï¼‰ï¼šå°†åƒç´ ç©ºé—´å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´
4. **Scheduler**ï¼šæ§åˆ¶å»å™ªæ­¥æ•°

**åŸºç¡€ä½¿ç”¨**ï¼š
```python
from diffusers import StableDiffusionPipeline
import torch

# åŠ è½½æ¨¡å‹
pipe = StableDiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-1",
    torch_dtype=torch.float16
)
pipe = pipe.to("cuda")

# ç”Ÿæˆå›¾åƒ
prompt = "a beautiful sunset over mountains, highly detailed, 4k"
image = pipe(
    prompt,
    num_inference_steps=50,  # å»å™ªæ­¥æ•°
    guidance_scale=7.5,      # CFGï¼ˆClassifier-Free Guidanceï¼‰å¼ºåº¦
    height=512,
    width=512
).images[0]

image.save("generated_image.png")
```

### 26.4.2 Text-to-Image Pipeline

**å®Œæ•´æµç¨‹**ï¼š
1. **æ–‡æœ¬ç¼–ç **ï¼š
   ```python
   text_embeddings = pipe.text_encoder(text_input_ids)
   ```
2. **åˆå§‹åŒ–å™ªå£°**ï¼š
   ```python
   latents = torch.randn((batch_size, 4, 64, 64))  # æ½œåœ¨ç©ºé—´
   ```
3. **å»å™ªå¾ªç¯**ï¼š
   ```python
   for t in pipe.scheduler.timesteps:
       # é¢„æµ‹å™ªå£°
       noise_pred = pipe.unet(latents, t, text_embeddings).sample
       
       # æ›´æ–° latents
       latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample
   ```
4. **è§£ç åˆ°åƒç´ ç©ºé—´**ï¼š
   ```python
   image = pipe.vae.decode(latents / 0.18215).sample
   ```

**é«˜çº§å‚æ•°**ï¼š
```python
image = pipe(
    prompt="a cyberpunk city at night",
    negative_prompt="blurry, low quality, ugly",  # è´Ÿæç¤ºè¯
    num_inference_steps=100,  # æ›´å¤šæ­¥æ•° â†’ æ›´é«˜è´¨é‡
    guidance_scale=9.0,       # æ›´é«˜ CFG â†’ æ›´ç¬¦åˆæç¤ºè¯
    generator=torch.Generator("cuda").manual_seed(42)  # å›ºå®šéšæœºç§å­
).images[0]
```

### 26.4.3 ControlNet é›†æˆ

**ControlNet** å…è®¸ä½¿ç”¨é¢å¤–çš„æ¡ä»¶ï¼ˆå¦‚è¾¹ç¼˜å›¾ã€æ·±åº¦å›¾ï¼‰æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ã€‚

```python
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from diffusers.utils import load_image
import cv2
import numpy as np

# 1. åŠ è½½ ControlNet
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-canny",
    torch_dtype=torch.float16
)

# 2. åˆ›å»º Pipeline
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-1",
    controlnet=controlnet,
    torch_dtype=torch.float16
)
pipe = pipe.to("cuda")

# 3. å‡†å¤‡æ§åˆ¶å›¾åƒï¼ˆCanny è¾¹ç¼˜æ£€æµ‹ï¼‰
original_image = load_image("input.jpg")
image_array = np.array(original_image)
edges = cv2.Canny(image_array, 100, 200)
edges = Image.fromarray(edges)

# 4. ç”Ÿæˆ
output = pipe(
    prompt="a beautiful painting of a house",
    image=edges,  # æ§åˆ¶æ¡ä»¶
    num_inference_steps=50
).images[0]

output.save("controlled_output.png")
```

**å¸¸ç”¨ ControlNet ç±»å‹**ï¼š
- **Canny**ï¼šè¾¹ç¼˜æ£€æµ‹
- **Depth**ï¼šæ·±åº¦å›¾
- **Pose**ï¼šäººä½“å§¿æ€ï¼ˆOpenPoseï¼‰
- **Scribble**ï¼šæ¶‚é¸¦
- **Seg**ï¼šè¯­ä¹‰åˆ†å‰²

---

## 26.5 éŸ³é¢‘æ¨¡å‹

### 26.5.1 Whisperï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰

**Whisper**ï¼ˆOpenAI 2022ï¼‰æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œæ”¯æŒ**è½¬å½•**å’Œ**ç¿»è¯‘**ã€‚

**æ¶æ„**ï¼š
- **Encoder-Decoder Transformer**
- è®­ç»ƒæ•°æ®ï¼š680k å°æ—¶å¤šè¯­è¨€éŸ³é¢‘

**åŸºç¡€ä½¿ç”¨**ï¼š
```python
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import librosa

# åŠ è½½æ¨¡å‹
processor = WhisperProcessor.from_pretrained("openai/whisper-small")
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")

# åŠ è½½éŸ³é¢‘
audio, sr = librosa.load("audio.mp3", sr=16000)  # é‡é‡‡æ ·åˆ° 16kHz

# é¢„å¤„ç†
inputs = processor(audio, sampling_rate=16000, return_tensors="pt")

# ç”Ÿæˆè½¬å½•
generated_ids = model.generate(inputs["input_features"])
transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

print(f"Transcription: {transcription}")
```

**å¤šè¯­è¨€æ”¯æŒ**ï¼š
```python
# æŒ‡å®šè¯­è¨€ï¼ˆä¸­æ–‡ï¼‰
forced_decoder_ids = processor.get_decoder_prompt_ids(language="zh", task="transcribe")
generated_ids = model.generate(
    inputs["input_features"],
    forced_decoder_ids=forced_decoder_ids
)
```

**ç¿»è¯‘åˆ°è‹±æ–‡**ï¼š
```python
forced_decoder_ids = processor.get_decoder_prompt_ids(language="zh", task="translate")
generated_ids = model.generate(
    inputs["input_features"],
    forced_decoder_ids=forced_decoder_ids
)
translation = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(f"Translation: {translation}")
```

**Pipeline ä½¿ç”¨**ï¼š
```python
from transformers import pipeline

# è‡ªåŠ¨è¯­éŸ³è¯†åˆ« Pipeline
pipe = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-medium",
    chunk_length_s=30,  # é•¿éŸ³é¢‘åˆ†å—
    device=0  # GPU
)

# è½¬å½•
result = pipe("long_audio.wav")
print(result["text"])

# å¸¦æ—¶é—´æˆ³
result = pipe("audio.wav", return_timestamps=True)
for chunk in result["chunks"]:
    print(f"[{chunk['timestamp'][0]:.2f}s - {chunk['timestamp'][1]:.2f}s]: {chunk['text']}")
```

### 26.5.2 Wav2Vec2ï¼ˆè‡ªç›‘ç£å­¦ä¹ ï¼‰

**Wav2Vec2**ï¼ˆMeta 2020ï¼‰ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ ä»æœªæ ‡æ³¨éŸ³é¢‘ä¸­å­¦ä¹ è¡¨ç¤ºã€‚

**é¢„è®­ç»ƒç›®æ ‡**ï¼š
- **Masked Prediction**ï¼šç±»ä¼¼ BERTï¼Œé®è”½éƒ¨åˆ†éŸ³é¢‘ç‰‡æ®µå¹¶é¢„æµ‹

**å¾®è°ƒç”¨äº ASR**ï¼š
```python
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
import torch

# åŠ è½½æ¨¡å‹
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

# åŠ è½½éŸ³é¢‘
audio, sr = librosa.load("audio.wav", sr=16000)

# é¢„å¤„ç†
inputs = processor(audio, sampling_rate=16000, return_tensors="pt", padding=True)

# å‰å‘ä¼ æ’­
with torch.no_grad():
    logits = model(inputs.input_values).logits

# CTC è§£ç 
predicted_ids = torch.argmax(logits, dim=-1)
transcription = processor.batch_decode(predicted_ids)[0]

print(f"Transcription: {transcription}")
```

**å¾®è°ƒè‡ªå®šä¹‰æ•°æ®**ï¼š
```python
from transformers import Trainer, TrainingArguments
from datasets import load_dataset

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("common_voice", "zh-CN", split="train")

# é¢„å¤„ç†å‡½æ•°
def prepare_dataset(batch):
    audio = batch["audio"]
    
    # å¤„ç†éŸ³é¢‘
    batch["input_values"] = processor(
        audio["array"],
        sampling_rate=audio["sampling_rate"]
    ).input_values[0]
    
    # å¤„ç†æ–‡æœ¬æ ‡ç­¾
    with processor.as_target_processor():
        batch["labels"] = processor(batch["sentence"]).input_ids
    
    return batch

# åº”ç”¨é¢„å¤„ç†
dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)

# è®­ç»ƒ
training_args = TrainingArguments(
    output_dir="./wav2vec2-zh-CN",
    per_device_train_batch_size=8,
    learning_rate=1e-4,
    num_train_epochs=5,
    fp16=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset
)

trainer.train()
```

### 26.5.3 éŸ³é¢‘åˆ†ç±»ä¸è½¬å½•

**éŸ³é¢‘åˆ†ç±»ç¤ºä¾‹ï¼ˆæƒ…æ„Ÿè¯†åˆ«ï¼‰**ï¼š
```python
from transformers import pipeline

# éŸ³é¢‘åˆ†ç±» Pipeline
classifier = pipeline(
    "audio-classification",
    model="superb/wav2vec2-base-superb-er"
)

# é¢„æµ‹æƒ…æ„Ÿ
result = classifier("happy_speech.wav")
print(result)
# [{'label': 'hap', 'score': 0.85}, {'label': 'neu', 'score': 0.10}, ...]
```

**å®æ—¶è½¬å½•ï¼ˆæµå¼å¤„ç†ï¼‰**ï¼š
```python
import pyaudio
import numpy as np

# åˆå§‹åŒ–éŸ³é¢‘æµ
p = pyaudio.PyAudio()
stream = p.open(
    format=pyaudio.paInt16,
    channels=1,
    rate=16000,
    input=True,
    frames_per_buffer=1024
)

print("ğŸ¤ Start speaking...")

while True:
    # è¯»å–éŸ³é¢‘å—
    audio_chunk = stream.read(1024)
    audio_array = np.frombuffer(audio_chunk, dtype=np.int16).astype(np.float32) / 32768.0
    
    # å¤„ç†
    inputs = processor(audio_array, sampling_rate=16000, return_tensors="pt")
    
    # æ¨ç†
    with torch.no_grad():
        logits = model(inputs.input_values).logits
    
    # è§£ç 
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)[0]
    
    if transcription.strip():
        print(f"Transcription: {transcription}")
```

---

## 26.6 å®æˆ˜æ¡ˆä¾‹ï¼šæ„å»ºå›¾åƒé—®ç­”ç³»ç»Ÿ

ç»“åˆæ‰€å­¦çŸ¥è¯†ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„å›¾åƒé—®ç­” Web åº”ç”¨ã€‚

```python
import gradio as gr
from transformers import BlipProcessor, BlipForQuestionAnswering
from PIL import Image

# åŠ è½½æ¨¡å‹
processor = BlipProcessor.from_pretrained("Salesforce/blip-vqa-base")
model = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base")

def answer_image_question(image, question):
    """
    å›¾åƒé—®ç­”å‡½æ•°
    
    Args:
        image: PIL Image
        question: str
    
    Returns:
        answer: str
    """
    # é¢„å¤„ç†
    inputs = processor(images=image, text=question, return_tensors="pt")
    
    # ç”Ÿæˆç­”æ¡ˆ
    outputs = model.generate(**inputs, max_length=50)
    answer = processor.decode(outputs[0], skip_special_tokens=True)
    
    return answer

# åˆ›å»º Gradio ç•Œé¢
iface = gr.Interface(
    fn=answer_image_question,
    inputs=[
        gr.Image(type="pil", label="Upload Image"),
        gr.Textbox(label="Ask a Question", placeholder="What is in the image?")
    ],
    outputs=gr.Textbox(label="Answer"),
    title="ğŸ–¼ï¸ Image Question Answering with BLIP",
    description="Upload an image and ask questions about it!",
    examples=[
        ["example1.jpg", "What is the main object?"],
        ["example2.jpg", "How many people are there?"],
        ["example3.jpg", "What color is the car?"]
    ]
)

# å¯åŠ¨
iface.launch(share=True)
```

---

## 26.7 æ€§èƒ½ä¼˜åŒ–ä¸éƒ¨ç½²

### 1. **é‡åŒ–åŠ é€Ÿ**

```python
from transformers import BitsAndBytesConfig

# 4-bit é‡åŒ–
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

model = BlipForQuestionAnswering.from_pretrained(
    "Salesforce/blip-vqa-base",
    quantization_config=quantization_config,
    device_map="auto"
)
```

### 2. **æ‰¹é‡æ¨ç†**

```python
def batch_inference(images, questions, batch_size=8):
    """æ‰¹é‡å¤„ç†å›¾åƒé—®ç­”"""
    results = []
    
    for i in range(0, len(images), batch_size):
        batch_images = images[i:i+batch_size]
        batch_questions = questions[i:i+batch_size]
        
        inputs = processor(
            images=batch_images,
            text=batch_questions,
            return_tensors="pt",
            padding=True
        )
        
        outputs = model.generate(**inputs, max_length=50)
        answers = processor.batch_decode(outputs, skip_special_tokens=True)
        
        results.extend(answers)
    
    return results
```

### 3. **TorchScript å¯¼å‡º**

```python
# å¯¼å‡ºä¸º TorchScriptï¼ˆä»…æ”¯æŒéƒ¨åˆ†æ¨¡å‹ï¼‰
traced_model = torch.jit.trace(model, example_inputs)
traced_model.save("blip_vqa.pt")

# åŠ è½½
loaded_model = torch.jit.load("blip_vqa.pt")
```

---

## 26.8 ç« èŠ‚æ€»ç»“

æœ¬ç« æˆ‘ä»¬æ·±å…¥å­¦ä¹ äº†å¤šæ¨¡æ€æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ï¼š

âœ… **æ ¸å¿ƒæŠ€èƒ½**ï¼š
- ç†è§£ CLIP å¯¹æ¯”å­¦ä¹ æ¶æ„ï¼ˆå›¾åƒ-æ–‡æœ¬å¯¹é½ï¼‰
- ä½¿ç”¨ BLIP/LLaVA è¿›è¡Œè§†è§‰é—®ç­”å’Œå›¾åƒæè¿°
- æŒæ¡ ViTï¼ˆVision Transformerï¼‰å›¾åƒç¼–ç åŸç†
- ä½¿ç”¨ Stable Diffusion ç”Ÿæˆå›¾åƒï¼ˆText-to-Imageï¼‰
- ä½¿ç”¨ Whisper è¿›è¡Œå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«
- å¾®è°ƒ Wav2Vec2 è¿›è¡Œ ASR ä»»åŠ¡

âœ… **å®æˆ˜èƒ½åŠ›**ï¼š
- æ„å»ºå›¾åƒé—®ç­”ç³»ç»Ÿï¼ˆBLIP VQAï¼‰
- ControlNet æ¡ä»¶å›¾åƒç”Ÿæˆ
- å®æ—¶è¯­éŸ³è½¬å½•
- å¤šæ¨¡æ€ç‰¹å¾å¯¹é½

âœ… **æœ€ä½³å®è·µ**ï¼š
- Processor ç»Ÿä¸€å¤„ç†å¤šæ¨¡æ€è¾“å…¥
- ä½¿ç”¨ Pipeline ç®€åŒ–æ¨ç†æµç¨‹
- é‡åŒ–åŠ é€Ÿï¼ˆ4-bitï¼‰é™ä½æ˜¾å­˜
- æ‰¹é‡æ¨ç†æå‡ååé‡

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼šChapter 27 å°†å­¦ä¹ **å¼ºåŒ–å­¦ä¹ ä¸ RLHF**ï¼ŒåŒ…æ‹¬ InstructGPT çš„ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼ˆSFT â†’ RM â†’ PPOï¼‰ã€TRL åº“ä½¿ç”¨ã€DPOï¼ˆDirect Preference Optimizationï¼‰ã€ä»¥åŠå®æˆ˜æŒ‡ä»¤å¾®è°ƒ LLaMAã€‚
