# Chapter 25: è‡ªå®šä¹‰ Trainer ä¸è®­ç»ƒå¾ªç¯ (Custom Trainer & Training Loop)

åœ¨å‰é¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ `Trainer` API è¿›è¡Œäº†å¤§é‡è®­ç»ƒå®éªŒã€‚æœ¬ç« å°†æ·±å…¥æ¢è®¨ **Trainer çš„å†…éƒ¨æœºåˆ¶**ï¼Œå­¦ä¹ å¦‚ä½•é€šè¿‡ç»§æ‰¿ `Trainer` ç±»æ¥è‡ªå®šä¹‰è®­ç»ƒé€»è¾‘ã€å®ç°è‡ªå®šä¹‰ Callbackã€å®Œå…¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼ˆä½¿ç”¨ Accelerateï¼‰ä»¥åŠå®ç°é«˜çº§æŸå¤±å‡½æ•°ï¼ˆFocal Lossã€Contrastive Lossã€KL Divergence ç­‰ï¼‰ã€‚è¿™äº›æŠ€èƒ½å¯¹äºç ”ç©¶å‰æ²¿æ–¹æ³•ã€é€‚é…ç‰¹æ®Šä»»åŠ¡è‡³å…³é‡è¦ã€‚

---

## 25.1 Trainer å†…éƒ¨æœºåˆ¶

### 25.1.1 è®­ç»ƒå¾ªç¯æºç èµ°è¯»

`Trainer` çš„æ ¸å¿ƒè®­ç»ƒå¾ªç¯ä½äº `train()` æ–¹æ³•ä¸­ï¼Œç®€åŒ–åçš„é€»è¾‘å¦‚ä¸‹ï¼š

```python
# transformers/trainer.py (ç®€åŒ–ç‰ˆ)
class Trainer:
    def train(self, resume_from_checkpoint=None):
        # 1. å‡†å¤‡é˜¶æ®µ
        train_dataloader = self.get_train_dataloader()
        optimizer = self.create_optimizer()
        lr_scheduler = self.create_scheduler(optimizer)
        
        # 2. åˆ†å¸ƒå¼å‡†å¤‡
        model, optimizer, train_dataloader = self.accelerator.prepare(
            self.model, optimizer, train_dataloader
        )
        
        # 3. å¼€å§‹è®­ç»ƒ
        for epoch in range(num_epochs):
            model.train()
            for step, inputs in enumerate(train_dataloader):
                # å‰å‘ä¼ æ’­
                outputs = model(**inputs)
                loss = outputs.loss
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ¢¯åº¦ç´¯ç§¯
                if (step + 1) % gradient_accumulation_steps == 0:
                    optimizer.step()
                    lr_scheduler.step()
                    optimizer.zero_grad()
                
                # æ—¥å¿—è®°å½•
                if step % logging_steps == 0:
                    self.log({"loss": loss.item()})
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if step % save_steps == 0:
                    self.save_model(output_dir)
            
            # æ¯ä¸ª epoch ç»“æŸåè¯„ä¼°
            if self.args.evaluation_strategy == "epoch":
                self.evaluate()
        
        return TrainOutput(...)
```

**å…³é”®ç»„ä»¶**ï¼š
1. **DataLoader**ï¼šé€šè¿‡ `get_train_dataloader()` åˆ›å»º
2. **Optimizer**ï¼šé€šè¿‡ `create_optimizer()` åˆ›å»ºï¼ˆé»˜è®¤ AdamWï¼‰
3. **Scheduler**ï¼šé€šè¿‡ `create_scheduler()` åˆ›å»ºï¼ˆçº¿æ€§è¡°å‡æˆ–ä½™å¼¦é€€ç«ï¼‰
4. **Accelerator**ï¼šå¤„ç†æ··åˆç²¾åº¦ã€åˆ†å¸ƒå¼ã€æ¢¯åº¦ç´¯ç§¯
5. **Checkpointing**ï¼šè‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹å’Œ optimizer çŠ¶æ€

### 25.1.2 é’©å­å‡½æ•°ï¼ˆHooksï¼‰ä½ç½®

`Trainer` æä¾›äº†å¤§é‡é’©å­å‡½æ•°ä¾›å­ç±»é‡å†™ï¼š

<div data-component="TrainerHookFlow"></div>

| é’©å­å‡½æ•° | è°ƒç”¨æ—¶æœº | ç”¨é€”ç¤ºä¾‹ |
|----------|----------|----------|
| **`compute_loss()`** | æ¯ä¸ª batch å‰å‘ä¼ æ’­å | è‡ªå®šä¹‰æŸå¤±å‡½æ•° |
| **`training_step()`** | æ¯ä¸ªè®­ç»ƒæ­¥éª¤ | è‡ªå®šä¹‰æ¢¯åº¦è®¡ç®— |
| **`prediction_step()`** | æ¯ä¸ªè¯„ä¼°æ­¥éª¤ | è‡ªå®šä¹‰è¯„ä¼°é€»è¾‘ |
| **`evaluation_loop()`** | æ•´ä¸ªè¯„ä¼°å¾ªç¯ | è‡ªå®šä¹‰è¯„ä¼°æµç¨‹ |
| **`create_optimizer()`** | è®­ç»ƒå¼€å§‹å‰ | ä½¿ç”¨è‡ªå®šä¹‰ä¼˜åŒ–å™¨ |
| **`create_scheduler()`** | è®­ç»ƒå¼€å§‹å‰ | è‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦ |
| **`save_model()`** | ä¿å­˜æ£€æŸ¥ç‚¹æ—¶ | ä¿å­˜é¢å¤–çŠ¶æ€ |
| **`log()`** | è®°å½•æ—¥å¿—æ—¶ | è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼ |

### 25.1.3 è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡

é»˜è®¤æƒ…å†µä¸‹ï¼Œ`Trainer` åªè®°å½• lossã€‚è¦æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡ï¼š

```python
from transformers import Trainer, TrainingArguments
from datasets import load_metric
import numpy as np

# åŠ è½½æŒ‡æ ‡
accuracy_metric = load_metric("accuracy")
f1_metric = load_metric("f1")

def compute_metrics(eval_pred):
    """
    è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡å‡½æ•°
    
    Args:
        eval_pred: EvalPrediction å¯¹è±¡ï¼ŒåŒ…å« predictions å’Œ label_ids
    
    Returns:
        dict: æŒ‡æ ‡å­—å…¸
    """
    predictions, labels = eval_pred
    
    # predictions æ˜¯ logitsï¼Œéœ€è¦è½¬æ¢ä¸ºç±»åˆ«
    preds = np.argmax(predictions, axis=1)
    
    # è®¡ç®—å¤šä¸ªæŒ‡æ ‡
    accuracy = accuracy_metric.compute(predictions=preds, references=labels)
    f1 = f1_metric.compute(predictions=preds, references=labels, average="weighted")
    
    return {
        "accuracy": accuracy["accuracy"],
        "f1": f1["f1"]
    }

# ä½¿ç”¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics  # ä¼ å…¥è‡ªå®šä¹‰å‡½æ•°
)

# è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨è¾“å‡º accuracy å’Œ f1
trainer.train()
```

**é«˜çº§ç¤ºä¾‹ï¼šå¤šä»»åŠ¡è¯„ä¼°**
```python
def compute_multitask_metrics(eval_pred):
    """
    å¤šä»»åŠ¡å­¦ä¹ çš„è¯„ä¼°æŒ‡æ ‡
    """
    predictions, labels = eval_pred
    
    # å‡è®¾ predictions æ˜¯ (batch, num_tasks, num_classes)
    task1_preds = np.argmax(predictions[:, 0, :], axis=1)
    task2_preds = np.argmax(predictions[:, 1, :], axis=1)
    
    task1_labels = labels[:, 0]
    task2_labels = labels[:, 1]
    
    return {
        "task1_accuracy": (task1_preds == task1_labels).mean(),
        "task2_accuracy": (task2_preds == task2_labels).mean(),
        "combined_accuracy": ((task1_preds == task1_labels) & (task2_preds == task2_labels)).mean()
    }
```

---

## 25.2 ç»§æ‰¿ Trainer ç±»

### 25.2.1 é‡å†™ compute_loss()

æœ€å¸¸è§çš„è‡ªå®šä¹‰éœ€æ±‚æ˜¯ä½¿ç”¨éæ ‡å‡†æŸå¤±å‡½æ•°ã€‚

**ç¤ºä¾‹ï¼šLabel Smoothing**
```python
import torch.nn.functional as F

class LabelSmoothingTrainer(Trainer):
    def __init__(self, label_smoothing=0.1, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.label_smoothing = label_smoothing
    
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        ä½¿ç”¨ Label Smoothing çš„äº¤å‰ç†µæŸå¤±
        """
        labels = inputs.pop("labels")
        
        # å‰å‘ä¼ æ’­
        outputs = model(**inputs)
        logits = outputs.logits
        
        # Label Smoothing æŸå¤±
        # å…¬å¼ï¼š(1 - Îµ) * NLL(y_true) + Îµ * NLL(uniform)
        log_probs = F.log_softmax(logits, dim=-1)
        
        # One-hot encoding
        num_classes = logits.size(-1)
        one_hot = torch.zeros_like(log_probs).scatter_(1, labels.unsqueeze(1), 1)
        
        # Smooth labels
        smooth_labels = one_hot * (1 - self.label_smoothing) + self.label_smoothing / num_classes
        
        # Compute loss
        loss = -(smooth_labels * log_probs).sum(dim=-1).mean()
        
        return (loss, outputs) if return_outputs else loss
```

**ä½¿ç”¨**ï¼š
```python
trainer = LabelSmoothingTrainer(
    label_smoothing=0.1,
    model=model,
    args=training_args,
    train_dataset=train_dataset
)
trainer.train()
```

### 25.2.2 é‡å†™ training_step()

æ§åˆ¶æ•´ä¸ªè®­ç»ƒæ­¥éª¤ï¼ˆåŒ…æ‹¬æ¢¯åº¦è£å‰ªã€å¯¹æŠ—è®­ç»ƒç­‰ï¼‰ã€‚

**ç¤ºä¾‹ï¼šå¯¹æŠ—è®­ç»ƒï¼ˆFGMï¼‰**
```python
class AdversarialTrainer(Trainer):
    def __init__(self, adv_epsilon=1.0, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.adv_epsilon = adv_epsilon
    
    def training_step(self, model, inputs):
        """
        Fast Gradient Method (FGM) å¯¹æŠ—è®­ç»ƒ
        
        æ­¥éª¤ï¼š
        1. æ­£å¸¸å‰å‘ + åå‘ï¼Œè®¡ç®—æ¢¯åº¦
        2. åœ¨ embedding ä¸Šæ·»åŠ å¯¹æŠ—æ‰°åŠ¨
        3. å†æ¬¡å‰å‘ï¼Œè®¡ç®—å¯¹æŠ—æŸå¤±
        4. åå‘ä¼ æ’­å¯¹æŠ—æŸå¤±
        5. æ¢å¤åŸå§‹ embedding
        """
        model.train()
        inputs = self._prepare_inputs(inputs)
        
        # === ç¬¬ä¸€æ­¥ï¼šæ­£å¸¸è®­ç»ƒ ===
        with self.compute_loss_context_manager():
            loss = self.compute_loss(model, inputs)
        
        if self.args.gradient_accumulation_steps > 1:
            loss = loss / self.args.gradient_accumulation_steps
        
        # åå‘ä¼ æ’­ï¼ˆä½†ä¸æ›´æ–°å‚æ•°ï¼‰
        self.accelerator.backward(loss)
        
        # === ç¬¬äºŒæ­¥ï¼šå¯¹æŠ—è®­ç»ƒ ===
        # ä¿å­˜åŸå§‹ embedding
        embedding_layer = model.get_input_embeddings()
        original_embedding = embedding_layer.weight.data.clone()
        
        # è®¡ç®—å¯¹æŠ—æ‰°åŠ¨
        # r_adv = epsilon * g / ||g||_2
        grad = embedding_layer.weight.grad
        if grad is not None:
            norm = torch.norm(grad)
            if norm != 0:
                r_adv = self.adv_epsilon * grad / norm
                embedding_layer.weight.data = original_embedding + r_adv
        
        # å¯¹æŠ—æ ·æœ¬å‰å‘ä¼ æ’­
        with self.compute_loss_context_manager():
            adv_loss = self.compute_loss(model, inputs)
        
        if self.args.gradient_accumulation_steps > 1:
            adv_loss = adv_loss / self.args.gradient_accumulation_steps
        
        # åå‘ä¼ æ’­å¯¹æŠ—æŸå¤±
        self.accelerator.backward(adv_loss)
        
        # æ¢å¤åŸå§‹ embedding
        embedding_layer.weight.data = original_embedding
        
        # æ€»æŸå¤±ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        return (loss + adv_loss).detach()
```

### 25.2.3 é‡å†™ evaluation_loop()

å®Œå…¨è‡ªå®šä¹‰è¯„ä¼°æµç¨‹ã€‚

**ç¤ºä¾‹ï¼šTop-K å‡†ç¡®ç‡**
```python
class TopKTrainer(Trainer):
    def __init__(self, top_k=5, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.top_k = top_k
    
    def evaluation_loop(
        self,
        dataloader,
        description,
        prediction_loss_only=None,
        ignore_keys=None,
        metric_key_prefix="eval"
    ):
        """
        è‡ªå®šä¹‰è¯„ä¼°å¾ªç¯ï¼Œè®¡ç®— Top-K å‡†ç¡®ç‡
        """
        model = self.model
        model.eval()
        
        all_preds = []
        all_labels = []
        total_loss = 0.0
        
        for step, inputs in enumerate(dataloader):
            inputs = self._prepare_inputs(inputs)
            labels = inputs.pop("labels")
            
            with torch.no_grad():
                outputs = model(**inputs)
                loss = outputs.loss
                logits = outputs.logits
            
            # æ”¶é›†é¢„æµ‹å’Œæ ‡ç­¾
            all_preds.append(logits.cpu())
            all_labels.append(labels.cpu())
            total_loss += loss.item()
        
        # åˆå¹¶æ‰€æœ‰ batch
        all_preds = torch.cat(all_preds, dim=0)
        all_labels = torch.cat(all_labels, dim=0)
        
        # è®¡ç®— Top-K å‡†ç¡®ç‡
        _, top_k_preds = torch.topk(all_preds, self.top_k, dim=1)
        correct = (top_k_preds == all_labels.unsqueeze(1)).any(dim=1).float()
        top_k_accuracy = correct.mean().item()
        
        # è®¡ç®— Top-1 å‡†ç¡®ç‡ï¼ˆæ ‡å‡†å‡†ç¡®ç‡ï¼‰
        top_1_preds = all_preds.argmax(dim=1)
        top_1_accuracy = (top_1_preds == all_labels).float().mean().item()
        
        metrics = {
            f"{metric_key_prefix}_loss": total_loss / len(dataloader),
            f"{metric_key_prefix}_top1_accuracy": top_1_accuracy,
            f"{metric_key_prefix}_top{self.top_k}_accuracy": top_k_accuracy
        }
        
        return EvalLoopOutput(
            predictions=all_preds.numpy(),
            label_ids=all_labels.numpy(),
            metrics=metrics,
            num_samples=len(all_labels)
        )
```

### 25.2.4 ç¤ºä¾‹ï¼šå¯¹æ¯”å­¦ä¹  Trainer

å®ç° Contrastive Learningï¼ˆå¦‚ SimCLRï¼‰çš„ Trainerï¼š

```python
import torch
import torch.nn.functional as F

class ContrastiveLearningTrainer(Trainer):
    def __init__(self, temperature=0.07, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.temperature = temperature
    
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆNT-Xent Lossï¼‰
        
        è¾“å…¥ï¼š
        - inputs: {"input_ids_1": ..., "input_ids_2": ...}ï¼ˆåŒä¸€æ ·æœ¬çš„ä¸¤ç§å¢å¼ºï¼‰
        
        æŸå¤±ï¼š
        L = -log( exp(sim(z_i, z_j) / Ï„) / Î£_k exp(sim(z_i, z_k) / Ï„) )
        """
        # è·å–ä¸¤ä¸ªå¢å¼ºè§†å›¾
        input_ids_1 = inputs.pop("input_ids_1")
        input_ids_2 = inputs.pop("input_ids_2")
        attention_mask_1 = inputs.pop("attention_mask_1", None)
        attention_mask_2 = inputs.pop("attention_mask_2", None)
        
        # å‰å‘ä¼ æ’­è·å– embeddings
        outputs_1 = model(
            input_ids=input_ids_1,
            attention_mask=attention_mask_1
        )
        outputs_2 = model(
            input_ids=input_ids_2,
            attention_mask=attention_mask_2
        )
        
        # æå– [CLS] è¡¨ç¤ºå¹¶å½’ä¸€åŒ–
        z1 = F.normalize(outputs_1.pooler_output, dim=1)  # (batch, hidden)
        z2 = F.normalize(outputs_2.pooler_output, dim=1)
        
        batch_size = z1.size(0)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        # æ‹¼æ¥æ­£è´Ÿæ ·æœ¬ï¼š[z1, z2] â†’ (2*batch, hidden)
        embeddings = torch.cat([z1, z2], dim=0)
        
        # è®¡ç®— cosine similarityï¼š(2*batch, 2*batch)
        similarity_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature
        
        # åˆ›å»º maskï¼ˆæ’é™¤è‡ªèº«ï¼‰
        mask = torch.eye(2 * batch_size, device=z1.device).bool()
        similarity_matrix.masked_fill_(mask, -1e9)
        
        # æ­£æ ·æœ¬å¯¹çš„ç´¢å¼•
        # z1[i] çš„æ­£æ ·æœ¬æ˜¯ z2[i]ï¼Œç´¢å¼•ä¸º i + batch_size
        positive_indices = torch.arange(batch_size, device=z1.device)
        
        # è®¡ç®—æŸå¤±ï¼ˆåˆ†ä¸¤éƒ¨åˆ†ï¼šz1â†’z2 å’Œ z2â†’z1ï¼‰
        # Part 1: z1 ä½œä¸º anchor
        logits_1 = similarity_matrix[:batch_size]  # (batch, 2*batch)
        labels_1 = positive_indices + batch_size
        loss_1 = F.cross_entropy(logits_1, labels_1)
        
        # Part 2: z2 ä½œä¸º anchor
        logits_2 = similarity_matrix[batch_size:]
        labels_2 = positive_indices
        loss_2 = F.cross_entropy(logits_2, labels_2)
        
        # æ€»æŸå¤±
        loss = (loss_1 + loss_2) / 2
        
        return (loss, outputs_1) if return_outputs else loss
```

**ä½¿ç”¨**ï¼š
```python
# æ•°æ®é›†éœ€è¦è¿”å›ä¸¤ä¸ªå¢å¼ºè§†å›¾
class ContrastiveDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=128):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        
        # ä¸¤ç§å¢å¼ºç­–ç•¥ï¼ˆç¤ºä¾‹ï¼šéšæœº dropoutï¼‰
        encoding_1 = self.tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        encoding_2 = self.tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        
        return {
            "input_ids_1": encoding_1["input_ids"].squeeze(0),
            "attention_mask_1": encoding_1["attention_mask"].squeeze(0),
            "input_ids_2": encoding_2["input_ids"].squeeze(0),
            "attention_mask_2": encoding_2["attention_mask"].squeeze(0)
        }
    
    def __len__(self):
        return len(self.texts)

# è®­ç»ƒ
trainer = ContrastiveLearningTrainer(
    temperature=0.07,
    model=model,
    args=training_args,
    train_dataset=contrastive_dataset
)
trainer.train()
```

---

## 25.3 è‡ªå®šä¹‰ Callback

`TrainerCallback` å…è®¸åœ¨è®­ç»ƒçš„å…³é”®èŠ‚ç‚¹æ’å…¥è‡ªå®šä¹‰é€»è¾‘ã€‚

### 25.3.1 TrainerCallback åŸºç±»

```python
from transformers import TrainerCallback, TrainerState, TrainerControl

class MyCallback(TrainerCallback):
    """
    è‡ªå®šä¹‰å›è°ƒåŸºç±»
    """
    def on_train_begin(self, args, state, control, **kwargs):
        """è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨"""
        pass
    
    def on_train_end(self, args, state, control, **kwargs):
        """è®­ç»ƒç»“æŸæ—¶è°ƒç”¨"""
        pass
    
    def on_epoch_begin(self, args, state, control, **kwargs):
        """æ¯ä¸ª epoch å¼€å§‹æ—¶è°ƒç”¨"""
        pass
    
    def on_epoch_end(self, args, state, control, **kwargs):
        """æ¯ä¸ª epoch ç»“æŸæ—¶è°ƒç”¨"""
        pass
    
    def on_step_begin(self, args, state, control, **kwargs):
        """æ¯ä¸ªè®­ç»ƒæ­¥éª¤å¼€å§‹æ—¶è°ƒç”¨"""
        pass
    
    def on_step_end(self, args, state, control, **kwargs):
        """æ¯ä¸ªè®­ç»ƒæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨"""
        pass
    
    def on_evaluate(self, args, state, control, **kwargs):
        """è¯„ä¼°æ—¶è°ƒç”¨"""
        pass
    
    def on_save(self, args, state, control, **kwargs):
        """ä¿å­˜æ£€æŸ¥ç‚¹æ—¶è°ƒç”¨"""
        pass
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        """è®°å½•æ—¥å¿—æ—¶è°ƒç”¨"""
        pass
```

**å…³é”®å‚æ•°**ï¼š
- **`args`**ï¼š`TrainingArguments` å¯¹è±¡
- **`state`**ï¼š`TrainerState` å¯¹è±¡ï¼ˆåŒ…å« global_step, epoch, best_metric ç­‰ï¼‰
- **`control`**ï¼š`TrainerControl` å¯¹è±¡ï¼ˆå¯ä»¥æ§åˆ¶è®­ç»ƒæµç¨‹ï¼‰
- **`kwargs`**ï¼šé¢å¤–å‚æ•°ï¼ˆå¦‚ model, optimizer, logsï¼‰

### 25.3.2 äº‹ä»¶è§¦å‘ç‚¹

å®Œæ•´çš„å›è°ƒæ‰§è¡Œé¡ºåºï¼š

```
on_train_begin
â”œâ”€ on_epoch_begin (epoch 1)
â”‚  â”œâ”€ on_step_begin (step 1)
â”‚  â”œâ”€ on_step_end
â”‚  â”œâ”€ on_log (if logging_steps)
â”‚  â”œâ”€ on_evaluate (if evaluation_strategy)
â”‚  â”œâ”€ on_save (if save_steps)
â”‚  â”œâ”€ ... (more steps)
â”‚  â””â”€ on_epoch_end
â”œâ”€ on_epoch_begin (epoch 2)
â”‚  â””â”€ ...
â””â”€ on_train_end
```

### 25.3.3 ç¤ºä¾‹ï¼šè‡ªå®šä¹‰å­¦ä¹ ç‡é¢„çƒ­

å®ç° Warmup + Linear Decayï¼ˆè™½ç„¶å†…ç½®ï¼Œä½†ä½œä¸ºç¤ºä¾‹ï¼‰ï¼š

```python
class WarmupCallback(TrainerCallback):
    def __init__(self, warmup_steps=1000, total_steps=10000):
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.base_lr = None
    
    def on_train_begin(self, args, state, control, **kwargs):
        # ä¿å­˜åˆå§‹å­¦ä¹ ç‡
        optimizer = kwargs.get("optimizer")
        self.base_lr = optimizer.param_groups[0]["lr"]
        print(f"âœ… Warmup Callback initialized: {self.warmup_steps} steps warmup")
    
    def on_step_end(self, args, state, control, **kwargs):
        optimizer = kwargs.get("optimizer")
        current_step = state.global_step
        
        if current_step < self.warmup_steps:
            # Warmup é˜¶æ®µï¼šçº¿æ€§å¢é•¿
            lr = self.base_lr * (current_step / self.warmup_steps)
        else:
            # Decay é˜¶æ®µï¼šçº¿æ€§è¡°å‡
            progress = (current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)
            lr = self.base_lr * (1 - progress)
        
        # æ›´æ–°å­¦ä¹ ç‡
        for param_group in optimizer.param_groups:
            param_group["lr"] = lr

# ä½¿ç”¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    callbacks=[WarmupCallback(warmup_steps=500, total_steps=5000)]
)
```

**æ›´å¤šç¤ºä¾‹**ï¼š

**1. æ—©åœï¼ˆEarly Stoppingï¼‰**
```python
class EarlyStoppingCallback(TrainerCallback):
    def __init__(self, patience=3, threshold=0.001):
        self.patience = patience
        self.threshold = threshold
        self.best_metric = None
        self.wait = 0
    
    def on_evaluate(self, args, state, control, metrics, **kwargs):
        current_metric = metrics.get("eval_loss")
        
        if self.best_metric is None or current_metric < self.best_metric - self.threshold:
            self.best_metric = current_metric
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= self.patience:
                print(f"ğŸ›‘ Early stopping triggered! Best metric: {self.best_metric}")
                control.should_training_stop = True  # åœæ­¢è®­ç»ƒ
        
        return control
```

**2. æ¢¯åº¦ç›‘æ§**
```python
class GradientMonitorCallback(TrainerCallback):
    def on_step_end(self, args, state, control, **kwargs):
        model = kwargs.get("model")
        
        total_norm = 0.0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** 0.5
        
        if state.global_step % args.logging_steps == 0:
            print(f"Step {state.global_step}: Gradient norm = {total_norm:.4f}")
            
            if total_norm > 10.0:
                print("âš ï¸  Warning: Gradient explosion detected!")
```

**3. æ¨¡å‹æ£€æŸ¥ç‚¹ç‰ˆæœ¬ç®¡ç†**
```python
import shutil

class VersionedCheckpointCallback(TrainerCallback):
    def __init__(self, keep_last_n=3):
        self.keep_last_n = keep_last_n
        self.checkpoints = []
    
    def on_save(self, args, state, control, **kwargs):
        checkpoint_dir = f"{args.output_dir}/checkpoint-{state.global_step}"
        self.checkpoints.append(checkpoint_dir)
        
        # åªä¿ç•™æœ€å N ä¸ªæ£€æŸ¥ç‚¹
        if len(self.checkpoints) > self.keep_last_n:
            old_checkpoint = self.checkpoints.pop(0)
            shutil.rmtree(old_checkpoint)
            print(f"ğŸ—‘ï¸  Removed old checkpoint: {old_checkpoint}")
```

---

## 25.4 å®Œå…¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯

æœ‰æ—¶ `Trainer` çš„çµæ´»æ€§ä¸å¤Ÿï¼Œéœ€è¦å®Œå…¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚ä½¿ç”¨ **Accelerate** å¯ä»¥è½»æ¾å®ç°ã€‚

### 25.4.1 ä½¿ç”¨ Accelerate æ›¿ä»£ Trainer

åŸºç¡€è®­ç»ƒå¾ªç¯ï¼š

```python
from accelerate import Accelerator
from torch.utils.data import DataLoader
from transformers import AdamW, get_linear_schedule_with_warmup
import torch

# 1. åˆå§‹åŒ– Accelerator
accelerator = Accelerator()

# 2. å‡†å¤‡æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æ•°æ®
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
optimizer = AdamW(model.parameters(), lr=5e-5)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
eval_dataloader = DataLoader(eval_dataset, batch_size=64)

# 3. ä½¿ç”¨ Accelerator åŒ…è£…
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)

# 4. å­¦ä¹ ç‡è°ƒåº¦å™¨
num_training_steps = len(train_dataloader) * num_epochs
lr_scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=500,
    num_training_steps=num_training_steps
)

# 5. è®­ç»ƒå¾ªç¯
num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    for batch in train_dataloader:
        # å‰å‘ä¼ æ’­
        outputs = model(**batch)
        loss = outputs.loss
        
        # åå‘ä¼ æ’­ï¼ˆAccelerator è‡ªåŠ¨å¤„ç†æ··åˆç²¾åº¦ï¼‰
        accelerator.backward(loss)
        
        # æ›´æ–°å‚æ•°
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        
        # æ‰“å°æ—¥å¿—
        if accelerator.is_main_process:
            print(f"Epoch {epoch}, Loss: {loss.item():.4f}")
    
    # è¯„ä¼°
    model.eval()
    total_correct = 0
    total_samples = 0
    
    for batch in eval_dataloader:
        with torch.no_grad():
            outputs = model(**batch)
            predictions = outputs.logits.argmax(dim=-1)
            
            # æ”¶é›†æ‰€æœ‰ GPU çš„ç»“æœ
            predictions, labels = accelerator.gather_for_metrics((predictions, batch["labels"]))
            total_correct += (predictions == labels).sum().item()
            total_samples += labels.size(0)
    
    accuracy = total_correct / total_samples
    if accelerator.is_main_process:
        print(f"Epoch {epoch} - Accuracy: {accuracy:.4f}")

# 6. ä¿å­˜æ¨¡å‹
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained("./my_model", save_function=accelerator.save)
```

**å…³é”® API**ï¼š
- **`accelerator.prepare()`**ï¼šè‡ªåŠ¨å¤„ç†è®¾å¤‡æ”¾ç½®ã€åˆ†å¸ƒå¼åŒ…è£…
- **`accelerator.backward()`**ï¼šè‡ªåŠ¨å¤„ç†æ··åˆç²¾åº¦çš„æ¢¯åº¦ç¼©æ”¾
- **`accelerator.gather_for_metrics()`**ï¼šä»æ‰€æœ‰è®¾å¤‡æ”¶é›†ç»“æœ
- **`accelerator.is_main_process`**ï¼šåˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ï¼ˆç”¨äºæ—¥å¿—ï¼‰

### 25.4.2 æ‰‹åŠ¨å®ç°æ¢¯åº¦ç´¯ç§¯

```python
gradient_accumulation_steps = 4

for epoch in range(num_epochs):
    model.train()
    for step, batch in enumerate(train_dataloader):
        # å‰å‘ä¼ æ’­
        outputs = model(**batch)
        loss = outputs.loss
        
        # æ¢¯åº¦ç´¯ç§¯ï¼šloss éœ€è¦é™¤ä»¥ç´¯ç§¯æ­¥æ•°
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        
        # æ¯ N æ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
        if (step + 1) % gradient_accumulation_steps == 0:
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
```

### 25.4.3 æ··åˆç²¾åº¦é›†æˆ

Accelerate è‡ªåŠ¨å¤„ç†æ··åˆç²¾åº¦ï¼Œåªéœ€åœ¨åˆå§‹åŒ–æ—¶æŒ‡å®šï¼š

```python
# æ–¹å¼ 1ï¼šå‘½ä»¤è¡Œå¯åŠ¨
# accelerate launch --mixed_precision fp16 train.py

# æ–¹å¼ 2ï¼šä»£ç ä¸­æŒ‡å®š
accelerator = Accelerator(mixed_precision="fp16")

# å…¶ä»–ä»£ç ä¸å˜ï¼ŒAccelerator ä¼šè‡ªåŠ¨ï¼š
# 1. æ¨¡å‹è½¬æ¢ä¸º fp16
# 2. æ¢¯åº¦ç¼©æ”¾ï¼ˆgradient scalingï¼‰
# 3. åŠ¨æ€æŸå¤±ç¼©æ”¾
```

### 25.4.4 åˆ†å¸ƒå¼è®­ç»ƒé€‚é…

ä½¿ç”¨ Accelerate çš„åˆ†å¸ƒå¼è®­ç»ƒï¼š

```bash
# å•æœºå¤šå¡
accelerate launch --multi_gpu --num_processes 4 train.py

# å¤šæœºå¤šå¡
accelerate launch \
    --multi_gpu \
    --num_machines 2 \
    --machine_rank 0 \
    --main_process_ip xxx.xxx.xxx.xxx \
    --num_processes 8 \
    train.py
```

**ä»£ç æ— éœ€ä¿®æ”¹**ï¼ŒAccelerate è‡ªåŠ¨å¤„ç†ï¼š
- è¿›ç¨‹åˆå§‹åŒ–
- æ¢¯åº¦åŒæ­¥
- æ•°æ®åˆ†ç‰‡

---

## 25.5 é«˜çº§æŸå¤±å‡½æ•°

<div data-component="LossFunctionExplorer"></div>

### 25.5.1 Focal Loss

ç”¨äºè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼ˆæå‡ºäº RetinaNetï¼‰ã€‚

**å…¬å¼**ï¼š
$$
\text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
$$

å…¶ä¸­ï¼š
- $p_t$ æ˜¯çœŸå®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡
- $\gamma$ æ˜¯èšç„¦å‚æ•°ï¼ˆé€šå¸¸ä¸º 2ï¼‰
- $\alpha_t$ æ˜¯ç±»åˆ«æƒé‡

**å®ç°**ï¼š
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        """
        Focal Loss for imbalanced classification
        
        Args:
            alpha (float): ç±»åˆ«æƒé‡
            gamma (float): èšç„¦å‚æ•°ï¼Œè¶Šå¤§è¶Šå…³æ³¨éš¾åˆ†æ ·æœ¬
            reduction (str): 'mean' or 'sum'
        """
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        """
        Args:
            inputs: (batch, num_classes) logits
            targets: (batch,) class indices
        """
        # è®¡ç®—æ¦‚ç‡
        p = F.softmax(inputs, dim=1)
        
        # è·å–çœŸå®ç±»åˆ«çš„æ¦‚ç‡
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        p_t = p.gather(1, targets.unsqueeze(1)).squeeze(1)
        
        # Focal weight: (1 - p_t)^gamma
        focal_weight = (1 - p_t) ** self.gamma
        
        # Focal Loss
        focal_loss = self.alpha * focal_weight * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


# åœ¨ Trainer ä¸­ä½¿ç”¨
class FocalLossTrainer(Trainer):
    def __init__(self, alpha=0.25, gamma=2.0, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.focal_loss = FocalLoss(alpha=alpha, gamma=gamma)
    
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        
        loss = self.focal_loss(logits, labels)
        
        return (loss, outputs) if return_outputs else loss
```

**æ•ˆæœ**ï¼š
- **å›°éš¾æ ·æœ¬**ï¼ˆ$p_t$ ä½ï¼‰ï¼šæŸå¤±æƒé‡å¤§ï¼Œæ¨¡å‹æ›´å…³æ³¨
- **ç®€å•æ ·æœ¬**ï¼ˆ$p_t$ é«˜ï¼‰ï¼šæŸå¤±æƒé‡å°ï¼ˆ$(1-p_t)^\gamma$ æ¥è¿‘ 0ï¼‰
- **ç±»åˆ«ä¸å¹³è¡¡**ï¼š$\alpha$ è°ƒæ•´æ­£è´Ÿæ ·æœ¬æƒé‡

### 25.5.2 Contrastive Loss

ç”¨äºå¯¹æ¯”å­¦ä¹ ï¼ˆSimCLRã€MoCoã€CLIPï¼‰ã€‚

**InfoNCE Loss**ï¼ˆNoise Contrastive Estimationï¼‰ï¼š
$$
\mathcal{L} = -\log \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k=1}^{2N} \mathbb{1}_{k \neq i} \exp(\text{sim}(z_i, z_k) / \tau)}
$$

**å®ç°**ï¼ˆè§ 25.2.4 å¯¹æ¯”å­¦ä¹  Trainerï¼‰ã€‚

**å…³é”®ç‚¹**ï¼š
- **æ­£æ ·æœ¬å¯¹**ï¼šåŒä¸€æ ·æœ¬çš„ä¸åŒå¢å¼º
- **è´Ÿæ ·æœ¬å¯¹**ï¼šbatch å†…å…¶ä»–æ ·æœ¬
- **æ¸©åº¦å‚æ•°** $\tau$ï¼šæ§åˆ¶åˆ†å¸ƒå¹³æ»‘åº¦ï¼ˆé€šå¸¸ 0.07ï¼‰

### 25.5.3 KL Divergenceï¼ˆçŸ¥è¯†è’¸é¦ï¼‰

ç”¨äºæ¨¡å‹è’¸é¦ï¼ˆDistilBERTã€TinyBERTï¼‰ã€‚

**è’¸é¦æŸå¤±**ï¼š
$$
\mathcal{L}_{\text{distill}} = \text{KL}(\text{softmax}(z_s / T) \| \text{softmax}(z_t / T))
$$

å…¶ä¸­ï¼š
- $z_s$ æ˜¯å­¦ç”Ÿæ¨¡å‹çš„ logits
- $z_t$ æ˜¯æ•™å¸ˆæ¨¡å‹çš„ logits
- $T$ æ˜¯æ¸©åº¦å‚æ•°ï¼ˆè½¯åŒ–åˆ†å¸ƒï¼‰

**å®ç°**ï¼š
```python
class DistillationTrainer(Trainer):
    def __init__(self, teacher_model, temperature=2.0, alpha=0.5, *args, **kwargs):
        """
        çŸ¥è¯†è’¸é¦ Trainer
        
        Args:
            teacher_model: æ•™å¸ˆæ¨¡å‹ï¼ˆå·²è®­ç»ƒå¥½ï¼‰
            temperature: æ¸©åº¦å‚æ•°ï¼ˆè½¯åŒ–åˆ†å¸ƒï¼‰
            alpha: è’¸é¦æŸå¤±æƒé‡ï¼ˆæ€»æŸå¤± = alpha * distill_loss + (1-alpha) * ce_lossï¼‰
        """
        super().__init__(*args, **kwargs)
        self.teacher_model = teacher_model
        self.teacher_model.eval()  # æ•™å¸ˆæ¨¡å‹å§‹ç»ˆåœ¨è¯„ä¼°æ¨¡å¼
        self.temperature = temperature
        self.alpha = alpha
    
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        
        # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
        student_outputs = model(**inputs)
        student_logits = student_outputs.logits
        
        # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
        with torch.no_grad():
            teacher_outputs = self.teacher_model(**inputs)
            teacher_logits = teacher_outputs.logits
        
        # 1. Hard Lossï¼ˆæ ‡å‡†äº¤å‰ç†µï¼‰
        hard_loss = F.cross_entropy(student_logits, labels)
        
        # 2. Soft Lossï¼ˆKL æ•£åº¦ï¼‰
        # ä½¿ç”¨æ¸©åº¦è½¯åŒ–åˆ†å¸ƒ
        student_probs = F.log_softmax(student_logits / self.temperature, dim=-1)
        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)
        
        # KL Divergence: D_KL(P || Q) = Î£ P(x) log(P(x) / Q(x))
        soft_loss = F.kl_div(
            student_probs,
            teacher_probs,
            reduction='batchmean'
        ) * (self.temperature ** 2)  # ç¼©æ”¾å› å­ï¼ˆæ¸©åº¦å¹³æ–¹ï¼‰
        
        # 3. æ€»æŸå¤±
        loss = self.alpha * soft_loss + (1 - self.alpha) * hard_loss
        
        return (loss, student_outputs) if return_outputs else loss


# ä½¿ç”¨
teacher_model = BertForSequenceClassification.from_pretrained("bert-base-uncased")
student_model = BertForSequenceClassification.from_pretrained("bert-tiny-uncased")

trainer = DistillationTrainer(
    teacher_model=teacher_model,
    temperature=2.0,
    alpha=0.7,
    model=student_model,
    args=training_args,
    train_dataset=train_dataset
)
trainer.train()
```

**ä¸ºä»€ä¹ˆä½¿ç”¨æ¸©åº¦**ï¼Ÿ
- **ä½æ¸©ï¼ˆT=1ï¼‰**ï¼šåˆ†å¸ƒå°–é”ï¼Œæ¥è¿‘ one-hot
- **é«˜æ¸©ï¼ˆT>1ï¼‰**ï¼šåˆ†å¸ƒå¹³æ»‘ï¼ŒåŒ…å«æ›´å¤šç±»é—´å…³ç³»ä¿¡æ¯
- **$T^2$ ç¼©æ”¾**ï¼šæŠµæ¶ˆæ¸©åº¦å¯¹æ¢¯åº¦å¹…åº¦çš„å½±å“

### 25.5.4 å¤šä»»åŠ¡å­¦ä¹ æŸå¤±ç»„åˆ

åŒæ—¶è®­ç»ƒå¤šä¸ªä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿåˆ†ç±» + NERï¼‰ã€‚

```python
class MultiTaskTrainer(Trainer):
    def __init__(self, task_weights=None, *args, **kwargs):
        """
        å¤šä»»åŠ¡å­¦ä¹  Trainer
        
        Args:
            task_weights: dictï¼Œä¾‹å¦‚ {"classification": 1.0, "ner": 0.5}
        """
        super().__init__(*args, **kwargs)
        self.task_weights = task_weights or {"classification": 1.0, "ner": 1.0}
    
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        å‡è®¾æ¨¡å‹è¾“å‡ºå¤šä¸ª lossï¼š
        outputs = {
            "classification_loss": ...,
            "ner_loss": ...,
            "logits": ...
        }
        """
        outputs = model(**inputs)
        
        # åŠ æƒç»„åˆå¤šä¸ªæŸå¤±
        total_loss = 0.0
        for task, weight in self.task_weights.items():
            task_loss = outputs.get(f"{task}_loss")
            if task_loss is not None:
                total_loss += weight * task_loss
        
        # è¿”å›æ€»æŸå¤±å’ŒåŸå§‹è¾“å‡º
        if return_outputs:
            outputs["loss"] = total_loss
            return total_loss, outputs
        else:
            return total_loss
```

**åŠ¨æ€ä»»åŠ¡æƒé‡**ï¼ˆUncertainty Weightingï¼‰ï¼š
```python
class UncertaintyWeightedTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # å¯å­¦ä¹ çš„ä»»åŠ¡æƒé‡ï¼ˆlog(ÏƒÂ²)ï¼‰
        self.log_vars = nn.Parameter(torch.zeros(2))  # 2 ä¸ªä»»åŠ¡
    
    def compute_loss(self, model, inputs, return_outputs=False):
        outputs = model(**inputs)
        
        loss1 = outputs["classification_loss"]
        loss2 = outputs["ner_loss"]
        
        # Uncertainty weighting:
        # L_total = (1 / 2Ïƒâ‚Â²) Lâ‚ + (1 / 2Ïƒâ‚‚Â²) Lâ‚‚ + log(Ïƒâ‚Ïƒâ‚‚)
        precision1 = torch.exp(-self.log_vars[0])
        precision2 = torch.exp(-self.log_vars[1])
        
        total_loss = (
            precision1 * loss1 +
            precision2 * loss2 +
            self.log_vars[0] + self.log_vars[1]  # æ­£åˆ™åŒ–é¡¹
        )
        
        return (total_loss, outputs) if return_outputs else total_loss
```

---

## 25.6 å®æˆ˜æ¡ˆä¾‹ï¼šæƒ…æ„Ÿåˆ†æè‡ªå®šä¹‰è®­ç»ƒ

ç»“åˆæ‰€æœ‰æŠ€æœ¯ï¼Œå®ç°ä¸€ä¸ªå®Œæ•´çš„è‡ªå®šä¹‰è®­ç»ƒæµç¨‹ã€‚

```python
import torch
import torch.nn as nn
from transformers import (
    Trainer,
    TrainingArguments,
    BertForSequenceClassification,
    BertTokenizer,
    TrainerCallback
)
from datasets import load_dataset
import numpy as np

# 1. è‡ªå®šä¹‰ Focal Loss Trainer
class SentimentTrainer(Trainer):
    def __init__(self, focal_gamma=2.0, label_smoothing=0.1, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.focal_gamma = focal_gamma
        self.label_smoothing = label_smoothing
    
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        
        # Focal Loss + Label Smoothing
        log_probs = F.log_softmax(logits, dim=-1)
        probs = torch.exp(log_probs)
        
        # è·å–çœŸå®ç±»åˆ«æ¦‚ç‡
        true_probs = probs.gather(1, labels.unsqueeze(1)).squeeze(1)
        
        # Focal weight
        focal_weight = (1 - true_probs) ** self.focal_gamma
        
        # Label smoothing
        num_classes = logits.size(-1)
        smooth_labels = torch.zeros_like(probs).scatter_(
            1, labels.unsqueeze(1), 1 - self.label_smoothing
        ) + self.label_smoothing / num_classes
        
        # æŸå¤±
        loss = -(focal_weight.unsqueeze(1) * smooth_labels * log_probs).sum(dim=-1).mean()
        
        return (loss, outputs) if return_outputs else loss

# 2. è‡ªå®šä¹‰ Callbackï¼ˆæ¢¯åº¦ç›‘æ§ + æ—©åœï¼‰
class MonitorCallback(TrainerCallback):
    def __init__(self, patience=3):
        self.patience = patience
        self.best_loss = float('inf')
        self.wait = 0
    
    def on_step_end(self, args, state, control, **kwargs):
        # æ¢¯åº¦ç›‘æ§
        if state.global_step % 100 == 0:
            model = kwargs["model"]
            total_norm = sum(
                p.grad.norm(2).item() ** 2 
                for p in model.parameters() if p.grad is not None
            ) ** 0.5
            print(f"ğŸ“Š Step {state.global_step}: Gradient norm = {total_norm:.4f}")
    
    def on_evaluate(self, args, state, control, metrics, **kwargs):
        # æ—©åœ
        current_loss = metrics.get("eval_loss")
        if current_loss < self.best_loss:
            self.best_loss = current_loss
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= self.patience:
                print(f"ğŸ›‘ Early stopping! Best loss: {self.best_loss:.4f}")
                control.should_training_stop = True

# 3. å‡†å¤‡æ•°æ®
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
dataset = load_dataset("imdb")

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=256
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# 4. è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./sentiment_model",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    learning_rate=2e-5,
    warmup_steps=500,
    weight_decay=0.01,
    logging_steps=100,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=500,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    fp16=True  # æ··åˆç²¾åº¦
)

# 5. æ¨¡å‹å’Œ Trainer
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

trainer = SentimentTrainer(
    focal_gamma=2.0,
    label_smoothing=0.1,
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    callbacks=[MonitorCallback(patience=3)]
)

# 6. è®­ç»ƒ
trainer.train()

# 7. è¯„ä¼°
results = trainer.evaluate()
print(f"âœ… Final results: {results}")
```

---

## 25.7 ç« èŠ‚æ€»ç»“

æœ¬ç« æˆ‘ä»¬æ·±å…¥å­¦ä¹ äº† Trainer çš„é«˜çº§å®šåˆ¶æŠ€æœ¯ï¼š

âœ… **æ ¸å¿ƒæŠ€èƒ½**ï¼š
- ç†è§£ `Trainer` å†…éƒ¨è®­ç»ƒå¾ªç¯ï¼ˆDataLoader â†’ Forward â†’ Backward â†’ Optimizer Stepï¼‰
- é‡å†™ `compute_loss()`ã€`training_step()`ã€`evaluation_loop()`
- å®ç°è‡ªå®šä¹‰ Callbackï¼ˆæ—©åœã€æ¢¯åº¦ç›‘æ§ã€å­¦ä¹ ç‡è°ƒåº¦ï¼‰
- ä½¿ç”¨ Accelerate å®Œå…¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯
- å®ç°é«˜çº§æŸå¤±å‡½æ•°ï¼ˆFocal Lossã€Contrastive Lossã€KL Divergenceï¼‰

âœ… **å®æˆ˜èƒ½åŠ›**ï¼š
- å¯¹æŠ—è®­ç»ƒï¼ˆFGMï¼‰
- å¯¹æ¯”å­¦ä¹ ï¼ˆSimCLRï¼‰
- çŸ¥è¯†è’¸é¦ï¼ˆKL Divergenceï¼‰
- å¤šä»»åŠ¡å­¦ä¹ ï¼ˆåŠ¨æ€ä»»åŠ¡æƒé‡ï¼‰

âœ… **æœ€ä½³å®è·µ**ï¼š
- æ¢¯åº¦ç´¯ç§¯ï¼š`loss = loss / gradient_accumulation_steps`
- æ··åˆç²¾åº¦ï¼šä½¿ç”¨ `Accelerator(mixed_precision="fp16")`
- åˆ†å¸ƒå¼è®­ç»ƒï¼š`accelerate launch --multi_gpu`
- æ—©åœï¼š`control.should_training_stop = True`

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼šChapter 26 å°†è¿›å…¥**å¤šæ¨¡æ€æ¨¡å‹**é¢†åŸŸï¼Œå­¦ä¹  Vision-Language æ¨¡å‹ï¼ˆCLIPã€BLIPã€LLaVAï¼‰ã€å›¾åƒç¼–ç å™¨ï¼ˆViTï¼‰ã€è§†è§‰é—®ç­”å¾®è°ƒã€å›¾åƒç”Ÿæˆï¼ˆStable Diffusionï¼‰ä»¥åŠéŸ³é¢‘æ¨¡å‹ï¼ˆWhisperã€Wav2Vec2ï¼‰ã€‚
