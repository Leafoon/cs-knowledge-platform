---
title: "Chapter 17. é«˜æ•ˆæ¨ç†åŸºç¡€"
description: "ç†è§£æ¨ç†æ€§èƒ½æŒ‡æ ‡ã€æŒæ¡ Flash Attentionã€BetterTransformerã€torch.compile ä¼˜åŒ–æŠ€æœ¯"
updated: "2026-01-22"
---

---

## 17.1 æ¨ç†æ€§èƒ½æŒ‡æ ‡

### 17.1.1 æ ¸å¿ƒæŒ‡æ ‡å®šä¹‰

åœ¨æ¨ç†ä¼˜åŒ–ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å…³æ³¨ä»¥ä¸‹å…³é”®æŒ‡æ ‡ï¼š

| **æŒ‡æ ‡** | **å®šä¹‰** | **å•ä½** | **é€‚ç”¨åœºæ™¯** |
|---------|---------|---------|-------------|
| **Latencyï¼ˆå»¶è¿Ÿï¼‰** | å•æ¬¡æ¨ç†çš„æ€»è€—æ—¶ | æ¯«ç§’ï¼ˆmsï¼‰ | äº¤äº’å¼åº”ç”¨ï¼ˆèŠå¤©æœºå™¨äººï¼‰ |
| **Throughputï¼ˆååé‡ï¼‰** | å•ä½æ—¶é—´å¤„ç†çš„æ ·æœ¬æ•° | samples/s æˆ– tokens/s | æ‰¹å¤„ç†ä»»åŠ¡ï¼ˆæ‰¹é‡ç¿»è¯‘ï¼‰ |
| **TTFTï¼ˆTime to First Tokenï¼‰** | ç”Ÿæˆç¬¬ä¸€ä¸ª token çš„æ—¶é—´ | æ¯«ç§’ï¼ˆmsï¼‰ | æµå¼ç”Ÿæˆä½“éªŒ |
| **TPSï¼ˆTokens Per Secondï¼‰** | æ¯ç§’ç”Ÿæˆçš„ token æ•°é‡ | tokens/s | ç”Ÿæˆä»»åŠ¡æ•ˆç‡ |
| **Memory Footprint** | æ¨¡å‹å ç”¨æ˜¾å­˜ | GB | éƒ¨ç½²æˆæœ¬ |

---

#### **1. Latency vs Throughput æƒè¡¡**

**å»¶è¿Ÿï¼ˆLatencyï¼‰**ï¼šä»è¾“å…¥åˆ°è¾“å‡ºçš„æ—¶é—´

$$
\text{Latency} = \text{Preprocessing Time} + \text{Model Inference Time} + \text{Postprocessing Time}
$$

**ååé‡ï¼ˆThroughputï¼‰**ï¼šå•ä½æ—¶é—´å¤„ç†çš„æ ·æœ¬æ•°

$$
\text{Throughput} = \frac{\text{Batch Size}}{\text{Latency}}
$$

**å…³é”®çŸ›ç›¾**ï¼š
- **å° Batch Size**ï¼ˆå¦‚ 1ï¼‰ï¼šå»¶è¿Ÿæœ€ä½ï¼Œä½†ååé‡ä½ï¼ˆGPU åˆ©ç”¨ç‡ä¸è¶³ï¼‰
- **å¤§ Batch Size**ï¼ˆå¦‚ 128ï¼‰ï¼šååé‡é«˜ï¼Œä½†å•æ ·æœ¬å»¶è¿Ÿé«˜

**å®æµ‹æ•°æ®ï¼ˆBERT-baseï¼ŒV100ï¼‰**ï¼š

| **Batch Size** | **Latencyï¼ˆms/sampleï¼‰** | **Throughputï¼ˆsamples/sï¼‰** | **GPU åˆ©ç”¨ç‡** |
|---------------|------------------------|--------------------------|--------------|
| 1 | 5 ms | 200 | 15% |
| 8 | 12 ms | 667 | 45% |
| 32 | 35 ms | 914 | 70% |
| 128 | 120 ms | 1067 | 95% |

**å»ºè®®**ï¼š
- **å®æ—¶åº”ç”¨**ï¼šBatch Size = 1-4ï¼ˆä¼˜å…ˆä½å»¶è¿Ÿï¼‰
- **æ‰¹å¤„ç†ä»»åŠ¡**ï¼šBatch Size = 32-128ï¼ˆä¼˜å…ˆé«˜ååï¼‰
- **åœ¨çº¿æœåŠ¡**ï¼šåŠ¨æ€æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰

---

#### **2. Time to First Token (TTFT)**

**å®šä¹‰**ï¼šåœ¨ç”Ÿæˆå¼ä»»åŠ¡ä¸­ï¼Œä»è¾“å…¥ prompt åˆ°ç”Ÿæˆç¬¬ä¸€ä¸ª token çš„æ—¶é—´ã€‚

$$
\text{TTFT} = \text{Prompt Processing Time} + \text{First Token Generation Time}
$$

**å½±å“å› ç´ **ï¼š
- **Prompt é•¿åº¦**ï¼šè¶Šé•¿ï¼ŒTTFT è¶Šé«˜ï¼ˆéœ€è¦å®Œæ•´å‰å‘ä¼ æ’­ï¼‰
- **æ¨¡å‹å¤§å°**ï¼šå‚æ•°è¶Šå¤šï¼Œè®¡ç®—è¶Šæ…¢
- **KV Cache åˆå§‹åŒ–**ï¼šéœ€è¦ç¼“å­˜æ‰€æœ‰ prompt tokens çš„ Kã€V

**ä¼˜åŒ–ç›®æ ‡**ï¼šTTFT < 100msï¼ˆç”¨æˆ·æ„ŸçŸ¥æµç•…ï¼‰

---

#### **3. Tokens Per Second (TPS)**

**å®šä¹‰**ï¼šç”Ÿæˆé˜¶æ®µæ¯ç§’ç”Ÿæˆçš„ token æ•°é‡ã€‚

$$
\text{TPS} = \frac{\text{Output Tokens}}{\text{Generation Time}}
$$

**ä¸å»¶è¿Ÿçš„å…³ç³»**ï¼š

$$
\text{Per-Token Latency} = \frac{1}{\text{TPS}}
$$

**å®æµ‹æ•°æ®ï¼ˆLLaMA-7Bï¼ŒA100ï¼Œç”Ÿæˆ 100 tokensï¼‰**ï¼š

| **ä¼˜åŒ–æ–¹æ³•** | **TTFTï¼ˆmsï¼‰** | **TPS** | **æ€»è€—æ—¶ï¼ˆmsï¼‰** |
|------------|--------------|---------|----------------|
| åŸå§‹ PyTorch | 350 | 25 | 4350 |
| BetterTransformer | 280 | 35 | 3130 |
| Flash Attention 2 | 200 | 50 | 2200 |
| torch.compile | 150 | 60 | 1816 |
| **ç»„åˆä¼˜åŒ–** | **120** | **80** | **1370** |

---

### 17.1.2 å»¶è¿Ÿåˆ†è§£åˆ†æ

<div data-component="InferenceLatencyBreakdown"></div>

**æ ‡å‡† Transformer æ¨ç†å»¶è¿Ÿåˆ†è§£**ï¼š

| **é˜¶æ®µ** | **å æ¯”** | **ä¼˜åŒ–æ–¹æ³•** |
|---------|---------|------------|
| **Tokenization** | 5% | Fast Tokenizerï¼ˆRust å®ç°ï¼‰ |
| **Embedding Lookup** | 5% | æ— æ˜¾è‘—ä¼˜åŒ–ç©ºé—´ |
| **Attention** | 60%-70% | Flash Attentionã€Multi-Query Attention |
| **FFN** | 20%-25% | Kernel èåˆã€torch.compile |
| **Sampling** | 5%-10% | Top-K/Top-P åŠ é€Ÿã€é™æ€ KV Cache |
| **Detokenization** | <1% | å¯å¿½ç•¥ |

**æ ¸å¿ƒä¼˜åŒ–æ–¹å‘**ï¼š
1. **Attention åŠ é€Ÿ**ï¼ˆæœ€é‡è¦ï¼‰ï¼šFlash Attentionã€PagedAttention
2. **FFN ä¼˜åŒ–**ï¼šç®—å­èåˆã€é‡åŒ–
3. **å‡å°‘å†…å­˜è®¿é—®**ï¼šKV Cache ä¼˜åŒ–ã€æ¿€æ´»å€¼é‡è®¡ç®—

---

### 17.1.3 æ‰¹å¤„ç†æ•ˆç‡

**æ‰¹å¤„ç†çš„æŒ‘æˆ˜**ï¼š
- **Padding æµªè´¹**ï¼šåºåˆ—é•¿åº¦ä¸ä¸€è‡´å¯¼è‡´æ— æ•ˆè®¡ç®—
- **æ˜¾å­˜å ç”¨**ï¼šbatch size è¶Šå¤§ï¼ŒKV Cache è¶Šå¤§

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **åŠ¨æ€ Batching**ï¼šç›¸ä¼¼é•¿åº¦çš„æ ·æœ¬ç»„æˆ batch
2. **Continuous Batching**ï¼ˆvLLM å¼•å…¥ï¼‰ï¼šåŠ¨æ€æ·»åŠ /ç§»é™¤å®Œæˆçš„æ ·æœ¬
3. **FlashAttention çš„ variable-length æ”¯æŒ**

---

## 17.2 BetterTransformer

### 17.2.1 FastPath æ‰§è¡Œè·¯å¾„

**BetterTransformer** æ˜¯ PyTorch 1.12+ å¼•å…¥çš„ä¼˜åŒ–ï¼Œé€šè¿‡**ç›´æ¥è°ƒç”¨ C++ åº•å±‚ç®—å­**ï¼ˆFastPathï¼‰ç»•è¿‡ Python å±‚çš„å¼€é”€ã€‚

**æ ¸å¿ƒä¼˜åŒ–**ï¼š
- **Native Attention**ï¼šä½¿ç”¨ `torch._native_multi_head_attention`ï¼ˆC++ å®ç°ï¼‰
- **Fused Operations**ï¼šLayerNorm + Residual Connection èåˆ
- **Padding Mask ä¼˜åŒ–**ï¼šé¿å…ä¸å¿…è¦çš„ Softmax è®¡ç®—

**æ”¯æŒçš„æ¨¡å‹æ¶æ„**ï¼š
- BERTã€RoBERTaã€ALBERTã€DistilBERT
- GPT-2ã€GPT-Neoã€OPT
- BARTã€T5ã€Whisperã€ViT

---

### 17.2.2 å¯ç”¨ BetterTransformer

**æ–¹æ³• 1ï¼šä½¿ç”¨ `to_bettertransformer()`**

```python
from transformers import AutoModelForSequenceClassification
import torch

model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased"
).to("cuda")

# å¯ç”¨ BetterTransformer
model = model.to_bettertransformer()

# æ¨ç†
inputs = tokenizer("Hello world!", return_tensors="pt").to("cuda")
with torch.inference_mode():
    outputs = model(**inputs)
```

**æ³¨æ„**ï¼š
- å¿…é¡»åœ¨ `model.eval()` æ¨¡å¼ä¸‹ä½¿ç”¨
- ä¸æ”¯æŒè®­ç»ƒï¼ˆä»…æ¨ç†ï¼‰
- éœ€è¦ PyTorch >= 1.12

---

**æ–¹æ³• 2ï¼šé€šè¿‡ `from_pretrained()` è‡ªåŠ¨å¯ç”¨**

```python
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    torch_dtype=torch.float16,
    device_map="auto",
    # è‡ªåŠ¨å¯ç”¨ BetterTransformer
    use_bettertransformer=True
)
```

---

### 17.2.3 æ€§èƒ½å¯¹æ¯”

**BERT-base æ¨ç†æ€§èƒ½ï¼ˆV100ï¼ŒBatch Size=32ï¼‰**ï¼š

| **é…ç½®** | **Latencyï¼ˆmsï¼‰** | **Throughputï¼ˆsamples/sï¼‰** | **åŠ é€Ÿæ¯”** |
|---------|-----------------|--------------------------|----------|
| åŸå§‹ PyTorch | 35 ms | 914 | 1.0x |
| BetterTransformer | 22 ms | 1455 | **1.6x** |
| BetterTransformer + FP16 | 18 ms | 1778 | **1.9x** |

**GPT-2 ç”Ÿæˆæ€§èƒ½ï¼ˆA100ï¼Œç”Ÿæˆ 50 tokensï¼‰**ï¼š

| **é…ç½®** | **TTFTï¼ˆmsï¼‰** | **TPS** | **æ€»è€—æ—¶ï¼ˆmsï¼‰** |
|---------|--------------|---------|----------------|
| åŸå§‹ PyTorch | 180 ms | 30 | 1846 ms |
| BetterTransformer | 120 ms | 45 | 1231 ms |
| **åŠ é€Ÿæ¯”** | **1.5x** | **1.5x** | **1.5x** |

---

### 17.2.4 é™åˆ¶ä¸æ³¨æ„äº‹é¡¹

âŒ **ä¸æ”¯æŒçš„æƒ…å†µ**ï¼š
- è®­ç»ƒæ¨¡å¼ï¼ˆåªèƒ½ç”¨äºæ¨ç†ï¼‰
- è‡ªå®šä¹‰ Attention å®ç°
- æŸäº›ç‰¹æ®Š Attention Maskï¼ˆå¦‚ ALiBiï¼‰

âœ… **æœ€ä½³å®è·µ**ï¼š
- ç»“åˆ FP16/BF16 æ··åˆç²¾åº¦
- ä½¿ç”¨ `torch.inference_mode()` è€Œé `torch.no_grad()`
- å›ºå®šè¾“å…¥å½¢çŠ¶ï¼ˆé¿å…åŠ¨æ€ shape é‡ç¼–è¯‘ï¼‰

---

## 17.3 Flash Attention 2

### 17.3.1 IO-Aware æ³¨æ„åŠ›ç®—æ³•åŸç†

**Flash Attention** æ˜¯æ–¯å¦ç¦å¤§å­¦æå‡ºçš„é©å‘½æ€§ç®—æ³•ï¼Œé€šè¿‡**å‡å°‘ GPU å†…å­˜è®¿é—®**ï¼ˆHBM â†” SRAMï¼‰å®ç° 2-4 å€åŠ é€Ÿã€‚

#### **æ ‡å‡† Attention çš„ç“¶é¢ˆ**

æ ‡å‡† Attention è®¡ç®—ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

**å†…å­˜è®¿é—®æ¨¡å¼**ï¼š
1. ä» HBM è¯»å– $Q, K, V$
2. è®¡ç®— $S = QK^T$ï¼Œå†™å› HBM
3. ä» HBM è¯»å– $S$ï¼Œè®¡ç®— Softmaxï¼Œå†™å› HBM
4. ä» HBM è¯»å– Softmax ç»“æœå’Œ $V$ï¼Œè®¡ç®—æœ€ç»ˆè¾“å‡º

**é—®é¢˜**ï¼š
- **å¤šæ¬¡ HBM è®¿é—®**ï¼šæ¯æ­¥éƒ½éœ€è¦è¯»/å†™å¤§çŸ©é˜µ
- **æ˜¾å­˜å ç”¨**ï¼š$S$ çŸ©é˜µå¤§å°ä¸º $O(N^2)$ï¼ˆ$N$ ä¸ºåºåˆ—é•¿åº¦ï¼‰

**HBM vs SRAM é€Ÿåº¦å·®å¼‚**ï¼š
- **HBMï¼ˆHigh Bandwidth Memoryï¼‰**ï¼šä¸»æ˜¾å­˜ï¼Œå®¹é‡å¤§ï¼ˆ80GBï¼‰ï¼Œé€Ÿåº¦æ…¢ï¼ˆ~2 TB/sï¼‰
- **SRAMï¼ˆOn-Chip Memoryï¼‰**ï¼šç‰‡ä¸Šç¼“å­˜ï¼Œå®¹é‡å°ï¼ˆ20 MBï¼‰ï¼Œé€Ÿåº¦å¿«ï¼ˆ~19 TB/sï¼‰

HBM è®¿é—®é€Ÿåº¦ä»…ä¸º SRAM çš„ **1/10**ï¼

---

#### **Flash Attention çš„æ ¸å¿ƒåˆ›æ–°**

<div data-component="FlashAttentionIOComparison"></div>

**å…³é”®æ€æƒ³**ï¼š
1. **åˆ†å—è®¡ç®—ï¼ˆTilingï¼‰**ï¼šå°† $Q, K, V$ åˆ†æˆå°å—ï¼Œæ¯å—å®Œå…¨æ”¾å…¥ SRAM
2. **åœ¨çº¿ Softmax**ï¼šä¸å­˜å‚¨å®Œæ•´ $S$ çŸ©é˜µï¼Œä½¿ç”¨åœ¨çº¿ç®—æ³•é€å—è®¡ç®—
3. **é‡è®¡ç®—ï¼ˆRecomputationï¼‰**ï¼šåå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—æ¿€æ´»å€¼ï¼Œé¿å…å­˜å‚¨

**ç®—æ³•æµç¨‹**ï¼š

```
å¯¹äºæ¯ä¸ª Q çš„å— (Qi):
    å¯¹äºæ¯ä¸ª K, V çš„å— (Kj, Vj):
        1. ä» HBM åŠ è½½ Qi, Kj, Vj åˆ° SRAM
        2. åœ¨ SRAM ä¸­è®¡ç®— Sij = Qi @ Kj^T
        3. åœ¨çº¿æ›´æ–° Softmax ç»Ÿè®¡é‡ï¼ˆæœ€å¤§å€¼ã€ç´¯åŠ å’Œï¼‰
        4. è®¡ç®—éƒ¨åˆ†è¾“å‡º Oi += softmax(Sij) @ Vj
    5. å°†æœ€ç»ˆ Oi å†™å› HBM
```

**ä¼˜åŠ¿**ï¼š
- **HBM è®¿é—®æ¬¡æ•°**ï¼šä» $O(N^2)$ é™è‡³ $O(N)$
- **æ˜¾å­˜å ç”¨**ï¼šæ— éœ€å­˜å‚¨ $S$ çŸ©é˜µï¼ŒèŠ‚çœ $O(N^2)$ æ˜¾å­˜
- **é€Ÿåº¦æå‡**ï¼š2-4 å€ï¼ˆIO-bound ä»»åŠ¡ï¼‰

---

### 17.3.2 å®‰è£…ä¸å¯ç”¨ Flash Attention 2

#### **å®‰è£…**

```bash
# éœ€è¦ CUDA 11.8+
pip install flash-attn --no-build-isolation

# æˆ–ä»æºç ç¼–è¯‘
git clone https://github.com/Dao-AILab/flash-attention
cd flash-attention
python setup.py install
```

**ä¾èµ–**ï¼š
- PyTorch >= 2.0
- CUDA >= 11.8
- GPU æ¶æ„ >= Ampereï¼ˆA100ã€RTX 3090ã€H100ï¼‰

---

#### **å¯ç”¨æ–¹å¼ 1ï¼šfrom_pretrained()**

```python
from transformers import AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.float16,
    device_map="auto",
    use_flash_attention_2=True,  # å¯ç”¨ Flash Attention 2
)

# æ¨ç†
inputs = tokenizer("Hello", return_tensors="pt").to("cuda")
with torch.inference_mode():
    outputs = model.generate(**inputs, max_new_tokens=50)
```

---

#### **å¯ç”¨æ–¹å¼ 2ï¼šæ‰‹åŠ¨æ›¿æ¢ Attention**

```python
from transformers.models.llama.modeling_llama import LlamaAttention
from flash_attn import flash_attn_func

class FlashLlamaAttention(LlamaAttention):
    def forward(self, hidden_states, attention_mask=None, **kwargs):
        # è®¡ç®— Q, K, V
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)
        
        # ä½¿ç”¨ Flash Attention
        attn_output = flash_attn_func(
            query_states, key_states, value_states,
            dropout_p=0.0,
            softmax_scale=1.0 / math.sqrt(self.head_dim),
            causal=True,
        )
        
        return self.o_proj(attn_output)

# æ›¿æ¢æ‰€æœ‰ Attention å±‚
for layer in model.model.layers:
    layer.self_attn = FlashLlamaAttention(layer.self_attn.config)
```

---

### 17.3.3 æ€§èƒ½æå‡ä¸æ˜¾å­˜èŠ‚çœ

**LLaMA-7B æ¨ç†æ€§èƒ½ï¼ˆA100ï¼ŒBatch Size=1ï¼‰**ï¼š

| **ä¼˜åŒ–** | **åºåˆ—é•¿åº¦** | **TTFTï¼ˆmsï¼‰** | **TPS** | **æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰** |
|---------|------------|--------------|---------|------------------|
| æ ‡å‡† Attention | 512 | 280 | 35 | 16.2 |
| Flash Attention 2 | 512 | 150 | 58 | 14.8 |
| **åŠ é€Ÿæ¯”** | - | **1.87x** | **1.66x** | **-8.6%** |

**é•¿åºåˆ—ä¼˜åŠ¿æ›´æ˜æ˜¾ï¼ˆLLaMA-7Bï¼ŒBatch Size=1ï¼‰**ï¼š

| **åºåˆ—é•¿åº¦** | **æ ‡å‡† Attentionï¼ˆGBï¼‰** | **Flash Attention 2ï¼ˆGBï¼‰** | **æ˜¾å­˜èŠ‚çœ** |
|------------|------------------------|--------------------------|------------|
| 512 | 16.2 | 14.8 | 8.6% |
| 2048 | 22.4 | 16.5 | **26.3%** |
| 4096 | 34.8 | 19.2 | **44.8%** |
| 8192 | OOMï¼ˆ>80GBï¼‰ | 24.6 | **å¯è¿è¡Œï¼** |

---

### 17.3.4 Flash Attention 2 çš„é™åˆ¶

âŒ **ä¸æ”¯æŒ**ï¼š
- **è‡ªå®šä¹‰ Attention Mask**ï¼šä»…æ”¯æŒ causal å’Œ bidirectional
- **ALiBi ä½ç½®ç¼–ç **ï¼šéœ€è¦é¢å¤– bias çŸ©é˜µ
- **Sparse Attention**ï¼šå¦‚ Longformerã€BigBird

âœ… **å…¼å®¹æ€§**ï¼š
- RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰ï¼šå®Œå…¨æ”¯æŒ
- Multi-Query Attention (MQA)ï¼šæ”¯æŒ
- Grouped-Query Attention (GQA)ï¼šæ”¯æŒ

---

## 17.4 torch.compile (PyTorch 2.0+)

### 17.4.1 TorchDynamo + TorchInductor åŸç†

**torch.compile** æ˜¯ PyTorch 2.0 å¼•å…¥çš„ç¼–è¯‘å™¨ï¼Œé€šè¿‡**å³æ—¶ç¼–è¯‘ï¼ˆJIT Compilationï¼‰**ä¼˜åŒ–è®¡ç®—å›¾ã€‚

**æ¶æ„**ï¼š

```
Python ä»£ç  
  â†“ TorchDynamo (å›¾æ•è·)
è®¡ç®—å›¾ (FX Graph)
  â†“ TorchInductor (ä»£ç ç”Ÿæˆ)
ä¼˜åŒ–çš„ CUDA Kernel
  â†“ Triton (GPU ä»£ç )
é«˜æ€§èƒ½æ‰§è¡Œ
```

**æ ¸å¿ƒç»„ä»¶**ï¼š
1. **TorchDynamo**ï¼šæ•è· Python æ‰§è¡Œè¿‡ç¨‹ä¸­çš„è®¡ç®—å›¾
2. **TorchInductor**ï¼šç”Ÿæˆä¼˜åŒ–çš„ CUDA/C++ ä»£ç 
3. **Triton**ï¼šGPU ç¼–ç¨‹è¯­è¨€ï¼ˆç±»ä¼¼ CUDAï¼Œä½†æ›´æ˜“ä¼˜åŒ–ï¼‰

---

### 17.4.2 ç¼–è¯‘æ¨¡å¼è¯¦è§£

**ä¸‰ç§ç¼–è¯‘æ¨¡å¼**ï¼š

| **æ¨¡å¼** | **ä¼˜åŒ–ç¨‹åº¦** | **ç¼–è¯‘æ—¶é—´** | **é€‚ç”¨åœºæ™¯** |
|---------|------------|------------|-------------|
| `default` | ä¸­ç­‰ | çŸ­ï¼ˆ~30sï¼‰ | é€šç”¨åœºæ™¯ï¼Œå¹³è¡¡ç¼–è¯‘ä¸è¿è¡Œé€Ÿåº¦ |
| `reduce-overhead` | ä½ | æçŸ­ï¼ˆ~10sï¼‰ | é¢‘ç¹åŠ¨æ€ shapeï¼Œå‡å°‘ç¼–è¯‘å¼€é”€ |
| `max-autotune` | æé«˜ | é•¿ï¼ˆ~5minï¼‰ | å›ºå®š shapeï¼Œè¿½æ±‚æè‡´æ€§èƒ½ |

---

### 17.4.3 ä½¿ç”¨ç¤ºä¾‹

#### **åŸºç¡€ç”¨æ³•**

```python
import torch
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.float16,
    device_map="auto"
)

# ç¼–è¯‘æ¨¡å‹
model = torch.compile(model, mode="default")

# é¦–æ¬¡è¿è¡Œä¼šè§¦å‘ç¼–è¯‘ï¼ˆè¾ƒæ…¢ï¼‰
inputs = tokenizer("Hello", return_tensors="pt").to("cuda")
with torch.inference_mode():
    outputs = model.generate(**inputs, max_new_tokens=10)

# åç»­è¿è¡Œä½¿ç”¨ç¼–è¯‘åçš„ä»£ç ï¼ˆå¿«é€Ÿï¼‰
outputs = model.generate(**inputs, max_new_tokens=50)
```

---

#### **é«˜çº§é…ç½®**

```python
# æœ€å¤§è‡ªåŠ¨è°ƒä¼˜æ¨¡å¼
model = torch.compile(
    model,
    mode="max-autotune",
    fullgraph=True,  # å°è¯•ç¼–è¯‘æ•´ä¸ªå›¾ï¼ˆæ›´æ¿€è¿›ï¼‰
    dynamic=False,   # ç¦ç”¨åŠ¨æ€ shapeï¼ˆå›ºå®šè¾“å…¥å¤§å°ï¼‰
)

# ä»…ç¼–è¯‘ç‰¹å®šæ¨¡å—
model.model.layers = torch.compile(model.model.layers, mode="default")
```

---

### 17.4.4 é¦–æ¬¡è¿è¡Œå¼€é”€ï¼ˆWarm-upï¼‰

**é—®é¢˜**ï¼šé¦–æ¬¡è¿è¡Œéœ€è¦ç¼–è¯‘ï¼Œè€—æ—¶è¾ƒé•¿ï¼ˆ10s-5minï¼‰

**è§£å†³æ–¹æ¡ˆ 1ï¼šé¢„çƒ­ï¼ˆWarm-upï¼‰**

```python
# é¢„çƒ­ï¼šä½¿ç”¨å°è¾“å…¥è§¦å‘ç¼–è¯‘
dummy_input = torch.randint(0, 1000, (1, 10), device="cuda")
with torch.inference_mode():
    _ = model(dummy_input)

# æ­£å¼æ¨ç†ï¼ˆå·²ç¼–è¯‘ï¼Œé€Ÿåº¦å¿«ï¼‰
outputs = model.generate(**inputs, max_new_tokens=100)
```

---

**è§£å†³æ–¹æ¡ˆ 2ï¼šä¿å­˜ç¼–è¯‘ç¼“å­˜**

```python
# å¯ç”¨ç¼–è¯‘ç¼“å­˜
import torch._dynamo
torch._dynamo.config.cache_size_limit = 64

# ç¼–è¯‘ç»“æœä¼šç¼“å­˜åˆ° ~/.cache/torch/
```

---

### 17.4.5 æ€§èƒ½æå‡å®æµ‹

**LLaMA-7B æ¨ç†æ€§èƒ½ï¼ˆA100ï¼ŒBatch Size=1ï¼Œç”Ÿæˆ 100 tokensï¼‰**ï¼š

| **ä¼˜åŒ–** | **TTFTï¼ˆmsï¼‰** | **TPS** | **æ€»è€—æ—¶ï¼ˆmsï¼‰** | **åŠ é€Ÿæ¯”** |
|---------|--------------|---------|----------------|----------|
| åŸå§‹ PyTorch | 350 | 25 | 4350 | 1.0x |
| torch.compile (default) | 180 | 55 | 1998 | **2.2x** |
| torch.compile (max-autotune) | 150 | 60 | 1816 | **2.4x** |

**BERT-base åˆ†ç±»ï¼ˆV100ï¼ŒBatch Size=64ï¼‰**ï¼š

| **é…ç½®** | **Throughputï¼ˆsamples/sï¼‰** | **åŠ é€Ÿæ¯”** |
|---------|--------------------------|----------|
| eager æ¨¡å¼ | 914 | 1.0x |
| torch.compile | 1420 | **1.55x** |
| torch.compile + FP16 | 1850 | **2.02x** |

---

### 17.4.6 å…¼å®¹æ€§ä¸é™åˆ¶

âœ… **æ”¯æŒ**ï¼š
- å¤§å¤šæ•° Transformers æ¨¡å‹ï¼ˆBERTã€GPTã€T5ã€LLaMAï¼‰
- æ··åˆç²¾åº¦ï¼ˆFP16/BF16ï¼‰
- åŠ¨æ€ shapeï¼ˆmode="reduce-overhead"ï¼‰

âŒ **ä¸æ”¯æŒæˆ–æ€§èƒ½è¾ƒå·®**ï¼š
- é«˜åº¦åŠ¨æ€çš„æ§åˆ¶æµï¼ˆif/whileï¼‰
- è‡ªå®šä¹‰ CUDA ç®—å­
- é¢‘ç¹æ”¹å˜è¾“å…¥ shapeï¼ˆæ¯æ¬¡éƒ½é‡ç¼–è¯‘ï¼‰

**æœ€ä½³å®è·µ**ï¼š
- **å›ºå®šè¾“å…¥ shape**ï¼ˆå¦‚å›ºå®š max_lengthï¼‰
- **ä½¿ç”¨ fullgraph=True**ï¼ˆå•æ¬¡ç¼–è¯‘æ•´ä¸ªæ¨¡å‹ï¼‰
- **ç»“åˆ Flash Attention 2**ï¼ˆè¿›ä¸€æ­¥åŠ é€Ÿï¼‰

---

## 17.5 é™æ€ KV Cache

### 17.5.1 åŠ¨æ€ vs é™æ€ KV Cache

#### **åŠ¨æ€ KV Cacheï¼ˆé»˜è®¤ï¼‰**

**åŸç†**ï¼šé€ token ç”Ÿæˆæ—¶ï¼ŒåŠ¨æ€æ‰©å±• past_key_values å¼ é‡ã€‚

```python
# ç¬¬ 1 ä¸ª token
past_key_values = None
output_1 = model(input_ids[:, 0], past_key_values=None)
past_key_values = output_1.past_key_values  # shape: (batch, num_heads, 1, head_dim)

# ç¬¬ 2 ä¸ª token
output_2 = model(input_ids[:, 1], past_key_values=past_key_values)
past_key_values = output_2.past_key_values  # shape: (batch, num_heads, 2, head_dim)

# ... ä¾æ¬¡è¿½åŠ 
```

**é—®é¢˜**ï¼š
- **å†…å­˜ç¢ç‰‡**ï¼šæ¯æ¬¡ `torch.cat()` éƒ½éœ€è¦åˆ†é…æ–°å†…å­˜
- **åŠ¨æ€ shape**ï¼šå¯¼è‡´ GPU kernel æ— æ³•ä¼˜åŒ–

---

#### **é™æ€ KV Cacheï¼ˆä¼˜åŒ–ï¼‰**

<div data-component="KVCacheComparisonVisualizer"></div>

**åŸç†**ï¼šé¢„åˆ†é…å›ºå®šå¤§å°çš„ KV Cacheï¼Œé¿å…åŠ¨æ€æ‰©å±•ã€‚

```python
from transformers import StaticCache

# é¢„åˆ†é… cacheï¼ˆå‡è®¾æœ€å¤§ç”Ÿæˆ 512 tokensï¼‰
cache = StaticCache(
    config=model.config,
    max_batch_size=1,
    max_cache_len=512,
    device="cuda",
    dtype=torch.float16
)

# ç”Ÿæˆæ—¶å¤ç”¨ cache
outputs = model.generate(
    **inputs,
    max_new_tokens=100,
    past_key_values=cache,
    cache_implementation="static"
)
```

**ä¼˜åŠ¿**ï¼š
- **é›¶å†…å­˜åˆ†é…å¼€é”€**ï¼šé¢„åˆ†é…åä¸å†åŠ¨æ€æ‰©å±•
- **å›ºå®š shape**ï¼šGPU kernel å¯å……åˆ†ä¼˜åŒ–
- **ä¸ torch.compile å®Œç¾é…åˆ**

---

### 17.5.2 å¯ç”¨é™æ€ Cache

#### **æ–¹æ³• 1ï¼šé€šè¿‡ GenerationConfig**

```python
from transformers import GenerationConfig

generation_config = GenerationConfig(
    max_new_tokens=100,
    cache_implementation="static",  # å¯ç”¨é™æ€ cache
    cache_config={
        "batch_size": 1,
        "max_cache_len": 512
    }
)

outputs = model.generate(**inputs, generation_config=generation_config)
```

---

#### **æ–¹æ³• 2ï¼šæ‰‹åŠ¨åˆ›å»º Cache**

```python
from transformers import StaticCache

# åˆ›å»ºé™æ€ cache
static_cache = StaticCache(
    config=model.config,
    max_batch_size=4,  # æ”¯æŒ batch æ¨ç†
    max_cache_len=2048,
    device="cuda",
    dtype=torch.float16
)

# æ¨ç†æ—¶ä¼ å…¥
outputs = model.generate(
    **inputs,
    max_new_tokens=100,
    past_key_values=static_cache
)
```

---

### 17.5.3 æ€§èƒ½å¯¹æ¯”

**LLaMA-7B ç”Ÿæˆæ€§èƒ½ï¼ˆA100ï¼Œç”Ÿæˆ 100 tokensï¼‰**ï¼š

| **Cache ç±»å‹** | **æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰** | **TTFTï¼ˆmsï¼‰** | **TPS** | **æ€»è€—æ—¶ï¼ˆmsï¼‰** |
|--------------|----------------|--------------|---------|----------------|
| åŠ¨æ€ Cache | 16.2 | 200 | 50 | 2200 |
| é™æ€ Cache | 15.8 | 180 | 58 | 1903 |
| é™æ€ Cache + compile | 15.8 | 120 | 75 | 1453 |
| **åŠ é€Ÿæ¯”** | **-2.5%** | **1.67x** | **1.5x** | **1.51x** |

**å…³é”®å‘ç°**ï¼š
- é™æ€ Cache å•ç‹¬æå‡æœ‰é™ï¼ˆ~10%ï¼‰
- **ä¸ torch.compile ç»„åˆæ—¶æ•ˆæœæ˜¾è‘—**ï¼ˆ1.5x+ï¼‰
- æ˜¾å­˜å ç”¨ç•¥å¾®é™ä½ï¼ˆå‡å°‘ç¢ç‰‡ï¼‰

---

## 17.6 æ‰¹å¤„ç†ä¼˜åŒ–

### 17.6.1 åŠ¨æ€ Batching

**é—®é¢˜**ï¼šä¸åŒæ ·æœ¬çš„åºåˆ—é•¿åº¦å·®å¼‚å¤§ï¼Œå¯¼è‡´ padding æµªè´¹ã€‚

**ç¤ºä¾‹**ï¼š
```
Sample 1: "Hello"           â†’ 1 token  + 511 padding
Sample 2: "Hello, how are?" â†’ 4 tokens + 508 padding
Sample 3: "Hi"              â†’ 1 token  + 511 padding
```

æœ‰æ•ˆè®¡ç®—ç‡ï¼š$(1+4+1) / (512 \times 3) = 0.39\%$ ğŸ˜±

---

**è§£å†³æ–¹æ¡ˆï¼šæŒ‰é•¿åº¦åˆ†ç»„**

```python
from torch.utils.data import DataLoader
from transformers import DataCollatorWithPadding

# æŒ‰åºåˆ—é•¿åº¦åˆ†ç»„
def collate_fn(batch):
    # ä»…å¯¹å½“å‰ batch è¿›è¡Œ paddingï¼ˆæœ€å° paddingï¼‰
    return tokenizer.pad(
        batch,
        padding=True,
        max_length=None,  # åŠ¨æ€è®¡ç®—
        return_tensors="pt"
    )

dataloader = DataLoader(
    dataset,
    batch_size=32,
    collate_fn=collate_fn,
    # æŒ‰é•¿åº¦æ’åºï¼ˆå¯é€‰ï¼‰
    shuffle=False
)
```

**æ•ˆæœ**ï¼šæœ‰æ•ˆè®¡ç®—ç‡æå‡è‡³ 80%-95%ã€‚

---

### 17.6.2 Continuous Batchingï¼ˆvLLM å¼•å…¥ï¼‰

**ä¼ ç»Ÿ Static Batching çš„é—®é¢˜**ï¼š

```
æ—¶é—´è½´ï¼š
[Batch 1: Sample A (50 tokens), Sample B (10 tokens), Sample C (5 tokens)]
   â†“
ç­‰å¾…æœ€é•¿æ ·æœ¬ (A) å®Œæˆåï¼Œæ•´ä¸ª batch æ‰ç»“æŸ
   â†“
Sample B å’Œ C æå‰å®Œæˆï¼Œä½† GPU ç©ºé—²ç­‰å¾…
```

**Continuous Batching çš„åˆ›æ–°**ï¼š

```
æ—¶é—´è½´ï¼š
t=0:  [A, B, C]  â† 3 ä¸ªæ ·æœ¬åŒæ—¶ç”Ÿæˆ
t=5:  [A, B, D]  â† C å®Œæˆï¼Œç«‹å³åŠ å…¥æ–°æ ·æœ¬ D
t=10: [A, E, F]  â† B å®Œæˆï¼ŒåŠ å…¥ E å’Œ F
t=50: [G, H, I]  â† A å®Œæˆï¼ŒæŒç»­è¡¥å……æ–°æ ·æœ¬
```

**ä¼˜åŠ¿**ï¼š
- **GPU åˆ©ç”¨ç‡ 100%**ï¼šå§‹ç»ˆæœ‰æ–°æ ·æœ¬å¡«è¡¥ç©ºé—²
- **ååé‡æå‡ 2-10 å€**ï¼ˆå–å†³äºæ ·æœ¬é•¿åº¦åˆ†å¸ƒï¼‰
- **é™ä½å¹³å‡å»¶è¿Ÿ**ï¼šçŸ­æ ·æœ¬æ— éœ€ç­‰å¾…é•¿æ ·æœ¬

**å®ç°**ï¼švLLMã€TGIï¼ˆè¯¦è§ Chapter 18ï¼‰

---

### 17.6.3 Padding ç­–ç•¥å¯¹æ¯”

| **ç­–ç•¥** | **ä¼˜ç‚¹** | **ç¼ºç‚¹** | **é€‚ç”¨åœºæ™¯** |
|---------|---------|---------|------------|
| **Left Padding** | é€‚åˆç”Ÿæˆä»»åŠ¡ï¼ˆKV Cache å¯¹é½ï¼‰ | Tokenizer éœ€è¦æ”¯æŒ | GPT ç³»åˆ—ç”Ÿæˆ |
| **Right Padding** | é€‚åˆåˆ†ç±»ä»»åŠ¡ï¼ˆ[CLS] åœ¨å¼€å¤´ï¼‰ | ç”Ÿæˆä»»åŠ¡æ€§èƒ½å·® | BERT åˆ†ç±» |
| **Dynamic Padding** | æœ€å° padding æµªè´¹ | éœ€è¦è‡ªå®šä¹‰ collate_fn | é•¿åº¦å·®å¼‚å¤§çš„æ•°æ®é›† |
| **No Padding (Variable Length)** | é›¶æµªè´¹ | ä»… Flash Attention æ”¯æŒ | Flash Attention 2 æ¨ç† |

---

## 17.7 ç»„åˆä¼˜åŒ–ç­–ç•¥

### 17.7.1 æœ€ä½³å®è·µç»„åˆ

**æ¨èé…ç½®ï¼ˆLLaMA-7B æ¨ç†ï¼‰**ï¼š

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, StaticCache

# 1. åŠ è½½æ¨¡å‹ï¼ˆFP16 + Flash Attention 2ï¼‰
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.float16,
    device_map="auto",
    use_flash_attention_2=True,  # Flash Attention
)

# 2. ç¼–è¯‘æ¨¡å‹
model = torch.compile(model, mode="max-autotune", fullgraph=True)

# 3. åˆ›å»ºé™æ€ Cache
static_cache = StaticCache(
    config=model.config,
    max_batch_size=4,
    max_cache_len=2048,
    device="cuda",
    dtype=torch.float16
)

# 4. é¢„çƒ­ç¼–è¯‘
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
dummy_input = tokenizer("Warm-up", return_tensors="pt").to("cuda")
with torch.inference_mode():
    _ = model.generate(**dummy_input, max_new_tokens=10, past_key_values=static_cache)

# 5. æ­£å¼æ¨ç†
inputs = tokenizer("Hello, how are you?", return_tensors="pt").to("cuda")
with torch.inference_mode():
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        past_key_values=static_cache,
        do_sample=True,
        temperature=0.7
    )

print(tokenizer.decode(outputs[0]))
```

**é¢„æœŸæ€§èƒ½æå‡**ï¼š

| **ä¼˜åŒ–** | **å•ç‹¬æ•ˆæœ** | **ç´¯ç§¯åŠ é€Ÿ** |
|---------|------------|------------|
| Baseline (FP32) | 1.0x | 1.0x |
| + FP16 | 1.5x | 1.5x |
| + Flash Attention 2 | 1.8x | **2.7x** |
| + torch.compile | 1.6x | **4.3x** |
| + Static Cache | 1.2x | **5.2x** |

---

### 17.7.2 æƒè¡¡ä¸é€‰æ‹©

**ä¸åŒåœºæ™¯çš„ä¼˜åŒ–ä¼˜å…ˆçº§**ï¼š

#### **1. å®æ—¶äº¤äº’ï¼ˆèŠå¤©æœºå™¨äººï¼‰**

**ç›®æ ‡**ï¼šTTFT < 100ms

**ä¼˜å…ˆçº§**ï¼š
1. Flash Attention 2ï¼ˆé™ä½ TTFTï¼‰
2. torch.compileï¼ˆåŠ é€Ÿæ¨ç†ï¼‰
3. å° Batch Sizeï¼ˆ1-2ï¼‰
4. é™æ€ Cache

**é…ç½®**ï¼š
```python
model = torch.compile(
    AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        use_flash_attention_2=True
    ),
    mode="reduce-overhead"  # å¿«é€Ÿç¼–è¯‘
)
```

---

#### **2. æ‰¹é‡å¤„ç†ï¼ˆæ‰¹é‡ç¿»è¯‘ï¼‰**

**ç›®æ ‡**ï¼šååé‡æœ€å¤§åŒ–

**ä¼˜å…ˆçº§**ï¼š
1. å¤§ Batch Sizeï¼ˆ32-128ï¼‰
2. Dynamic Batchingï¼ˆæŒ‰é•¿åº¦åˆ†ç»„ï¼‰
3. torch.compile (max-autotune)
4. BetterTransformer

**é…ç½®**ï¼š
```python
model = torch.compile(
    AutoModelForSeq2SeqLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        use_bettertransformer=True
    ),
    mode="max-autotune",
    fullgraph=True
)
```

---

#### **3. é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆè®ºæ–‡å†™ä½œåŠ©æ‰‹ï¼‰**

**ç›®æ ‡**ï¼šæ”¯æŒé•¿åºåˆ—ï¼ˆ4K-8K tokensï¼‰

**ä¼˜å…ˆçº§**ï¼š
1. Flash Attention 2ï¼ˆæ˜¾å­˜ä¼˜åŒ–ï¼‰
2. Gradient Checkpointingï¼ˆè®­ç»ƒæ—¶ï¼‰
3. é™æ€ Cacheï¼ˆå›ºå®š max_lengthï¼‰
4. é‡åŒ–ï¼ˆ4-bit / 8-bitï¼‰

**é…ç½®**ï¼š
```python
from transformers import BitsAndBytesConfig

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=BitsAndBytesConfig(load_in_4bit=True),
    use_flash_attention_2=True,
    max_memory={0: "40GB"}  # é™åˆ¶æ˜¾å­˜
)
```

---

## 17.8 æ€§èƒ½å‰–æå·¥å…·

### 17.8.1 PyTorch Profiler

```python
import torch
from torch.profiler import profile, ProfilerActivity

model = ...
inputs = ...

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    with_stack=True
) as prof:
    with torch.inference_mode():
        outputs = model.generate(**inputs, max_new_tokens=50)

# æ‰“å°æŠ¥å‘Š
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))

# å¯¼å‡º Chrome Trace
prof.export_chrome_trace("trace.json")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
-----------------------  ------------  ------------  
Name                     CPU Time      CUDA Time     
-----------------------  ------------  ------------  
aten::matmul             5.23ms        120.45ms      
aten::softmax            1.12ms        35.67ms       
aten::layer_norm         0.85ms        22.34ms       
-----------------------  ------------  ------------  
```

---

### 17.8.2 NVIDIA Nsight Systems

```bash
# è¿è¡Œ profiling
nsys profile -o profile.qdrep python infer.py

# æŸ¥çœ‹ç»“æœï¼ˆGUIï¼‰
nsys-ui profile.qdrep
```

**åˆ†ææŒ‡æ ‡**ï¼š
- **Kernel æ‰§è¡Œæ—¶é—´**ï¼šå“ªäº› CUDA kernel æœ€æ…¢
- **å†…å­˜ä¼ è¾“**ï¼šHBM â†” SRAM æ•°æ®é‡
- **GPU åˆ©ç”¨ç‡**ï¼šæ˜¯å¦å……åˆ†åˆ©ç”¨ GPU

---

## 17.9 æ€»ç»“ä¸æœ€ä½³å®è·µ

### 17.9.1 ä¼˜åŒ–æ¸…å•

âœ… **å¿…åšä¼˜åŒ–**ï¼ˆé€‚ç”¨æ‰€æœ‰åœºæ™¯ï¼‰ï¼š
- [ ] ä½¿ç”¨ FP16/BF16 æ··åˆç²¾åº¦
- [ ] å¯ç”¨ Flash Attention 2ï¼ˆAmpere+ GPUï¼‰
- [ ] ä½¿ç”¨ torch.inference_mode() è€Œé torch.no_grad()
- [ ] é¢„çƒ­æ¨¡å‹ï¼ˆwarm-upï¼‰

âœ… **é«˜ä¼˜å…ˆçº§**ï¼ˆå¤§å¤šæ•°åœºæ™¯ï¼‰ï¼š
- [ ] torch.compileï¼ˆPyTorch 2.0+ï¼‰
- [ ] BetterTransformerï¼ˆç®€å•æ¨¡å‹ï¼‰
- [ ] é™æ€ KV Cacheï¼ˆå›ºå®šç”Ÿæˆé•¿åº¦ï¼‰
- [ ] æ‰¹å¤„ç†ä¼˜åŒ–

âœ… **å¯é€‰ä¼˜åŒ–**ï¼ˆç‰¹å®šåœºæ™¯ï¼‰ï¼š
- [ ] é‡åŒ–ï¼ˆ4-bit/8-bitï¼Œæ˜¾å­˜å—é™æ—¶ï¼‰
- [ ] æ¨¡å‹å¯¼å‡ºï¼ˆONNX/TensorRTï¼Œç”Ÿäº§éƒ¨ç½²ï¼‰
- [ ] vLLM/TGIï¼ˆåœ¨çº¿æœåŠ¡ï¼‰

---

### 17.9.2 æ€§èƒ½åŸºå‡†

**LLaMA-7B æ¨ç†æ€§èƒ½æ€»ç»“ï¼ˆA100ï¼Œç”Ÿæˆ 100 tokensï¼‰**ï¼š

| **é…ç½®** | **TTFTï¼ˆmsï¼‰** | **TPS** | **æ˜¾å­˜ï¼ˆGBï¼‰** | **æ€»è€—æ—¶ï¼ˆmsï¼‰** |
|---------|--------------|---------|--------------|----------------|
| Baseline (FP32) | 500 | 18 | 28.0 | 6056 |
| FP16 | 350 | 25 | 16.2 | 4350 |
| + BetterTransformer | 280 | 35 | 16.2 | 3130 |
| + Flash Attention 2 | 200 | 50 | 14.8 | 2200 |
| + torch.compile | 150 | 60 | 14.8 | 1816 |
| + Static Cache | 120 | 75 | 14.5 | 1453 |
| **æ€»åŠ é€Ÿæ¯”** | **4.2x** | **4.2x** | **-48%** | **4.2x** |

---

### 17.9.3 å¸¸è§è¯¯åŒº

âŒ **è¯¯åŒº 1**ï¼šåªä¼˜åŒ–æ¨¡å‹ï¼Œå¿½ç•¥æ•°æ®å¤„ç†

**æ­£ç¡®åšæ³•**ï¼š
- ä½¿ç”¨ Fast Tokenizerï¼ˆRust å®ç°ï¼‰
- ä¼˜åŒ–æ•°æ® collate å‡½æ•°
- å‡å°‘ CPU â†” GPU æ•°æ®ä¼ è¾“

---

âŒ **è¯¯åŒº 2**ï¼šç›²ç›®å¢å¤§ Batch Size

**æ­£ç¡®åšæ³•**ï¼š
- æ ¹æ®ä»»åŠ¡é€‰æ‹©ï¼ˆå®æ—¶ vs æ‰¹å¤„ç†ï¼‰
- ç›‘æ§ GPU åˆ©ç”¨ç‡ï¼ˆ`nvidia-smi dmon`ï¼‰
- æµ‹è¯•ä¸åŒ batch size çš„ååé‡

---

âŒ **è¯¯åŒº 3**ï¼šå¿½ç•¥é¦–æ¬¡è¿è¡Œå¼€é”€

**æ­£ç¡®åšæ³•**ï¼š
- é¢„çƒ­æ¨¡å‹ï¼ˆwarm-upï¼‰
- ç¼“å­˜ç¼–è¯‘ç»“æœï¼ˆtorch.compileï¼‰
- æœåŠ¡å¯åŠ¨æ—¶å®Œæˆæ‰€æœ‰åˆå§‹åŒ–

---

## 17.10 æ‰©å±•é˜…è¯»

- **Flash Attention è®ºæ–‡**ï¼š[arXiv:2205.14135](https://arxiv.org/abs/2205.14135)
- **Flash Attention 2**ï¼š[arXiv:2307.08691](https://arxiv.org/abs/2307.08691)
- **PyTorch 2.0 Blog**ï¼šhttps://pytorch.org/blog/pytorch-2.0-release/
- **BetterTransformer æ–‡æ¡£**ï¼šhttps://huggingface.co/docs/transformers/perf_infer_gpu_one
- **Triton æ•™ç¨‹**ï¼šhttps://triton-lang.org/main/getting-started/tutorials/index.html

---

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼šChapter 18 å°†æ·±å…¥æ¢è®¨ **vLLM ä¸ TGI**ï¼Œå­¦ä¹  PagedAttentionã€Continuous Batchingã€åœ¨çº¿æœåŠ¡éƒ¨ç½²ç­‰ç”Ÿäº§çº§æ¨ç†ä¼˜åŒ–æŠ€æœ¯ï¼Œå®ç° 10-20 å€ååé‡æå‡ã€‚
