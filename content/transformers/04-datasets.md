# Chapter 4: Datasets åº“ä¸æ•°æ®é¢„å¤„ç†

> **æœ¬ç« ç›®æ ‡**ï¼šå…¨é¢æŒæ¡ Hugging Face Datasets åº“çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œå­¦ä¹ é«˜æ•ˆçš„æ•°æ®åŠ è½½ã€é¢„å¤„ç†å’Œç®¡ç†æŠ€å·§ï¼Œä¸ºæ¨¡å‹è®­ç»ƒåšå¥½æ•°æ®å‡†å¤‡ã€‚

---

## 4.1 Datasets åº“åŸºç¡€

### 4.1.1 ä¸ºä»€ä¹ˆéœ€è¦ Datasetsï¼Ÿ

åœ¨æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­ï¼Œæ•°æ®å¤„ç†å¾€å¾€å æ®å¤§é‡æ—¶é—´å’Œå†…å­˜ã€‚Hugging Face Datasets åº“é€šè¿‡ä»¥ä¸‹åˆ›æ–°è§£å†³äº†ä¼ ç»Ÿç—›ç‚¹ï¼š

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š

1. **å†…å­˜æ˜ å°„ï¼ˆMemory Mappingï¼‰**
   - æ•°æ®å­˜å‚¨åœ¨ç£ç›˜ä¸Šï¼ŒæŒ‰éœ€åŠ è½½åˆ°å†…å­˜
   - å¤„ç† TB çº§æ•°æ®é›†æ—¶å†…å­˜å ç”¨æå°
   - åŸºäº Apache Arrow é«˜æ€§èƒ½åˆ—å¼å­˜å‚¨

2. **é›¶æ‹·è´è¯»å–ï¼ˆZero-Copy Readsï¼‰**
   - é¿å…æ•°æ®åœ¨å†…å­˜ä¸­é‡å¤å¤åˆ¶
   - æ˜¾è‘—æå‡æ•°æ®åŠ è½½é€Ÿåº¦

3. **æ™ºèƒ½ç¼“å­˜**
   - è‡ªåŠ¨ç¼“å­˜é¢„å¤„ç†ç»“æœï¼ˆå¦‚ tokenizationï¼‰
   - é¿å…é‡å¤è®¡ç®—

4. **äº’æ“ä½œæ€§**
   - æ— ç¼è½¬æ¢ä¸º PyTorch/TensorFlow å¼ é‡
   - ä¸ Pandasã€NumPy äº’é€š

**ä¼ ç»Ÿæ–¹å¼ vs Datasets åº“å¯¹æ¯”**ï¼š

```python
# âŒ ä¼ ç»Ÿæ–¹å¼ï¼šå…¨éƒ¨åŠ è½½åˆ°å†…å­˜
import pandas as pd

# å ç”¨å¤§é‡å†…å­˜ï¼ˆå‡è®¾ 10GB æ•°æ®ï¼‰
df = pd.read_csv("large_dataset.csv")  # å†…å­˜å ç”¨ ~10GB
texts = df["text"].tolist()

# ğŸŸ¢ Datasets åº“ï¼šå†…å­˜æ˜ å°„
from datasets import load_dataset

# å†…å­˜å ç”¨ ~å‡ ç™¾ MBï¼ˆæ•°æ®å­˜åœ¨ç£ç›˜ï¼‰
dataset = load_dataset("csv", data_files="large_dataset.csv")
print(f"æ•°æ®é›†å¤§å°: {dataset.num_rows} è¡Œ")
print(f"å†…å­˜å ç”¨: ~0.5GB")  # ä»…ç´¢å¼•å’Œå…ƒæ•°æ®
```

**æ€§èƒ½å¯¹æ¯”å®éªŒ**ï¼š

```python
import time
import psutil
import os

def measure_memory():
    """æµ‹é‡å½“å‰è¿›ç¨‹å†…å­˜å ç”¨"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024  # MB

# æµ‹è¯•æ•°æ®é›†ï¼šIMDB ç”µå½±è¯„è®ºï¼ˆ~500MBï¼‰
dataset_name = "imdb"

# æ–¹å¼1: Datasets åº“
start_mem = measure_memory()
start_time = time.time()

dataset = load_dataset(dataset_name, split="train")
print(f"Datasets åº“:")
print(f"  åŠ è½½æ—¶é—´: {time.time() - start_time:.2f}s")
print(f"  å†…å­˜å¢é‡: {measure_memory() - start_mem:.2f}MB")
print(f"  æ•°æ®é›†å¤§å°: {len(dataset)} æ¡\n")

# è¾“å‡ºç¤ºä¾‹:
# Datasets åº“:
#   åŠ è½½æ—¶é—´: 2.3s
#   å†…å­˜å¢é‡: 45MB
#   æ•°æ®é›†å¤§å°: 25000 æ¡
```

### 4.1.2 åŠ è½½æ•°æ®é›†ï¼ˆload_datasetï¼‰

**åŸºæœ¬ç”¨æ³•**ï¼š

```python
from datasets import load_dataset

# æ–¹å¼1: ä» Hub åŠ è½½ï¼ˆæœ€å¸¸ç”¨ï¼‰
dataset = load_dataset("glue", "mrpc")  # GLUE åŸºå‡†çš„ MRPC ä»»åŠ¡

# æ–¹å¼2: æŒ‡å®šæ•°æ®é›†é…ç½®
dataset = load_dataset(
    "glue",
    "mrpc",
    split="train",           # åªåŠ è½½è®­ç»ƒé›†
    cache_dir="./my_cache"   # è‡ªå®šä¹‰ç¼“å­˜ç›®å½•
)

# æ–¹å¼3: ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
dataset = load_dataset("csv", data_files="my_data.csv")

# æ–¹å¼4: ä»å¤šä¸ªæ–‡ä»¶åŠ è½½
dataset = load_dataset(
    "json",
    data_files={
        "train": ["train1.json", "train2.json"],
        "test": "test.json"
    }
)

# æ–¹å¼5: æµå¼åŠ è½½ï¼ˆå¤§æ•°æ®é›†ï¼‰
dataset = load_dataset("c4", "en", split="train", streaming=True)
```

**æŸ¥çœ‹æ•°æ®é›†ç»“æ„**ï¼š

```python
dataset = load_dataset("imdb", split="train")

print("=== æ•°æ®é›†ä¿¡æ¯ ===")
print(dataset)
# è¾“å‡º:
# Dataset({
#     features: ['text', 'label'],
#     num_rows: 25000
# })

print("\n=== æ•°æ®é›†ç‰¹å¾ ===")
print(dataset.features)
# è¾“å‡º:
# {'text': Value(dtype='string', id=None),
#  'label': ClassLabel(names=['neg', 'pos'], id=None)}

print("\n=== ç¬¬ä¸€æ¡æ ·æœ¬ ===")
print(dataset[0])
# è¾“å‡º:
# {'text': 'Bromwell High is a cartoon comedy...', 'label': 1}

print("\n=== å‰3æ¡æ ·æœ¬ ===")
print(dataset[:3])
```

### 4.1.3 Hub æ•°æ®é›†æµè§ˆ

**åœ¨ Hub ä¸Šæœç´¢æ•°æ®é›†**ï¼š

```python
from datasets import list_datasets

# åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ•°æ®é›†ï¼ˆ10000+ ä¸ªï¼‰
all_datasets = list_datasets()
print(f"æ€»æ•°æ®é›†æ•°é‡: {len(all_datasets)}")

# æœç´¢ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†
sentiment_datasets = [d for d in all_datasets if 'sentiment' in d.lower()]
print(f"æƒ…æ„Ÿåˆ†ææ•°æ®é›†: {sentiment_datasets[:5]}")

# è¾“å‡ºç¤ºä¾‹:
# ['amazon_polarity', 'imdb', 'yelp_review_full', 'sst2', 'tweet_eval']
```

**æŸ¥çœ‹æ•°æ®é›†å…ƒæ•°æ®**ï¼š

```python
from datasets import load_dataset_builder

# è·å–æ•°æ®é›†ä¿¡æ¯ï¼ˆä¸ä¸‹è½½æ•°æ®ï¼‰
builder = load_dataset_builder("squad")

print(f"æè¿°: {builder.info.description[:100]}...")
print(f"å¼•ç”¨: {builder.info.citation[:100]}...")
print(f"ä¸»é¡µ: {builder.info.homepage}")
print(f"è®¸å¯è¯: {builder.info.license}")
print(f"æ•°æ®é›†å¤§å°: {builder.info.dataset_size / 1e6:.2f}MB")
print(f"ä¸‹è½½å¤§å°: {builder.info.download_size / 1e6:.2f}MB")
```

---

## 4.2 æ•°æ®é›†æ“ä½œ

<div data-component="DatasetPipeline"></div>

### 4.2.1 map()ï¼šæ‰¹é‡è½¬æ¢

`map()` æ˜¯ Datasets åº“æœ€å¼ºå¤§çš„æ–¹æ³•ï¼Œç”¨äºå¯¹æ•°æ®é›†çš„æ¯ä¸ªæ ·æœ¬åº”ç”¨å‡½æ•°ã€‚

**åŸºç¡€ç”¨æ³•**ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("imdb", split="train[:1000]")  # åªåŠ è½½1000æ¡

# å®šä¹‰è½¬æ¢å‡½æ•°
def add_length(example):
    """æ·»åŠ æ–‡æœ¬é•¿åº¦å­—æ®µ"""
    example["length"] = len(example["text"])
    return example

# åº”ç”¨è½¬æ¢
dataset = dataset.map(add_length)

print(dataset[0])
# è¾“å‡º: {'text': '...', 'label': 1, 'length': 1234}
```

**æ‰¹é‡å¤„ç†ï¼ˆæ¨èï¼‰**ï¼š

```python
# æ‰¹é‡å¤„ç†æ¯”é€æ¡å¤„ç†å¿« 10-100 å€
def add_length_batch(examples):
    """æ‰¹é‡å¤„ç†ç‰ˆæœ¬"""
    examples["length"] = [len(text) for text in examples["text"]]
    return examples

dataset = dataset.map(
    add_length_batch,
    batched=True,        # å¯ç”¨æ‰¹é‡å¤„ç†
    batch_size=1000      # æ¯æ‰¹ 1000 ä¸ªæ ·æœ¬
)

# æ€§èƒ½å¯¹æ¯”
import time

# é€æ¡å¤„ç†
start = time.time()
dataset.map(add_length, batched=False)
time_single = time.time() - start

# æ‰¹é‡å¤„ç†
start = time.time()
dataset.map(add_length_batch, batched=True, batch_size=1000)
time_batch = time.time() - start

print(f"é€æ¡å¤„ç†: {time_single:.2f}s")
print(f"æ‰¹é‡å¤„ç†: {time_batch:.2f}s")
print(f"åŠ é€Ÿæ¯”: {time_single / time_batch:.2f}x")

# å…¸å‹è¾“å‡º:
# é€æ¡å¤„ç†: 12.5s
# æ‰¹é‡å¤„ç†: 0.8s
# åŠ é€Ÿæ¯”: 15.6x
```

**å¤šè¿›ç¨‹åŠ é€Ÿ**ï¼š

```python
# ä½¿ç”¨å¤šæ ¸ CPU å¹¶è¡Œå¤„ç†
dataset = dataset.map(
    add_length_batch,
    batched=True,
    num_proc=4  # ä½¿ç”¨ 4 ä¸ªè¿›ç¨‹
)

# è‡ªåŠ¨æ£€æµ‹ CPU æ ¸å¿ƒæ•°
import os
num_cores = os.cpu_count()
dataset = dataset.map(
    add_length_batch,
    batched=True,
    num_proc=num_cores
)
```

**Tokenization é›†æˆ**ï¼š

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def tokenize_function(examples):
    """æ‰¹é‡ tokenization"""
    return tokenizer(
        examples["text"],
        padding=False,       # ä¸åœ¨è¿™é‡Œ paddingï¼ˆç•™ç»™ DataCollatorï¼‰
        truncation=True,
        max_length=512
    )

# åº”ç”¨ tokenization
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    num_proc=4,
    remove_columns=["text"]  # ç§»é™¤åŸå§‹æ–‡æœ¬ï¼ˆèŠ‚çœå†…å­˜ï¼‰
)

print(tokenized_dataset[0])
# è¾“å‡º: {'input_ids': [101, 2023, ...], 'attention_mask': [1, 1, ...], 'label': 1}
```

**è¿›åº¦æ¡ä¸ç¼“å­˜**ï¼š

```python
dataset = dataset.map(
    tokenize_function,
    batched=True,
    desc="Tokenizing",           # è¿›åº¦æ¡æè¿°
    load_from_cache_file=True,   # ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤ï¼‰
    cache_file_name="tokenized_cache.arrow"  # è‡ªå®šä¹‰ç¼“å­˜æ–‡ä»¶å
)

# ç¬¬äºŒæ¬¡è¿è¡Œä¼šç›´æ¥ä»ç¼“å­˜åŠ è½½ï¼Œé€Ÿåº¦æå¿«
```

### 4.2.2 filter()ï¼šæ¡ä»¶ç­›é€‰

**åŸºç¡€è¿‡æ»¤**ï¼š

```python
# åªä¿ç•™é•¿æ–‡æœ¬ï¼ˆ>100 å­—ç¬¦ï¼‰
filtered_dataset = dataset.filter(lambda x: len(x["text"]) > 100)

print(f"åŸå§‹å¤§å°: {len(dataset)}")
print(f"è¿‡æ»¤å: {len(filtered_dataset)}")

# è¾“å‡º:
# åŸå§‹å¤§å°: 1000
# è¿‡æ»¤å: 856
```

**æ‰¹é‡è¿‡æ»¤ï¼ˆæ›´å¿«ï¼‰**ï¼š

```python
def filter_long_texts(examples):
    """æ‰¹é‡è¿‡æ»¤"""
    return [len(text) > 100 for text in examples["text"]]

filtered_dataset = dataset.filter(
    filter_long_texts,
    batched=True,
    batch_size=1000
)
```

**å¤æ‚æ¡ä»¶**ï¼š

```python
# è¿‡æ»¤ï¼šæ­£é¢è¯„è®º ä¸” é•¿åº¦åœ¨ 100-500 ä¹‹é—´
def complex_filter(example):
    return (
        example["label"] == 1 and           # æ­£é¢è¯„è®º
        100 < len(example["text"]) < 500    # é•¿åº¦é™åˆ¶
    )

dataset = dataset.filter(complex_filter)
```

### 4.2.3 select()ã€shuffle()ã€train_test_split()

**select() - é€‰æ‹©ç‰¹å®šç´¢å¼•**ï¼š

```python
# é€‰æ‹©å‰100æ¡
subset = dataset.select(range(100))

# é€‰æ‹©ç‰¹å®šç´¢å¼•
indices = [0, 10, 20, 30, 40]
subset = dataset.select(indices)

# éšæœºé‡‡æ ·ï¼ˆä½¿ç”¨ shuffle + selectï¼‰
import random
indices = random.sample(range(len(dataset)), k=100)
subset = dataset.select(indices)
```

**shuffle() - éšæœºæ‰“ä¹±**ï¼š

```python
# å®Œå…¨æ‰“ä¹±
shuffled_dataset = dataset.shuffle(seed=42)

# éƒ¨åˆ†æ‰“ä¹±ï¼ˆåªæ‰“ä¹±å‰1000æ¡ï¼‰
shuffled_dataset = dataset.shuffle(seed=42).select(range(1000))
```

**train_test_split() - åˆ’åˆ†æ•°æ®é›†**ï¼š

```python
# æŒ‰æ¯”ä¾‹åˆ’åˆ†
split_dataset = dataset.train_test_split(test_size=0.2, seed=42)

print(split_dataset)
# è¾“å‡º:
# DatasetDict({
#     train: Dataset({features: [...], num_rows: 800})
#     test: Dataset({features: [...], num_rows: 200})
# })

train_dataset = split_dataset["train"]
test_dataset = split_dataset["test"]

# ä¸‰åˆ†æ³•ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•ï¼‰
train_test = dataset.train_test_split(test_size=0.3, seed=42)
test_valid = train_test["test"].train_test_split(test_size=0.5, seed=42)

final_dataset = {
    "train": train_test["train"],       # 70%
    "validation": test_valid["train"],  # 15%
    "test": test_valid["test"]          # 15%
}
```

### 4.2.4 æ•°æ®é›†æ‹¼æ¥ä¸äº¤ç»‡

**concatenate_datasets - å‚ç›´æ‹¼æ¥**ï¼š

```python
from datasets import concatenate_datasets

dataset1 = load_dataset("imdb", split="train[:1000]")
dataset2 = load_dataset("imdb", split="test[:1000]")

# æ‹¼æ¥
combined = concatenate_datasets([dataset1, dataset2])
print(f"åˆå¹¶åå¤§å°: {len(combined)}")  # 2000
```

**interleave_datasets - äº¤ç»‡æ··åˆ**ï¼š

```python
from datasets import interleave_datasets

# ä»å¤šä¸ªæ•°æ®é›†äº¤æ›¿é‡‡æ ·
dataset1 = load_dataset("imdb", split="train", streaming=True)
dataset2 = load_dataset("yelp_review_full", split="train", streaming=True)

# äº¤ç»‡ï¼ˆ1:1 æ¯”ä¾‹ï¼‰
interleaved = interleave_datasets([dataset1, dataset2])

# è‡ªå®šä¹‰é‡‡æ ·æ¦‚ç‡ï¼ˆ70% æ¥è‡ª dataset1ï¼Œ30% æ¥è‡ª dataset2ï¼‰
interleaved = interleave_datasets(
    [dataset1, dataset2],
    probabilities=[0.7, 0.3],
    seed=42
)
```

---

## 4.3 Tokenization é›†æˆ

### 4.3.1 ä½¿ç”¨ map() æ‰¹é‡ tokenize

**æ ‡å‡†æµç¨‹**ï¼š

```python
from datasets import load_dataset
from transformers import AutoTokenizer

# åŠ è½½æ•°æ®é›†å’Œ tokenizer
dataset = load_dataset("glue", "sst2", split="train")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# å®šä¹‰ tokenization å‡½æ•°
def tokenize_function(examples):
    return tokenizer(
        examples["sentence"],
        padding="max_length",  # å›ºå®šé•¿åº¦ padding
        truncation=True,
        max_length=128
    )

# æ‰¹é‡å¤„ç†
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    batch_size=1000,
    num_proc=4
)

print(tokenized_dataset.column_names)
# è¾“å‡º: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask']
```

### 4.3.2 remove_columns() æ¸…ç†åŸå§‹å­—æ®µ

**ç§»é™¤ä¸éœ€è¦çš„åˆ—**ï¼š

```python
# åªä¿ç•™æ¨¡å‹éœ€è¦çš„å­—æ®µ
tokenized_dataset = tokenized_dataset.remove_columns(["sentence", "idx"])

print(tokenized_dataset.column_names)
# è¾“å‡º: ['label', 'input_ids', 'token_type_ids', 'attention_mask']

# æˆ–è€…åœ¨ map() æ—¶ç›´æ¥ç§»é™¤
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=dataset.column_names  # ç§»é™¤æ‰€æœ‰åŸå§‹åˆ—
)
```

**é‡å‘½ååˆ—**ï¼š

```python
# å°† 'label' é‡å‘½åä¸º 'labels'ï¼ˆTrainer æœŸæœ›çš„åç§°ï¼‰
tokenized_dataset = tokenized_dataset.rename_column("label", "labels")
```

### 4.3.3 set_format()ï¼šPyTorch/TensorFlow æ ¼å¼

**è½¬æ¢ä¸ºå¼ é‡æ ¼å¼**ï¼š

```python
# æ–¹å¼1: set_format() - ä¸´æ—¶è½¬æ¢
tokenized_dataset.set_format(
    type="torch",  # 'torch', 'tensorflow', 'numpy', 'pandas'
    columns=["input_ids", "attention_mask", "labels"]
)

# ç°åœ¨è®¿é—®æ•°æ®æ—¶è‡ªåŠ¨è¿”å› torch.Tensor
print(type(tokenized_dataset[0]["input_ids"]))
# è¾“å‡º: <class 'torch.Tensor'>

# æ–¹å¼2: with_format() - è¿”å›æ–°å¯¹è±¡
torch_dataset = tokenized_dataset.with_format("torch")

# é‡ç½®æ ¼å¼
tokenized_dataset.reset_format()
```

**å®æˆ˜ç¤ºä¾‹ï¼šå‡†å¤‡ DataLoader**ï¼š

```python
from torch.utils.data import DataLoader

# è®¾ç½®æ ¼å¼
tokenized_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

# åˆ›å»º DataLoader
dataloader = DataLoader(tokenized_dataset, batch_size=16, shuffle=True)

# ä½¿ç”¨
for batch in dataloader:
    print(batch.keys())  # dict_keys(['input_ids', 'attention_mask', 'labels'])
    print(batch["input_ids"].shape)  # torch.Size([16, 128])
    break
```

---

## 4.4 DataCollator å®¶æ—

DataCollator æ˜¯æ•°æ®æ‰¹å¤„ç†çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£å°†å¤šä¸ªæ ·æœ¬æ•´ç†æˆæ¨¡å‹å¯æ¥å—çš„æ‰¹æ¬¡æ ¼å¼ã€‚

<div data-component="DataCollatorDemo"></div>

### 4.4.1 DataCollatorWithPaddingï¼šåŠ¨æ€ padding

**ä¸ºä»€ä¹ˆéœ€è¦åŠ¨æ€ paddingï¼Ÿ**

```python
# é—®é¢˜ï¼šåºåˆ—é•¿åº¦ä¸ä¸€è‡´
samples = [
    {"input_ids": [101, 2023, 2003, 102]},           # é•¿åº¦ 4
    {"input_ids": [101, 7592, 2088, 999, 102]},      # é•¿åº¦ 5
    {"input_ids": [101, 1045, 2293, 2023, 3185, 102]} # é•¿åº¦ 6
]

# âŒ æ— æ³•ç›´æ¥å †å æˆå¼ é‡
import torch
try:
    torch.tensor([s["input_ids"] for s in samples])
except ValueError as e:
    print(f"é”™è¯¯: {e}")
# è¾“å‡º: é”™è¯¯: expected sequence of equal length tensors

# âœ… ä½¿ç”¨ DataCollator è‡ªåŠ¨ padding
from transformers import DataCollatorWithPadding

collator = DataCollatorWithPadding(tokenizer=tokenizer)
batch = collator(samples)

print(batch["input_ids"])
# è¾“å‡º: tensor([
#     [101, 2023, 2003,  102,    0,    0],  # padding åˆ°æœ€é•¿
#     [101, 7592, 2088,  999,  102,    0],
#     [101, 1045, 2293, 2023, 3185,  102]
# ])

print(batch["attention_mask"])
# è¾“å‡º: tensor([
#     [1, 1, 1, 1, 0, 0],  # 0 è¡¨ç¤º padding ä½ç½®
#     [1, 1, 1, 1, 1, 0],
#     [1, 1, 1, 1, 1, 1]
# ])
```

**å®Œæ•´è®­ç»ƒæµç¨‹**ï¼š

```python
from transformers import DataCollatorWithPadding, Trainer, TrainingArguments
from torch.utils.data import DataLoader

# 1. Tokenizationï¼ˆä¸åš paddingï¼‰
def tokenize_function(examples):
    return tokenizer(
        examples["sentence"],
        truncation=True,
        padding=False  # é‡è¦ï¼šä¸åœ¨è¿™é‡Œ padding
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# 2. åˆ›å»º DataCollator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# 3. æ‰‹åŠ¨ä½¿ç”¨ï¼ˆPyTorch DataLoaderï¼‰
dataloader = DataLoader(
    tokenized_dataset,
    batch_size=8,
    collate_fn=data_collator  # å…³é”®å‚æ•°
)

for batch in dataloader:
    print(f"Batch shape: {batch['input_ids'].shape}")
    # æ¯ä¸ªæ‰¹æ¬¡çš„é•¿åº¦ä¸åŒï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰
    break

# 4. æˆ–ç›´æ¥ä¼ ç»™ Trainerï¼ˆæ¨èï¼‰
training_args = TrainingArguments(output_dir="./results")
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator  # Trainer è‡ªåŠ¨ä½¿ç”¨
)
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

```python
# å›ºå®š padding åˆ° 512ï¼ˆæµªè´¹è®¡ç®—ï¼‰
def tokenize_fixed(examples):
    return tokenizer(examples["sentence"], padding="max_length", max_length=512, truncation=True)

# åŠ¨æ€ paddingï¼ˆä»… padding åˆ°æ‰¹æ¬¡å†…æœ€é•¿ï¼‰
def tokenize_dynamic(examples):
    return tokenizer(examples["sentence"], truncation=True, padding=False)

# å‡è®¾å¹³å‡é•¿åº¦ 50ï¼Œæ‰¹æ¬¡å¤§å° 32
# å›ºå®š padding: 32 * 512 = 16,384 tokens/batch
# åŠ¨æ€ padding: 32 * ~60 = ~1,920 tokens/batch
# è®¡ç®—èŠ‚çœ: ~88%
```

### 4.4.2 DataCollatorForLanguageModelingï¼šMLM æ©ç 

ç”¨äºè®­ç»ƒ BERT ç±»æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMasked Language Modelingï¼‰ã€‚

**å·¥ä½œåŸç†**ï¼š

```python
from transformers import DataCollatorForLanguageModeling

# åˆ›å»º MLM collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=True,           # å¯ç”¨ MLM
    mlm_probability=0.15  # æ©ç  15% çš„ token
)

# ç¤ºä¾‹æ•°æ®
texts = ["Hello world", "Machine learning is amazing"]
tokenized = tokenizer(texts, return_tensors="pt", padding=True)

# åº”ç”¨æ©ç 
batch = data_collator([
    {k: v[i] for k, v in tokenized.items()} for i in range(len(texts))
])

print("åŸå§‹ input_ids:")
print(tokenized["input_ids"])

print("\næ©ç å input_ids (éƒ¨åˆ†è¢«æ›¿æ¢ä¸º [MASK]):")
print(batch["input_ids"])

print("\nlabels (ç”¨äºè®¡ç®—æŸå¤±):")
print(batch["labels"])
# -100 è¡¨ç¤ºä¸è®¡ç®—æŸå¤±çš„ä½ç½®ï¼ˆæœªè¢«æ©ç çš„ tokenï¼‰
```

**æ©ç ç­–ç•¥è¯¦è§£**ï¼š

```python
# BERT çš„æ©ç ç­–ç•¥ï¼š
# é€‰ä¸­çš„ 15% token ä¸­ï¼š
#   - 80% æ›¿æ¢ä¸º [MASK]
#   - 10% æ›¿æ¢ä¸ºéšæœº token
#   - 10% ä¿æŒä¸å˜

# ç¤ºä¾‹
original_text = "The quick brown fox jumps"
# å‡è®¾ 'quick' è¢«é€‰ä¸­æ©ç 
# å¯èƒ½ç»“æœï¼š
# - "The [MASK] brown fox jumps"  (80% æ¦‚ç‡)
# - "The dog brown fox jumps"     (10% æ¦‚ç‡ï¼Œéšæœºè¯)
# - "The quick brown fox jumps"   (10% æ¦‚ç‡ï¼Œä¿æŒä¸å˜)
```

**ä»å¤´è®­ç»ƒ BERT ç¤ºä¾‹**ï¼š

```python
from transformers import BertForMaskedLM, Trainer, TrainingArguments

# åŠ è½½æœªè®­ç»ƒçš„æ¨¡å‹
model = BertForMaskedLM(config=config)

# å‡†å¤‡æ•°æ®
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")

def tokenize_function(examples):
    return tokenizer(examples["text"], return_special_tokens_mask=True)

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# MLM DataCollator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm_probability=0.15
)

# è®­ç»ƒ
training_args = TrainingArguments(
    output_dir="./bert-mlm",
    per_device_train_batch_size=8,
    num_train_epochs=3
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator
)

trainer.train()
```

### 4.4.3 DataCollatorForSeq2Seqï¼šEncoder-Decoder ä¸“ç”¨

ç”¨äºåºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼ˆå¦‚ç¿»è¯‘ã€æ‘˜è¦ï¼‰ã€‚

**æ ¸å¿ƒåŠŸèƒ½**ï¼š

1. **Decoder input è‡ªåŠ¨æ„å»º**ï¼šå°† labels å³ç§»ä¸€ä½ä½œä¸º decoder_input_ids
2. **Label padding å¤„ç†**ï¼šç”¨ -100 å¡«å……ï¼ˆCrossEntropyLoss ä¼šå¿½ç•¥ï¼‰
3. **åŒæ—¶å¤„ç† encoder å’Œ decoder åºåˆ—**

```python
from transformers import DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")
tokenizer = AutoTokenizer.from_pretrained("t5-small")

# åˆ›å»º Seq2Seq collator
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,  # ç”¨äºè·å– pad_token_id
    label_pad_token_id=-100  # labels çš„ padding å€¼
)

# ç¤ºä¾‹æ•°æ®ï¼ˆç¿»è¯‘ä»»åŠ¡ï¼‰
samples = [
    {
        "input_ids": [1, 2, 3, 4, 5],      # æºè¯­è¨€
        "labels": [10, 11, 12]              # ç›®æ ‡è¯­è¨€
    },
    {
        "input_ids": [1, 2, 3],
        "labels": [10, 11, 12, 13, 14]
    }
]

batch = data_collator(samples)

print("Input IDs (encoder):")
print(batch["input_ids"])
# tensor([[1, 2, 3, 4, 5],
#         [1, 2, 3, 0, 0]])  # padding åˆ°ç›¸åŒé•¿åº¦

print("\nLabels (decoder output):")
print(batch["labels"])
# tensor([[ 10,  11,  12, -100, -100],
#         [ 10,  11,  12,   13,   14]])  # -100 è¡¨ç¤º padding
```

**å®Œæ•´ç¿»è¯‘å¾®è°ƒç¤ºä¾‹**ï¼š

```python
from datasets import load_dataset

# åŠ è½½ç¿»è¯‘æ•°æ®é›†
dataset = load_dataset("wmt16", "de-en", split="train[:1000]")

def preprocess_function(examples):
    inputs = [f"translate German to English: {ex}" for ex in examples["de"]]
    targets = examples["en"]
    
    model_inputs = tokenizer(inputs, max_length=128, truncation=True)
    
    # Tokenize targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=128, truncation=True)
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(preprocess_function, batched=True)

# Seq2Seq DataCollator
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

# è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator
)
```

### 4.4.4 è‡ªå®šä¹‰ DataCollator

**åœºæ™¯ï¼šæ·»åŠ è‡ªå®šä¹‰æ•°æ®å¢å¼º**

```python
from dataclasses import dataclass
from transformers.data.data_collator import DataCollatorMixin
import torch

@dataclass
class CustomDataCollator(DataCollatorMixin):
    tokenizer: AutoTokenizer
    
    def __call__(self, features):
        # 1. æ ‡å‡† padding
        batch = self.tokenizer.pad(
            features,
            padding=True,
            return_tensors="pt"
        )
        
        # 2. è‡ªå®šä¹‰å¢å¼ºï¼šéšæœºæ©ç  10% çš„ tokenï¼ˆæ•°æ®å¢å¼ºï¼‰
        if self.training:
            mask_prob = 0.1
            input_ids = batch["input_ids"]
            probability_matrix = torch.full(input_ids.shape, mask_prob)
            
            # ä¸æ©ç ç‰¹æ®Š token
            special_tokens_mask = [
                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)
                for val in input_ids.tolist()
            ]
            probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)
            
            masked_indices = torch.bernoulli(probability_matrix).bool()
            batch["input_ids"][masked_indices] = self.tokenizer.mask_token_id
        
        # 3. æ·»åŠ è‡ªå®šä¹‰å­—æ®µ
        batch["custom_weight"] = torch.tensor([len(f["input_ids"]) for f in features])
        
        return batch

# ä½¿ç”¨
collator = CustomDataCollator(tokenizer=tokenizer)
```

---

## 4.5 æµå¼æ•°æ®é›†ï¼ˆStreamingï¼‰

### 4.5.1 ä½•æ—¶ä½¿ç”¨æµå¼æ¨¡å¼

**é€‚ç”¨åœºæ™¯**ï¼š

1. **è¶…å¤§æ•°æ®é›†**ï¼šæ•°ç™¾ GB æˆ– TB çº§åˆ«
2. **å¿«é€Ÿå®éªŒ**ï¼šæ— éœ€ä¸‹è½½å®Œæ•´æ•°æ®é›†å³å¯å¼€å§‹è®­ç»ƒ
3. **åŠ¨æ€æ•°æ®**ï¼šå®æ—¶æ›´æ–°çš„æ•°æ®æµ

**ä¼ ç»Ÿ vs æµå¼å¯¹æ¯”**ï¼š

```python
# âŒ ä¼ ç»Ÿæ¨¡å¼ï¼šéœ€è¦ä¸‹è½½æ•´ä¸ªæ•°æ®é›†ï¼ˆ~800GBï¼‰
dataset = load_dataset("c4", "en", split="train")  # ç­‰å¾…æ•°å°æ—¶ä¸‹è½½

# âœ… æµå¼æ¨¡å¼ï¼šç«‹å³å¼€å§‹ï¼ŒæŒ‰éœ€åŠ è½½
dataset = load_dataset("c4", "en", split="train", streaming=True)  # ç§’çº§å¯åŠ¨
```

### 4.5.2 IterableDataset vs Dataset

**æ ¸å¿ƒå·®å¼‚**ï¼š

| ç‰¹æ€§ | Datasetï¼ˆæ ‡å‡†ï¼‰ | IterableDatasetï¼ˆæµå¼ï¼‰ |
|------|----------------|------------------------|
| æ•°æ®å­˜å‚¨ | å®Œæ•´ä¸‹è½½åˆ°ç£ç›˜ | æŒ‰éœ€ä¸‹è½½ |
| éšæœºè®¿é—® | æ”¯æŒ `dataset[i]` | ä¸æ”¯æŒ |
| é•¿åº¦æŸ¥è¯¢ | `len(dataset)` | ä¸æ”¯æŒï¼ˆæœªçŸ¥é•¿åº¦ï¼‰ |
| Shuffle | æ”¯æŒå…¨å±€ shuffle | ä»…æ”¯æŒç¼“å†²åŒº shuffle |
| å†…å­˜å ç”¨ | å›ºå®šï¼ˆç´¢å¼•ï¼‰ | æå° |

**ä½¿ç”¨æ–¹å¼**ï¼š

```python
from datasets import load_dataset

# åŠ è½½æµå¼æ•°æ®é›†
dataset = load_dataset("oscar", "unshuffled_deduplicated_en", split="train", streaming=True)

# è¿­ä»£è®¿é—®ï¼ˆç±»ä¼¼ç”Ÿæˆå™¨ï¼‰
for i, example in enumerate(dataset):
    print(example)
    if i >= 5:  # åªæŸ¥çœ‹å‰5æ¡
        break

# âŒ ä¸æ”¯æŒçš„æ“ä½œ
# print(len(dataset))  # TypeError
# print(dataset[0])    # TypeError
```

**ä¸ Trainer é›†æˆ**ï¼š

```python
from transformers import Trainer

# æµå¼æ•°æ®é›†å¯ä»¥ç›´æ¥ä¼ ç»™ Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=streaming_dataset,  # IterableDataset
    data_collator=data_collator
)

# Trainer ä¼šè‡ªåŠ¨å¤„ç†æµå¼è¿­ä»£
trainer.train()
```

### 4.5.3 æµå¼æ•°æ®çš„ shuffle ä¸ç¼“å†²

**ç¼“å†²åŒº shuffle**ï¼š

```python
# æµå¼æ•°æ®æ— æ³•å…¨å±€ shuffleï¼Œåªèƒ½åœ¨ç¼“å†²åŒºå†… shuffle
shuffled_dataset = dataset.shuffle(
    seed=42,
    buffer_size=10000  # ç¼“å­˜ 10000 ä¸ªæ ·æœ¬è¿›è¡Œ shuffle
)

# å·¥ä½œåŸç†ï¼š
# 1. ä»æ•°æ®æµä¸­è¯»å– 10000 ä¸ªæ ·æœ¬åˆ°ç¼“å†²åŒº
# 2. åœ¨ç¼“å†²åŒºå†…éšæœº shuffle
# 3. é€ä¸ªè¿”å›æ ·æœ¬
# 4. æ¯è¿”å›ä¸€ä¸ªï¼Œä»æ•°æ®æµä¸­è¡¥å……ä¸€ä¸ªåˆ°ç¼“å†²åŒº
# 5. é‡å¤æ­¥éª¤ 2-4
```

**take() å’Œ skip()**ï¼š

```python
# åªå–å‰ 1000 æ¡
subset = dataset.take(1000)

# è·³è¿‡å‰ 5000 æ¡
dataset_after_5k = dataset.skip(5000)

# ç»„åˆï¼šè·³è¿‡å‰ 5000ï¼Œå–æ¥ä¸‹æ¥çš„ 1000
subset = dataset.skip(5000).take(1000)
```

**map() åœ¨æµå¼æ•°æ®ä¸Šçš„åº”ç”¨**ï¼š

```python
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

# æµå¼ mapï¼ˆæƒ°æ€§æ‰§è¡Œï¼‰
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# è¿­ä»£æ—¶æ‰å®é™…æ‰§è¡Œ tokenization
for example in tokenized_dataset:
    print(example.keys())
    break
```

---

## 4.6 è‡ªå®šä¹‰æ•°æ®é›†

### 4.6.1 ä» CSV/JSON åŠ è½½

**CSV æ–‡ä»¶**ï¼š

```python
# å•ä¸ªæ–‡ä»¶
dataset = load_dataset("csv", data_files="my_data.csv")

# å¤šä¸ªæ–‡ä»¶
dataset = load_dataset("csv", data_files=["file1.csv", "file2.csv"])

# æŒ‡å®šåˆ†å‰²
dataset = load_dataset(
    "csv",
    data_files={
        "train": "train.csv",
        "test": "test.csv"
    }
)

# è‡ªå®šä¹‰å‚æ•°ï¼ˆä¼ é€’ç»™ pandas.read_csvï¼‰
dataset = load_dataset(
    "csv",
    data_files="data.csv",
    delimiter=";",           # åˆ†éš”ç¬¦
    quotechar='"',
    column_names=["text", "label"]  # è‡ªå®šä¹‰åˆ—å
)
```

**JSON æ–‡ä»¶**ï¼š

```python
# JSON Lines æ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰
dataset = load_dataset("json", data_files="data.jsonl")

# æ ‡å‡† JSON æ•°ç»„
dataset = load_dataset("json", data_files="data.json", field="data")

# ç¤ºä¾‹ data.jsonl å†…å®¹ï¼š
# {"text": "Example 1", "label": 0}
# {"text": "Example 2", "label": 1}
```

### 4.6.2 ä» Python å­—å…¸åˆ›å»º

**åŸºç¡€åˆ›å»º**ï¼š

```python
from datasets import Dataset

# æ–¹å¼1ï¼šä»å­—å…¸åˆ›å»º
data_dict = {
    "text": ["Hello", "World", "Test"],
    "label": [0, 1, 0]
}

dataset = Dataset.from_dict(data_dict)
print(dataset)
# Dataset({
#     features: ['text', 'label'],
#     num_rows: 3
# })

# æ–¹å¼2ï¼šä»åˆ—è¡¨åˆ›å»º
data_list = [
    {"text": "Hello", "label": 0},
    {"text": "World", "label": 1},
    {"text": "Test", "label": 0}
]

dataset = Dataset.from_list(data_list)

# æ–¹å¼3ï¼šä» Pandas DataFrame
import pandas as pd

df = pd.DataFrame({
    "text": ["Hello", "World", "Test"],
    "label": [0, 1, 0]
})

dataset = Dataset.from_pandas(df)
```

**æŒ‡å®šç‰¹å¾ç±»å‹**ï¼š

```python
from datasets import Dataset, Features, Value, ClassLabel

# å®šä¹‰ç‰¹å¾ schema
features = Features({
    "text": Value("string"),
    "label": ClassLabel(names=["negative", "positive"]),
    "score": Value("float32")
})

data = {
    "text": ["Good", "Bad"],
    "label": [1, 0],
    "score": [0.9, 0.1]
}

dataset = Dataset.from_dict(data, features=features)
print(dataset.features)
```

### 4.6.3 ä¸Šä¼ è‡ªå®šä¹‰æ•°æ®é›†åˆ° Hub

**å‡†å¤‡æ•°æ®é›†**ï¼š

```python
from datasets import Dataset, DatasetDict

# åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
train_data = {"text": [...], "label": [...]}
test_data = {"text": [...], "label": [...]}

train_dataset = Dataset.from_dict(train_data)
test_dataset = Dataset.from_dict(test_data)

# ç»„åˆæˆ DatasetDict
dataset_dict = DatasetDict({
    "train": train_dataset,
    "test": test_dataset
})
```

**ä¸Šä¼ åˆ° Hub**ï¼š

```python
# éœ€è¦å…ˆç™»å½•
from huggingface_hub import login
login()  # ä¼šæç¤ºè¾“å…¥ token

# ä¸Šä¼ æ•°æ®é›†
dataset_dict.push_to_hub("my_username/my_dataset_name")

# æ·»åŠ æ•°æ®é›†å¡ç‰‡ï¼ˆREADME.mdï¼‰
dataset_dict.push_to_hub(
    "my_username/my_dataset_name",
    config_name="default",
    private=False  # å…¬å¼€æ•°æ®é›†
)
```

**åŠ è½½è‡ªå·±çš„æ•°æ®é›†**ï¼š

```python
# å…¶ä»–äººå¯ä»¥è¿™æ ·åŠ è½½ä½ çš„æ•°æ®é›†
dataset = load_dataset("my_username/my_dataset_name")
```

---

## æœ¬ç« æ€»ç»“

**æ ¸å¿ƒè¦ç‚¹**ï¼š

1. âœ… **Datasets åº“ä¼˜åŠ¿**ï¼šå†…å­˜æ˜ å°„ã€é›¶æ‹·è´ã€æ™ºèƒ½ç¼“å­˜ï¼Œå¤„ç†å¤§æ•°æ®é›†é«˜æ•ˆ
2. âœ… **æ ¸å¿ƒæ“ä½œ**ï¼šmap()ã€filter()ã€select()ã€shuffle()ã€train_test_split()
3. âœ… **Tokenization é›†æˆ**ï¼šæ‰¹é‡å¤„ç†ã€å¤šè¿›ç¨‹åŠ é€Ÿã€ç§»é™¤åŸå§‹åˆ—
4. âœ… **DataCollator å®¶æ—**ï¼š
   - `DataCollatorWithPadding` - åŠ¨æ€ paddingï¼ˆæ¨èï¼‰
   - `DataCollatorForLanguageModeling` - MLM ä»»åŠ¡
   - `DataCollatorForSeq2Seq` - ç¿»è¯‘/æ‘˜è¦
5. âœ… **æµå¼æ•°æ®é›†**ï¼šé€‚åˆè¶…å¤§æ•°æ®é›†ï¼ŒIterableDataset æŒ‰éœ€åŠ è½½
6. âœ… **è‡ªå®šä¹‰æ•°æ®é›†**ï¼šCSV/JSON åŠ è½½ã€Python å­—å…¸åˆ›å»ºã€ä¸Šä¼ åˆ° Hub

**æœ€ä½³å®è·µ**ï¼š

- ä½¿ç”¨ `batched=True` + `num_proc` åŠ é€Ÿæ•°æ®å¤„ç†
- Tokenization æ—¶ä¸åš paddingï¼Œç•™ç»™ DataCollator
- ç§»é™¤ä¸éœ€è¦çš„åˆ—èŠ‚çœå†…å­˜
- å¤§æ•°æ®é›†ä½¿ç”¨æµå¼æ¨¡å¼
- å–„ç”¨ç¼“å­˜é¿å…é‡å¤è®¡ç®—

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š  
Chapter 5 å°†æ·±å…¥ **Trainer API**ï¼Œå­¦ä¹ å®Œæ•´çš„è®­ç»ƒæµç¨‹ã€TrainingArguments å‚æ•°è¯¦è§£ã€å›è°ƒå‡½æ•°ã€å¤š GPU è®­ç»ƒç­‰é«˜çº§ç‰¹æ€§ã€‚

---

## ç»ƒä¹ é¢˜

1. **åŸºç¡€é¢˜**ï¼šä½¿ç”¨ Datasets åº“åŠ è½½ GLUE çš„ MRPC ä»»åŠ¡ï¼Œtokenize åæŸ¥çœ‹ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ input_idsã€attention_mask å’Œ labelsã€‚

2. **è¿›é˜¶é¢˜**ï¼šå®ç°ä¸€ä¸ªè‡ªå®šä¹‰ DataCollatorï¼Œå¯¹æ–‡æœ¬è¿›è¡Œéšæœºå¤§å°å†™è½¬æ¢ï¼ˆæ•°æ®å¢å¼ºï¼‰ï¼Œå¹¶ä¸æ ‡å‡† DataCollator å¯¹æ¯”è®­ç»ƒæ•ˆæœã€‚

3. **æŒ‘æˆ˜é¢˜**ï¼šä» CSV æ–‡ä»¶åŠ è½½è‡ªå®šä¹‰æ•°æ®é›†ï¼Œåˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼ˆ70%/15%/15%ï¼‰ï¼Œä½¿ç”¨æµå¼æ¨¡å¼å¤„ç†ï¼Œè®¡ç®—æ¯ä¸ªåˆ†å‰²çš„å¹³å‡æ–‡æœ¬é•¿åº¦ï¼Œæœ€åä¸Šä¼ åˆ° Hugging Face Hubã€‚

4. **æ€è€ƒé¢˜**ï¼šä¸ºä»€ä¹ˆåŠ¨æ€ padding æ¯”å›ºå®š padding æ›´é«˜æ•ˆï¼Ÿåœ¨ä»€ä¹ˆæƒ…å†µä¸‹å›ºå®š padding å¯èƒ½æ›´åˆé€‚ï¼Ÿè®¡ç®—ä¸¤ç§æ–¹å¼åœ¨ IMDB æ•°æ®é›†ä¸Šçš„ç†è®ºè®¡ç®—é‡å·®å¼‚ï¼ˆå‡è®¾å¹³å‡é•¿åº¦ 200ï¼Œæœ€å¤§é•¿åº¦ 512ï¼‰ã€‚
