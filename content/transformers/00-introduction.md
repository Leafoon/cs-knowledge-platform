---
title: "Chapter 0. Transformers ç”Ÿæ€ç³»ç»Ÿæ¦‚è§ˆ"
description: "å…¨é¢äº†è§£ Hugging Face Transformers åº“çš„è®¾è®¡å“²å­¦ã€ç”Ÿæ€ç»„ä»¶ä¸ç¯å¢ƒå‡†å¤‡"
updated: "2026-01-22"
---

# Chapter 0. Transformers ç”Ÿæ€ç³»ç»Ÿæ¦‚è§ˆ

> **Learning Objectives**
> * ç†è§£ Hugging Face Transformers çš„è®¾è®¡å“²å­¦ä¸æ ¸å¿ƒä¼˜åŠ¿
> * æŒæ¡ç¯å¢ƒå®‰è£…ä¸ç‰ˆæœ¬å…¼å®¹æ€§ç®¡ç†
> * ç†Ÿæ‚‰ Hugging Face Hub çš„æ¨¡å‹ä»“åº“ç»“æ„ä¸ç¼“å­˜æœºåˆ¶
> * è¿è¡Œç¬¬ä¸€ä¸ª Pipeline ç¤ºä¾‹ï¼Œå»ºç«‹å…¨å±€è®¤çŸ¥

---

## 0.1 ä»€ä¹ˆæ˜¯ Hugging Face Transformersï¼Ÿ

### 0.1.1 è®¾è®¡å“²å­¦ï¼šç»Ÿä¸€çš„ API æ¥å£

Hugging Face Transformers æ˜¯ç›®å‰**æœ€æµè¡Œçš„é¢„è®­ç»ƒæ¨¡å‹åº“**ï¼Œæä¾›äº†ä¸€ä¸ªç»Ÿä¸€ã€ç®€æ´çš„æ¥å£æ¥è®¿é—®æ•°åƒä¸ªé¢„è®­ç»ƒæ¨¡å‹ã€‚

**æ ¸å¿ƒè®¾è®¡åŸåˆ™**ï¼š

1. **API ç»Ÿä¸€æ€§ (Unified API)**  
   æ— è®ºæ˜¯ BERTã€GPTã€T5 è¿˜æ˜¯ LLaMAï¼Œéƒ½ä½¿ç”¨ç›¸åŒçš„æ¥å£æ¨¡å¼ï¼š
   ```python
   from transformers import AutoTokenizer, AutoModel
   
   # æ‰€æœ‰æ¨¡å‹éƒ½éµå¾ªè¿™ä¸ªæ¨¡å¼
   tokenizer = AutoTokenizer.from_pretrained("model-name")
   model = AutoModel.from_pretrained("model-name")
   ```

2. **æ¡†æ¶æ— å…³æ€§ (Framework Agnostic)**  
   åŒæ—¶æ”¯æŒ PyTorchã€TensorFlowã€JAXï¼Œä»£ç å‡ ä¹é›¶ä¿®æ”¹ï¼š
   ```python
   # PyTorch
   from transformers import TFAutoModel  # TensorFlow
   from transformers import FlaxAutoModel  # JAX
   ```

3. **å¼€ç®±å³ç”¨ (Out-of-the-Box)**  
   ä¸€è¡Œä»£ç å³å¯å®Œæˆå¤æ‚ä»»åŠ¡ï¼š
   ```python
   from transformers import pipeline
   classifier = pipeline("sentiment-analysis")
   result = classifier("I love this library!")
   ```

4. **ç¤¾åŒºé©±åŠ¨ (Community-Driven)**  
   æ‹¥æœ‰è¶…è¿‡ **200,000+ æ¨¡å‹**ã€**30,000+ æ•°æ®é›†**ï¼ˆæˆªè‡³ 2026 å¹´ 1 æœˆï¼‰

### 0.1.2 ä¸å…¶ä»–æ¡†æ¶å¯¹æ¯”

<div data-component="TransformersEcosystemComparison"></div>

| ç‰¹æ€§ | Transformers | Fairseq | AllenNLP | PaddleNLP |
|------|-------------|---------|----------|-----------|
| **æ¨¡å‹æ•°é‡** | 200,000+ | ~100 | ~50 | 500+ |
| **æ”¯æŒæ¡†æ¶** | PyTorch/TF/JAX | PyTorch | PyTorch | PaddlePaddle |
| **æ˜“ç”¨æ€§** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| **æ–‡æ¡£è´¨é‡** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| **å·¥ä¸šåº”ç”¨** | å¹¿æ³› | å­¦æœ¯ä¸ºä¸» | ä¸­ç­‰ | ä¸­å›½å¸‚åœº |
| **æ›´æ–°é¢‘ç‡** | æ¯å‘¨ | æ¯æœˆ | ä¸å®šæœŸ | æ¯æœˆ |

**ä¸ºä»€ä¹ˆé€‰æ‹© Transformersï¼Ÿ**
- âœ… æœ€ä¸°å¯Œçš„æ¨¡å‹ä»“åº“ï¼ˆBERTã€GPT ç³»åˆ—ã€LLaMAã€Mistralã€Qwen ç­‰ï¼‰
- âœ… æ´»è·ƒçš„ç¤¾åŒºæ”¯æŒï¼ˆGitHub 120k+ starsï¼‰
- âœ… ä¸ç°ä»£è®­ç»ƒåº“æ— ç¼é›†æˆï¼ˆAccelerateã€PEFTã€DeepSpeedï¼‰
- âœ… ä¸€æµçš„æ–‡æ¡£ä¸æ•™ç¨‹
- âœ… å·¥ä¸šç•Œäº‹å®æ ‡å‡†

### 0.1.3 ç”Ÿæ€ç»„ä»¶å…¨æ™¯å›¾

Hugging Face ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ¨¡å‹åº“ï¼Œè€Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„ ML ç”Ÿæ€ç³»ç»Ÿï¼š

<div data-component="HuggingFaceEcosystemMap"></div>

**æ ¸å¿ƒåº“**ï¼š
1. **ğŸ¤— Transformers**ï¼šé¢„è®­ç»ƒæ¨¡å‹åº“ï¼ˆæœ¬è¯¾ç¨‹é‡ç‚¹ï¼‰
2. **ğŸ¤— Datasets**ï¼šæ•°æ®é›†åŠ è½½ä¸é¢„å¤„ç†
3. **ğŸ¤— Tokenizers**ï¼šæé€Ÿåˆ†è¯å™¨ï¼ˆRust å®ç°ï¼‰
4. **ğŸ¤— Accelerate**ï¼šåˆ†å¸ƒå¼è®­ç»ƒæŠ½è±¡å±‚
5. **ğŸ¤— PEFT**ï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆLoRAã€QLoRAï¼‰
6. **ğŸ¤— Optimum**ï¼šç¡¬ä»¶åŠ é€Ÿä¼˜åŒ–
7. **ğŸ¤— Diffusers**ï¼šæ‰©æ•£æ¨¡å‹ï¼ˆStable Diffusionï¼‰
8. **ğŸ¤— TRL**ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰

**å¹³å°æœåŠ¡**ï¼š
- **Hub**ï¼šæ¨¡å‹ä¸æ•°æ®é›†æ‰˜ç®¡å¹³å°
- **Spaces**ï¼šML åº”ç”¨æ‰˜ç®¡ï¼ˆGradio/Streamlitï¼‰
- **Inference API**ï¼šæ— æœåŠ¡å™¨æ¨ç†æœåŠ¡
- **AutoTrain**ï¼šæ— ä»£ç è®­ç»ƒå¹³å°

---

## 0.2 ç¯å¢ƒå‡†å¤‡ä¸å®‰è£…

### 0.2.1 å®‰è£…ç­–ç•¥

#### **æ–¹å¼ä¸€ï¼špip å®‰è£…ï¼ˆæ¨èï¼‰**

```bash
# åŸºç¡€å®‰è£…ï¼ˆä»… PyTorch åç«¯ï¼‰
pip install transformers

# å®Œæ•´å®‰è£…ï¼ˆåŒ…å«æ‰€æœ‰ä¾èµ–ï¼‰
pip install transformers[torch]

# å¼€å‘å®‰è£…ï¼ˆåŒ…å«æµ‹è¯•ã€è´¨é‡æ£€æŸ¥å·¥å…·ï¼‰
pip install transformers[dev]

# TensorFlow ç”¨æˆ·
pip install transformers[tf-cpu]  # CPU ç‰ˆæœ¬
pip install transformers[tf]       # GPU ç‰ˆæœ¬
```

#### **æ–¹å¼äºŒï¼šconda å®‰è£…**

```bash
conda install -c huggingface transformers
```

#### **æ–¹å¼ä¸‰ï¼šä»æºç å®‰è£…ï¼ˆè·å–æœ€æ–°ç‰¹æ€§ï¼‰**

```bash
git clone https://github.com/huggingface/transformers
cd transformers
pip install -e .
```

> [!TIP]
> **æ¨èå®‰è£…é¡ºåº**ï¼š
> 1. å…ˆå®‰è£… PyTorchï¼ˆä» pytorch.org è·å–é€‚é…æ‚¨ CUDA ç‰ˆæœ¬çš„å‘½ä»¤ï¼‰
> 2. å†å®‰è£… transformers
> 3. æŒ‰éœ€å®‰è£…å…¶ä»–åº“ï¼ˆdatasetsã€accelerateã€peftï¼‰

### 0.2.2 ç‰ˆæœ¬å…¼å®¹æ€§çŸ©é˜µ

<div data-component="VersionCompatibilityMatrix"></div>

| Transformers | PyTorch | Python | CUDA | é‡è¦ç‰¹æ€§ |
|--------------|---------|--------|------|---------|
| **v4.40+** (2026) | 2.0+ | 3.9+ | 11.8+ | Gemma 2, Qwen 2.5 æ”¯æŒ |
| **v4.35-4.39** | 2.0+ | 3.8+ | 11.8+ | Mixtral, Phi-3 |
| **v4.30-4.34** | 1.13+ | 3.8+ | 11.7+ | LLaMA 2, Mistral |
| **v4.25-4.29** | 1.11+ | 3.7+ | 11.6+ | BLOOM, OPT |
| **< v4.25** | 1.9+ | 3.7+ | 11.3+ | Legacy |

**æ£€æŸ¥ç‰ˆæœ¬**ï¼š
```python
import transformers
import torch

print(f"Transformers: {transformers.__version__}")
print(f"PyTorch: {torch.__version__}")
print(f"CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA Version: {torch.version.cuda}")
```

**é¢„æœŸè¾“å‡º**ï¼š
```
Transformers: 4.40.1
PyTorch: 2.2.0
CUDA Available: True
CUDA Version: 12.1
```

> [!CAUTION]
> **å¸¸è§é™·é˜±**ï¼š
> - CUDA ç‰ˆæœ¬ä¸ PyTorch ä¸åŒ¹é…ä¼šå¯¼è‡´ GPU ä¸å¯ç”¨
> - Python 3.7 å·²ä¸å†æ”¯æŒï¼ˆä½¿ç”¨ 3.9+ è·å¾—æœ€ä½³å…¼å®¹æ€§ï¼‰
> - M1/M2 Mac ç”¨æˆ·ä½¿ç”¨ `torch` è€Œé `torch-cpu`

### 0.2.3 éªŒè¯å®‰è£…ï¼šå¿«é€Ÿæµ‹è¯•è„šæœ¬

åˆ›å»ºæ–‡ä»¶ `test_installation.py`ï¼š

```python
#!/usr/bin/env python3
"""
å¿«é€ŸéªŒè¯ Transformers å®‰è£…æ˜¯å¦æ­£å¸¸
"""
import sys

def test_import():
    """æµ‹è¯•åŸºç¡€å¯¼å…¥"""
    try:
        import transformers
        print(f"âœ… Transformers {transformers.__version__} imported successfully")
        return True
    except ImportError as e:
        print(f"âŒ Failed to import transformers: {e}")
        return False

def test_pytorch():
    """æµ‹è¯• PyTorch åç«¯"""
    try:
        import torch
        print(f"âœ… PyTorch {torch.__version__} available")
        if torch.cuda.is_available():
            print(f"   ğŸ® CUDA {torch.version.cuda} detected ({torch.cuda.device_count()} GPU(s))")
        else:
            print(f"   ğŸ’» CPU-only mode")
        return True
    except ImportError:
        print(f"âŒ PyTorch not found")
        return False

def test_pipeline():
    """æµ‹è¯• Pipeline åŠŸèƒ½"""
    try:
        from transformers import pipeline
        classifier = pipeline(
            "sentiment-analysis",
            model="distilbert-base-uncased-finetuned-sst-2-english"
        )
        result = classifier("This is a test")[0]
        print(f"âœ… Pipeline test passed: {result['label']} ({result['score']:.2f})")
        return True
    except Exception as e:
        print(f"âŒ Pipeline test failed: {e}")
        return False

if __name__ == "__main__":
    print("ğŸ” Testing Hugging Face Transformers Installation\n")
    
    tests = [
        test_import(),
        test_pytorch(),
        test_pipeline()
    ]
    
    if all(tests):
        print("\nğŸ‰ All tests passed! Your installation is ready.")
        sys.exit(0)
    else:
        print("\nâš ï¸  Some tests failed. Please check the errors above.")
        sys.exit(1)
```

è¿è¡Œï¼š
```bash
python test_installation.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
ğŸ” Testing Hugging Face Transformers Installation

âœ… Transformers 4.40.1 imported successfully
âœ… PyTorch 2.2.0 available
   ğŸ® CUDA 12.1 detected (1 GPU(s))
âœ… Pipeline test passed: POSITIVE (0.99)

ğŸ‰ All tests passed! Your installation is ready.
```

---

## 0.3 Hugging Face Hub å…¥é—¨

### 0.3.1 æ¨¡å‹ä»“åº“ç»“æ„

æ¯ä¸ªæ¨¡å‹ä»“åº“éƒ½éµå¾ªæ ‡å‡†åŒ–ç»“æ„ï¼Œè¿™æ˜¯ç†è§£æ¨¡å‹åŠ è½½çš„å…³é”®ã€‚

<div data-component="ModelRepoStructureExplorer"></div>

**å…¸å‹ä»“åº“ç»“æ„**ï¼ˆä»¥ `bert-base-uncased` ä¸ºä¾‹ï¼‰ï¼š

```
bert-base-uncased/
â”œâ”€â”€ config.json              # æ¨¡å‹é…ç½®ï¼ˆæ¶æ„å‚æ•°ï¼‰
â”œâ”€â”€ pytorch_model.bin        # PyTorch æƒé‡ï¼ˆæ—§æ ¼å¼ï¼‰
â”œâ”€â”€ model.safetensors        # Safetensors æƒé‡ï¼ˆæ–°æ ¼å¼ï¼Œæ¨èï¼‰
â”œâ”€â”€ tokenizer_config.json    # Tokenizer é…ç½®
â”œâ”€â”€ vocab.txt                # è¯æ±‡è¡¨
â”œâ”€â”€ tokenizer.json           # Fast Tokenizer æ–‡ä»¶
â”œâ”€â”€ special_tokens_map.json  # ç‰¹æ®Š token æ˜ å°„
â”œâ”€â”€ README.md                # æ¨¡å‹å¡ç‰‡ï¼ˆModel Cardï¼‰
â””â”€â”€ .gitattributes           # Git LFS é…ç½®
```

**å¤§æ¨¡å‹åˆ†ç‰‡ç»“æ„**ï¼ˆä»¥ `meta-llama/Llama-2-7b-hf` ä¸ºä¾‹ï¼‰ï¼š

```
Llama-2-7b-hf/
â”œâ”€â”€ config.json
â”œâ”€â”€ generation_config.json         # ç”Ÿæˆå‚æ•°é…ç½®
â”œâ”€â”€ model-00001-of-00002.safetensors  # åˆ†ç‰‡æƒé‡ 1
â”œâ”€â”€ model-00002-of-00002.safetensors  # åˆ†ç‰‡æƒé‡ 2
â”œâ”€â”€ model.safetensors.index.json   # åˆ†ç‰‡ç´¢å¼•
â”œâ”€â”€ tokenizer.model                # SentencePiece æ¨¡å‹
â”œâ”€â”€ tokenizer_config.json
â””â”€â”€ README.md
```

**å…³é”®æ–‡ä»¶è¯´æ˜**ï¼š

1. **config.json** - æ¨¡å‹æ¶æ„é…ç½®
   ```json
   {
     "architectures": ["BertForMaskedLM"],
     "hidden_size": 768,
     "num_attention_heads": 12,
     "num_hidden_layers": 12,
     "vocab_size": 30522,
     ...
   }
   ```

2. **model.safetensors** - Safetensors æ ¼å¼æƒé‡
   - æ¯” `.bin` æ›´å®‰å…¨ï¼ˆé˜²æ­¢ä»»æ„ä»£ç æ‰§è¡Œï¼‰
   - åŠ è½½é€Ÿåº¦æ›´å¿«ï¼ˆé›¶æ‹·è´ï¼‰
   - è·¨æ¡†æ¶å…¼å®¹æ€§å¥½

3. **tokenizer.json** - Fast Tokenizer å®Œæ•´çŠ¶æ€
   - Rust å®ç°ï¼Œé€Ÿåº¦å¿« 10-100 å€
   - åŒ…å«è¯æ±‡è¡¨ã€åˆå¹¶è§„åˆ™ã€ç‰¹æ®Š token

### 0.3.2 è®¿é—®ä»¤ç‰Œï¼ˆAccess Tokenï¼‰ä¸ç§æœ‰æ¨¡å‹

æŸäº›æ¨¡å‹ï¼ˆå¦‚ LLaMA 2ã€Gemmaï¼‰éœ€è¦æ¥å—è®¸å¯åè®®å¹¶ä½¿ç”¨è®¿é—®ä»¤ç‰Œã€‚

**è·å– Access Token**ï¼š
1. è®¿é—® https://huggingface.co/settings/tokens
2. ç‚¹å‡» "New token" â†’ é€‰æ‹© "Read" æƒé™
3. å¤åˆ¶ç”Ÿæˆçš„ tokenï¼ˆæ ¼å¼ï¼š`hf_xxxxxxxxxxxx`ï¼‰

**ä½¿ç”¨æ–¹å¼**ï¼š

```python
from transformers import AutoTokenizer

# æ–¹å¼ä¸€ï¼šç›´æ¥ä¼ å…¥ token
tokenizer = AutoTokenizer.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    token="hf_xxxxxxxxxxxx"  # ä½ çš„ token
)

# æ–¹å¼äºŒï¼šä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰
import os
os.environ["HF_TOKEN"] = "hf_xxxxxxxxxxxx"
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# æ–¹å¼ä¸‰ï¼šCLI ç™»å½•ï¼ˆæ°¸ä¹…ï¼‰
# åœ¨ç»ˆç«¯è¿è¡Œï¼šhuggingface-cli login
```

> [!WARNING]
> **å®‰å…¨æç¤º**ï¼š
> - æ°¸è¿œä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç  token
> - ä¸è¦æäº¤åŒ…å« token çš„æ–‡ä»¶åˆ° Git
> - ä½¿ç”¨ `.env` æ–‡ä»¶ + `.gitignore` ç®¡ç†æ•æ„Ÿä¿¡æ¯

### 0.3.3 æœ¬åœ°ç¼“å­˜æœºåˆ¶

Transformers ä½¿ç”¨æ™ºèƒ½ç¼“å­˜é¿å…é‡å¤ä¸‹è½½ã€‚

**é»˜è®¤ç¼“å­˜ä½ç½®**ï¼š
- **Linux/Mac**: `~/.cache/huggingface/hub/`
- **Windows**: `C:\Users\<username>\.cache\huggingface\hub\`

**ç¼“å­˜ç»“æ„**ï¼š
```bash
~/.cache/huggingface/hub/
â”œâ”€â”€ models--bert-base-uncased/
â”‚   â”œâ”€â”€ blobs/                    # å®é™…æ–‡ä»¶å†…å®¹ï¼ˆé€šè¿‡å“ˆå¸Œå»é‡ï¼‰
â”‚   â”‚   â”œâ”€â”€ abc123def456...       # config.json
â”‚   â”‚   â””â”€â”€ 789xyz...             # pytorch_model.bin
â”‚   â”œâ”€â”€ refs/
â”‚   â”‚   â””â”€â”€ main                  # æŒ‡å‘æœ€æ–°æäº¤
â”‚   â””â”€â”€ snapshots/
â”‚       â””â”€â”€ commit_hash/          # ç¬¦å·é“¾æ¥åˆ° blobs/
â”‚           â”œâ”€â”€ config.json -> ../../blobs/abc123def456...
â”‚           â””â”€â”€ pytorch_model.bin -> ../../blobs/789xyz...
```

**ç¼“å­˜ç®¡ç†**ï¼š

```python
from transformers import AutoModel

# æŸ¥çœ‹ç¼“å­˜è·¯å¾„
import transformers
print(transformers.file_utils.default_cache_path)

# è‡ªå®šä¹‰ç¼“å­˜ç›®å½•
import os
os.environ["HF_HOME"] = "/custom/cache/path"

# å¼ºåˆ¶é‡æ–°ä¸‹è½½ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
model = AutoModel.from_pretrained(
    "bert-base-uncased",
    force_download=True
)

# ä»…ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
model = AutoModel.from_pretrained(
    "bert-base-uncased",
    local_files_only=True
)
```

**æ¸…ç†ç¼“å­˜**ï¼š

```bash
# æŸ¥çœ‹ç¼“å­˜å ç”¨
huggingface-cli scan-cache

# äº¤äº’å¼åˆ é™¤ä¸ç”¨çš„æ¨¡å‹
huggingface-cli delete-cache

# æ‰‹åŠ¨åˆ é™¤ï¼ˆè°¨æ…ï¼ï¼‰
rm -rf ~/.cache/huggingface/hub/models--bert-base-uncased
```

<div data-component="CacheManagementVisualizer"></div>

---

## 0.4 ç¬¬ä¸€ä¸ªç¤ºä¾‹ï¼šæƒ…æ„Ÿåˆ†æ Pipeline

### 0.4.1 é›¶ä»£ç ä½“éªŒï¼špipeline() ä¸€è¡Œè°ƒç”¨

```python
from transformers import pipeline

# åˆ›å»ºæƒ…æ„Ÿåˆ†æ Pipelineï¼ˆè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼‰
classifier = pipeline("sentiment-analysis")

# å•æ¡æ–‡æœ¬
result = classifier("I love using Transformers library!")
print(result)

# æ‰¹é‡å¤„ç†
texts = [
    "This is amazing!",
    "I'm feeling frustrated.",
    "The weather is okay."
]
results = classifier(texts)
for text, result in zip(texts, results):
    print(f"{text:30} â†’ {result['label']:8} ({result['score']:.3f})")
```

**è¾“å‡º**ï¼š
```
[{'label': 'POSITIVE', 'score': 0.9998}]

This is amazing!               â†’ POSITIVE (0.999)
I'm feeling frustrated.        â†’ NEGATIVE (0.998)
The weather is okay.           â†’ POSITIVE (0.731)
```

**å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ**

<div data-component="PipelineInternalFlow"></div>

Pipeline åœ¨å¹•åè‡ªåŠ¨å®Œæˆäº† **3 ä¸ªæ ¸å¿ƒæ­¥éª¤**ï¼š

1. **Tokenizationï¼ˆåˆ†è¯ï¼‰**ï¼š
   ```python
   "I love Transformers" â†’ [101, 1045, 2293, 19081, 102]
   ```

2. **Model Inferenceï¼ˆæ¨¡å‹æ¨ç†ï¼‰**ï¼š
   ```python
   [101, 1045, ...] â†’ logits: [-4.23, 4.56]  # [negative, positive]
   ```

3. **Post-processingï¼ˆåå¤„ç†ï¼‰**ï¼š
   ```python
   logits â†’ softmax â†’ {"POSITIVE": 0.9998, "NEGATIVE": 0.0002}
   ```

### 0.4.2 è¾“å‡ºè§£æ

Pipeline è¿”å›çš„å­—å…¸åŒ…å«ï¼š

```python
{
    'label': 'POSITIVE',    # é¢„æµ‹ç±»åˆ«
    'score': 0.9998         # ç½®ä¿¡åº¦ï¼ˆæ¦‚ç‡ï¼‰
}
```

**è·å–åŸå§‹ logits**ï¼ˆéœ€è¦æ‰‹åŠ¨è°ƒç”¨æ¨¡å‹ï¼‰ï¼š

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# ç¼–ç 
inputs = tokenizer("I love this!", return_tensors="pt")
print(f"Input IDs: {inputs['input_ids']}")

# æ¨ç†
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    probabilities = torch.softmax(logits, dim=-1)

print(f"Logits: {logits[0].tolist()}")
print(f"Probabilities: {probabilities[0].tolist()}")
print(f"Prediction: {model.config.id2label[logits.argmax().item()]}")
```

**è¾“å‡º**ï¼š
```
Input IDs: tensor([[  101,  1045,  2293,  2023,   999,   102]])
Logits: [-4.2341, 4.5623]
Probabilities: [0.0002, 0.9998]
Prediction: POSITIVE
```

### 0.4.3 æ”¯æŒçš„ä»»åŠ¡ç±»å‹å…¨åˆ—è¡¨

<div data-component="TaskTypeGallery"></div>

Transformers æ”¯æŒ **30+ ç§ä»»åŠ¡**ï¼Œåˆ†ä¸ºä»¥ä¸‹ç±»åˆ«ï¼š

**è‡ªç„¶è¯­è¨€å¤„ç† (NLP)**ï¼š
- `text-classification` / `sentiment-analysis`
- `token-classification` / `ner`ï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰
- `question-answering`
- `fill-mask`ï¼ˆå®Œå½¢å¡«ç©ºï¼‰
- `summarization`
- `translation`
- `text-generation`
- `text2text-generation`
- `zero-shot-classification`
- `conversational`ï¼ˆå¯¹è¯ï¼‰

**è®¡ç®—æœºè§†è§‰ (CV)**ï¼š
- `image-classification`
- `object-detection`
- `image-segmentation`
- `depth-estimation`
- `zero-shot-image-classification`

**éŸ³é¢‘ (Audio)**ï¼š
- `automatic-speech-recognition`
- `audio-classification`
- `text-to-speech`

**å¤šæ¨¡æ€ (Multimodal)**ï¼š
- `visual-question-answering`
- `document-question-answering`
- `image-to-text`ï¼ˆå›¾åƒæè¿°ï¼‰

**æŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡**ï¼š
```python
from transformers.pipelines import SUPPORTED_TASKS
print(list(SUPPORTED_TASKS.keys()))
```

---

## 0.5 æ€»ç»“ä¸ä¸‹ä¸€æ­¥

### çŸ¥è¯†å›é¡¾

âœ… **æŒæ¡äº†**ï¼š
- Transformers çš„è®¾è®¡å“²å­¦ä¸ç”Ÿæ€ç³»ç»Ÿ
- ç¯å¢ƒå®‰è£…ä¸ç‰ˆæœ¬å…¼å®¹æ€§
- Hub æ¨¡å‹ä»“åº“ç»“æ„ä¸ç¼“å­˜æœºåˆ¶
- Pipeline å¿«é€Ÿä¸Šæ‰‹

ğŸ¯ **å…³é”®è¦ç‚¹**ï¼š
1. Transformers = ç»Ÿä¸€ API + ä¸°å¯Œæ¨¡å‹ + æ´»è·ƒç¤¾åŒº
2. ä¼˜å…ˆä½¿ç”¨ Safetensors æ ¼å¼
3. ç†è§£æœ¬åœ°ç¼“å­˜å¯èŠ‚çœå¸¦å®½ä¸æ—¶é—´
4. Pipeline æ˜¯å¿«é€ŸåŸå‹çš„æœ€ä½³é€‰æ‹©

### ç»ƒä¹ é¢˜

1. **ç¯å¢ƒæ£€æŸ¥**ï¼šè¿è¡Œ `test_installation.py`ï¼Œæˆªå›¾ä¿å­˜è¾“å‡º
2. **ç¼“å­˜æ¢ç´¢**ï¼šä½¿ç”¨ `huggingface-cli scan-cache` æŸ¥çœ‹æœ¬åœ°ç¼“å­˜å ç”¨
3. **Pipeline å®éªŒ**ï¼šå°è¯•è‡³å°‘ 3 ç§ä¸åŒä»»åŠ¡çš„ Pipelineï¼ˆå¦‚ NERã€æ‘˜è¦ã€é—®ç­”ï¼‰
4. **æ¨¡å‹å¡ç‰‡é˜…è¯»**ï¼šè®¿é—® https://huggingface.co/bert-base-uncasedï¼Œé˜…è¯» Model Card

### æ€è€ƒé¢˜

â“ **ä¸ºä»€ä¹ˆ Safetensors æ¯” pickle (.bin) æ›´å®‰å…¨ï¼Ÿ**  
ğŸ’¡ æç¤ºï¼šè€ƒè™‘ Python pickle çš„ååºåˆ—åŒ–æœºåˆ¶

â“ **å¦‚æœæœ¬åœ°ç¼“å­˜è¢«åˆ é™¤ï¼Œ`from_pretrained()` ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ**  
ğŸ’¡ æç¤ºï¼šè§‚å¯Ÿç½‘ç»œæµé‡

â“ **Pipeline çš„æ€§èƒ½ç“¶é¢ˆåœ¨å“ªé‡Œï¼Ÿä½•æ—¶åº”è¯¥é¿å…ä½¿ç”¨ï¼Ÿ**  
ğŸ’¡ æç¤ºï¼šè€ƒè™‘æ‰¹å¤„ç†ã€åŠ¨æ€ paddingã€æ¨¡å‹é‡å¤åŠ è½½

### æ‰©å±•é˜…è¯»

ğŸ“– **å®˜æ–¹æ–‡æ¡£**ï¼š
- [Transformers å¿«é€Ÿä¸Šæ‰‹](https://huggingface.co/docs/transformers/quicktour)
- [Pipeline å®Œæ•´æŒ‡å—](https://huggingface.co/docs/transformers/main_classes/pipelines)
- [æ¨¡å‹ä»“åº“æ–‡æ¡£](https://huggingface.co/docs/hub/models)

ğŸ“„ **é‡è¦è®ºæ–‡**ï¼š
- Attention Is All You Need (Vaswani et al., 2017)
- BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)

---

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼šChapter 1 å°†æ·±å…¥ Pipeline å†…éƒ¨æœºåˆ¶ï¼Œå­¦ä¹ å¦‚ä½•æ§åˆ¶æ¯ä¸ªå¤„ç†é˜¶æ®µï¼Œç†è§£ Tokenizerã€Modelã€Post-processing çš„ç»†èŠ‚ã€‚

