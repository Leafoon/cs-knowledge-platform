---
title: "Chapter 15. FSDP ï¼ˆFully Sharded Data Parallelï¼‰"
description: "æ·±å…¥ç†è§£ FSDP åˆ†ç‰‡æœºåˆ¶ã€é›¶å†—ä½™ä¼˜åŒ–å™¨ã€ä¸ DeepSpeed å¯¹æ¯”"
updated: "2026-01-22"
---

> **å®˜æ–¹æ–‡æ¡£**: https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html  
> **Hugging Face é›†æˆ**: https://huggingface.co/docs/transformers/main_classes/trainer#fsdp  
> **PyTorch ç‰ˆæœ¬**: PyTorch 2.0+ (FSDP ç¨³å®šç‰ˆæœ¬)

## 15.1 FSDP åŸç†æ·±åº¦è§£æ

### 15.1.1 ZeRO ä¼˜åŒ–å™¨çš„ä¸‰ä¸ªé˜¶æ®µ

FSDP (Fully Sharded Data Parallel) åŸºäºå¾®è½¯ DeepSpeed æå‡ºçš„ **ZeROï¼ˆZero Redundancy Optimizerï¼‰** ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡**åˆ†ç‰‡**æ¨¡å‹çŠ¶æ€æ¥èŠ‚çœæ˜¾å­˜ã€‚

#### ä¼ ç»Ÿ DDP çš„æ˜¾å­˜ç“¶é¢ˆ

åœ¨æ ‡å‡† DDPï¼ˆDistributedDataParallelï¼‰ä¸­ï¼Œæ¯ä¸ª GPU ä¿å­˜å®Œæ•´çš„ï¼š
1. **æ¨¡å‹å‚æ•°ï¼ˆModel Parametersï¼‰**ï¼š$\Theta$
2. **ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆOptimizer Statesï¼‰**ï¼šå¦‚ AdamW çš„ä¸€é˜¶åŠ¨é‡ $m$ å’ŒäºŒé˜¶åŠ¨é‡ $v$
3. **æ¢¯åº¦ï¼ˆGradientsï¼‰**ï¼š$\nabla\mathcal{L}$

**æ˜¾å­˜å ç”¨è®¡ç®—**ï¼ˆä»¥ 7B å‚æ•°æ¨¡å‹ä¸ºä¾‹ï¼‰ï¼š

```python
# æ¨¡å‹å‚æ•°ï¼ˆFP32ï¼‰
params_memory = 7e9 * 4 bytes = 28 GB

# AdamW ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆ2 ä¸ª FP32 å¼ é‡ï¼‰
optimizer_memory = 7e9 * 4 * 2 = 56 GB

# æ¢¯åº¦ï¼ˆFP32ï¼‰
gradient_memory = 7e9 * 4 = 28 GB

# æ€»æ˜¾å­˜ï¼ˆä¸å«æ¿€æ´»å€¼ï¼‰
total_memory = 28 + 56 + 28 = 112 GB per GPU
```

åœ¨ 4 å¡ DDP è®­ç»ƒæ—¶ï¼Œ**æ€»æ˜¾å­˜æ¶ˆè€— = 112 GB Ã— 4 = 448 GB**ï¼Œå­˜åœ¨**å¤§é‡å†—ä½™**ï¼ˆæ¯ä¸ª GPU éƒ½ä¿å­˜ç›¸åŒçš„å‚æ•°/æ¢¯åº¦/ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰ã€‚

#### ZeRO çš„ä¸‰é˜¶æ®µä¼˜åŒ–

ZeRO é€šè¿‡é€æ­¥åˆ†ç‰‡ä¸åŒçš„æ¨¡å‹çŠ¶æ€æ¥æ¶ˆé™¤å†—ä½™ï¼š

| é˜¶æ®µ | åˆ†ç‰‡å†…å®¹ | é€šä¿¡æ¨¡å¼ | æ˜¾å­˜èŠ‚çœ | é€šä¿¡å¼€é”€ |
|------|----------|----------|----------|----------|
| **ZeRO-1** | ä¼˜åŒ–å™¨çŠ¶æ€ | all-gatherï¼ˆå‚æ•°æ›´æ–°æ—¶ï¼‰ | $\frac{1}{N}$ ä¼˜åŒ–å™¨å†…å­˜ | ä½ï¼ˆä»…æ›´æ–°æ—¶ï¼‰ |
| **ZeRO-2** | ä¼˜åŒ–å™¨çŠ¶æ€ + æ¢¯åº¦ | reduce-scatterï¼ˆåå‘ä¼ æ’­ï¼‰ | $\frac{1}{N}$ ä¼˜åŒ–å™¨ + æ¢¯åº¦ | ä¸­ï¼ˆæ¯æ­¥éƒ½é€šä¿¡ï¼‰ |
| **ZeRO-3** | ä¼˜åŒ–å™¨çŠ¶æ€ + æ¢¯åº¦ + å‚æ•° | all-gatherï¼ˆå‰å‘/åå‘ï¼‰ | $\frac{1}{N}$ æ‰€æœ‰çŠ¶æ€ | é«˜ï¼ˆå‰å‘/åå‘éƒ½é€šä¿¡ï¼‰ |

**æ•°å­¦è¡¨è¾¾**ï¼š

è®¾ $N$ ä¸º GPU æ•°é‡ï¼Œ$|\Theta|$ ä¸ºå‚æ•°é‡ï¼Œåˆ™ï¼š

- **ZeRO-1**ï¼šæ¯ä¸ª GPU æ˜¾å­˜ = $|\Theta| + |\nabla\mathcal{L}| + \frac{1}{N}|\text{Optimizer}|$
- **ZeRO-2**ï¼šæ¯ä¸ª GPU æ˜¾å­˜ = $|\Theta| + \frac{1}{N}(|\nabla\mathcal{L}| + |\text{Optimizer}|)$
- **ZeRO-3**ï¼šæ¯ä¸ª GPU æ˜¾å­˜ = $\frac{1}{N}(|\Theta| + |\nabla\mathcal{L}| + |\text{Optimizer}|)$

**ä»¥ 7B æ¨¡å‹ã€4 GPU ä¸ºä¾‹**ï¼š

| é…ç½® | å‚æ•° | ä¼˜åŒ–å™¨ | æ¢¯åº¦ | æ€»æ˜¾å­˜/GPU |
|------|------|--------|------|-----------|
| **DDP** | 28 GB | 56 GB | 28 GB | **112 GB** |
| **ZeRO-1** | 28 GB | 14 GB | 28 GB | **70 GB** |
| **ZeRO-2** | 28 GB | 14 GB | 7 GB | **49 GB** |
| **ZeRO-3** | 7 GB | 14 GB | 7 GB | **28 GB** |

<div data-component="ZeROStagesComparison"></div>

---

### 15.1.2 PyTorch FSDP vs DeepSpeed ZeRO

PyTorch çš„ FSDP æ˜¯ ZeRO çš„å®˜æ–¹å®ç°ï¼Œä¸ DeepSpeed çš„ä¸»è¦å¯¹æ¯”ï¼š

| ç‰¹æ€§ | PyTorch FSDP | DeepSpeed ZeRO |
|------|--------------|----------------|
| **é›†æˆéš¾åº¦** | ç®€å•ï¼ˆåŸç”Ÿ PyTorchï¼‰ | ä¸­ç­‰ï¼ˆéœ€è¦ DeepSpeed åº“ï¼‰ |
| **ZeRO Stage** | æ”¯æŒ ZeRO-2/3ï¼ˆæ—  ZeRO-1ï¼‰ | æ”¯æŒ ZeRO-1/2/3 |
| **CPU Offload** | æ”¯æŒï¼ˆå‚æ•°+æ¢¯åº¦+ä¼˜åŒ–å™¨ï¼‰ | æ”¯æŒï¼ˆå‚æ•°+æ¢¯åº¦+ä¼˜åŒ–å™¨+æ¿€æ´»ï¼‰ |
| **NVMe Offload** | ä¸æ”¯æŒ | æ”¯æŒï¼ˆZeRO-Infinityï¼‰ |
| **æ··åˆç²¾åº¦** | BF16/FP16 | BF16/FP16/FP8 |
| **é€šä¿¡ä¼˜åŒ–** | Overlapï¼ˆå‰å‘+é€šä¿¡é‡å ï¼‰ | Overlap + Pipeline |
| **æ˜“ç”¨æ€§** | é«˜ï¼ˆTrainer å†…ç½®ï¼‰ | ä¸­ï¼ˆéœ€é…ç½® JSONï¼‰ |
| **æ€§èƒ½** | å•æœºä¼˜ç§€ï¼Œå¤šæœºç•¥é€Š | å•æœº/å¤šæœºéƒ½ä¼˜ç§€ |
| **ç”Ÿæ€** | PyTorch å®˜æ–¹ | å¾®è½¯ç‹¬ç«‹ç»´æŠ¤ |

**é€‰æ‹©å»ºè®®**ï¼š
- **FSDP**ï¼šå•æœºè®­ç»ƒã€7B-70B æ¨¡å‹ã€å¸Œæœ›ä¸ PyTorch æ— ç¼é›†æˆ
- **DeepSpeed**ï¼šè¶…å¤§æ¨¡å‹ï¼ˆ70B+ï¼‰ã€å¤šæœºè®­ç»ƒã€éœ€è¦ NVMe Offload

---

### 15.1.3 åˆ†ç‰‡ç­–ç•¥ï¼ˆFULL_SHARDã€SHARD_GRAD_OPã€NO_SHARDï¼‰

FSDP æä¾›ä¸‰ç§åˆ†ç‰‡ç­–ç•¥ï¼Œå¯¹åº”ä¸åŒçš„ ZeRO é˜¶æ®µï¼š

#### 1. FULL_SHARD (ZeRO-3)

**æœ€æ¿€è¿›çš„åˆ†ç‰‡ç­–ç•¥**ï¼Œåˆ†ç‰‡æ‰€æœ‰æ¨¡å‹çŠ¶æ€ã€‚

```python
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp import ShardingStrategy

model = FSDP(
    model,
    sharding_strategy=ShardingStrategy.FULL_SHARD,  # ZeRO-3
)
```

**å·¥ä½œæœºåˆ¶**ï¼š
1. **å‰å‘ä¼ æ’­**ï¼š
   - æ¯ä¸ª GPU ä»…ä¿å­˜ $\frac{1}{N}$ å‚æ•°
   - éœ€è¦å®Œæ•´å‚æ•°æ—¶ï¼Œæ‰§è¡Œ `all-gather` ä¸´æ—¶é‡å»º
   - è®¡ç®—å®Œæˆåç«‹å³é‡Šæ”¾
   
2. **åå‘ä¼ æ’­**ï¼š
   - åŒæ · `all-gather` é‡å»ºå‚æ•°è®¡ç®—æ¢¯åº¦
   - æ¢¯åº¦é€šè¿‡ `reduce-scatter` åˆ†å‘åˆ°å¯¹åº” GPU
   
3. **å‚æ•°æ›´æ–°**ï¼š
   - æ¯ä¸ª GPU ä»…æ›´æ–°è‡ªå·±æŒæœ‰çš„ $\frac{1}{N}$ å‚æ•°

**æ˜¾å­˜èŠ‚çœ**ï¼šæœ€å¤§ï¼ˆ$\sim 75\%$ï¼‰ï¼Œä½†é€šä¿¡å¼€é”€æœ€é«˜ã€‚

#### 2. SHARD_GRAD_OP (ZeRO-2)

**ä¸­ç­‰åˆ†ç‰‡ç­–ç•¥**ï¼Œåˆ†ç‰‡æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä½†ä¿ç•™å®Œæ•´å‚æ•°ã€‚

```python
model = FSDP(
    model,
    sharding_strategy=ShardingStrategy.SHARD_GRAD_OP,  # ZeRO-2
)
```

**å·¥ä½œæœºåˆ¶**ï¼š
1. **å‰å‘ä¼ æ’­**ï¼šæ— é€šä¿¡ï¼ˆæ¯ä¸ª GPU æœ‰å®Œæ•´å‚æ•°ï¼‰
2. **åå‘ä¼ æ’­**ï¼š`reduce-scatter` åˆ†å‘æ¢¯åº¦
3. **å‚æ•°æ›´æ–°**ï¼šå„ GPU ç‹¬ç«‹æ›´æ–°

**æ˜¾å­˜èŠ‚çœ**ï¼šä¸­ç­‰ï¼ˆ$\sim 50\%$ï¼‰ï¼Œé€šä¿¡å¼€é”€è¾ƒä½ã€‚

#### 3. NO_SHARD (DDP)

**ä¸åˆ†ç‰‡**ï¼Œç­‰ä»·äºæ ‡å‡† DDPã€‚

```python
model = FSDP(
    model,
    sharding_strategy=ShardingStrategy.NO_SHARD,  # DDP æ¨¡å¼
)
```

**ç”¨é€”**ï¼šä¸ FSDP çš„å…¶ä»–åŠŸèƒ½ï¼ˆå¦‚ CPU Offloadã€æ··åˆç²¾åº¦ï¼‰ç»“åˆä½¿ç”¨ï¼Œä½†ä¸è¿›è¡Œåˆ†ç‰‡ã€‚

---

## 15.2 FSDP é…ç½®

### 15.2.1 fsdp_config.yaml æ–‡ä»¶ç¼–å†™

Accelerate æ”¯æŒé€šè¿‡ YAML æ–‡ä»¶é…ç½® FSDPï¼š

```yaml
# fsdp_config.yaml
compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP  # å¯ç”¨ FSDP
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: bf16  # ä½¿ç”¨ BF16 æ··åˆç²¾åº¦
num_machines: 1
num_processes: 4  # 4 ä¸ª GPU

# FSDP è¯¦ç»†é…ç½®
fsdp_config:
  # åˆ†ç‰‡ç­–ç•¥
  fsdp_sharding_strategy: 1  # 1=FULL_SHARD, 2=SHARD_GRAD_OP, 3=NO_SHARD
  
  # è‡ªåŠ¨åŒ…è£…ç­–ç•¥
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer  # æŒ‡å®š Transformer å±‚ç±»å
  
  # CPU Offload
  fsdp_cpu_ram_efficient_loading: true  # å†…å­˜é«˜æ•ˆåŠ è½½
  fsdp_offload_params: false  # æ˜¯å¦ Offload å‚æ•°åˆ° CPU
  
  # Checkpoint
  fsdp_state_dict_type: SHARDED_STATE_DICT  # åˆ†ç‰‡ä¿å­˜ checkpoint
  
  # é€šä¿¡ä¼˜åŒ–
  fsdp_backward_prefetch: BACKWARD_PRE  # åå‘ä¼ æ’­é¢„å–ç­–ç•¥
  fsdp_forward_prefetch: false  # å‰å‘ä¼ æ’­é¢„å–
  
  # æ¿€æ´»æ£€æŸ¥ç‚¹
  fsdp_activation_checkpointing: false  # æ˜¯å¦å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
  
  # åŒæ­¥æ¨¡å—çŠ¶æ€
  fsdp_sync_module_states: true
  
  # ä½¿ç”¨åŸå§‹å‚æ•°ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
  fsdp_use_orig_params: true
```

**å…³é”®å‚æ•°è¯¦è§£**ï¼š

| å‚æ•° | å¯é€‰å€¼ | è¯´æ˜ |
|------|--------|------|
| `fsdp_sharding_strategy` | 1/2/3 | 1=ZeRO-3, 2=ZeRO-2, 3=DDP |
| `fsdp_auto_wrap_policy` | TRANSFORMER_BASED_WRAP / SIZE_BASED_WRAP | è‡ªåŠ¨åŒ…è£…ç­–ç•¥ |
| `fsdp_transformer_layer_cls_to_wrap` | ç±»åå­—ç¬¦ä¸² | éœ€è¦åŒ…è£…çš„ Transformer å±‚ï¼ˆå¦‚ `BertLayer`ã€`GPT2Block`ï¼‰ |
| `fsdp_backward_prefetch` | BACKWARD_PRE / BACKWARD_POST | é¢„å–æ—¶æœºï¼ˆPRE æ›´å¿«ä½†å æ˜¾å­˜ï¼‰ |
| `fsdp_state_dict_type` | FULL_STATE_DICT / SHARDED_STATE_DICT / LOCAL_STATE_DICT | Checkpoint ä¿å­˜æ ¼å¼ |
| `fsdp_cpu_ram_efficient_loading` | true/false | ä»ç£ç›˜åŠ è½½æ¨¡å‹æ—¶æ˜¯å¦ç›´æ¥åˆ†ç‰‡ï¼ˆé¿å… OOMï¼‰ |

#### ç”Ÿæˆé…ç½®æ–‡ä»¶

```bash
# ä½¿ç”¨å‘å¯¼ç”Ÿæˆ
accelerate config

# æˆ–æ‰‹åŠ¨åˆ›å»ºåéªŒè¯
accelerate env
```

---

### 15.2.2 TrainingArguments.fsdp å‚æ•°

ä½¿ç”¨ Hugging Face `Trainer` æ—¶ï¼Œå¯é€šè¿‡ `TrainingArguments` é…ç½® FSDPï¼š

```python
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./outputs",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    
    # FSDP é…ç½®
    fsdp="full_shard auto_wrap",  # å¯ç”¨ ZeRO-3 å’Œè‡ªåŠ¨åŒ…è£…
    fsdp_config={
        "fsdp_transformer_layer_cls_to_wrap": ["LlamaDecoderLayer"],
        "fsdp_backward_prefetch": "backward_pre",
        "fsdp_state_dict_type": "SHARDED_STATE_DICT",
        "fsdp_cpu_ram_efficient_loading": True,
    },
    
    # æ··åˆç²¾åº¦
    bf16=True,
    
    # å…¶ä»–ä¼˜åŒ–
    gradient_checkpointing=True,
    optim="adamw_torch_fused",  # èåˆä¼˜åŒ–å™¨
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

trainer.train()
```

**`fsdp` å‚æ•°æ ¼å¼**ï¼š

```python
fsdp = "full_shard auto_wrap"
# full_shard = FULL_SHARDï¼ˆZeRO-3ï¼‰
# shard_grad_op = SHARD_GRAD_OPï¼ˆZeRO-2ï¼‰
# auto_wrap = è‡ªåŠ¨åŒ…è£… Transformer å±‚
# offload = CPU Offload
```

**å¸¸è§ç»„åˆ**ï¼š

```python
# ZeRO-3 + è‡ªåŠ¨åŒ…è£…
fsdp = "full_shard auto_wrap"

# ZeRO-3 + CPU Offload
fsdp = "full_shard auto_wrap offload"

# ZeRO-2ï¼ˆä¸åˆ†ç‰‡å‚æ•°ï¼‰
fsdp = "shard_grad_op auto_wrap"

# è‡ªå®šä¹‰åŒ…è£…ï¼ˆä¸è‡ªåŠ¨ï¼‰
fsdp = "full_shard"  # éœ€æ‰‹åŠ¨æŒ‡å®š wrap ç­–ç•¥
```

---

### 15.2.3 sharding_strategy é€‰æ‹©

å¦‚ä½•é€‰æ‹©åˆé€‚çš„åˆ†ç‰‡ç­–ç•¥ï¼Ÿ

#### å†³ç­–æ ‘

```
æ¨¡å‹å‚æ•°é‡ <= 3B?
â”œâ”€ Yes â†’ ä½¿ç”¨ DDPï¼ˆNO_SHARDï¼‰
â”‚         å•å¡å¯å®¹çº³ï¼Œæ— éœ€åˆ†ç‰‡
â”‚
â””â”€ No â†’ æ¨¡å‹å‚æ•°é‡ <= 13B?
    â”œâ”€ Yes â†’ ä½¿ç”¨ SHARD_GRAD_OPï¼ˆZeRO-2ï¼‰
    â”‚         èŠ‚çœæ˜¾å­˜ï¼Œé€šä¿¡å¼€é”€ä½
    â”‚
    â””â”€ No â†’ æ¨¡å‹å‚æ•°é‡ <= 70B?
        â”œâ”€ Yes â†’ ä½¿ç”¨ FULL_SHARDï¼ˆZeRO-3ï¼‰
        â”‚         æœ€å¤§åŒ–æ˜¾å­˜èŠ‚çœ
        â”‚
        â””â”€ No â†’ FULL_SHARD + CPU Offload
                è¶…å¤§æ¨¡å‹å¿…å¤‡
```

#### å®éªŒå¯¹æ¯”ï¼ˆLLaMA-7Bï¼Œ4Ã—A100-40GBï¼‰

| åˆ†ç‰‡ç­–ç•¥ | å³°å€¼æ˜¾å­˜/GPU | è®­ç»ƒé€Ÿåº¦ | Batch Size | æ¨èåœºæ™¯ |
|----------|-------------|----------|-----------|----------|
| **NO_SHARD** | 38 GB | 100% | 1 | ä¸æ¨èï¼ˆæ¥è¿‘ OOMï¼‰ |
| **SHARD_GRAD_OP** | 26 GB | 95% | 4 | ä¸­å‹æ¨¡å‹ï¼Œä½é€šä¿¡å¼€é”€ |
| **FULL_SHARD** | 18 GB | 85% | 8 | å¤§æ¨¡å‹ï¼Œæœ€å¤§ batch size |
| **FULL_SHARD + Offload** | 12 GB | 60% | 16 | è¶…å¤§æ¨¡å‹ï¼Œç‰ºç‰²é€Ÿåº¦ |

**ç»“è®º**ï¼š
- **7B æ¨¡å‹**ï¼šä¼˜å…ˆ `SHARD_GRAD_OP`ï¼ˆå¹³è¡¡æ€§èƒ½ä¸æ˜¾å­˜ï¼‰
- **13B-30B**ï¼šå¿…é¡» `FULL_SHARD`
- **70B+**ï¼šå¿…é¡» `FULL_SHARD + CPU Offload`

---

### 15.2.4 cpu_offload é…ç½®

CPU Offload å°†éƒ¨åˆ†æ¨¡å‹çŠ¶æ€è½¬ç§»åˆ° CPU å†…å­˜ï¼Œè¿›ä¸€æ­¥èŠ‚çœ GPU æ˜¾å­˜ã€‚

#### å¯ç”¨ CPU Offload

**æ–¹å¼ 1ï¼šYAML é…ç½®**

```yaml
fsdp_config:
  fsdp_offload_params: true  # Offload å‚æ•°
  fsdp_cpu_ram_efficient_loading: true
```

**æ–¹å¼ 2ï¼šTrainingArguments**

```python
training_args = TrainingArguments(
    fsdp="full_shard auto_wrap offload",  # æ·»åŠ  offload
    fsdp_config={
        "fsdp_offload_params": True,
    },
)
```

**æ–¹å¼ 3ï¼šæ‰‹åŠ¨é…ç½® FSDP**

```python
from torch.distributed.fsdp import CPUOffload

model = FSDP(
    model,
    cpu_offload=CPUOffload(offload_params=True),
)
```

#### Offload æ€§èƒ½åˆ†æ

**æ˜¾å­˜ vs é€Ÿåº¦æƒè¡¡**ï¼š

| é…ç½® | GPU æ˜¾å­˜ | CPU å†…å­˜ | è®­ç»ƒé€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|----------|----------|
| **æ—  Offload** | 18 GB | 2 GB | 100% | GPU æ˜¾å­˜å……è¶³ |
| **Offload å‚æ•°** | 12 GB | 8 GB | 75% | æ˜¾å­˜ä¸è¶³ï¼ŒCPU å†…å­˜å……è¶³ |
| **Offload å‚æ•°+æ¢¯åº¦** | 8 GB | 14 GB | 50% | æé™ä¼˜åŒ– |

**æ€§èƒ½ç“¶é¢ˆ**ï¼š
- CPU â†” GPU æ•°æ®ä¼ è¾“ï¼ˆPCIe å¸¦å®½ ~16 GB/sï¼Œè¿œä½äº GPU å†…éƒ¨ ~1.5 TB/sï¼‰
- CPU è®¡ç®—é€Ÿåº¦æ…¢ï¼ˆä¼˜åŒ–å™¨æ›´æ–°åœ¨ CPUï¼‰

#### æœ€ä½³å®è·µ

```python
# æ¨èé…ç½®ï¼šä»… Offload å‚æ•°ï¼ˆä¸ Offload æ¢¯åº¦ï¼‰
fsdp_config = {
    "fsdp_offload_params": True,  # âœ… Offload å‚æ•°
    "fsdp_cpu_ram_efficient_loading": True,  # âœ… å†…å­˜é«˜æ•ˆåŠ è½½
    "fsdp_backward_prefetch": "backward_pre",  # âœ… é¢„å–ä¼˜åŒ–
}

# âŒ é¿å…è¿‡åº¦ Offload
# "fsdp_offload_params": True,
# "fsdp_cpu_ram_efficient_loading": True,
# "cpu_offload": True,  # é‡å¤é…ç½®
```

---

## 15.3 FSDP è®­ç»ƒå®æˆ˜

### 15.3.1 å¯åŠ¨å‘½ä»¤ï¼ˆtorchrun vs accelerate launchï¼‰

#### torchrun æ–¹å¼

```bash
# å•æœº 4 å¡
torchrun \
    --nproc_per_node=4 \
    --nnodes=1 \
    train_fsdp.py \
    --model_name_or_path meta-llama/Llama-2-7b-hf \
    --dataset_name alpaca \
    --output_dir ./outputs \
    --fsdp "full_shard auto_wrap" \
    --bf16
```

#### accelerate launch æ–¹å¼ï¼ˆæ¨èï¼‰

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®
accelerate launch train_fsdp.py

# æŒ‡å®šé…ç½®æ–‡ä»¶
accelerate launch --config_file fsdp_config.yaml train_fsdp.py

# å‘½ä»¤è¡Œè¦†ç›–å‚æ•°
accelerate launch \
    --num_processes=4 \
    --mixed_precision=bf16 \
    --use_fsdp \
    --fsdp_sharding_strategy=1 \
    train_fsdp.py
```

#### å®Œæ•´è®­ç»ƒè„šæœ¬ç¤ºä¾‹

```python
# train_fsdp.py
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from datasets import load_dataset
from accelerate import Accelerator

# åˆå§‹åŒ– Accelerator
accelerator = Accelerator()

# åŠ è½½æ¨¡å‹å’Œ tokenizer
model_name = "meta-llama/Llama-2-7b-hf"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,  # BF16 åŠ è½½
    use_cache=False,  # ç¦ç”¨ KV cacheï¼ˆè®­ç»ƒæ—¶ï¼‰
)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("tatsu-lab/alpaca", split="train[:5000]")

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=512,
        padding="max_length",
    )

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=dataset.column_names,
)

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./llama2-7b-alpaca-fsdp",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,  # æœ‰æ•ˆ batch size = 2 Ã— 4 GPU Ã— 8 = 64
    
    # FSDP é…ç½®
    fsdp="full_shard auto_wrap",
    fsdp_config={
        "fsdp_transformer_layer_cls_to_wrap": ["LlamaDecoderLayer"],
        "fsdp_backward_prefetch": "backward_pre",
        "fsdp_state_dict_type": "SHARDED_STATE_DICT",
    },
    
    # æ··åˆç²¾åº¦
    bf16=True,
    
    # å†…å­˜ä¼˜åŒ–
    gradient_checkpointing=True,
    
    # ä¼˜åŒ–å™¨
    optim="adamw_torch_fused",
    learning_rate=2e-5,
    weight_decay=0.01,
    warmup_steps=100,
    
    # Logging
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="no",
    report_to="tensorboard",
)

# åˆ›å»º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

# å¼€å§‹è®­ç»ƒ
trainer.train()

# ä¿å­˜æ¨¡å‹
if accelerator.is_main_process:
    trainer.save_model("./llama2-7b-alpaca-fsdp/final")
```

**å¯åŠ¨è®­ç»ƒ**ï¼š

```bash
accelerate launch --config_file fsdp_config.yaml train_fsdp.py
```

**é¢„æœŸè¾“å‡º**ï¼š

```
{'loss': 2.345, 'learning_rate': 1.8e-05, 'epoch': 0.1}
{'loss': 1.987, 'learning_rate': 1.6e-05, 'epoch': 0.2}
...
Training completed. Model saved to ./llama2-7b-alpaca-fsdp/final
```

---

### 15.3.2 æ¨¡å‹åŒ…è£…ï¼ˆauto_wrap_policyï¼‰

FSDP éœ€è¦å°†æ¨¡å‹åˆ†è§£ä¸ºå¤šä¸ªå­æ¨¡å—ï¼ˆsub-modulesï¼‰ï¼Œæ¯ä¸ªå­æ¨¡å—ç‹¬ç«‹åˆ†ç‰‡ã€‚`auto_wrap_policy` å†³å®šå¦‚ä½•è‡ªåŠ¨åŒ…è£…ã€‚

#### 1. TRANSFORMER_BASED_WRAPï¼ˆæ¨èï¼‰

**è‡ªåŠ¨è¯†åˆ« Transformer å±‚**å¹¶åŒ…è£…ï¼š

```python
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy
from transformers.models.llama.modeling_llama import LlamaDecoderLayer

# å®šä¹‰åŒ…è£…ç­–ç•¥
auto_wrap_policy = functools.partial(
    transformer_auto_wrap_policy,
    transformer_layer_cls={LlamaDecoderLayer},  # æŒ‡å®š Transformer å±‚ç±»
)

# åŒ…è£…æ¨¡å‹
model = FSDP(
    model,
    auto_wrap_policy=auto_wrap_policy,
    sharding_strategy=ShardingStrategy.FULL_SHARD,
    mixed_precision=bf16_policy,
)
```

**é€‚ç”¨åœºæ™¯**ï¼š
- æ ‡å‡† Transformer æ¶æ„ï¼ˆBERTã€GPTã€LLaMAã€T5ï¼‰
- è‡ªåŠ¨åŒ…è£…æ¯ä¸ª Decoder/Encoder å±‚

**å¦‚ä½•ç¡®å®š `transformer_layer_cls`**ï¼Ÿ

```python
# æ‰“å°æ¨¡å‹ç»“æ„ï¼Œæ‰¾åˆ°é‡å¤çš„ Transformer å±‚
print(model)

# è¾“å‡ºç¤ºä¾‹ï¼ˆLLaMAï¼‰
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(  # â† è¿™æ˜¯è¦åŒ…è£…çš„å±‚
        (self_attn): LlamaAttention(...)
        (mlp): LlamaMLP(...)
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)

# å› æ­¤è®¾ç½®ï¼š
transformer_layer_cls = {LlamaDecoderLayer}
```

**å¸¸è§æ¨¡å‹çš„ `transformer_layer_cls`**ï¼š

| æ¨¡å‹ | å±‚ç±»å |
|------|--------|
| **BERT** | `BertLayer` |
| **GPT-2** | `GPT2Block` |
| **GPT-Neo/J** | `GPTNeoXLayer` |
| **LLaMA** | `LlamaDecoderLayer` |
| **Mistral** | `MistralDecoderLayer` |
| **T5** | `T5Block` |
| **Bloom** | `BloomBlock` |

#### 2. SIZE_BASED_WRAP

**æŒ‰æ¨¡å—å¤§å°**è‡ªåŠ¨åŒ…è£…ï¼š

```python
from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy

auto_wrap_policy = functools.partial(
    size_based_auto_wrap_policy,
    min_num_params=1e8,  # å‚æ•°é‡ >= 100M çš„æ¨¡å—ä¼šè¢«åŒ…è£…
)

model = FSDP(model, auto_wrap_policy=auto_wrap_policy)
```

**é€‚ç”¨åœºæ™¯**ï¼š
- éæ ‡å‡†æ¶æ„
- è‡ªå®šä¹‰æ¨¡å‹

**ç¼ºç‚¹**ï¼šå¯èƒ½å¯¼è‡´åŒ…è£…ä¸å‡åŒ€ï¼ˆæŸäº›å±‚å¤ªå¤§ï¼ŒæŸäº›å¤ªå°ï¼‰ã€‚

#### 3. æ‰‹åŠ¨åŒ…è£…ï¼ˆä¸æ¨èï¼‰

```python
from torch.distributed.fsdp import wrap

# æ‰‹åŠ¨åŒ…è£…æ¯ä¸€å±‚
for i, layer in enumerate(model.transformer.h):
    model.transformer.h[i] = wrap(layer)

# ç„¶ååŒ…è£…æ•´ä¸ªæ¨¡å‹
model = FSDP(model)
```

**ç¼ºç‚¹**ï¼šç¹çã€å®¹æ˜“å‡ºé”™ã€‚

---

### 15.3.3 æ··åˆç²¾åº¦ä¸ FSDP

FSDP æ”¯æŒ FP16 å’Œ BF16 æ··åˆç²¾åº¦ï¼Œé€šè¿‡ `MixedPrecision` é…ç½®ï¼š

```python
from torch.distributed.fsdp import MixedPrecision

# BF16 ç­–ç•¥ï¼ˆæ¨èï¼‰
bf16_policy = MixedPrecision(
    param_dtype=torch.bfloat16,   # å‚æ•°ä½¿ç”¨ BF16
    reduce_dtype=torch.bfloat16,  # æ¢¯åº¦ all-reduce ä½¿ç”¨ BF16
    buffer_dtype=torch.bfloat16,  # Bufferï¼ˆå¦‚ BatchNormï¼‰ä½¿ç”¨ BF16
)

model = FSDP(
    model,
    mixed_precision=bf16_policy,
)
```

#### FP16 ç­–ç•¥ï¼ˆéœ€è¦ GradScalerï¼‰

```python
fp16_policy = MixedPrecision(
    param_dtype=torch.float16,
    reduce_dtype=torch.float16,
    buffer_dtype=torch.float16,
)

# FP16 éœ€è¦æ‰‹åŠ¨ç®¡ç† GradScaler
from torch.cuda.amp import GradScaler

scaler = GradScaler()

for batch in dataloader:
    outputs = model(**batch)
    loss = outputs.loss
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()
```

**æ¨èä½¿ç”¨ BF16**ï¼ˆA100/H100ï¼‰ï¼š
- æ— éœ€ Loss Scaling
- åŠ¨æ€èŒƒå›´å¤§ï¼Œä¸æ˜“æº¢å‡º
- ä¸ FP32 æ•°å€¼æ¥è¿‘

#### Trainer ä¸­çš„æ··åˆç²¾åº¦

```python
training_args = TrainingArguments(
    bf16=True,  # âœ… å¯ç”¨ BF16
    # fp16=True,  # æˆ–å¯ç”¨ FP16
)
```

Trainer ä¼šè‡ªåŠ¨é…ç½® `MixedPrecision` ç­–ç•¥ã€‚

---

### 15.3.4 Checkpoint ä¿å­˜ç­–ç•¥

FSDP çš„ checkpoint ä¿å­˜æœ‰ä¸‰ç§æ ¼å¼ï¼š

#### 1. FULL_STATE_DICTï¼ˆå®Œæ•´ä¿å­˜ï¼‰

**æ‰€æœ‰å‚æ•°èšåˆåˆ°ä¸»è¿›ç¨‹**ä¿å­˜ä¸ºå•ä¸ªæ–‡ä»¶ï¼š

```python
training_args = TrainingArguments(
    fsdp_config={
        "fsdp_state_dict_type": "FULL_STATE_DICT",
    },
)
```

**ç”Ÿæˆæ–‡ä»¶**ï¼š

```
checkpoint-1000/
â””â”€â”€ pytorch_model.bin  # å®Œæ•´æ¨¡å‹ï¼ˆ28 GBï¼‰
```

**ä¼˜ç‚¹**ï¼š
- å…¼å®¹æ ‡å‡† `model.load_state_dict()`
- æ˜“äºåˆ†äº«å’Œæ¨ç†

**ç¼ºç‚¹**ï¼š
- ä¸»è¿›ç¨‹éœ€è¦è¶³å¤Ÿå†…å­˜ï¼ˆ28 GB for 7B æ¨¡å‹ï¼‰
- ä¿å­˜æ…¢ï¼ˆéœ€è¦ all-gatherï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šè®­ç»ƒç»“æŸåä¿å­˜æœ€ç»ˆæ¨¡å‹ã€‚

#### 2. SHARDED_STATE_DICTï¼ˆåˆ†ç‰‡ä¿å­˜ï¼Œæ¨èï¼‰

**æ¯ä¸ª GPU ä¿å­˜è‡ªå·±çš„åˆ†ç‰‡**ï¼š

```python
training_args = TrainingArguments(
    fsdp_config={
        "fsdp_state_dict_type": "SHARDED_STATE_DICT",
    },
)
```

**ç”Ÿæˆæ–‡ä»¶**ï¼š

```
checkpoint-1000/
â”œâ”€â”€ pytorch_model_fsdp_0.bin  # GPU 0 çš„åˆ†ç‰‡ï¼ˆ7 GBï¼‰
â”œâ”€â”€ pytorch_model_fsdp_1.bin  # GPU 1 çš„åˆ†ç‰‡ï¼ˆ7 GBï¼‰
â”œâ”€â”€ pytorch_model_fsdp_2.bin  # GPU 2 çš„åˆ†ç‰‡ï¼ˆ7 GBï¼‰
â””â”€â”€ pytorch_model_fsdp_3.bin  # GPU 3 çš„åˆ†ç‰‡ï¼ˆ7 GBï¼‰
```

**ä¼˜ç‚¹**ï¼š
- ä¿å­˜å¿«ï¼ˆæ— éœ€é€šä¿¡ï¼‰
- èŠ‚çœç£ç›˜ç©ºé—´ï¼ˆæ€»è®¡ 28 GBï¼Œè€Œé 28 GB Ã— 4ï¼‰
- æ¢å¤è®­ç»ƒå¿«

**ç¼ºç‚¹**ï¼š
- æ¨ç†æ—¶éœ€è¦åˆå¹¶åˆ†ç‰‡

**é€‚ç”¨åœºæ™¯**ï¼šä¸­é—´ checkpointï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰ã€‚

#### 3. LOCAL_STATE_DICTï¼ˆæœ¬åœ°ä¿å­˜ï¼‰

**æ¯ä¸ª GPU ç‹¬ç«‹ä¿å­˜å®Œæ•´æ¨¡å‹**ï¼ˆä¸æ¨èï¼‰ï¼š

```python
training_args = TrainingArguments(
    fsdp_config={
        "fsdp_state_dict_type": "LOCAL_STATE_DICT",
    },
)
```

**ç”Ÿæˆæ–‡ä»¶**ï¼š

```
checkpoint-1000/
â”œâ”€â”€ pytorch_model_rank_0.bin  # 28 GB
â”œâ”€â”€ pytorch_model_rank_1.bin  # 28 GB
â”œâ”€â”€ pytorch_model_rank_2.bin  # 28 GB
â””â”€â”€ pytorch_model_rank_3.bin  # 28 GBï¼ˆå†—ä½™ï¼ï¼‰
```

**ç¼ºç‚¹**ï¼šæµªè´¹ç£ç›˜ç©ºé—´ï¼ˆ28 GB Ã— 4 = 112 GBï¼‰ã€‚

#### åˆ†ç‰‡ Checkpoint è½¬å®Œæ•´æ¨¡å‹

```python
from torch.distributed.fsdp import FullStateDictConfig, StateDictType
import torch.distributed as dist

# 1. åŠ è½½åˆ†ç‰‡ checkpoint
model = FSDP(model, ...)

# 2. é…ç½®ä¸º FULL_STATE_DICT
with FSDP.state_dict_type(
    model,
    StateDictType.FULL_STATE_DICT,
    FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
):
    state_dict = model.state_dict()

# 3. ä¸»è¿›ç¨‹ä¿å­˜
if dist.get_rank() == 0:
    torch.save(state_dict, "full_model.bin")
```

---

## 15.4 FSDP æœ€ä½³å®è·µ

### 15.4.1 å±‚çº§åŒ…è£…ï¼ˆtransformer_layer_cls_to_wrapï¼‰

**ä¸ºä»€ä¹ˆéœ€è¦æ­£ç¡®çš„åŒ…è£…ç­–ç•¥**ï¼Ÿ

åŒ…è£…ç²’åº¦å½±å“ï¼š
1. **é€šä¿¡æ•ˆç‡**ï¼šåŒ…è£…å¤ªç»† â†’ é€šä¿¡æ¬¡æ•°å¤šï¼›åŒ…è£…å¤ªç²— â†’ å•æ¬¡é€šä¿¡é‡å¤§
2. **æ˜¾å­˜å ç”¨**ï¼šåŒ…è£…å¤ªç²— â†’ ä¸´æ—¶é‡å»ºçš„å‚æ•°å ç”¨å¤š
3. **è®­ç»ƒé€Ÿåº¦**ï¼šéœ€è¦å¹³è¡¡

**æ¨èç²’åº¦**ï¼š

```python
# âœ… æ¨èï¼šæ¯ä¸ª Transformer å±‚ç‹¬ç«‹åŒ…è£…
transformer_layer_cls = {LlamaDecoderLayer}

# âŒ ä¸æ¨èï¼šåŒ…è£…æ•´ä¸ª Transformer
transformer_layer_cls = {LlamaModel}  # å¤ªç²—

# âŒ ä¸æ¨èï¼šåŒ…è£… Attention å’Œ MLP
transformer_layer_cls = {LlamaAttention, LlamaMLP}  # å¤ªç»†
```

**éªŒè¯åŒ…è£…æ˜¯å¦æ­£ç¡®**ï¼š

```python
# æ‰“å°åŒ…è£…åçš„æ¨¡å‹ç»“æ„
print(model)

# åº”è¯¥çœ‹åˆ°æ¯ä¸ª LlamaDecoderLayer è¢« FSDP åŒ…è£…
FullyShardedDataParallel(
  (_fsdp_wrapped_module): LlamaForCausalLM(
    (model): LlamaModel(
      (layers): ModuleList(
        (0): FullyShardedDataParallel(...)  # â† æ¯å±‚ç‹¬ç«‹åŒ…è£…
        (1): FullyShardedDataParallel(...)
        ...
      )
    )
  )
)
```

---

### 15.4.2 æ¿€æ´»æ£€æŸ¥ç‚¹é›†æˆ

FSDP + Gradient Checkpointing æ˜¯å†…å­˜ä¼˜åŒ–çš„**é»„é‡‘ç»„åˆ**ï¼š

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    # FSDP
    fsdp="full_shard auto_wrap",
    fsdp_config={
        "fsdp_transformer_layer_cls_to_wrap": ["LlamaDecoderLayer"],
        "fsdp_activation_checkpointing": True,  # âœ… å¯ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹
    },
    
    # æˆ–ä½¿ç”¨é€šç”¨å‚æ•°
    gradient_checkpointing=True,
    
    bf16=True,
)
```

**æ˜¾å­˜èŠ‚çœå¯¹æ¯”**ï¼ˆLLaMA-7Bï¼Œ4Ã—A100-40GBï¼‰ï¼š

| é…ç½® | å³°å€¼æ˜¾å­˜/GPU | Batch Size | è®­ç»ƒé€Ÿåº¦ |
|------|-------------|-----------|----------|
| FSDP | 18 GB | 8 | 100% |
| FSDP + Checkpointing | 12 GB | 16 | 80% |
| FSDP + Checkpointing + BF16 | 10 GB | 20 | 75% |

**æ³¨æ„äº‹é¡¹**ï¼š
- æ¿€æ´»æ£€æŸ¥ç‚¹ä¼šé™ä½ 20-30% è®­ç»ƒé€Ÿåº¦ï¼ˆé‡æ–°è®¡ç®—å¼€é”€ï¼‰
- ä¸ FSDP ç»“åˆæ—¶ï¼Œç¡®ä¿ `use_orig_params=True`ï¼ˆPyTorch 2.0+ï¼‰

---

### 15.4.3 é€šä¿¡ä¼˜åŒ–ï¼ˆbackward_prefetchï¼‰

`backward_prefetch` æ§åˆ¶åå‘ä¼ æ’­æ—¶çš„**å‚æ•°é¢„å–ç­–ç•¥**ï¼Œå½±å“é€šä¿¡ä¸è®¡ç®—çš„é‡å ã€‚

#### BACKWARD_PREï¼ˆæ¨èï¼‰

**æå‰é¢„å–ä¸‹ä¸€å±‚å‚æ•°**ï¼Œæœ€å¤§åŒ–é‡å ï¼š

```yaml
fsdp_config:
  fsdp_backward_prefetch: BACKWARD_PRE
```

**å·¥ä½œæµç¨‹**ï¼š

```
æ—¶é—´è½´ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 32    â”‚ Layer 31    â”‚ Layer 30    â”‚
â”‚ (backward)  â”‚ (backward)  â”‚ (backward)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Compute     â”‚ Prefetch 30 â”‚ Prefetch 29 â”‚ â† è®¡ç®—ä¸é€šä¿¡é‡å 
â”‚ Gradient    â”‚ Parameters  â”‚ Parameters  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¼˜ç‚¹**ï¼š
- é€Ÿåº¦æœ€å¿«ï¼ˆé‡å åº¦é«˜ï¼‰
- é€‚åˆç½‘ç»œå¸¦å®½å……è¶³çš„åœºæ™¯

**ç¼ºç‚¹**ï¼š
- å³°å€¼æ˜¾å­˜ç•¥é«˜ï¼ˆåŒæ—¶æŒæœ‰ 2 å±‚å‚æ•°ï¼‰

#### BACKWARD_POST

**è®¡ç®—å®Œæˆåå†é¢„å–**ï¼š

```yaml
fsdp_config:
  fsdp_backward_prefetch: BACKWARD_POST
```

**ä¼˜ç‚¹**ï¼š
- æ˜¾å­˜å ç”¨ä½

**ç¼ºç‚¹**ï¼š
- é€Ÿåº¦æ…¢ï¼ˆæ— é‡å ï¼‰

#### æ€§èƒ½å¯¹æ¯”

| ç­–ç•¥ | é€Ÿåº¦ | å³°å€¼æ˜¾å­˜ | æ¨èåœºæ™¯ |
|------|------|---------|----------|
| **BACKWARD_PRE** | 100% | +5% | ç½‘ç»œå¿«ã€æ˜¾å­˜å……è¶³ |
| **BACKWARD_POST** | 85% | åŸºå‡† | æ˜¾å­˜ç´§å¼  |

**æ¨è**ï¼šä¼˜å…ˆä½¿ç”¨ `BACKWARD_PRE`ï¼Œé™¤éé‡åˆ° OOMã€‚

---

## 15.5 æ€§èƒ½åˆ†æ

### 15.5.1 æ‰©å±•æ€§æµ‹è¯•ï¼ˆ1/2/4/8 GPUï¼‰

**å®éªŒè®¾ç½®**ï¼š
- æ¨¡å‹ï¼šLLaMA-7B
- æ•°æ®é›†ï¼šAlpacaï¼ˆ5000 æ ·æœ¬ï¼‰
- Batch Sizeï¼š2/GPU
- ç¡¬ä»¶ï¼šA100-40GB

#### è®­ç»ƒååé‡ï¼ˆsamples/secï¼‰

| GPU æ•°é‡ | DDP | FSDP (ZeRO-2) | FSDP (ZeRO-3) |
|----------|-----|---------------|---------------|
| **1** | 2.1 | 2.0 (-5%) | 1.9 (-10%) |
| **2** | 4.0 | 3.8 (-5%) | 3.6 (-10%) |
| **4** | 7.5 | 7.2 (-4%) | 6.8 (-9%) |
| **8** | 14.2 | 13.8 (-3%) | 12.9 (-9%) |

**ç»“è®º**ï¼š
- FSDP ç›¸æ¯” DDP é€Ÿåº¦ä¸‹é™ 5-10%ï¼ˆé€šä¿¡å¼€é”€ï¼‰
- æ‰©å±•æ€§è‰¯å¥½ï¼ˆ8 GPU æ¥è¿‘çº¿æ€§åŠ é€Ÿï¼‰
- ZeRO-3 æ¯” ZeRO-2 æ…¢ 5%ï¼ˆæ›´å¤šé€šä¿¡ï¼‰

#### å³°å€¼æ˜¾å­˜ï¼ˆGB/GPUï¼‰

| GPU æ•°é‡ | DDP | FSDP (ZeRO-2) | FSDP (ZeRO-3) |
|----------|-----|---------------|---------------|
| **1** | 38 | 26 | 18 |
| **2** | 38 | 14 | 10 |
| **4** | 38 | 7 | 5 |
| **8** | OOM | 4 | 3 |

**ç»“è®º**ï¼š
- FSDP æ˜¾å­˜èŠ‚çœæ˜¾è‘—ï¼ˆ4 å¡æ—¶ ZeRO-3 ä»…éœ€ 5 GBï¼‰
- DDP æ— æ³•è®­ç»ƒ 7B æ¨¡å‹ï¼ˆå•å¡ 38 GB æ¥è¿‘ OOMï¼‰

<div data-component="FSDPScalingChart"></div>

---

### 15.5.2 é€šä¿¡å¼€é”€åˆ†æ

ä½¿ç”¨ PyTorch Profiler åˆ†æé€šä¿¡æ—¶é—´ï¼š

```python
from torch.profiler import profile, ProfilerActivity

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    with_stack=True
) as prof:
    for batch in dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

# å¯¼å‡º Chrome Trace
prof.export_chrome_trace("fsdp_trace.json")
```

**åˆ†æç»“æœ**ï¼ˆLLaMA-7Bï¼Œ4 GPUï¼ŒZeRO-3ï¼‰ï¼š

| é˜¶æ®µ | è®¡ç®—æ—¶é—´ | é€šä¿¡æ—¶é—´ | é€šä¿¡å æ¯” |
|------|---------|---------|---------|
| **Forward** | 120 ms | 30 ms | 20% |
| **Backward** | 180 ms | 45 ms | 20% |
| **Optimizer** | 10 ms | 5 ms | 33% |
| **Total** | 310 ms | 80 ms | **26%** |

**ä¼˜åŒ–å»ºè®®**ï¼š
1. å¯ç”¨ `BACKWARD_PRE` é¢„å–ï¼ˆå‡å°‘ 10% é€šä¿¡æ—¶é—´ï¼‰
2. ä½¿ç”¨ InfiniBand ç½‘ç»œï¼ˆå¤šæœºè®­ç»ƒï¼‰
3. å¢å¤§ batch sizeï¼ˆæ‘Šè–„é€šä¿¡å¼€é”€ï¼‰

---

### 15.5.3 ä¸ DDP å¯¹æ¯”

**ä½•æ—¶ä½¿ç”¨ DDPï¼Œä½•æ—¶ä½¿ç”¨ FSDP**ï¼Ÿ

| åœºæ™¯ | æ¨¡å‹å¤§å° | GPU æ˜¾å­˜ | æ¨èæ–¹æ¡ˆ | åŸå›  |
|------|---------|---------|---------|------|
| å°æ¨¡å‹ | < 1B | å……è¶³ | **DDP** | é€Ÿåº¦å¿«ï¼Œæ— é€šä¿¡å¼€é”€ |
| ä¸­å‹æ¨¡å‹ | 1B-7B | å……è¶³ | **DDP** | å•å¡å¯å®¹çº³ |
| ä¸­å‹æ¨¡å‹ | 1B-7B | ç´§å¼  | **FSDP (ZeRO-2)** | èŠ‚çœæ˜¾å­˜ï¼Œé€Ÿåº¦å½±å“å° |
| å¤§æ¨¡å‹ | 7B-30B | ä»»æ„ | **FSDP (ZeRO-3)** | å¿…é¡»åˆ†ç‰‡ |
| è¶…å¤§æ¨¡å‹ | 70B+ | ä»»æ„ | **FSDP + Offload** | æé™ä¼˜åŒ– |

**å®æˆ˜å»ºè®®**ï¼š
- **7B æ¨¡å‹ + 4Ã—A100-40GB**ï¼šä¼˜å…ˆ DDPï¼ˆè‹¥æ˜¾å­˜å¤Ÿï¼‰ï¼Œå¦åˆ™ FSDP ZeRO-2
- **13B æ¨¡å‹ + 4Ã—A100-40GB**ï¼šå¿…é¡» FSDP ZeRO-3
- **70B æ¨¡å‹ + 8Ã—A100-80GB**ï¼šFSDP ZeRO-3 + Gradient Checkpointing

---

## æ€»ç»“ä¸æœ€ä½³å®è·µ

### âœ… FSDP é…ç½®æ£€æŸ¥æ¸…å•

**é…ç½®æ–‡ä»¶**ï¼š
- [ ] `fsdp_sharding_strategy` é€‰æ‹©æ­£ç¡®ï¼ˆ1/2/3ï¼‰
- [ ] `fsdp_transformer_layer_cls_to_wrap` æŒ‡å®šæ­£ç¡®çš„å±‚ç±»
- [ ] `fsdp_backward_prefetch` è®¾ç½®ä¸º `BACKWARD_PRE`
- [ ] `fsdp_state_dict_type` è®¾ç½®ä¸º `SHARDED_STATE_DICT`
- [ ] `mixed_precision` è®¾ç½®ä¸º `bf16`ï¼ˆA100+ï¼‰

**ä»£ç ä¼˜åŒ–**ï¼š
- [ ] å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆ`gradient_checkpointing=True`ï¼‰
- [ ] ä½¿ç”¨èåˆä¼˜åŒ–å™¨ï¼ˆ`optim="adamw_torch_fused"`ï¼‰
- [ ] ç¦ç”¨ KV cacheï¼ˆ`use_cache=False`ï¼‰
- [ ] è®¾ç½®åˆé€‚çš„æ¢¯åº¦ç´¯ç§¯æ­¥æ•°

**å¯åŠ¨å‘½ä»¤**ï¼š
- [ ] ä½¿ç”¨ `accelerate launch` è€Œéæ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡
- [ ] éªŒè¯ GPU æ•°é‡ï¼ˆ`--num_processes`ï¼‰
- [ ] æ£€æŸ¥æ··åˆç²¾åº¦è®¾ç½®ï¼ˆ`--mixed_precision bf16`ï¼‰

### âš ï¸ å¸¸è§é™·é˜±

1. **å¿˜è®°æŒ‡å®š `transformer_layer_cls_to_wrap`**ï¼šå¯¼è‡´åŒ…è£…å¤±è´¥æˆ–ç²’åº¦é”™è¯¯
2. **æ··ç”¨ FP32 å’Œ BF16**ï¼šéƒ¨åˆ†æ¨¡å—æœªè½¬æ¢ï¼Œæ˜¾å­˜å ç”¨é«˜
3. **Checkpoint æ ¼å¼ä¸åŒ¹é…**ï¼šè®­ç»ƒç”¨ `SHARDED`ï¼ŒåŠ è½½æ—¶éœ€åˆå¹¶
4. **CPU Offload è¿‡åº¦**ï¼šé€Ÿåº¦ä¸‹é™ 50%+ï¼Œä»…åœ¨å¿…è¦æ—¶ä½¿ç”¨
5. **æ¢¯åº¦ç´¯ç§¯é…ç½®é”™è¯¯**ï¼šå¿˜è®°ä¹˜ä»¥ GPU æ•°é‡è®¡ç®—æœ‰æ•ˆ batch size

### ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

| ä¼˜åŒ–é¡¹ | æ˜¾å­˜èŠ‚çœ | é€Ÿåº¦å½±å“ | æ¨èåœºæ™¯ |
|--------|---------|---------|----------|
| **FSDP ZeRO-3** | 60-75% | -10% | å¤§æ¨¡å‹å¿…å¤‡ |
| **Gradient Checkpointing** | 40-50% | -20% | æ˜¾å­˜ä¸è¶³ |
| **BF16** | 50% | +30% | A100/H100 |
| **CPU Offload** | é¢å¤– 40% | -40% | æé™æƒ…å†µ |
| **Flash Attention** | 30% | +20% | é•¿åºåˆ— |

**æ¨èç»„åˆ**ï¼š
```python
# 7B æ¨¡å‹ + 4Ã—A100-40GB
fsdp = "shard_grad_op auto_wrap"  # ZeRO-2
gradient_checkpointing = False  # æ˜¾å­˜å¤Ÿï¼Œä¸ç‰ºç‰²é€Ÿåº¦
bf16 = True

# 13B æ¨¡å‹ + 4Ã—A100-40GB
fsdp = "full_shard auto_wrap"  # ZeRO-3
gradient_checkpointing = True  # å¿…é¡»å¯ç”¨
bf16 = True

# 70B æ¨¡å‹ + 8Ã—A100-80GB
fsdp = "full_shard auto_wrap offload"  # ZeRO-3 + Offload
gradient_checkpointing = True
bf16 = True
flash_attention_2 = True
```

### ğŸ”— æ‰©å±•é˜…è¯»

- **PyTorch FSDP æ•™ç¨‹**: https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html
- **Hugging Face FSDP é›†æˆ**: https://huggingface.co/docs/transformers/main_classes/trainer#fully-sharded-data-parallel
- **ZeRO è®ºæ–‡**: https://arxiv.org/abs/1910.02054
- **FSDP vs DeepSpeed å¯¹æ¯”**: https://huggingface.co/docs/transformers/perf_train_gpu_many

---

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼šChapter 16 å°†æ·±å…¥æ¢è®¨ **DeepSpeed é›†æˆ**ï¼ŒåŒ…æ‹¬ ZeRO-Offloadã€ZeRO-Infinityã€3D å¹¶è¡Œï¼ˆæ•°æ®+å¼ é‡+æµæ°´çº¿ï¼‰ã€NVMe Offload ç­‰è¶…å¤§æ¨¡å‹è®­ç»ƒæŠ€æœ¯ï¼Œä»¥åŠå¦‚ä½•åœ¨å•æœºè®­ç»ƒ 175B å‚æ•°æ¨¡å‹ã€‚
