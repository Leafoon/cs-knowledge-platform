---
title: "Chapter 5. Trainer API å®Œæ•´æŒ‡å—"
description: "æŒæ¡ Trainer API å…¨æµç¨‹ã€æ·±å…¥ç†è§£è®­ç»ƒå¾ªç¯ã€å›è°ƒæœºåˆ¶ä¸è‡ªå®šä¹‰æ‰©å±•"
updated: "2026-01-22"
---

---

## 5.1 Trainer æ ¸å¿ƒæ¦‚å¿µ

### 5.1.1 ä¸ºä»€ä¹ˆä½¿ç”¨ Trainerï¼Ÿ

**ä¼ ç»Ÿ PyTorch è®­ç»ƒå¾ªç¯ vs Trainer å¯¹æ¯”**ï¼š

```python
# âŒ ä¼ ç»Ÿ PyTorchï¼šéœ€è¦æ‰‹åŠ¨å®ç°å¤§é‡ç»†èŠ‚
import torch
from torch.utils.data import DataLoader

model = MyModel()
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
dataloader = DataLoader(dataset, batch_size=16)

model.train()
for epoch in range(num_epochs):
    for batch in dataloader:
        # æ‰‹åŠ¨å¤„ç†è®¾å¤‡è¿ç§»
        batch = {k: v.to(device) for k, v in batch.items()}
        
        # å‰å‘ä¼ æ’­
        outputs = model(**batch)
        loss = outputs.loss
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        # æ‰‹åŠ¨å®ç°æ¢¯åº¦è£å‰ªã€å­¦ä¹ ç‡è°ƒåº¦ã€æ—¥å¿—è®°å½•ã€ä¿å­˜æ£€æŸ¥ç‚¹...
        # æ•°ç™¾è¡Œä»£ç 

# âœ… Trainerï¼šä¸€é”®è®­ç»ƒ
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    learning_rate=5e-5
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

trainer.train()  # å®Œæˆæ‰€æœ‰è®­ç»ƒé€»è¾‘
```

**Trainer è‡ªåŠ¨å¤„ç†çš„åŠŸèƒ½**ï¼š

1. âœ… **è®­ç»ƒå¾ªç¯**ï¼šå‰å‘/åå‘ä¼ æ’­ã€æ¢¯åº¦æ›´æ–°
2. âœ… **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šå¤š GPUã€å¤šèŠ‚ç‚¹è‡ªåŠ¨æ”¯æŒ
3. âœ… **æ··åˆç²¾åº¦**ï¼šFP16/BF16 è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ
4. âœ… **æ¢¯åº¦ç´¯ç§¯**ï¼šæ¨¡æ‹Ÿå¤§æ‰¹æ¬¡è®­ç»ƒ
5. âœ… **æ¢¯åº¦è£å‰ª**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
6. âœ… **å­¦ä¹ ç‡è°ƒåº¦**ï¼šwarmupã€çº¿æ€§/ä½™å¼¦è¡°å‡
7. âœ… **æ£€æŸ¥ç‚¹ç®¡ç†**ï¼šè‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
8. âœ… **æ—¥å¿—è®°å½•**ï¼šTensorBoardã€WandB é›†æˆ
9. âœ… **è¯„ä¼°å¾ªç¯**ï¼šéªŒè¯é›†æ€§èƒ½ç›‘æ§
10. âœ… **æ—©åœ**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ

<div data-component="TrainingLoopVisualizer"></div>

### 5.1.2 Trainer æ¶æ„æ¦‚è§ˆ

**æ ¸å¿ƒç»„ä»¶å…³ç³»å›¾**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Trainer                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ TrainingArgs â”‚  â”‚    Model     â”‚  â”‚   Dataset    â”‚      â”‚
â”‚  â”‚   å‚æ•°é…ç½®    â”‚  â”‚   æ¨¡å‹å®ä¾‹    â”‚  â”‚   è®­ç»ƒæ•°æ®    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚DataCollator  â”‚  â”‚  Optimizer   â”‚  â”‚   Scheduler  â”‚      â”‚
â”‚  â”‚  æ‰¹å¤„ç†é€»è¾‘   â”‚  â”‚   ä¼˜åŒ–å™¨     â”‚  â”‚  å­¦ä¹ ç‡è°ƒåº¦   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Callbacks   â”‚  â”‚compute_metricsâ”‚ â”‚   Logging    â”‚      â”‚
â”‚  â”‚  å›è°ƒå‡½æ•°     â”‚  â”‚  è¯„ä¼°æŒ‡æ ‡    â”‚  â”‚   æ—¥å¿—ç³»ç»Ÿ    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.1.3 æœ€ç®€è®­ç»ƒç¤ºä¾‹

```python
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments
)
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹å’Œ tokenizer
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# 2. å‡†å¤‡æ•°æ®é›†
dataset = load_dataset("glue", "sst2")

def tokenize_function(examples):
    return tokenizer(examples["sentence"], truncation=True, padding=False)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# 3. é…ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",          # è¾“å‡ºç›®å½•
    num_train_epochs=3,              # è®­ç»ƒè½®æ•°
    per_device_train_batch_size=16,  # æ‰¹æ¬¡å¤§å°
    evaluation_strategy="epoch",     # æ¯ä¸ª epoch è¯„ä¼°ä¸€æ¬¡
    save_strategy="epoch",           # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡
    logging_dir="./logs"             # æ—¥å¿—ç›®å½•
)

# 4. åˆ›å»º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"]
)

# 5. å¼€å§‹è®­ç»ƒ
trainer.train()

# 6. è¯„ä¼°
metrics = trainer.evaluate()
print(metrics)

# 7. ä¿å­˜æ¨¡å‹
trainer.save_model("./final_model")
```

**è®­ç»ƒè¾“å‡ºç¤ºä¾‹**ï¼š

```
Epoch | Training Loss | Validation Loss | Runtime | Samples/s
------|---------------|-----------------|---------|----------
  1   |    0.4523     |     0.3891      | 120.5s  |  558.2
  2   |    0.3012     |     0.3654      | 118.3s  |  568.9
  3   |    0.2145     |     0.3721      | 119.1s  |  564.8

***** eval metrics *****
  eval_loss         = 0.3721
  eval_runtime      = 12.34s
  eval_samples_per_second = 687.5
```

---

## 5.2 TrainingArguments å‚æ•°è¯¦è§£

`TrainingArguments` åŒ…å« **100+ ä¸ªå‚æ•°**ï¼Œæ§åˆ¶è®­ç»ƒçš„æ–¹æ–¹é¢é¢ã€‚

<div data-component="TrainingArgumentsExplorer"></div>

### 5.2.1 åŸºç¡€å‚æ•°

**å¿…éœ€å‚æ•°**ï¼š

```python
from transformers import TrainingArguments

args = TrainingArguments(
    output_dir="./results",  # å¿…éœ€ï¼šæ¨¡å‹å’Œæ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
)
```

**æ ¸å¿ƒè®­ç»ƒå‚æ•°**ï¼š

```python
args = TrainingArguments(
    output_dir="./results",
    
    # === è®­ç»ƒè½®æ•° ===
    num_train_epochs=3,              # æ€» epoch æ•°
    # æˆ–ä½¿ç”¨
    max_steps=10000,                 # æ€»è®­ç»ƒæ­¥æ•°ï¼ˆä¸ num_train_epochs äºŒé€‰ä¸€ï¼‰
    
    # === æ‰¹æ¬¡å¤§å° ===
    per_device_train_batch_size=16,  # æ¯ä¸ª GPU çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°
    per_device_eval_batch_size=32,   # æ¯ä¸ª GPU çš„è¯„ä¼°æ‰¹æ¬¡å¤§å°ï¼ˆå¯ä»¥æ›´å¤§ï¼‰
    
    # === å­¦ä¹ ç‡ ===
    learning_rate=5e-5,              # åˆå§‹å­¦ä¹ ç‡
    weight_decay=0.01,               # L2 æ­£åˆ™åŒ–ç³»æ•°
    
    # === ä¼˜åŒ–å™¨ ===
    optim="adamw_torch",             # ä¼˜åŒ–å™¨ç±»å‹
    # å¯é€‰: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused,
    #      adafactor, adamw_anyprecision, sgd, adagrad
    
    # === å­¦ä¹ ç‡è°ƒåº¦ ===
    lr_scheduler_type="linear",      # å­¦ä¹ ç‡è°ƒåº¦å™¨
    # å¯é€‰: linear, cosine, cosine_with_restarts, polynomial, constant,
    #      constant_with_warmup
    warmup_ratio=0.1,                # warmup æ¯”ä¾‹ï¼ˆæ€»æ­¥æ•°çš„ 10%ï¼‰
    # æˆ–ä½¿ç”¨
    warmup_steps=500,                # warmup æ­¥æ•°ï¼ˆä¸ warmup_ratio äºŒé€‰ä¸€ï¼‰
)
```

### 5.2.2 è¯„ä¼°ä¸ä¿å­˜

```python
args = TrainingArguments(
    output_dir="./results",
    
    # === è¯„ä¼°ç­–ç•¥ ===
    evaluation_strategy="steps",     # è¯„ä¼°æ—¶æœº
    # å¯é€‰: "no"ï¼ˆä¸è¯„ä¼°ï¼‰, "steps"ï¼ˆæ¯ N æ­¥ï¼‰, "epoch"ï¼ˆæ¯ä¸ª epochï¼‰
    eval_steps=500,                  # æ¯ 500 æ­¥è¯„ä¼°ä¸€æ¬¡
    eval_delay=0,                    # å»¶è¿Ÿè¯„ä¼°ï¼ˆä»ç¬¬ N æ­¥å¼€å§‹ï¼‰
    
    # === ä¿å­˜ç­–ç•¥ ===
    save_strategy="steps",           # ä¿å­˜æ—¶æœºï¼ˆåŒ evaluation_strategyï¼‰
    save_steps=500,                  # æ¯ 500 æ­¥ä¿å­˜ä¸€æ¬¡
    save_total_limit=3,              # æœ€å¤šä¿ç•™ 3 ä¸ªæ£€æŸ¥ç‚¹ï¼ˆåˆ é™¤æ—§çš„ï¼‰
    
    # === æœ€ä½³æ¨¡å‹ ===
    load_best_model_at_end=True,     # è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹
    metric_for_best_model="eval_loss",  # ç”¨äºåˆ¤æ–­æœ€ä½³æ¨¡å‹çš„æŒ‡æ ‡
    greater_is_better=False,         # eval_loss è¶Šå°è¶Šå¥½
    
    # === æ—¥å¿—è®°å½• ===
    logging_dir="./logs",            # TensorBoard æ—¥å¿—ç›®å½•
    logging_strategy="steps",        # æ—¥å¿—è®°å½•æ—¶æœº
    logging_steps=100,               # æ¯ 100 æ­¥è®°å½•ä¸€æ¬¡
    logging_first_step=True,         # è®°å½•ç¬¬ä¸€æ­¥
    
    # === æŠ¥å‘Šé›†æˆ ===
    report_to=["tensorboard", "wandb"],  # ä¸ŠæŠ¥åˆ°å“ªäº›å¹³å°
    # å¯é€‰: tensorboard, wandb, mlflow, comet_ml, clearml, all, none
)
```

**ä¿å­˜ç›®å½•ç»“æ„**ï¼š

```
./results/
â”œâ”€â”€ checkpoint-500/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ model.safetensors
â”‚   â”œâ”€â”€ optimizer.pt
â”‚   â”œâ”€â”€ scheduler.pt
â”‚   â”œâ”€â”€ trainer_state.json
â”‚   â””â”€â”€ training_args.bin
â”œâ”€â”€ checkpoint-1000/
â”œâ”€â”€ checkpoint-1500/
â””â”€â”€ runs/  # TensorBoard æ—¥å¿—
    â””â”€â”€ ...
```

### 5.2.3 æ··åˆç²¾åº¦ä¸ä¼˜åŒ–

```python
args = TrainingArguments(
    output_dir="./results",
    
    # === æ··åˆç²¾åº¦è®­ç»ƒ ===
    fp16=True,                       # å¯ç”¨ FP16 æ··åˆç²¾åº¦ï¼ˆNvidia GPUï¼‰
    # æˆ–
    bf16=True,                       # å¯ç”¨ BF16 æ··åˆç²¾åº¦ï¼ˆAmpere+ GPUï¼Œæ›´ç¨³å®šï¼‰
    fp16_opt_level="O1",             # FP16 ä¼˜åŒ–çº§åˆ«ï¼ˆapexï¼‰
    
    # === æ¢¯åº¦ç´¯ç§¯ ===
    gradient_accumulation_steps=4,   # ç´¯ç§¯ 4 æ­¥å†æ›´æ–°ï¼ˆç›¸å½“äº batch_size Ã— 4ï¼‰
    
    # === æ¢¯åº¦è£å‰ª ===
    max_grad_norm=1.0,               # æ¢¯åº¦èŒƒæ•°è£å‰ªé˜ˆå€¼
    
    # === ä¼˜åŒ–æŠ€å·§ ===
    gradient_checkpointing=True,     # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼Œä½†é€Ÿåº¦æ…¢ 20%ï¼‰
    
    # === DataLoader ä¼˜åŒ– ===
    dataloader_num_workers=4,        # æ•°æ®åŠ è½½è¿›ç¨‹æ•°
    dataloader_pin_memory=True,      # å›ºå®šå†…å­˜ï¼ˆåŠ é€Ÿ GPU ä¼ è¾“ï¼‰
    
    # === ç¼–è¯‘ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰===
    torch_compile=True,              # å¯ç”¨ torch.compile ç¼–è¯‘
    torch_compile_backend="inductor", # ç¼–è¯‘åç«¯
    torch_compile_mode="default",    # ç¼–è¯‘æ¨¡å¼ï¼ˆdefault, reduce-overhead, max-autotuneï¼‰
)
```

**æ··åˆç²¾åº¦æ€§èƒ½å¯¹æ¯”**ï¼š

```python
# FP32ï¼ˆé»˜è®¤ï¼‰
# - æ˜¾å­˜å ç”¨: 100%
# - è®­ç»ƒé€Ÿåº¦: 1.0x
# - æ•°å€¼ç¨³å®šæ€§: æœ€ä½³

# FP16
# - æ˜¾å­˜å ç”¨: ~50%
# - è®­ç»ƒé€Ÿåº¦: 2-3x
# - æ•°å€¼ç¨³å®šæ€§: å¯èƒ½æº¢å‡ºï¼ˆéœ€è¦ loss scalingï¼‰

# BF16ï¼ˆæ¨èï¼ŒAmpere+ GPUï¼‰
# - æ˜¾å­˜å ç”¨: ~50%
# - è®­ç»ƒé€Ÿåº¦: 2-3x
# - æ•°å€¼ç¨³å®šæ€§: ä¼˜äº FP16ï¼ˆæ›´å¤§åŠ¨æ€èŒƒå›´ï¼‰
```

<div data-component="MixedPrecisionComparison"></div>

### 5.2.4 åˆ†å¸ƒå¼è®­ç»ƒ

```python
args = TrainingArguments(
    output_dir="./results",
    
    # === å¤š GPU ===
    # è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å¯ç”¨ GPUï¼ˆä½¿ç”¨ DataParallel æˆ– DistributedDataParallelï¼‰
    # æ— éœ€é¢å¤–é…ç½®
    
    # === DeepSpeed ===
    deepspeed="ds_config.json",      # DeepSpeed é…ç½®æ–‡ä»¶
    
    # === FSDPï¼ˆFully Sharded Data Parallelï¼‰===
    fsdp="full_shard auto_wrap",     # FSDP ç­–ç•¥
    fsdp_config={
        "min_num_params": 1e6,       # æœ€å°åˆ†ç‰‡å‚æ•°é‡
        "backward_prefetch": "backward_pre",
        "forward_prefetch": False,
        "cpu_offload": False
    },
    
    # === åˆ†å¸ƒå¼é‡‡æ · ===
    ddp_find_unused_parameters=False,  # æ˜¯å¦æŸ¥æ‰¾æœªä½¿ç”¨çš„å‚æ•°ï¼ˆæ…¢ï¼‰
    ddp_bucket_cap_mb=25,            # DDP é€šä¿¡æ¡¶å¤§å°
    
    # === æœ¬åœ°è¿›ç¨‹æ•° ===
    local_rank=-1,                   # æœ¬åœ°è¿›ç¨‹æ’åï¼ˆè‡ªåŠ¨è®¾ç½®ï¼‰
)
```

**å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ**ï¼š

```bash
# æ–¹å¼1ï¼šä½¿ç”¨ accelerateï¼ˆæ¨èï¼‰
accelerate launch train.py

# æ–¹å¼2ï¼šä½¿ç”¨ torchrunï¼ˆPyTorch åŸç”Ÿï¼‰
torchrun --nproc_per_node=4 train.py

# æ–¹å¼3ï¼šä½¿ç”¨ deepspeed
deepspeed train.py --deepspeed ds_config.json
```

### 5.2.5 å…¶ä»–é‡è¦å‚æ•°

```python
args = TrainingArguments(
    output_dir="./results",
    
    # === ç§å­ ===
    seed=42,                         # éšæœºç§å­ï¼ˆä¿è¯å¯å¤ç°ï¼‰
    data_seed=42,                    # æ•°æ®é‡‡æ ·ç§å­
    
    # === æ—©åœ ===
    # éœ€è¦é…åˆ EarlyStoppingCallback ä½¿ç”¨
    # metric_for_best_model å’Œ load_best_model_at_end å·²åœ¨å‰é¢è®¾ç½®
    
    # === æ¨é€åˆ° Hub ===
    push_to_hub=True,                # è®­ç»ƒç»“æŸåæ¨é€åˆ° Hugging Face Hub
    hub_model_id="my-model",         # Hub ä¸Šçš„æ¨¡å‹åç§°
    hub_strategy="every_save",       # æ¨é€ç­–ç•¥
    # å¯é€‰: end, every_save, checkpoint, all_checkpoints
    
    # === å…¶ä»– ===
    remove_unused_columns=True,      # è‡ªåŠ¨ç§»é™¤æ¨¡å‹ä¸éœ€è¦çš„åˆ—
    label_names=["labels"],          # æ ‡ç­¾åˆ—åç§°
    include_inputs_for_metrics=False, # æ˜¯å¦åœ¨ compute_metrics ä¸­åŒ…å«è¾“å…¥
    
    # === è°ƒè¯• ===
    debug="underflow_overflow",      # è°ƒè¯•æ¨¡å¼
    # å¯é€‰: underflow_overflowï¼ˆæ£€æµ‹æ•°å€¼é—®é¢˜ï¼‰
)
```

---

## 5.3 è®­ç»ƒå¾ªç¯è¯¦è§£

<div data-component="TrainingStepBreakdown"></div>

### 5.3.1 è®­ç»ƒæµç¨‹å‰–æ

**å®Œæ•´è®­ç»ƒå¾ªç¯ä¼ªä»£ç **ï¼š

```python
# Trainer.train() å†…éƒ¨æµç¨‹ï¼ˆç®€åŒ–ç‰ˆï¼‰

for epoch in range(num_epochs):
    model.train()
    
    for step, batch in enumerate(train_dataloader):
        # 1. å‰å‘ä¼ æ’­
        outputs = model(**batch)
        loss = outputs.loss
        
        # 2. æŸå¤±ç¼©æ”¾ï¼ˆæ··åˆç²¾åº¦ï¼‰
        if fp16:
            with amp.scale_loss(loss, optimizer) as scaled_loss:
                scaled_loss.backward()
        else:
            loss.backward()
        
        # 3. æ¢¯åº¦ç´¯ç§¯
        if (step + 1) % gradient_accumulation_steps == 0:
            # 4. æ¢¯åº¦è£å‰ª
            if max_grad_norm > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            
            # 5. ä¼˜åŒ–å™¨æ›´æ–°
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
            
            # 6. æ—¥å¿—è®°å½•
            if (step + 1) % logging_steps == 0:
                log_metrics({"loss": loss.item(), "lr": scheduler.get_last_lr()[0]})
            
            # 7. è¯„ä¼°
            if (step + 1) % eval_steps == 0:
                eval_metrics = evaluate()
            
            # 8. ä¿å­˜æ£€æŸ¥ç‚¹
            if (step + 1) % save_steps == 0:
                save_checkpoint()
```

### 5.3.2 è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤

**è¦†ç›– `training_step()` æ–¹æ³•**ï¼š

```python
from transformers import Trainer

class CustomTrainer(Trainer):
    def training_step(self, model, inputs):
        """è‡ªå®šä¹‰å•æ­¥è®­ç»ƒé€»è¾‘"""
        model.train()
        inputs = self._prepare_inputs(inputs)
        
        # å‰å‘ä¼ æ’­
        with self.compute_loss_context_manager():
            loss = self.compute_loss(model, inputs)
        
        # è‡ªå®šä¹‰æŸå¤±è°ƒæ•´ï¼ˆä¾‹å¦‚ï¼šæ·»åŠ æ­£åˆ™é¡¹ï¼‰
        l2_reg = sum(p.pow(2.0).sum() for p in model.parameters())
        loss = loss + 0.001 * l2_reg
        
        # åå‘ä¼ æ’­
        if self.args.gradient_accumulation_steps > 1:
            loss = loss / self.args.gradient_accumulation_steps
        
        self.accelerator.backward(loss)
        
        return loss.detach()
```

**è¦†ç›– `compute_loss()` æ–¹æ³•**ï¼š

```python
class CustomLossTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        """è‡ªå®šä¹‰æŸå¤±è®¡ç®—"""
        labels = inputs.pop("labels")
        
        # å‰å‘ä¼ æ’­
        outputs = model(**inputs)
        logits = outputs.logits
        
        # è‡ªå®šä¹‰æŸå¤±ï¼ˆä¾‹å¦‚ï¼šFocal Lossï¼‰
        ce_loss = torch.nn.functional.cross_entropy(logits, labels, reduction="none")
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** 2 * ce_loss).mean()
        
        return (focal_loss, outputs) if return_outputs else focal_loss
```

### 5.3.3 è¯„ä¼°å¾ªç¯

**è¦†ç›– `evaluation_loop()`**ï¼š

```python
class CustomEvalTrainer(Trainer):
    def evaluation_loop(self, dataloader, description, prediction_loss_only=None, ...):
        """è‡ªå®šä¹‰è¯„ä¼°å¾ªç¯"""
        model = self.model
        model.eval()
        
        all_preds = []
        all_labels = []
        total_loss = 0
        
        for step, inputs in enumerate(dataloader):
            with torch.no_grad():
                outputs = model(**inputs)
                loss = outputs.loss
                logits = outputs.logits
            
            total_loss += loss.item()
            all_preds.append(logits.cpu())
            all_labels.append(inputs["labels"].cpu())
        
        # è®¡ç®—æŒ‡æ ‡
        all_preds = torch.cat(all_preds)
        all_labels = torch.cat(all_labels)
        
        metrics = self.compute_metrics(
            EvalPrediction(predictions=all_preds, label_ids=all_labels)
        )
        metrics["eval_loss"] = total_loss / len(dataloader)
        
        return metrics
```

---

## 5.4 è¯„ä¼°æŒ‡æ ‡ï¼ˆcompute_metricsï¼‰

### 5.4.1 åŸºç¡€è¯„ä¼°å‡½æ•°

```python
from datasets import load_metric
import numpy as np

# æ–¹å¼1ï¼šä½¿ç”¨ datasets åº“çš„ metricï¼ˆæ¨èï¼‰
metric = load_metric("accuracy")

def compute_metrics(eval_pred):
    """è®¡ç®—å‡†ç¡®ç‡"""
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# ä½¿ç”¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics
)
```

**å¤šæŒ‡æ ‡è¯„ä¼°**ï¼š

```python
from sklearn.metrics import precision_recall_fscore_support, accuracy_score

def compute_metrics(eval_pred):
    """è®¡ç®—å¤šä¸ªåˆ†ç±»æŒ‡æ ‡"""
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='weighted'
    )
    acc = accuracy_score(labels, predictions)
    
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }
```

### 5.4.2 NLP ä»»åŠ¡è¯„ä¼°

**åºåˆ—æ ‡æ³¨ï¼ˆNERï¼‰**ï¼š

```python
from datasets import load_metric

seqeval = load_metric("seqeval")

def compute_metrics(eval_pred):
    """NER è¯„ä¼°"""
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    # ç§»é™¤ paddingï¼ˆ-100ï¼‰
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_preds = [
        [label_names[p] for (p, l) in zip(pred, label) if l != -100]
        for pred, label in zip(predictions, labels)
    ]
    
    results = seqeval.compute(predictions=true_preds, references=true_labels)
    
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"]
    }
```

**é—®ç­”ï¼ˆSQuADï¼‰**ï¼š

```python
from datasets import load_metric

squad_metric = load_metric("squad")

def compute_metrics(eval_pred):
    """SQuAD è¯„ä¼°"""
    predictions, labels = eval_pred
    
    # predictions: (start_logits, end_logits)
    # labels: (start_positions, end_positions)
    
    # è§£ç é¢„æµ‹ç­”æ¡ˆï¼ˆéœ€è¦å®ç° postprocess é€»è¾‘ï¼‰
    predictions = postprocess_qa_predictions(
        examples=eval_examples,
        features=eval_features,
        predictions=predictions
    )
    
    references = [{"id": ex["id"], "answers": ex["answers"]} for ex in eval_examples]
    
    return squad_metric.compute(predictions=predictions, references=references)
```

**æ–‡æœ¬ç”Ÿæˆï¼ˆROUGEï¼‰**ï¼š

```python
from datasets import load_metric

rouge = load_metric("rouge")

def compute_metrics(eval_pred):
    """æ‘˜è¦ç”Ÿæˆè¯„ä¼°"""
    predictions, labels = eval_pred
    
    # è§£ç  token IDs ä¸ºæ–‡æœ¬
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    
    # æ›¿æ¢ -100ï¼ˆç”¨äº label paddingï¼‰
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    
    # ROUGE éœ€è¦æ¢è¡Œåˆ†éš”
    decoded_preds = ["\n".join(pred.split()) for pred in decoded_preds]
    decoded_labels = ["\n".join(label.split()) for label in decoded_labels]
    
    result = rouge.compute(
        predictions=decoded_preds,
        references=decoded_labels,
        use_stemmer=True
    )
    
    return {
        "rouge1": result["rouge1"].mid.fmeasure,
        "rouge2": result["rouge2"].mid.fmeasure,
        "rougeL": result["rougeL"].mid.fmeasure
    }
```

### 5.4.3 è‡ªå®šä¹‰å¤æ‚æŒ‡æ ‡

```python
def compute_metrics(eval_pred):
    """è‡ªå®šä¹‰å¤æ‚æŒ‡æ ‡"""
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    # 1. æ•´ä½“å‡†ç¡®ç‡
    acc = accuracy_score(labels, predictions)
    
    # 2. æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    per_class_acc = {}
    for i, label_name in enumerate(label_names):
        mask = labels == i
        if mask.sum() > 0:
            per_class_acc[f"acc_{label_name}"] = accuracy_score(
                labels[mask], predictions[mask]
            )
    
    # 3. æ··æ·†çŸ©é˜µ
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(labels, predictions)
    
    # 4. ç½®ä¿¡åº¦åˆ†æ
    probs = softmax(logits, axis=-1)
    max_probs = np.max(probs, axis=-1)
    avg_confidence = max_probs.mean()
    correct_confidence = max_probs[predictions == labels].mean()
    wrong_confidence = max_probs[predictions != labels].mean()
    
    return {
        "accuracy": acc,
        **per_class_acc,
        "avg_confidence": avg_confidence,
        "correct_confidence": correct_confidence,
        "wrong_confidence": wrong_confidence
    }
```

---

## 5.5 å›è°ƒå‡½æ•°ï¼ˆCallbacksï¼‰

å›è°ƒå‡½æ•°å…è®¸åœ¨è®­ç»ƒçš„ç‰¹å®šæ—¶åˆ»æ’å…¥è‡ªå®šä¹‰é€»è¾‘ï¼ˆå¦‚æ—©åœã€å­¦ä¹ ç‡è°ƒæ•´ã€è‡ªå®šä¹‰æ—¥å¿—ç­‰ï¼‰ã€‚

<div data-component="CallbackFlow"></div>

### 5.5.1 å†…ç½®å›è°ƒ

**1. EarlyStoppingCallback - æ—©åœ**

```python
from transformers import EarlyStoppingCallback, Trainer

# åˆ›å»ºæ—©åœå›è°ƒ
early_stopping = EarlyStoppingCallback(
    early_stopping_patience=3,  # è¿ç»­ 3 æ¬¡è¯„ä¼°æ²¡æœ‰æ”¹å–„åˆ™åœæ­¢
    early_stopping_threshold=0.0  # æ”¹å–„é˜ˆå€¼
)

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    callbacks=[early_stopping]  # æ·»åŠ å›è°ƒ
)

trainer.train()
```

**2. TensorBoardCallback - TensorBoard é›†æˆ**

```python
from transformers import TrainerCallback

# é»˜è®¤å·²åŒ…å«ï¼Œæ— éœ€æ‰‹åŠ¨æ·»åŠ 
# æŸ¥çœ‹æ—¥å¿—ï¼š
# tensorboard --logdir ./logs
```

**3. ProgressCallback - è¿›åº¦æ¡**

```python
# é»˜è®¤åŒ…å«ï¼Œæ˜¾ç¤ºè®­ç»ƒè¿›åº¦
# å¯ä»¥ç¦ç”¨ï¼š
training_args = TrainingArguments(
    output_dir="./results",
    disable_tqdm=True  # ç¦ç”¨è¿›åº¦æ¡
)
```

### 5.5.2 è‡ªå®šä¹‰å›è°ƒ

**åŸºç¡€è‡ªå®šä¹‰å›è°ƒ**ï¼š

```python
from transformers import TrainerCallback

class CustomCallback(TrainerCallback):
    """è‡ªå®šä¹‰å›è°ƒç¤ºä¾‹"""
    
    def on_train_begin(self, args, state, control, **kwargs):
        """è®­ç»ƒå¼€å§‹æ—¶"""
        print("ğŸš€ è®­ç»ƒå¼€å§‹ï¼")
    
    def on_epoch_begin(self, args, state, control, **kwargs):
        """æ¯ä¸ª epoch å¼€å§‹æ—¶"""
        print(f"ğŸ“Š Epoch {state.epoch} å¼€å§‹")
    
    def on_step_end(self, args, state, control, **kwargs):
        """æ¯æ­¥ç»“æŸæ—¶"""
        if state.global_step % 100 == 0:
            print(f"âœ… å®Œæˆ {state.global_step} æ­¥")
    
    def on_evaluate(self, args, state, control, metrics, **kwargs):
        """è¯„ä¼°å"""
        print(f"ğŸ“ˆ è¯„ä¼°ç»“æœ: {metrics}")
    
    def on_save(self, args, state, control, **kwargs):
        """ä¿å­˜æ£€æŸ¥ç‚¹å"""
        print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {state.global_step} æ­¥")
    
    def on_train_end(self, args, state, control, **kwargs):
        """è®­ç»ƒç»“æŸæ—¶"""
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")

# ä½¿ç”¨
trainer = Trainer(
    model=model,
    args=training_args,
    callbacks=[CustomCallback()]
)
```

**é«˜çº§å›è°ƒï¼šå­¦ä¹ ç‡é‡å¯**

```python
class CosineRestartCallback(TrainerCallback):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡é‡å¯"""
    
    def __init__(self, restart_every=1000):
        self.restart_every = restart_every
    
    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step % self.restart_every == 0:
            # é‡ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
            optimizer = kwargs["optimizer"]
            for param_group in optimizer.param_groups:
                param_group["lr"] = args.learning_rate
            print(f"ğŸ”„ å­¦ä¹ ç‡é‡å¯: {args.learning_rate}")
```

**æ¢¯åº¦ç›‘æ§å›è°ƒ**ï¼š

```python
class GradientMonitorCallback(TrainerCallback):
    """ç›‘æ§æ¢¯åº¦èŒƒæ•°"""
    
    def on_step_end(self, args, state, control, model, **kwargs):
        if state.global_step % args.logging_steps == 0:
            total_norm = 0
            for p in model.parameters():
                if p.grad is not None:
                    param_norm = p.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
            total_norm = total_norm ** 0.5
            
            print(f"ğŸ“ æ¢¯åº¦èŒƒæ•°: {total_norm:.4f}")
            
            # è®°å½•åˆ° TensorBoard
            if hasattr(state, "log_history"):
                state.log_history[-1]["grad_norm"] = total_norm
```

**WandB é›†æˆå›è°ƒ**ï¼š

```python
import wandb

class WandbCallback(TrainerCallback):
    """Weights & Biases é›†æˆ"""
    
    def on_train_begin(self, args, state, control, model, **kwargs):
        wandb.init(
            project="my-project",
            name=args.run_name,
            config=args.to_dict()
        )
        wandb.watch(model)
    
    def on_log(self, args, state, control, logs, **kwargs):
        wandb.log(logs)
    
    def on_train_end(self, args, state, control, **kwargs):
        wandb.finish()
```

### 5.5.3 å›è°ƒæ‰§è¡Œé¡ºåº

**æ‰€æœ‰å›è°ƒäº‹ä»¶**ï¼š

```python
# è®­ç»ƒç”Ÿå‘½å‘¨æœŸäº‹ä»¶ï¼ˆæŒ‰è°ƒç”¨é¡ºåºï¼‰
on_init_end              # Trainer åˆå§‹åŒ–å®Œæˆ
on_train_begin           # è®­ç»ƒå¼€å§‹å‰
on_epoch_begin           # æ¯ä¸ª epoch å¼€å§‹å‰
  on_step_begin          # æ¯æ­¥å¼€å§‹å‰
  on_substep_end         # æ¢¯åº¦ç´¯ç§¯å­æ­¥éª¤ç»“æŸï¼ˆå¦‚æœå¯ç”¨ï¼‰
  on_step_end            # æ¯æ­¥ç»“æŸå
  on_log                 # æ—¥å¿—è®°å½•æ—¶
  on_evaluate            # è¯„ä¼°æ—¶
  on_save                # ä¿å­˜æ£€æŸ¥ç‚¹æ—¶
  on_prediction_step     # é¢„æµ‹æ­¥éª¤æ—¶
on_epoch_end             # æ¯ä¸ª epoch ç»“æŸå
on_train_end             # è®­ç»ƒç»“æŸå
```

---

## 5.6 ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡è°ƒåº¦

### 5.6.1 è‡ªå®šä¹‰ä¼˜åŒ–å™¨

**ä½¿ç”¨ä¸åŒä¼˜åŒ–å™¨**ï¼š

```python
from transformers import Trainer, TrainingArguments
from torch.optim import SGD, Adam

class CustomOptimizerTrainer(Trainer):
    def create_optimizer(self):
        """è‡ªå®šä¹‰ä¼˜åŒ–å™¨"""
        # æ–¹å¼1ï¼šä½¿ç”¨ PyTorch åŸç”Ÿä¼˜åŒ–å™¨
        self.optimizer = SGD(
            self.model.parameters(),
            lr=self.args.learning_rate,
            momentum=0.9,
            weight_decay=self.args.weight_decay
        )
        
        # æ–¹å¼2ï¼šä¸åŒå±‚ä¸åŒå­¦ä¹ ç‡
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in self.model.named_parameters() if "bias" not in n],
                "weight_decay": self.args.weight_decay,
            },
            {
                "params": [p for n, p in self.model.named_parameters() if "bias" in n],
                "weight_decay": 0.0,
            },
        ]
        self.optimizer = Adam(optimizer_grouped_parameters, lr=self.args.learning_rate)
```

**å±‚çº§å­¦ä¹ ç‡ï¼ˆLayer-wise Learning Rate Decayï¼‰**ï¼š

```python
def get_optimizer_grouped_parameters(model, learning_rate, weight_decay, layerwise_lr_decay=0.95):
    """ä¸ºä¸åŒå±‚è®¾ç½®ä¸åŒå­¦ä¹ ç‡"""
    no_decay = ["bias", "LayerNorm.weight"]
    
    # BERT æœ‰ 12 å±‚ï¼ˆlayer.0 åˆ° layer.11ï¼‰
    num_layers = model.config.num_hidden_layers
    layers = [model.bert.embeddings] + list(model.bert.encoder.layer)
    layers.reverse()  # ä»é¡¶å±‚åˆ°åº•å±‚
    
    optimizer_grouped_parameters = []
    
    for i, layer in enumerate(layers):
        # è®¡ç®—å½“å‰å±‚çš„å­¦ä¹ ç‡ï¼ˆé¡¶å±‚æœ€å¤§ï¼Œåº•å±‚é€æ¸è¡°å‡ï¼‰
        lr = learning_rate * (layerwise_lr_decay ** i)
        
        optimizer_grouped_parameters += [
            {
                "params": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],
                "weight_decay": weight_decay,
                "lr": lr,
            },
            {
                "params": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
                "lr": lr,
            },
        ]
    
    return optimizer_grouped_parameters

# ä½¿ç”¨
class LayerwiseLRTrainer(Trainer):
    def create_optimizer(self):
        self.optimizer = torch.optim.AdamW(
            get_optimizer_grouped_parameters(
                self.model,
                learning_rate=self.args.learning_rate,
                weight_decay=self.args.weight_decay,
                layerwise_lr_decay=0.95
            )
        )
```

### 5.6.2 è‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨

```python
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts

class CustomSchedulerTrainer(Trainer):
    def create_scheduler(self, num_training_steps, optimizer=None):
        """è‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if optimizer is None:
            optimizer = self.optimizer
        
        # ä½™å¼¦é€€ç« + é‡å¯
        self.lr_scheduler = CosineAnnealingWarmRestarts(
            optimizer,
            T_0=1000,  # ç¬¬ä¸€æ¬¡é‡å¯å‘¨æœŸ
            T_mult=2,  # æ¯æ¬¡é‡å¯å‘¨æœŸç¿»å€
            eta_min=1e-6  # æœ€å°å­¦ä¹ ç‡
        )
```

**Warmup + Linear Decayï¼ˆæ‰‹åŠ¨å®ç°ï¼‰**ï¼š

```python
from torch.optim.lr_scheduler import LambdaLR

def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):
    """Warmup + çº¿æ€§è¡°å‡"""
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            # Warmup é˜¶æ®µï¼šçº¿æ€§å¢é•¿
            return float(current_step) / float(max(1, num_warmup_steps))
        # è¡°å‡é˜¶æ®µï¼šçº¿æ€§è¡°å‡åˆ° 0
        return max(
            0.0,
            float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))
        )
    
    return LambdaLR(optimizer, lr_lambda)

# ä½¿ç”¨
class WarmupLinearTrainer(Trainer):
    def create_scheduler(self, num_training_steps, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        
        num_warmup_steps = int(num_training_steps * 0.1)  # 10% warmup
        self.lr_scheduler = get_linear_schedule_with_warmup(
            optimizer, num_warmup_steps, num_training_steps
        )
```

<div data-component="LearningRateScheduler"></div>

---

## 5.7 é«˜çº§è®­ç»ƒæŠ€å·§

### 5.7.1 æ¢¯åº¦ç´¯ç§¯

**åŸç†**ï¼šæ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡å¤§å°ï¼Œé¿å…æ˜¾å­˜ä¸è¶³ã€‚

```python
# å‡è®¾æ˜¾å­˜åªèƒ½å®¹çº³ batch_size=8ï¼Œä½†æƒ³è¦ batch_size=32 çš„æ•ˆæœ
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,     # å®é™…æ‰¹æ¬¡å¤§å°
    gradient_accumulation_steps=4,     # ç´¯ç§¯ 4 æ­¥
    # ç­‰æ•ˆæ‰¹æ¬¡å¤§å° = 8 Ã— 4 = 32
)

# Trainer è‡ªåŠ¨å¤„ç†ï¼š
# - æ¯ 4 æ­¥æ‰æ›´æ–°ä¸€æ¬¡å‚æ•°
# - æŸå¤±è‡ªåŠ¨é™¤ä»¥ 4
# - å­¦ä¹ ç‡è°ƒåº¦å™¨åœ¨ç´¯ç§¯å®Œæˆåæ‰æ­¥è¿›
```

**å®æˆ˜ç¤ºä¾‹**ï¼š

```python
# ç›®æ ‡ï¼šåœ¨ 1 å¼  GPUï¼ˆ16GBï¼‰ä¸Šè®­ç»ƒ GPT-2 Large
# é—®é¢˜ï¼šå®Œæ•´æ‰¹æ¬¡éœ€è¦ 32GB æ˜¾å­˜

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=1,     # å•å¼  GPU åªèƒ½æ”¾ 1 ä¸ªæ ·æœ¬
    gradient_accumulation_steps=32,    # ç´¯ç§¯ 32 æ­¥
    # ç­‰æ•ˆæ‰¹æ¬¡å¤§å° = 32ï¼ˆä¸å¤š GPU è®­ç»ƒç›¸åŒï¼‰
    
    fp16=True,  # æ··åˆç²¾åº¦è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜
    gradient_checkpointing=True  # ç‰ºç‰² 20% é€Ÿåº¦æ¢å– 50% æ˜¾å­˜
)
```

### 5.7.2 æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰

**åŸç†**ï¼šä¸ä¿å­˜æ‰€æœ‰ä¸­é—´æ¿€æ´»å€¼ï¼Œåå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—ã€‚

```python
# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model.gradient_checkpointing_enable()

# æˆ–åœ¨ TrainingArguments ä¸­è®¾ç½®
training_args = TrainingArguments(
    output_dir="./results",
    gradient_checkpointing=True
)

# æ•ˆæœï¼š
# - æ˜¾å­˜å ç”¨å‡å°‘ ~50%
# - è®­ç»ƒé€Ÿåº¦é™ä½ ~20%
# - é€‚åˆæ˜¾å­˜ä¸è¶³æ—¶ä½¿ç”¨
```

**è‡ªå®šä¹‰æ£€æŸ¥ç‚¹ç­–ç•¥**ï¼š

```python
# åªå¯¹éƒ¨åˆ†å±‚å¯ç”¨æ£€æŸ¥ç‚¹
from torch.utils.checkpoint import checkpoint

class SelectiveCheckpointModel(torch.nn.Module):
    def forward(self, x):
        # å‰å‡ å±‚æ­£å¸¸è®¡ç®—
        x = self.layer1(x)
        x = self.layer2(x)
        
        # ä¸­é—´å±‚ä½¿ç”¨æ£€æŸ¥ç‚¹ï¼ˆæœ€è€—æ˜¾å­˜ï¼‰
        x = checkpoint(self.layer3, x)
        x = checkpoint(self.layer4, x)
        
        # æœ€åå‡ å±‚æ­£å¸¸è®¡ç®—
        x = self.layer5(x)
        return x
```

### 5.7.3 æ¨¡å‹å¹¶è¡Œï¼ˆModel Parallelismï¼‰

**è®¾å¤‡æ˜ å°„ï¼ˆDevice Mapï¼‰**ï¼š

```python
from transformers import AutoModelForCausalLM

# è‡ªåŠ¨è·¨ GPU åˆ†é…å±‚
model = AutoModelForCausalLM.from_pretrained(
    "gpt-j-6B",
    device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°å¤šå¼  GPU
    torch_dtype=torch.float16
)

# æ‰‹åŠ¨æŒ‡å®šè®¾å¤‡æ˜ å°„
device_map = {
    "transformer.wte": 0,      # Embedding å±‚åœ¨ GPU 0
    "transformer.h.0": 0,      # ç¬¬ 0 å±‚åœ¨ GPU 0
    "transformer.h.1": 0,
    "transformer.h.2": 1,      # ç¬¬ 2 å±‚åœ¨ GPU 1
    # ...
    "lm_head": 3               # è¾“å‡ºå±‚åœ¨ GPU 3
}

model = AutoModelForCausalLM.from_pretrained(
    "gpt-j-6B",
    device_map=device_map,
    torch_dtype=torch.float16
)
```

### 5.7.4 8-bit / 4-bit è®­ç»ƒ

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 8-bit é‡åŒ–åŠ è½½
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b",
    quantization_config=quantization_config,
    device_map="auto"
)

# 4-bit é‡åŒ–ï¼ˆQLoRAï¼‰
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b",
    quantization_config=quantization_config,
    device_map="auto"
)

# æ˜¾å­˜å ç”¨å¯¹æ¯”ï¼š
# - FP32: ~28GB
# - FP16: ~14GB
# - 8-bit: ~7GB
# - 4-bit: ~3.5GB
```

---

## 5.8 å®æˆ˜æ¡ˆä¾‹

<div data-component="TrainingMetricsPlot"></div>

### 5.8.1 æƒ…æ„Ÿåˆ†ç±»å®Œæ•´æµç¨‹

```python
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
    EarlyStoppingCallback
)
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

# === 1. æ•°æ®å‡†å¤‡ ===
dataset = load_dataset("imdb")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4)
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets = tokenized_datasets.remove_columns(["text"])

# è®­ç»ƒé›†é‡‡æ ·ï¼ˆå¿«é€Ÿå®éªŒï¼‰
small_train = tokenized_datasets["train"].shuffle(seed=42).select(range(5000))
small_eval = tokenized_datasets["test"].shuffle(seed=42).select(range(500))

# === 2. æ¨¡å‹åˆå§‹åŒ– ===
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

# === 3. è¯„ä¼°æŒ‡æ ‡ ===
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average="weighted")
    return {"accuracy": acc, "f1": f1}

# === 4. è®­ç»ƒé…ç½® ===
training_args = TrainingArguments(
    output_dir="./imdb-bert",
    
    # åŸºç¡€å‚æ•°
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    
    # ä¼˜åŒ–å™¨
    learning_rate=2e-5,
    weight_decay=0.01,
    warmup_ratio=0.1,
    
    # æ··åˆç²¾åº¦
    fp16=True,
    
    # è¯„ä¼°ä¸ä¿å­˜
    evaluation_strategy="steps",
    eval_steps=200,
    save_strategy="steps",
    save_steps=200,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    greater_is_better=True,
    
    # æ—¥å¿—
    logging_dir="./logs",
    logging_steps=50,
    report_to=["tensorboard"],
    
    # å…¶ä»–
    seed=42,
    dataloader_num_workers=4,
)

# === 5. DataCollator ===
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# === 6. Trainer ===
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train,
    eval_dataset=small_eval,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

# === 7. è®­ç»ƒ ===
trainer.train()

# === 8. è¯„ä¼° ===
metrics = trainer.evaluate()
print(f"Final Metrics: {metrics}")

# === 9. ä¿å­˜ ===
trainer.save_model("./imdb-bert-final")

# === 10. æ¨ç†æµ‹è¯• ===
test_texts = [
    "This movie was absolutely fantastic!",
    "Worst film I've ever seen."
]

inputs = tokenizer(test_texts, return_tensors="pt", padding=True, truncation=True)
outputs = model(**inputs)
predictions = torch.argmax(outputs.logits, dim=-1)

print(f"Predictions: {predictions}")  # tensor([1, 0]) - positive, negative
```

### 5.8.2 å¤š GPU è®­ç»ƒ

```python
# æ–¹å¼1ï¼šä½¿ç”¨ Accelerateï¼ˆæ¨èï¼‰
# accelerate config  # é¦–æ¬¡è¿è¡Œï¼Œé…ç½®åˆ†å¸ƒå¼ç­–ç•¥
# accelerate launch train.py

# train.py
from accelerate import Accelerator

accelerator = Accelerator()

# æ­£å¸¸å®šä¹‰æ¨¡å‹ã€æ•°æ®ã€ä¼˜åŒ–å™¨
model, optimizer, train_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader
)

# è®­ç»ƒå¾ªç¯ï¼ˆAccelerator è‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼ï¼‰
for batch in train_dataloader:
    outputs = model(**batch)
    loss = outputs.loss
    accelerator.backward(loss)
    optimizer.step()
    optimizer.zero_grad()

# æ–¹å¼2ï¼šTrainer è‡ªåŠ¨æ£€æµ‹
# æ— éœ€é¢å¤–ä»£ç ï¼ŒTrainer è‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰å¯è§ GPU
trainer = Trainer(...)
trainer.train()

# å¯åŠ¨ï¼š
# python -m torch.distributed.launch --nproc_per_node=4 train.py
# æˆ–
# torchrun --nproc_per_node=4 train.py
```

---

## æœ¬ç« æ€»ç»“

**æ ¸å¿ƒè¦ç‚¹**ï¼š

1. âœ… **Trainer ä¼˜åŠ¿**ï¼šè‡ªåŠ¨åŒ–è®­ç»ƒæµç¨‹ï¼Œæ”¯æŒåˆ†å¸ƒå¼ã€æ··åˆç²¾åº¦ã€æ£€æŸ¥ç‚¹ç®¡ç†
2. âœ… **TrainingArguments**ï¼š100+ å‚æ•°æ§åˆ¶è®­ç»ƒç»†èŠ‚
3. âœ… **è¯„ä¼°æŒ‡æ ‡**ï¼šcompute_metrics è‡ªå®šä¹‰ä»»åŠ¡è¯„ä¼°
4. âœ… **å›è°ƒå‡½æ•°**ï¼šæ—©åœã€è‡ªå®šä¹‰æ—¥å¿—ã€WandB é›†æˆ
5. âœ… **ä¼˜åŒ–æŠ€å·§**ï¼šæ¢¯åº¦ç´¯ç§¯ã€æ¢¯åº¦æ£€æŸ¥ç‚¹ã€å±‚çº§å­¦ä¹ ç‡ã€é‡åŒ–è®­ç»ƒ
6. âœ… **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šå¤š GPUã€FSDPã€DeepSpeed æ— ç¼é›†æˆ

**æœ€ä½³å®è·µ**ï¼š

- ä½¿ç”¨ `fp16=True` æˆ– `bf16=True` åŠ é€Ÿè®­ç»ƒ
- æ˜¾å­˜ä¸è¶³æ—¶ï¼šæ¢¯åº¦ç´¯ç§¯ + æ¢¯åº¦æ£€æŸ¥ç‚¹
- å¤§æ¨¡å‹è®­ç»ƒï¼šdevice_map="auto" + 8-bit/4-bit é‡åŒ–
- å®éªŒé˜¶æ®µï¼šå°æ•°æ®é›† + å¿«é€Ÿè¿­ä»£
- ç”Ÿäº§è®­ç»ƒï¼šæ—©åœ + å¤šæŒ‡æ ‡ç›‘æ§ + æ£€æŸ¥ç‚¹ç®¡ç†

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š  
Chapter 6 å°†å­¦ä¹  **PEFTï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰**ï¼ŒåŒ…æ‹¬ LoRAã€QLoRAã€Adapterã€Prefix Tuning ç­‰æŠ€æœ¯ï¼Œç”¨ 1% çš„å‚æ•°é‡å®ç°æ¥è¿‘å…¨é‡å¾®è°ƒçš„æ•ˆæœã€‚

---

## ç»ƒä¹ é¢˜

1. **åŸºç¡€é¢˜**ï¼šä½¿ç”¨ Trainer åœ¨ GLUE çš„ SST-2 ä»»åŠ¡ä¸Šå¾®è°ƒ BERTï¼Œè®°å½•æœ€ä½³ F1 åˆ†æ•°å’Œè®­ç»ƒæ—¶é•¿ã€‚

2. **è¿›é˜¶é¢˜**ï¼šå®ç°ä¸€ä¸ªè‡ªå®šä¹‰å›è°ƒå‡½æ•°ï¼Œç›‘æ§æ¯ä¸ª epoch çš„æ¢¯åº¦èŒƒæ•°ã€å­¦ä¹ ç‡ã€éªŒè¯é›† lossï¼Œå¹¶ç»˜åˆ¶æ›²çº¿å›¾ã€‚

3. **æŒ‘æˆ˜é¢˜**ï¼šåœ¨å•å¼  GPUï¼ˆ12GBï¼‰ä¸Šä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ + æ¢¯åº¦æ£€æŸ¥ç‚¹è®­ç»ƒ RoBERTa-Largeï¼ˆ355M å‚æ•°ï¼‰ã€‚å¯¹æ¯”ä¸åŒ `gradient_accumulation_steps`ï¼ˆ2/4/8ï¼‰å’Œ `gradient_checkpointing`ï¼ˆå¼€/å…³ï¼‰ç»„åˆçš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦ã€‚

4. **æ€è€ƒé¢˜**ï¼šä¸ºä»€ä¹ˆ BF16 æ¯” FP16 æ›´ç¨³å®šï¼Ÿåœ¨ä»€ä¹ˆç¡¬ä»¶ä¸Šå¯ä»¥ä½¿ç”¨ BF16ï¼Ÿæ¢¯åº¦ç´¯ç§¯ä¼šå½±å“ BatchNorm çš„è¡Œä¸ºå—ï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ
