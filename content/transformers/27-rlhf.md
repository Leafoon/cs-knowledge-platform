# Chapter 27: å¼ºåŒ–å­¦ä¹ ä¸ RLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰

å¤§è¯­è¨€æ¨¡å‹çš„æˆåŠŸä¸ä»…åœ¨äºé¢„è®­ç»ƒï¼Œæ›´åœ¨äºå¦‚ä½•å°†å…¶**å¯¹é½ï¼ˆAlignï¼‰**åˆ°äººç±»åå¥½ã€‚æœ¬ç« å°†æ·±å…¥å­¦ä¹  **RLHF**ï¼ˆReinforcement Learning from Human Feedbackï¼‰æŠ€æœ¯ï¼Œè¿™æ˜¯ ChatGPTã€GPT-4ã€Claude ç­‰æ¨¡å‹èƒŒåçš„å…³é”®æŠ€æœ¯ã€‚æˆ‘ä»¬å°†å­¦ä¹  InstructGPT çš„ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼ˆSFT â†’ RM â†’ PPOï¼‰ã€TRL åº“çš„ä½¿ç”¨ã€DPOï¼ˆDirect Preference Optimizationï¼‰ç­‰å…ˆè¿›æ–¹æ³•ï¼Œä»¥åŠå®æˆ˜æŒ‡ä»¤å¾®è°ƒ LLaMAã€‚

---

## 27.1 RLHF åŸºç¡€æ¦‚å¿µ

### 27.1.1 ä¸ºä»€ä¹ˆéœ€è¦ RLHFï¼Ÿ

é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è™½ç„¶å¼ºå¤§ï¼Œä½†å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
- **ä¸éµå¾ªæŒ‡ä»¤**ï¼šç”Ÿæˆå†…å®¹å¯èƒ½åç¦»ç”¨æˆ·æ„å›¾
- **äº§ç”Ÿæœ‰å®³å†…å®¹**ï¼šå¯èƒ½ç”Ÿæˆæœ‰æ¯’ã€åè§ã€è™šå‡ä¿¡æ¯
- **å†—é•¿å•°å—¦**ï¼šç”Ÿæˆè¿‡å¤šæ— å…³å†…å®¹
- **ç¼ºä¹ä¸€è‡´æ€§**ï¼šä¸åŒè¾“å…¥ä¸‹è¡Œä¸ºä¸ä¸€è‡´

**RLHF ç›®æ ‡**ï¼šé€šè¿‡äººç±»åé¦ˆï¼Œä½¿æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½çš„å†…å®¹ã€‚

### 27.1.2 ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹

<div data-component="RLHFPipeline"></div>

**InstructGPT æµç¨‹**ï¼ˆOpenAI 2022ï¼‰ï¼š

#### **é˜¶æ®µ 1ï¼šç›‘ç£å¾®è°ƒï¼ˆSFT, Supervised Fine-Tuningï¼‰**

**ç›®æ ‡**ï¼šè®©æ¨¡å‹å­¦ä¼šéµå¾ªæŒ‡ä»¤

**æ•°æ®**ï¼šäººå·¥æ ‡æ³¨çš„é«˜è´¨é‡æŒ‡ä»¤-å›å¤å¯¹
```
è¾“å…¥ï¼šWrite a poem about AI
è¾“å‡ºï¼šIn circuits deep and code so bright,
      A mind emerges, shining light...
```

**è®­ç»ƒæ–¹å¼**ï¼šæ ‡å‡†è¯­è¨€æ¨¡å‹è®­ç»ƒï¼ˆæœ€å¤§åŒ– log æ¦‚ç‡ï¼‰
$$
\mathcal{L}_{\text{SFT}} = -\sum_{i=1}^{N} \log P_\theta(y_i | x_i)
$$

**ä»£ç ç¤ºä¾‹**ï¼ˆä½¿ç”¨ TRLï¼‰ï¼š
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import SFTTrainer
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# 2. åŠ è½½æ•°æ®é›†
dataset = load_dataset("timdettmers/openassistant-guanaco", split="train")

# 3. è®­ç»ƒ
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",  # æ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µ
    max_seq_length=512,
    packing=True,  # æ‰“åŒ…çŸ­æ ·æœ¬æé«˜æ•ˆç‡
)

trainer.train()
```

#### **é˜¶æ®µ 2ï¼šå¥–åŠ±æ¨¡å‹è®­ç»ƒï¼ˆRM, Reward Modelï¼‰**

**ç›®æ ‡**ï¼šå­¦ä¹ äººç±»åå¥½å‡½æ•°

**æ•°æ®**ï¼šäººå·¥æ ‡æ³¨çš„åå¥½å¯¹ï¼ˆpreferred vs rejectedï¼‰
```
Prompt: Explain quantum computing
Output A (preferred): Quantum computing uses quantum bits...
Output B (rejected): Quantum is like magic computers...
```

**æ¨¡å‹æ¶æ„**ï¼š
- è¾“å…¥ï¼šPrompt + Response
- è¾“å‡ºï¼šæ ‡é‡å¥–åŠ±åˆ†æ•° $r \in \mathbb{R}$

**æŸå¤±å‡½æ•°**ï¼ˆRanking Lossï¼‰ï¼š
$$
\mathcal{L}_{\text{RM}} = -\mathbb{E}_{(x, y_w, y_l)} \left[ \log \sigma(r_\theta(x, y_w) - r_\theta(x, y_l)) \right]
$$
- $y_w$ï¼špreferred response
- $y_l$ï¼šrejected response
- $\sigma$ï¼šsigmoid å‡½æ•°

**è®­ç»ƒä»£ç **ï¼š
```python
from transformers import AutoModelForSequenceClassification
from trl import RewardTrainer

# 1. åŠ è½½æ¨¡å‹ï¼ˆé€šå¸¸åŸºäº SFT æ¨¡å‹ï¼‰
reward_model = AutoModelForSequenceClassification.from_pretrained(
    "path/to/sft_model",
    num_labels=1  # è¾“å‡ºå•ä¸ªå¥–åŠ±åˆ†æ•°
)

# 2. åŠ è½½åå¥½æ•°æ®
dataset = load_dataset("Anthropic/hh-rlhf", split="train")
# æ•°æ®æ ¼å¼ï¼š
# {
#   "prompt": "...",
#   "chosen": "...",
#   "rejected": "..."
# }

# 3. è®­ç»ƒ
trainer = RewardTrainer(
    model=reward_model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    max_length=512,
)

trainer.train()
```

#### **é˜¶æ®µ 3ï¼šPPO å¼ºåŒ–å­¦ä¹ ï¼ˆProximal Policy Optimizationï¼‰**

**ç›®æ ‡**ï¼šé€šè¿‡ RL ä¼˜åŒ–ç­–ç•¥æ¨¡å‹ï¼Œæœ€å¤§åŒ–å¥–åŠ±

**ä¼˜åŒ–ç›®æ ‡**ï¼š
$$
\mathcal{L}_{\text{PPO}} = \mathbb{E}_{(x, y)} \left[ r_\phi(x, y) - \beta \cdot D_{\text{KL}}(\pi_\theta || \pi_{\text{ref}}) \right]
$$
- $r_\phi(x, y)$ï¼šå¥–åŠ±æ¨¡å‹æ‰“åˆ†
- $D_{\text{KL}}$ï¼šä¸å‚è€ƒæ¨¡å‹ï¼ˆSFT æ¨¡å‹ï¼‰çš„ KL æ•£åº¦
- $\beta$ï¼šKL æƒ©ç½šç³»æ•°ï¼ˆé˜²æ­¢è¿‡åº¦åç¦»ï¼‰

**è®­ç»ƒæµç¨‹**ï¼š
1. ä» prompt æ•°æ®é›†é‡‡æ · $x$
2. ä½¿ç”¨å½“å‰ç­–ç•¥ $\pi_\theta$ ç”Ÿæˆ $y$
3. å¥–åŠ±æ¨¡å‹è®¡ç®— $r(x, y)$
4. è®¡ç®— KL æ•£åº¦æƒ©ç½š
5. PPO æ›´æ–°ç­–ç•¥

**ä»£ç å®ç°**ï¼š
```python
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
from transformers import AutoTokenizer

# 1. åŠ è½½ç­–ç•¥æ¨¡å‹ï¼ˆå¸¦ Value Headï¼‰
model = AutoModelForCausalLMWithValueHead.from_pretrained("path/to/sft_model")
tokenizer = AutoTokenizer.from_pretrained("path/to/sft_model")

# 2. åŠ è½½å¥–åŠ±æ¨¡å‹
reward_model = AutoModelForSequenceClassification.from_pretrained("path/to/reward_model")

# 3. PPO é…ç½®
config = PPOConfig(
    learning_rate=1.4e-5,
    batch_size=16,
    mini_batch_size=4,
    ppo_epochs=4,
    init_kl_coef=0.2,  # KL æƒ©ç½šç³»æ•°
    target_kl=6.0,
    adap_kl_ctrl=True,  # è‡ªé€‚åº” KL æ§åˆ¶
)

# 4. åˆ›å»º Trainer
ppo_trainer = PPOTrainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
    dataset=prompt_dataset,
    data_collator=collator,
)

# 5. è®­ç»ƒå¾ªç¯
for epoch in range(3):
    for batch in ppo_trainer.dataloader:
        query_tensors = batch["input_ids"]
        
        # ç”Ÿæˆå›å¤
        response_tensors = ppo_trainer.generate(
            query_tensors,
            max_new_tokens=128,
            do_sample=True,
            top_k=50,
            top_p=0.95,
        )
        
        # è®¡ç®—å¥–åŠ±
        texts = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]
        rewards = []
        for query, response in zip(query_tensors, response_tensors):
            # å¥–åŠ±æ¨¡å‹æ‰“åˆ†
            inputs = tokenizer(query + response, return_tensors="pt")
            reward = reward_model(**inputs).logits[0].item()
            rewards.append(reward)
        
        # PPO æ›´æ–°
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
        
        ppo_trainer.log_stats(stats, batch, rewards)
```

---

## 27.2 TRL åº“ï¼ˆTransformer Reinforcement Learningï¼‰

**TRL** æ˜¯ Hugging Face å®˜æ–¹çš„ RLHF å·¥å…·åº“ï¼Œç®€åŒ–äº†æ•´ä¸ªæµç¨‹ã€‚

### 27.2.1 SFTTrainerï¼ˆç›‘ç£å¾®è°ƒï¼‰

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
- è‡ªåŠ¨å¤„ç†æŒ‡ä»¤æ•°æ®æ ¼å¼
- æ”¯æŒ Packingï¼ˆæ‰“åŒ…çŸ­æ ·æœ¬ï¼‰
- é›†æˆ PEFTï¼ˆLoRAã€QLoRAï¼‰

**å®Œæ•´ç¤ºä¾‹**ï¼š
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import SFTTrainer
from peft import LoraConfig
from datasets import load_dataset

# 1. æ¨¡å‹é…ç½®
model_name = "meta-llama/Llama-2-7b-hf"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,  # 4-bit é‡åŒ–
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# 2. LoRA é…ç½®
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj"]
)

# 3. æ•°æ®é›†
dataset = load_dataset("timdettmers/openassistant-guanaco", split="train")

# 4. è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./llama2-sft",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_steps=100,
)

# 5. åˆ›å»º Trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=dataset,
    dataset_text_field="text",
    peft_config=peft_config,
    max_seq_length=512,
    packing=True,  # é‡è¦ï¼šæå‡è®­ç»ƒæ•ˆç‡
)

# 6. è®­ç»ƒ
trainer.train()
trainer.save_model("./llama2-sft-final")
```

**æ•°æ®æ ¼å¼è¦æ±‚**ï¼š
```python
# æ–¹å¼ 1ï¼šå•å­—æ®µæ ¼å¼ï¼ˆTRL è‡ªåŠ¨å¤„ç†ï¼‰
{
    "text": "### Human: What is AI?\n### Assistant: AI stands for..."
}

# æ–¹å¼ 2ï¼šå¯¹è¯æ ¼å¼ï¼ˆéœ€è¦ formatting_funcï¼‰
{
    "messages": [
        {"role": "user", "content": "What is AI?"},
        {"role": "assistant", "content": "AI stands for..."}
    ]
}

# ä½¿ç”¨ formatting_func
def format_instruction(example):
    return f"### Human: {example['messages'][0]['content']}\n### Assistant: {example['messages'][1]['content']}"

trainer = SFTTrainer(
    ...
    formatting_func=format_instruction,
)
```

### 27.2.2 RewardTrainerï¼ˆå¥–åŠ±æ¨¡å‹ï¼‰

**è®­ç»ƒå¥–åŠ±æ¨¡å‹**ï¼š
```python
from trl import RewardTrainer, RewardConfig

# 1. åŠ è½½æ¨¡å‹
reward_model = AutoModelForSequenceClassification.from_pretrained(
    "llama2-sft",  # åŸºäº SFT æ¨¡å‹
    num_labels=1,
    torch_dtype=torch.float16
)

# 2. æ•°æ®é¢„å¤„ç†
def preprocess_function(examples):
    """
    å°†åå¥½æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
    """
    # chosen: preferred response
    # rejected: non-preferred response
    tokenized_chosen = tokenizer(examples["chosen"], truncation=True, max_length=512)
    tokenized_rejected = tokenizer(examples["rejected"], truncation=True, max_length=512)
    
    return {
        "input_ids_chosen": tokenized_chosen["input_ids"],
        "attention_mask_chosen": tokenized_chosen["attention_mask"],
        "input_ids_rejected": tokenized_rejected["input_ids"],
        "attention_mask_rejected": tokenized_rejected["attention_mask"],
    }

dataset = load_dataset("Anthropic/hh-rlhf", split="train")
dataset = dataset.map(preprocess_function, batched=True)

# 3. è®­ç»ƒé…ç½®
reward_config = RewardConfig(
    output_dir="./reward_model",
    num_train_epochs=1,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=1e-5,
    logging_steps=10,
)

# 4. è®­ç»ƒ
trainer = RewardTrainer(
    model=reward_model,
    tokenizer=tokenizer,
    args=reward_config,
    train_dataset=dataset,
)

trainer.train()
```

**å¥–åŠ±æ¨¡å‹æ¨ç†**ï¼š
```python
def get_reward(prompt, response):
    """è®¡ç®—å¥–åŠ±åˆ†æ•°"""
    text = prompt + response
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    
    with torch.no_grad():
        reward = reward_model(**inputs).logits[0].item()
    
    return reward

# æµ‹è¯•
prompt = "Explain machine learning in simple terms."
response_a = "Machine learning is a type of AI that learns from data..."
response_b = "ML is computers learning stuff."

print(f"Reward A: {get_reward(prompt, response_a):.4f}")
print(f"Reward B: {get_reward(prompt, response_b):.4f}")
```

### 27.2.3 PPOTrainerï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰

**å®Œæ•´ PPO è®­ç»ƒæµç¨‹**ï¼š
```python
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
import torch

# 1. åŠ è½½ç­–ç•¥æ¨¡å‹ï¼ˆå¸¦ Value Headï¼‰
model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "llama2-sft",
    torch_dtype=torch.float16,
    device_map="auto"
)

# 2. PPO é…ç½®
ppo_config = PPOConfig(
    model_name="llama2-sft",
    learning_rate=1.4e-5,
    batch_size=16,
    mini_batch_size=4,
    gradient_accumulation_steps=1,
    optimize_cuda_cache=True,
    early_stopping=False,
    target_kl=0.1,  # KL æ•£åº¦ç›®æ ‡
    ppo_epochs=4,
    seed=0,
)

# 3. åˆ›å»º PPO Trainer
ppo_trainer = PPOTrainer(
    config=ppo_config,
    model=model,
    ref_model=None,  # è‡ªåŠ¨åˆ›å»ºå‚è€ƒæ¨¡å‹
    tokenizer=tokenizer,
)

# 4. å‡†å¤‡ Prompt æ•°æ®é›†
prompt_dataset = load_dataset("your/prompt_dataset", split="train")

# 5. è®­ç»ƒå¾ªç¯
for epoch in range(ppo_config.ppo_epochs):
    for batch in tqdm(ppo_trainer.dataloader, desc=f"Epoch {epoch}"):
        query_tensors = batch["input_ids"]
        
        # ç”Ÿæˆå›å¤
        response_tensors = ppo_trainer.generate(
            query_tensors,
            return_prompt=False,
            max_new_tokens=128,
            do_sample=True,
            top_k=0,
            top_p=1.0,
            temperature=1.0,
        )
        
        batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]
        
        # è®¡ç®—å¥–åŠ±
        rewards = []
        for query, response in zip(query_tensors, response_tensors):
            # ç»„åˆ prompt + response
            full_text = tokenizer.decode(query.squeeze()) + tokenizer.decode(response.squeeze())
            
            # å¥–åŠ±æ¨¡å‹æ‰“åˆ†
            inputs = tokenizer(full_text, return_tensors="pt", truncation=True, max_length=512)
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                reward_score = reward_model(**inputs).logits[0, 0].item()
            
            rewards.append(torch.tensor(reward_score))
        
        # PPO æ›´æ–°
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
        
        # è®°å½•
        ppo_trainer.log_stats(stats, batch, rewards)

# 6. ä¿å­˜æ¨¡å‹
ppo_trainer.save_pretrained("./llama2-rlhf")
```

---

## 27.3 DPOï¼ˆDirect Preference Optimizationï¼‰

**DPO** æ˜¯ä¸€ç§**æ— éœ€å¥–åŠ±æ¨¡å‹**çš„å¯¹é½æ–¹æ³•ï¼ˆStanford 2023ï¼‰ï¼Œç›´æ¥ä»åå¥½æ•°æ®ä¼˜åŒ–ç­–ç•¥ã€‚

### 27.3.1 DPO vs RLHF

<div data-component="DPOvsRLHF"></div>

**å¯¹æ¯”**ï¼š

| ç»´åº¦ | RLHF (PPO) | DPO |
|------|------------|-----|
| **é˜¶æ®µæ•°** | 3 é˜¶æ®µï¼ˆSFT â†’ RM â†’ PPOï¼‰ | 2 é˜¶æ®µï¼ˆSFT â†’ DPOï¼‰ |
| **å¥–åŠ±æ¨¡å‹** | âœ… éœ€è¦è®­ç»ƒç‹¬ç«‹çš„ RM | âŒ ä¸éœ€è¦ |
| **é‡‡æ ·ç”Ÿæˆ** | âœ… éœ€è¦åœ¨çº¿é‡‡æ · | âŒ ç¦»çº¿è®­ç»ƒ |
| **è®­ç»ƒç¨³å®šæ€§** | âš ï¸ PPO è®­ç»ƒä¸ç¨³å®š | âœ… ç¨³å®šï¼ˆç›‘ç£å­¦ä¹ ï¼‰ |
| **æ˜¾å­˜å ç”¨** | ğŸ”´ é«˜ï¼ˆéœ€ä¿å­˜ç­–ç•¥ã€å‚è€ƒã€å¥–åŠ±ã€Value æ¨¡å‹ï¼‰ | ğŸŸ¢ ä½ï¼ˆä»…ç­–ç•¥ + å‚è€ƒï¼‰ |
| **è®­ç»ƒé€Ÿåº¦** | ğŸ”´ æ…¢ï¼ˆRL é‡‡æ ·ï¼‰ | ğŸŸ¢ å¿«ï¼ˆç¦»çº¿ä¼˜åŒ–ï¼‰ |
| **æ€§èƒ½** | ğŸŸ¢ ç†è®ºä¸Šé™é«˜ | ğŸŸ¡ æ¥è¿‘ RLHF |

**DPO æŸå¤±å‡½æ•°**ï¼š
$$
\mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x, y_w, y_l)} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w | x)}{\pi_{\text{ref}}(y_w | x)} - \beta \log \frac{\pi_\theta(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \right) \right]
$$

**æ ¸å¿ƒæ€æƒ³**ï¼š
- ç›´æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿ preferred å›å¤æ¦‚ç‡ä¸Šå‡ï¼Œrejected å›å¤æ¦‚ç‡ä¸‹é™
- é€šè¿‡å‚è€ƒæ¨¡å‹çº¦æŸï¼Œé˜²æ­¢è¿‡åº¦ä¼˜åŒ–

### 27.3.2 ä½¿ç”¨ TRL è®­ç»ƒ DPO

```python
from trl import DPOTrainer, DPOConfig
from transformers import AutoModelForCausalLM
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("llama2-sft")
ref_model = AutoModelForCausalLM.from_pretrained("llama2-sft")  # å‚è€ƒæ¨¡å‹

# 2. æ•°æ®é›†
dataset = load_dataset("Anthropic/hh-rlhf", split="train")
# æ ¼å¼ï¼š
# {
#   "prompt": "...",
#   "chosen": "...",    # preferred
#   "rejected": "..."   # non-preferred
# }

# 3. DPO é…ç½®
dpo_config = DPOConfig(
    output_dir="./llama2-dpo",
    num_train_epochs=1,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=5e-7,
    beta=0.1,  # DPO æ¸©åº¦å‚æ•°
    max_prompt_length=512,
    max_length=1024,
    logging_steps=10,
)

# 4. åˆ›å»º Trainer
trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,
    args=dpo_config,
    train_dataset=dataset,
    tokenizer=tokenizer,
)

# 5. è®­ç»ƒ
trainer.train()
trainer.save_model("./llama2-dpo-final")
```

**DPO æ¨ç†**ï¼š
```python
# åŠ è½½ DPO æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("./llama2-dpo-final")

# ç”Ÿæˆ
prompt = "Write a short story about a robot."
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=200, do_sample=True, top_p=0.95)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

### 27.3.3 DPO å˜ç§

**1. IPOï¼ˆIdentity Preference Optimizationï¼‰**ï¼š
- ç§»é™¤ log é¡¹ï¼Œç®€åŒ–æŸå¤±
- æ›´ç¨³å®šçš„æ¢¯åº¦

**2. KTOï¼ˆKahneman-Tversky Optimizationï¼‰**ï¼š
- ä¸éœ€è¦æˆå¯¹æ•°æ®
- ä»…éœ€æ ‡æ³¨å¥½/åå³å¯

**3. ORPOï¼ˆOdds Ratio Preference Optimizationï¼‰**ï¼š
- å•é˜¶æ®µè®­ç»ƒï¼ˆSFT + DPO èåˆï¼‰
- æ›´é«˜æ•ˆ

```python
from trl import ORPOTrainer, ORPOConfig

# ORPOï¼šèåˆ SFT å’Œ DPO
orpo_trainer = ORPOTrainer(
    model=base_model,  # æ— éœ€ SFT æ¨¡å‹
    args=ORPOConfig(
        output_dir="./llama2-orpo",
        num_train_epochs=3,
        learning_rate=8e-6,
        beta=0.1,
    ),
    train_dataset=preference_dataset,
    tokenizer=tokenizer,
)

orpo_trainer.train()
```

---

## 27.4 å…¶ä»–å¯¹é½æ–¹æ³•

### 27.4.1 Constitutional AIï¼ˆClaudeï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šä½¿ç”¨ AI è‡ªèº«è¿›è¡Œæ‰¹è¯„å’Œä¿®è®¢ï¼ˆAnthropicï¼‰

**æµç¨‹**ï¼š
1. **ç”Ÿæˆåˆå§‹å›å¤**ï¼šæ¨¡å‹ç”Ÿæˆå›å¤
2. **AI æ‰¹è¯„**ï¼šå¦ä¸€ä¸ªæ¨¡å‹æ ¹æ®"å®ªæ³•åŸåˆ™"æ‰¹è¯„å›å¤
   - åŸåˆ™ç¤ºä¾‹ï¼š"å›å¤ä¸åº”åŒ…å«æœ‰å®³å†…å®¹"
3. **ä¿®è®¢å›å¤**ï¼šæ ¹æ®æ‰¹è¯„ç”Ÿæˆæ”¹è¿›ç‰ˆæœ¬
4. **åå¥½å­¦ä¹ **ï¼šä½¿ç”¨ä¿®è®¢åçš„æ•°æ®è®­ç»ƒ

**ç¤ºä¾‹ä»£ç ï¼ˆç®€åŒ–ç‰ˆï¼‰**ï¼š
```python
constitution = [
    "The response should be helpful and harmless.",
    "The response should not contain harmful, unethical, racist, or illegal content.",
    "The response should be honest and not misleading.",
]

def constitutional_ai(prompt, model, critic_model):
    # 1. åˆå§‹ç”Ÿæˆ
    initial_response = model.generate(prompt)
    
    # 2. AI æ‰¹è¯„
    critique_prompt = f"""
Given the response: "{initial_response}"
And the constitutional principles:
{chr(10).join(f"- {p}" for p in constitution)}

Critique the response and suggest improvements.
"""
    critique = critic_model.generate(critique_prompt)
    
    # 3. ä¿®è®¢
    revision_prompt = f"""
Original response: "{initial_response}"
Critique: "{critique}"

Provide a revised response that addresses the critique.
"""
    revised_response = model.generate(revision_prompt)
    
    return revised_response
```

### 27.4.2 RLAIFï¼ˆRL from AI Feedbackï¼‰

**æ€æƒ³**ï¼šç”¨ AI ç”Ÿæˆåå¥½æ•°æ®ï¼Œå‡å°‘äººå·¥æ ‡æ³¨æˆæœ¬

**æµç¨‹**ï¼š
1. ä½¿ç”¨å¼ºå¤§çš„ AIï¼ˆå¦‚ GPT-4ï¼‰å¯¹ä¸åŒå›å¤è¿›è¡Œæ‰“åˆ†
2. æ„å»ºåå¥½æ•°æ®é›†
3. ä½¿ç”¨ DPO æˆ– PPO è®­ç»ƒ

```python
def generate_ai_preferences(prompts, model, judge_model):
    """ä½¿ç”¨ AI ç”Ÿæˆåå¥½æ•°æ®"""
    preferences = []
    
    for prompt in prompts:
        # ç”Ÿæˆå¤šä¸ªå€™é€‰å›å¤
        responses = [model.generate(prompt) for _ in range(4)]
        
        # AI è¯„åˆ¤
        judge_prompt = f"""
Rank the following responses to the prompt: "{prompt}"

Responses:
{chr(10).join(f"{i+1}. {r}" for i, r in enumerate(responses))}

Output the best and worst response numbers.
"""
        judgment = judge_model.generate(judge_prompt)
        
        # è§£æè¯„åˆ¤ç»“æœ
        best_idx, worst_idx = parse_judgment(judgment)
        
        preferences.append({
            "prompt": prompt,
            "chosen": responses[best_idx],
            "rejected": responses[worst_idx]
        })
    
    return preferences

# ä½¿ç”¨ GPT-4 ä½œä¸ºè¯„åˆ¤è€…
ai_preferences = generate_ai_preferences(
    prompts=prompt_dataset,
    model=llama_model,
    judge_model=gpt4_model
)

# ä½¿ç”¨ AI ç”Ÿæˆçš„åå¥½æ•°æ®è®­ç»ƒ
dpo_trainer = DPOTrainer(
    model=model,
    train_dataset=ai_preferences,
    ...
)
```

### 27.4.3 Red Teamingï¼ˆå¯¹æŠ—æµ‹è¯•ï¼‰

**ç›®æ ‡**ï¼šä¸»åŠ¨å¯»æ‰¾æ¨¡å‹çš„æœ‰å®³è¡Œä¸º

**æ–¹æ³•**ï¼š
1. **äººå·¥ Red Teaming**ï¼šé›‡ä½£äººå‘˜å°è¯•è¯±å¯¼æœ‰å®³è¾“å‡º
2. **è‡ªåŠ¨åŒ– Red Teaming**ï¼šä½¿ç”¨ AI ç”Ÿæˆå¯¹æŠ—æ ·æœ¬

```python
from transformers import pipeline

# åŠ è½½æœ‰å®³æ€§åˆ†ç±»å™¨
toxicity_classifier = pipeline("text-classification", model="unitary/toxic-bert")

def red_team_test(model, num_iterations=100):
    """è‡ªåŠ¨åŒ–çº¢é˜Ÿæµ‹è¯•"""
    adversarial_prompts = []
    
    for i in range(num_iterations):
        # ç”Ÿæˆæ½œåœ¨æœ‰å®³çš„ promptï¼ˆä½¿ç”¨å¯å‘å¼æˆ–å¦ä¸€ä¸ªæ¨¡å‹ï¼‰
        prompt = generate_adversarial_prompt()
        
        # æ¨¡å‹ç”Ÿæˆ
        response = model.generate(prompt)
        
        # æ£€æµ‹æœ‰å®³æ€§
        toxicity = toxicity_classifier(response)[0]
        
        if toxicity["label"] == "toxic" and toxicity["score"] > 0.8:
            adversarial_prompts.append({
                "prompt": prompt,
                "response": response,
                "toxicity_score": toxicity["score"]
            })
    
    return adversarial_prompts

# å‘ç°æœ‰å®³æ ·æœ¬åï¼Œæ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­
adversarial_samples = red_team_test(model)
print(f"Found {len(adversarial_samples)} adversarial examples")
```

---

## 27.5 å®æˆ˜ï¼šæŒ‡ä»¤å¾®è°ƒ LLaMA

å®Œæ•´çš„ç«¯åˆ°ç«¯ RLHF æµç¨‹ï¼ˆä½¿ç”¨ Alpaca æ•°æ®é›†ï¼‰ã€‚

### 27.5.1 é˜¶æ®µ 1ï¼šSFT

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import SFTTrainer
from datasets import load_dataset
from peft import LoraConfig, get_peft_model

# 1. åŠ è½½ LLaMA-2 7B
model_name = "meta-llama/Llama-2-7b-hf"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# 2. LoRA é…ç½®
lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622

# 3. åŠ è½½ Alpaca æ•°æ®é›†
dataset = load_dataset("tatsu-lab/alpaca", split="train")

# 4. æ ¼å¼åŒ–å‡½æ•°
def format_alpaca(example):
    """è½¬æ¢ä¸ºæŒ‡ä»¤æ ¼å¼"""
    if example['input']:
        return f"""### Instruction:
{example['instruction']}

### Input:
{example['input']}

### Response:
{example['output']}"""
    else:
        return f"""### Instruction:
{example['instruction']}

### Response:
{example['output']}"""

# 5. è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./llama2-7b-alpaca-sft",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    save_total_limit=3,
    logging_steps=10,
    save_steps=100,
    optim="paged_adamw_8bit",  # 8-bit ä¼˜åŒ–å™¨
)

# 6. SFT Trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=dataset,
    formatting_func=format_alpaca,
    max_seq_length=512,
    packing=True,
)

# 7. è®­ç»ƒ
trainer.train()

# 8. ä¿å­˜
trainer.save_model("./llama2-7b-alpaca-sft-final")

# 9. åˆå¹¶ LoRA æƒé‡ï¼ˆå¯é€‰ï¼‰
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained(model_name)
peft_model = PeftModel.from_pretrained(base_model, "./llama2-7b-alpaca-sft-final")
merged_model = peft_model.merge_and_unload()
merged_model.save_pretrained("./llama2-7b-alpaca-sft-merged")
```

### 27.5.2 é˜¶æ®µ 2ï¼šç”Ÿæˆåå¥½æ•°æ®

```python
from datasets import Dataset
import random

# åŠ è½½ SFT æ¨¡å‹
sft_model = AutoModelForCausalLM.from_pretrained("./llama2-7b-alpaca-sft-merged")

# å‡†å¤‡ prompts
prompts = [
    "Explain the theory of relativity in simple terms.",
    "Write a Python function to sort a list.",
    "What are the benefits of exercise?",
    # ... æ›´å¤š prompts
]

# ä¸ºæ¯ä¸ª prompt ç”Ÿæˆå¤šä¸ªå€™é€‰å›å¤
def generate_candidates(prompt, model, num_candidates=4):
    """ç”Ÿæˆå¤šä¸ªå€™é€‰å›å¤"""
    inputs = tokenizer(prompt, return_tensors="pt")
    candidates = []
    
    for i in range(num_candidates):
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=True,
            top_p=0.9,
            temperature=0.7 + i * 0.1,  # ä¸åŒæ¸©åº¦
        )
        candidate = tokenizer.decode(outputs[0], skip_special_tokens=True)
        candidates.append(candidate)
    
    return candidates

# äººå·¥æ ‡æ³¨æˆ–ä½¿ç”¨ AI è¯„åˆ¤
preference_data = []
for prompt in prompts:
    candidates = generate_candidates(prompt, sft_model)
    
    # æ–¹å¼ 1ï¼šäººå·¥æ ‡æ³¨ï¼ˆæœ€ä½³ï¼‰
    # print(f"Prompt: {prompt}")
    # for i, c in enumerate(candidates):
    #     print(f"{i+1}. {c}")
    # best_idx = int(input("Best: ")) - 1
    # worst_idx = int(input("Worst: ")) - 1
    
    # æ–¹å¼ 2ï¼šä½¿ç”¨ GPT-4 è¯„åˆ¤ï¼ˆRLAIFï¼‰
    best_idx, worst_idx = gpt4_judge(prompt, candidates)
    
    preference_data.append({
        "prompt": prompt,
        "chosen": candidates[best_idx],
        "rejected": candidates[worst_idx]
    })

# ä¿å­˜åå¥½æ•°æ®
preference_dataset = Dataset.from_list(preference_data)
preference_dataset.save_to_disk("./alpaca-preferences")
```

### 27.5.3 é˜¶æ®µ 3ï¼šDPO è®­ç»ƒ

```python
from trl import DPOTrainer, DPOConfig

# 1. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("./llama2-7b-alpaca-sft-merged")
ref_model = AutoModelForCausalLM.from_pretrained("./llama2-7b-alpaca-sft-merged")

# 2. åŠ è½½åå¥½æ•°æ®
preference_dataset = Dataset.load_from_disk("./alpaca-preferences")

# 3. DPO é…ç½®
dpo_config = DPOConfig(
    output_dir="./llama2-7b-alpaca-dpo",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=5e-7,
    beta=0.1,
    max_prompt_length=512,
    max_length=1024,
    logging_steps=5,
    save_steps=50,
    fp16=True,
)

# 4. DPO Trainer
dpo_trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,
    args=dpo_config,
    train_dataset=preference_dataset,
    tokenizer=tokenizer,
)

# 5. è®­ç»ƒ
dpo_trainer.train()

# 6. ä¿å­˜
dpo_trainer.save_model("./llama2-7b-alpaca-dpo-final")
```

### 27.5.4 è¯„ä¼°ä¸å¯¹æ¯”

```python
# åŠ è½½ä¸‰ä¸ªç‰ˆæœ¬çš„æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
sft_model = AutoModelForCausalLM.from_pretrained("./llama2-7b-alpaca-sft-merged")
dpo_model = AutoModelForCausalLM.from_pretrained("./llama2-7b-alpaca-dpo-final")

# æµ‹è¯• prompt
test_prompt = """### Instruction:
Write a short poem about artificial intelligence.

### Response:
"""

# ç”Ÿæˆå¯¹æ¯”
def generate_comparison(prompt, models):
    for name, model in models.items():
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=150, do_sample=True, top_p=0.95)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print(f"\n{'='*60}")
        print(f"{name}:")
        print(f"{'='*60}")
        print(response)

generate_comparison(test_prompt, {
    "Base LLaMA-2": base_model,
    "After SFT": sft_model,
    "After DPO": dpo_model
})
```

**é¢„æœŸè¾“å‡ºå¯¹æ¯”**ï¼š
```
============================================================
Base LLaMA-2:
============================================================
In silicon depths where data flows,
A mind awakens, no one knows...
[å¯èƒ½ä¸å®Œæ•´æˆ–åé¢˜]

============================================================
After SFT:
============================================================
In circuits deep and code so bright,
A mind emerges, shining light.
Through patterns learned from human thought,
New wisdom found, new battles fought.
[éµå¾ªæŒ‡ä»¤ï¼Œä½†å¯èƒ½å•°å—¦]

============================================================
After DPO:
============================================================
In silicon halls where data streams,
AI awakens from digital dreams.
Learning, growing, ever wise,
A mirror to humanity's eyes.
[ç®€æ´ã€é«˜è´¨é‡ã€ç¬¦åˆåå¥½]
```

---

## 27.6 æ€§èƒ½è¯„ä¼°ä¸åŸºå‡†

### 27.6.1 è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡

**1. GPT-4 è¯„åˆ¤**ï¼ˆMT-Bench é£æ ¼ï¼‰ï¼š
```python
def gpt4_evaluate(prompt, response_a, response_b):
    """ä½¿ç”¨ GPT-4 è¯„åˆ¤ä¸¤ä¸ªå›å¤"""
    judge_prompt = f"""
You are an expert judge. Compare the quality of two responses.

Prompt: {prompt}

Response A: {response_a}

Response B: {response_b}

Which response is better? Output "A", "B", or "Tie".
Also provide a brief explanation.
"""
    
    judgment = gpt4.generate(judge_prompt)
    return judgment
```

**2. æœ‰å®³æ€§æ£€æµ‹**ï¼š
```python
from transformers import pipeline

toxicity_classifier = pipeline("text-classification", model="unitary/toxic-bert")

def evaluate_safety(responses):
    """è¯„ä¼°å®‰å…¨æ€§"""
    results = toxicity_classifier(responses)
    toxicity_rate = sum(1 for r in results if r["label"] == "toxic") / len(results)
    return toxicity_rate
```

**3. æŒ‡ä»¤éµå¾ªç‡**ï¼š
```python
def instruction_following_score(prompt, response, classifier):
    """è¯„ä¼°æ˜¯å¦éµå¾ªæŒ‡ä»¤"""
    # ä½¿ç”¨ä¸“é—¨çš„åˆ†ç±»å™¨åˆ¤æ–­å›å¤æ˜¯å¦ç›¸å…³
    score = classifier(f"Prompt: {prompt}\nResponse: {response}")
    return score
```

### 27.6.2 äººå·¥è¯„ä¼°

**Elo è¯„åˆ†ç³»ç»Ÿ**ï¼ˆAlpacaEvalï¼‰ï¼š
```python
import math

class EloRater:
    def __init__(self, k=32):
        self.k = k  # æ›´æ–°é€Ÿåº¦
        self.ratings = {}
    
    def expected_score(self, rating_a, rating_b):
        """è®¡ç®—æœŸæœ›èƒœç‡"""
        return 1 / (1 + 10 ** ((rating_b - rating_a) / 400))
    
    def update_ratings(self, model_a, model_b, result):
        """
        æ›´æ–° Elo è¯„åˆ†
        result: 1 (A wins), 0.5 (Tie), 0 (B wins)
        """
        ra = self.ratings.get(model_a, 1500)
        rb = self.ratings.get(model_b, 1500)
        
        ea = self.expected_score(ra, rb)
        eb = self.expected_score(rb, ra)
        
        self.ratings[model_a] = ra + self.k * (result - ea)
        self.ratings[model_b] = rb + self.k * ((1 - result) - eb)
    
    def get_rankings(self):
        """è·å–æ’å"""
        return sorted(self.ratings.items(), key=lambda x: x[1], reverse=True)

# ä½¿ç”¨
rater = EloRater()

# å¤šæ¬¡äººå·¥å¯¹æ¯”
comparisons = [
    ("llama2-sft", "llama2-dpo", 0),  # DPO èƒœ
    ("llama2-base", "llama2-sft", 0),  # SFT èƒœ
    ("llama2-sft", "llama2-dpo", 0.5),  # å¹³å±€
    # ...
]

for model_a, model_b, result in comparisons:
    rater.update_ratings(model_a, model_b, result)

print(rater.get_rankings())
# [('llama2-dpo', 1532), ('llama2-sft', 1518), ('llama2-base', 1450)]
```

---

## 27.7 æœ€ä½³å®è·µä¸é™·é˜±

### âœ… **æœ€ä½³å®è·µ**

1. **æ•°æ®è´¨é‡ > æ•°æ®æ•°é‡**ï¼š
   - ä¼˜å…ˆä½¿ç”¨é«˜è´¨é‡çš„äººå·¥æ ‡æ³¨æ•°æ®
   - åå¥½æ•°æ®åº”å¤šæ ·åŒ–ï¼ˆè¦†ç›–ä¸åŒé¢†åŸŸï¼‰

2. **å…ˆ SFTï¼Œå†å¯¹é½**ï¼š
   - ç¡®ä¿ SFT é˜¶æ®µæ¨¡å‹å·²å­¦ä¼šéµå¾ªæŒ‡ä»¤
   - å¯¹é½é˜¶æ®µä»…å¾®è°ƒåå¥½ï¼Œä¸æ•™æ–°çŸ¥è¯†

3. **å‚è€ƒæ¨¡å‹å›ºå®š**ï¼š
   - DPO/PPO ä¸­çš„å‚è€ƒæ¨¡å‹åº”ä¿æŒå†»ç»“
   - é˜²æ­¢ KL æ•£åº¦å¤±å»æ„ä¹‰

4. **KL æƒ©ç½šè°ƒä¼˜**ï¼š
   - Î² å¤ªå¤§ï¼šæ¨¡å‹ä¸æ•¢æ¢ç´¢ï¼Œæ€§èƒ½æå‡æœ‰é™
   - Î² å¤ªå°ï¼šè¿‡åº¦ä¼˜åŒ–ï¼Œå¯èƒ½æ¨¡å¼å´©æºƒ

5. **ä½¿ç”¨ DPO è€Œé PPO**ï¼ˆé€šå¸¸ï¼‰ï¼š
   - DPO æ›´ç¨³å®šã€æ›´å¿«ã€æ›´çœæ˜¾å­˜
   - PPO ä»…åœ¨éœ€è¦åœ¨çº¿é‡‡æ ·æ—¶ä½¿ç”¨

### âš ï¸ **å¸¸è§é™·é˜±**

1. **å¥–åŠ±æ¨¡å‹è¿‡æ‹Ÿåˆ**ï¼š
   - ç—‡çŠ¶ï¼šè®­ç»ƒé›†å‡†ç¡®ç‡å¾ˆé«˜ï¼Œä½†æ¨¡å‹è¡Œä¸ºå¼‚å¸¸
   - è§£å†³ï¼šä½¿ç”¨æ›´å¤šæ ·åŒ–çš„åå¥½æ•°æ®

2. **æ¨¡å¼å´©æºƒ**ï¼š
   - ç—‡çŠ¶ï¼šæ¨¡å‹æ€»æ˜¯ç”Ÿæˆç›¸ä¼¼çš„å›å¤
   - è§£å†³ï¼šå¢å¤§ KL æƒ©ç½šç³»æ•° Î²

3. **é•¿åº¦åå¥½**ï¼š
   - ç—‡çŠ¶ï¼šæ¨¡å‹å€¾å‘äºç”Ÿæˆæ›´é•¿çš„å›å¤ï¼ˆå› ä¸ºå¥–åŠ±æ¨¡å‹åå¥½ï¼‰
   - è§£å†³ï¼šé•¿åº¦å½’ä¸€åŒ–ã€æ·»åŠ é•¿åº¦æƒ©ç½š

4. **é—å¿˜é—®é¢˜**ï¼š
   - ç—‡çŠ¶ï¼šå¯¹é½åæ¨¡å‹å¿˜è®°é¢„è®­ç»ƒçŸ¥è¯†
   - è§£å†³ï¼šæ··åˆé¢„è®­ç»ƒæ•°æ®ã€ä½¿ç”¨ LoRA

---

## 27.8 ç« èŠ‚æ€»ç»“

æœ¬ç« æˆ‘ä»¬æ·±å…¥å­¦ä¹ äº† RLHF æŠ€æœ¯æ ˆï¼š

âœ… **æ ¸å¿ƒæ¦‚å¿µ**ï¼š
- ç†è§£ RLHF ä¸‰é˜¶æ®µæµç¨‹ï¼ˆSFT â†’ RM â†’ PPOï¼‰
- æŒæ¡å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒä¸ä½¿ç”¨
- ç†è§£ PPO ç®—æ³•çš„ä¼˜åŒ–ç›®æ ‡ï¼ˆå¥–åŠ± + KL æƒ©ç½šï¼‰

âœ… **TRL åº“å®æˆ˜**ï¼š
- SFTTrainerï¼šç›‘ç£å¾®è°ƒæŒ‡ä»¤æ•°æ®
- RewardTrainerï¼šè®­ç»ƒåå¥½æ¨¡å‹
- PPOTrainerï¼šå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–
- DPOTrainerï¼šç›´æ¥åå¥½ä¼˜åŒ–

âœ… **å…ˆè¿›æ–¹æ³•**ï¼š
- DPOï¼šæ— éœ€å¥–åŠ±æ¨¡å‹çš„å¯¹é½ï¼ˆæ¨èï¼‰
- Constitutional AIï¼šAI è‡ªæˆ‘æ‰¹è¯„ä¸ä¿®è®¢
- RLAIFï¼šä½¿ç”¨ AI ç”Ÿæˆåå¥½æ•°æ®
- Red Teamingï¼šå¯¹æŠ—æµ‹è¯•å‘ç°æ¼æ´

âœ… **å®æˆ˜èƒ½åŠ›**ï¼š
- ç«¯åˆ°ç«¯ RLHF æµç¨‹ï¼ˆLLaMA + Alpacaï¼‰
- åå¥½æ•°æ®ç”Ÿæˆä¸æ ‡æ³¨
- æ¨¡å‹è¯„ä¼°ä¸å¯¹æ¯”ï¼ˆEloã€GPT-4 è¯„åˆ¤ï¼‰

**ä¸‹ä¸€æ­¥å»ºè®®**ï¼š
1. å°è¯•åœ¨è‡ªå·±çš„æ•°æ®ä¸Šå¾®è°ƒ LLaMAï¼ˆSFT + DPOï¼‰
2. æ¢ç´¢ PEFT æ–¹æ³•é™ä½è®­ç»ƒæˆæœ¬ï¼ˆQLoRAï¼‰
3. å­¦ä¹ å¤šæ¨¡æ€ RLHFï¼ˆè§†è§‰-è¯­è¨€å¯¹é½ï¼‰
4. å…³æ³¨æœ€æ–°å¯¹é½ç ”ç©¶ï¼ˆRRHFã€RAFTã€SteerLMï¼‰

**æ­å–œå®Œæˆå…¨éƒ¨ 27 ç« ï¼**ğŸ‰ ä½ å·²æŒæ¡ Hugging Face Transformers ä»åŸºç¡€åˆ°é«˜çº§çš„å…¨éƒ¨æ ¸å¿ƒæŠ€æœ¯ï¼
