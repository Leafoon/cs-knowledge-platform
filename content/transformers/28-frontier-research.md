# Chapter 28: å‰æ²¿ç ”ç©¶ä¸æœªæ¥æ–¹å‘

> **æœ¬ç« ç›®æ ‡**ï¼šæ¢ç´¢ Transformers é¢†åŸŸçš„æœ€æ–°ç ”ç©¶æ–¹å‘ï¼Œäº†è§£é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ã€é«˜æ•ˆæ¶æ„ã€æ¨¡å‹åˆå¹¶ã€å¯è§£é‡Šæ€§ã€å®‰å…¨æ€§ç­‰å‰æ²¿ä¸»é¢˜ï¼Œå±•æœ›æœªæ¥å‘å±•è¶‹åŠ¿ã€‚

---

## 28.1 é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡

éšç€åº”ç”¨éœ€æ±‚çš„å¢é•¿ï¼Œå¤„ç†è¶…é•¿æ–‡æœ¬åºåˆ—æˆä¸ºå…³é”®æŒ‘æˆ˜ã€‚æ ‡å‡† Transformer çš„äºŒæ¬¡å¤æ‚åº¦é™åˆ¶äº†å…¶ä¸Šä¸‹æ–‡çª—å£é•¿åº¦ã€‚

### 28.1.1 ä½ç½®æ’å€¼ï¼ˆPosition Interpolationï¼‰

ä½ç½®æ’å€¼é€šè¿‡ç¼©æ”¾ä½ç½®ç¼–ç ï¼Œè®©æ¨¡å‹å¤„ç†æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
- å°†ä½ç½®ç´¢å¼•ä» `[0, L']` çº¿æ€§æ˜ å°„åˆ° `[0, L]`ï¼ˆL æ˜¯è®­ç»ƒé•¿åº¦ï¼ŒL' > Lï¼‰
- RoPEï¼ˆRotary Position Embeddingï¼‰å¤©ç„¶æ”¯æŒæ’å€¼

**å®ç°ç¤ºä¾‹**ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# LLaMA æ¨¡å‹æ”¯æŒä½ç½®æ’å€¼æ‰©å±•
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    trust_remote_code=True
)

# é€šè¿‡ä¿®æ”¹ config æ‰©å±•ä¸Šä¸‹æ–‡
model.config.max_position_embeddings = 8192  # ä» 4096 æ‰©å±•
model.config.rope_scaling = {
    "type": "linear",
    "factor": 2.0  # æ’å€¼å› å­
}

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# æµ‹è¯•è¶…é•¿è¾“å…¥
long_text = "This is a very long document. " * 1000
inputs = tokenizer(long_text, return_tensors="pt", truncation=False)

# ç”Ÿæˆæ—¶å¯ä»¥å¤„ç†æ›´é•¿åºåˆ—
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        use_cache=True
    )
```

**NTK-Aware Interpolation**ï¼ˆç¥ç»åˆ‡çº¿æ ¸æ„ŸçŸ¥æ’å€¼ï¼‰ï¼š

```python
# æ›´é«˜çº§çš„æ’å€¼ç­–ç•¥
model.config.rope_scaling = {
    "type": "ntk",
    "factor": 2.0,
    "alpha": 1.0  # NTK å‚æ•°
}
```

**æ•°å­¦åŸç†**ï¼š

å¯¹äº RoPEï¼Œä½ç½® $m$ çš„ç¼–ç ä¸ºï¼š
$$
\text{RoPE}(x_m, m) = x_m \cdot e^{im\theta}
$$

çº¿æ€§æ’å€¼ï¼š
$$
m' = \frac{m}{s}, \quad s = \frac{L'}{L}
$$

NTK æ’å€¼è°ƒæ•´é¢‘ç‡ï¼š
$$
\theta' = \theta \cdot s^{-\alpha}
$$

### 28.1.2 ALiBi ä¸ RoPE æ‰©å±•

**ALiBiï¼ˆAttention with Linear Biasesï¼‰**ï¼š

ä¸ä½¿ç”¨ä½ç½®ç¼–ç ï¼Œè€Œæ˜¯åœ¨æ³¨æ„åŠ›åˆ†æ•°ä¸Šç›´æ¥æ·»åŠ çº¿æ€§åç½®ã€‚

```python
# ALiBi å®ç°ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
import torch

def get_alibi_slopes(num_heads):
    """è®¡ç®— ALiBi çš„æ–œç‡"""
    def get_slopes_power_of_2(n):
        start = 2 ** (-2 ** -(math.log2(n) - 3))
        ratio = start
        return [start * (ratio ** i) for i in range(n)]
    
    if math.log2(num_heads).is_integer():
        return get_slopes_power_of_2(num_heads)
    else:
        # å¤„ç†é 2 çš„å¹‚æ¬¡
        closest_power_of_2 = 2 ** math.floor(math.log2(num_heads))
        return get_slopes_power_of_2(closest_power_of_2) + \
               get_alibi_slopes(2 * closest_power_of_2)[0::2][:num_heads - closest_power_of_2]

def apply_alibi(attention_scores, num_heads):
    """åº”ç”¨ ALiBi åç½®"""
    batch_size, seq_len = attention_scores.shape[0], attention_scores.shape[-1]
    
    # è®¡ç®—ç›¸å¯¹è·ç¦»
    position_ids = torch.arange(seq_len, device=attention_scores.device)
    relative_positions = position_ids[None, :] - position_ids[:, None]  # (seq_len, seq_len)
    
    # è·å–æ–œç‡
    slopes = torch.tensor(get_alibi_slopes(num_heads), device=attention_scores.device)
    
    # è®¡ç®—åç½®
    alibi = slopes[:, None, None] * relative_positions[None, :, :]  # (num_heads, seq_len, seq_len)
    
    # æ·»åŠ åˆ°æ³¨æ„åŠ›åˆ†æ•°
    attention_scores = attention_scores + alibi
    return attention_scores
```

**ALiBi çš„ä¼˜åŠ¿**ï¼š
- é›¶å¤–æ¨èƒ½åŠ›ï¼šè®­ç»ƒæ—¶çŸ­ï¼Œæ¨ç†æ—¶é•¿
- æ— éœ€ä½ç½®ç¼–ç å‚æ•°
- å¯¹ä¸åŒé•¿åº¦æ³›åŒ–è‰¯å¥½

**ä½¿ç”¨ ALiBi çš„æ¨¡å‹**ï¼š

```python
from transformers import AutoModelForCausalLM

# BLOOM ä½¿ç”¨ ALiBi
model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-560m")

# å¯ä»¥ç›´æ¥å¤„ç†æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—
long_input = tokenizer("A " * 5000, return_tensors="pt")
outputs = model(**long_input)
```

### 28.1.3 Sparse Attentionï¼ˆLongformerã€BigBirdï¼‰

ç¨€ç–æ³¨æ„åŠ›é€šè¿‡é™åˆ¶æ³¨æ„åŠ›èŒƒå›´ï¼Œå°†å¤æ‚åº¦ä» $O(n^2)$ é™ä½åˆ° $O(n)$ã€‚

**Longformer çš„ä¸‰ç§æ³¨æ„åŠ›æ¨¡å¼**ï¼š

1. **å±€éƒ¨çª—å£æ³¨æ„åŠ›**ï¼ˆSliding Windowï¼‰
2. **å…¨å±€æ³¨æ„åŠ›**ï¼ˆGlobal Attentionï¼‰
3. **æ‰©å¼ çª—å£æ³¨æ„åŠ›**ï¼ˆDilated Windowï¼‰

```python
from transformers import LongformerModel, LongformerTokenizer

# åŠ è½½ Longformer
model = LongformerModel.from_pretrained("allenai/longformer-base-4096")
tokenizer = LongformerTokenizer.from_pretrained("allenai/longformer-base-4096")

# å‡†å¤‡é•¿æ–‡æœ¬
text = "This is a long document. " * 500
inputs = tokenizer(text, return_tensors="pt", max_length=4096, truncation=True)

# è®¾ç½®å…¨å±€æ³¨æ„åŠ›ï¼ˆç¬¬ä¸€ä¸ª token é€šå¸¸æ˜¯ [CLS]ï¼‰
global_attention_mask = torch.zeros_like(inputs["input_ids"])
global_attention_mask[:, 0] = 1  # [CLS] token æœ‰å…¨å±€æ³¨æ„åŠ›

# å‰å‘ä¼ æ’­
outputs = model(
    input_ids=inputs["input_ids"],
    attention_mask=inputs["attention_mask"],
    global_attention_mask=global_attention_mask
)

# è¾“å‡º shape: (batch_size, sequence_length, hidden_size)
print(outputs.last_hidden_state.shape)  # torch.Size([1, 4096, 768])
```

**çª—å£å¤§å°é…ç½®**ï¼š

```python
# è‡ªå®šä¹‰çª—å£å¤§å°
from transformers import LongformerConfig

config = LongformerConfig.from_pretrained("allenai/longformer-base-4096")
config.attention_window = [256] * 12  # æ¯å±‚ 256 çš„çª—å£
model = LongformerModel(config)
```

**BigBird çš„éšæœºæ³¨æ„åŠ›**ï¼š

BigBird ç»“åˆäº†å±€éƒ¨ã€å…¨å±€å’Œéšæœºæ³¨æ„åŠ›ã€‚

```python
from transformers import BigBirdModel, BigBirdTokenizer

model = BigBirdModel.from_pretrained("google/bigbird-roberta-base")
tokenizer = BigBirdTokenizer.from_pretrained("google/bigbird-roberta-base")

# BigBird æ”¯æŒæœ€é•¿ 4096 tokens
inputs = tokenizer("Long text " * 500, return_tensors="pt", max_length=4096, truncation=True)

outputs = model(**inputs)
```

<div data-component="LongContextStrategies"></div>

### 28.1.4 Retrieval-Augmented Generation (RAG)

RAG é€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†åº“æ‰©å±•ä¸Šä¸‹æ–‡ï¼Œé¿å…å°†æ‰€æœ‰ä¿¡æ¯ç¼–ç åˆ°å‚æ•°ä¸­ã€‚

**æ¶æ„**ï¼š
1. **æ£€ç´¢å™¨**ï¼ˆRetrieverï¼‰ï¼šä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£
2. **ç”Ÿæˆå™¨**ï¼ˆGeneratorï¼‰ï¼šåŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ

```python
from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration
import torch

# åŠ è½½ RAG æ¨¡å‹
tokenizer = RagTokenizer.from_pretrained("facebook/rag-sequence-nq")
retriever = RagRetriever.from_pretrained(
    "facebook/rag-sequence-nq",
    index_name="exact",
    use_dummy_dataset=True  # æ¼”ç¤ºç”¨ï¼Œå®é™…éœ€è¦çœŸå®ç´¢å¼•
)
model = RagSequenceForGeneration.from_pretrained("facebook/rag-sequence-nq", retriever=retriever)

# é—®é¢˜
question = "What is the capital of France?"
inputs = tokenizer(question, return_tensors="pt")

# ç”Ÿæˆç­”æ¡ˆ
generated = model.generate(**inputs, num_return_sequences=1, max_new_tokens=50)
answer = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]
print(f"Answer: {answer}")
```

**è‡ªå®šä¹‰æ£€ç´¢å™¨**ï¼ˆä½¿ç”¨ FAISSï¼‰ï¼š

```python
from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer
import faiss
import numpy as np

# 1. å‡†å¤‡æ–‡æ¡£åµŒå…¥
doc_encoder = DPRQuestionEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
doc_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")

documents = [
    "Paris is the capital of France.",
    "Berlin is the capital of Germany.",
    "Madrid is the capital of Spain."
]

# ç¼–ç æ–‡æ¡£
doc_inputs = doc_tokenizer(documents, return_tensors="pt", padding=True, truncation=True)
with torch.no_grad():
    doc_embeddings = doc_encoder(**doc_inputs).pooler_output.numpy()

# 2. æ„å»º FAISS ç´¢å¼•
dimension = doc_embeddings.shape[1]
index = faiss.IndexFlatIP(dimension)  # Inner Product
index.add(doc_embeddings)

# 3. æ£€ç´¢æŸ¥è¯¢
question_encoder = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")

question = "What is the capital of France?"
q_inputs = question_tokenizer(question, return_tensors="pt")
with torch.no_grad():
    q_embedding = question_encoder(**q_inputs).pooler_output.numpy()

# æœç´¢ top-k
k = 2
scores, indices = index.search(q_embedding, k)
retrieved_docs = [documents[i] for i in indices[0]]

print(f"Retrieved: {retrieved_docs}")
# ['Paris is the capital of France.', 'Berlin is the capital of Germany.']

# 4. ç»„åˆæ£€ç´¢ç»“æœ + ç”Ÿæˆ
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

generator = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-base")
gen_tokenizer = AutoTokenizer.from_pretrained("facebook/bart-base")

# æ‹¼æ¥ä¸Šä¸‹æ–‡
context = " ".join(retrieved_docs)
input_text = f"question: {question} context: {context}"
inputs = gen_tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)

outputs = generator.generate(**inputs, max_new_tokens=50)
answer = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"Answer: {answer}")
```

---

## 28.2 é«˜æ•ˆæ¶æ„

### 28.2.1 Mixture of Experts (MoE)

MoE é€šè¿‡ç¨€ç–æ¿€æ´»ä¸“å®¶ç½‘ç»œï¼Œå¤§å¹…å¢åŠ æ¨¡å‹å®¹é‡è€Œä¸å¢åŠ è®¡ç®—é‡ã€‚

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š
- **ä¸“å®¶å±‚**ï¼ˆExpert Layerï¼‰ï¼šå¤šä¸ªå¹¶è¡Œçš„ FFN
- **è·¯ç”±å™¨**ï¼ˆRouter/Gateï¼‰ï¼šå†³å®šæ¯ä¸ª token æ¿€æ´»å“ªäº›ä¸“å®¶
- **Top-K è·¯ç”±**ï¼šæ¯ä¸ª token åªæ¿€æ´» K ä¸ªä¸“å®¶

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Mixtral 8x7B æ˜¯è‘—åçš„ MoE æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mixtral-8x7B-v0.1",
    device_map="auto",
    load_in_4bit=True,  # é‡åŒ–åŠ è½½
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mixtral-8x7B-v0.1")

# æ­£å¸¸ä½¿ç”¨
inputs = tokenizer("Once upon a time", return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0]))
```

**è‡ªå®šä¹‰ MoE å±‚**ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MoELayer(nn.Module):
    def __init__(self, hidden_size, num_experts=8, top_k=2, expert_hidden_size=2048):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        
        # è·¯ç”±å™¨
        self.gate = nn.Linear(hidden_size, num_experts, bias=False)
        
        # ä¸“å®¶ç½‘ç»œ
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size, expert_hidden_size),
                nn.GELU(),
                nn.Linear(expert_hidden_size, hidden_size)
            )
            for _ in range(num_experts)
        ])
    
    def forward(self, x):
        # x: (batch_size, seq_len, hidden_size)
        batch_size, seq_len, hidden_size = x.shape
        x_flat = x.view(-1, hidden_size)  # (batch_size * seq_len, hidden_size)
        
        # è·¯ç”±å†³ç­–
        router_logits = self.gate(x_flat)  # (batch_size * seq_len, num_experts)
        router_probs = F.softmax(router_logits, dim=-1)
        
        # Top-K é€‰æ‹©
        top_k_probs, top_k_indices = torch.topk(router_probs, self.top_k, dim=-1)
        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)  # é‡æ–°å½’ä¸€åŒ–
        
        # åˆå§‹åŒ–è¾“å‡º
        output = torch.zeros_like(x_flat)
        
        # åˆ†å‘åˆ°ä¸“å®¶
        for i in range(self.top_k):
            expert_idx = top_k_indices[:, i]  # (batch_size * seq_len,)
            weight = top_k_probs[:, i:i+1]  # (batch_size * seq_len, 1)
            
            # ä¸ºæ¯ä¸ªä¸“å®¶æ”¶é›†å¯¹åº”çš„ token
            for expert_id in range(self.num_experts):
                mask = (expert_idx == expert_id)
                if mask.any():
                    expert_input = x_flat[mask]
                    expert_output = self.experts[expert_id](expert_input)
                    output[mask] += weight[mask] * expert_output
        
        return output.view(batch_size, seq_len, hidden_size)

# ä½¿ç”¨ç¤ºä¾‹
moe_layer = MoELayer(hidden_size=768, num_experts=8, top_k=2)
x = torch.randn(2, 128, 768)  # (batch, seq_len, hidden_size)
output = moe_layer(x)
print(output.shape)  # torch.Size([2, 128, 768])
```

**è´Ÿè½½å‡è¡¡æŸå¤±**ï¼ˆLoad Balancing Lossï¼‰ï¼š

```python
def load_balancing_loss(router_probs, num_experts):
    """
    ç¡®ä¿ä¸“å®¶è¢«å‡åŒ€ä½¿ç”¨
    """
    # router_probs: (batch_size * seq_len, num_experts)
    expert_usage = router_probs.mean(dim=0)  # (num_experts,)
    target = torch.ones_like(expert_usage) / num_experts
    loss = F.mse_loss(expert_usage, target)
    return loss
```

<div data-component="MoERouting"></div>

### 28.2.2 State Space Modelsï¼ˆMambaã€S4ï¼‰

çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰æ˜¯ Transformer çš„æ›¿ä»£æ¶æ„ï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦ã€‚

**Mamba æ ¸å¿ƒæ€æƒ³**ï¼š
- é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´ï¼ˆSelective SSMï¼‰
- ç¡¬ä»¶æ„ŸçŸ¥ç®—æ³•ï¼ˆHardware-Aware Algorithmï¼‰
- çº¿æ€§æ—¶é—´æ¨ç†

```python
# Mamba æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹ï¼ˆéœ€è¦å®‰è£… mamba-ssmï¼‰
# pip install mamba-ssm

from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½ Mamba æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "state-spaces/mamba-2.8b",
    trust_remote_code=True,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("state-spaces/mamba-2.8b")

# ç”Ÿæˆ
prompt = "The future of AI is"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0]))
```

**S4ï¼ˆStructured State Spaceï¼‰å±‚ç®€åŒ–å®ç°**ï¼š

```python
import torch
import torch.nn as nn

class S4Layer(nn.Module):
    """ç®€åŒ–çš„ S4 å±‚ï¼ˆæ•™å­¦ç”¨ï¼‰"""
    def __init__(self, d_model, d_state=64):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        
        # çŠ¶æ€ç©ºé—´å‚æ•°ï¼ˆç®€åŒ–ä¸ºå¯å­¦ä¹ çŸ©é˜µï¼‰
        self.A = nn.Parameter(torch.randn(d_state, d_state) / d_state)
        self.B = nn.Parameter(torch.randn(d_state, d_model) / d_model)
        self.C = nn.Parameter(torch.randn(d_model, d_state) / d_state)
        self.D = nn.Parameter(torch.randn(d_model))
        
    def forward(self, x):
        # x: (batch, seq_len, d_model)
        batch, seq_len, _ = x.shape
        
        # åˆå§‹åŒ–çŠ¶æ€
        h = torch.zeros(batch, self.d_state, device=x.device)
        outputs = []
        
        # é€’å½’è®¡ç®—ï¼ˆå®é™… S4 ä½¿ç”¨å¹¶è¡Œæ‰«æï¼‰
        for t in range(seq_len):
            u = x[:, t, :]  # (batch, d_model)
            h = torch.matmul(h, self.A.T) + torch.matmul(u, self.B.T)  # (batch, d_state)
            y = torch.matmul(h, self.C.T) + u * self.D  # (batch, d_model)
            outputs.append(y)
        
        return torch.stack(outputs, dim=1)  # (batch, seq_len, d_model)

# æµ‹è¯•
layer = S4Layer(d_model=256, d_state=64)
x = torch.randn(4, 100, 256)
output = layer(x)
print(output.shape)  # torch.Size([4, 100, 256])
```

**S4 vs Transformer å¯¹æ¯”**ï¼š

| ç‰¹æ€§ | Transformer | S4/Mamba |
|------|------------|----------|
| æ—¶é—´å¤æ‚åº¦ | $O(n^2 d)$ | $O(nd)$ |
| æ¨ç†é€Ÿåº¦ | æ…¢ï¼ˆKV cacheï¼‰ | å¿«ï¼ˆçŠ¶æ€å›ºå®šï¼‰ |
| é•¿åºåˆ—å¤„ç† | å—é™ | æ“…é•¿ |
| è®­ç»ƒæ•ˆç‡ | é«˜ï¼ˆå¹¶è¡Œï¼‰ | ä¸­ï¼ˆéœ€å¹¶è¡Œæ‰«æï¼‰ |

### 28.2.3 RetNetï¼ˆRetentive Networksï¼‰

RetNet ç»“åˆäº† Transformer å’Œ RNN çš„ä¼˜ç‚¹ã€‚

**ä¸‰ç§è®¡ç®—æ¨¡å¼**ï¼š
1. **å¹¶è¡Œæ¨¡å¼**ï¼ˆè®­ç»ƒï¼‰
2. **å¾ªç¯æ¨¡å¼**ï¼ˆæ¨ç†ï¼‰
3. **åˆ†å—å¾ªç¯æ¨¡å¼**ï¼ˆé•¿åºåˆ—ï¼‰

```python
# RetNet æ¦‚å¿µå®ç°ï¼ˆç®€åŒ–ï¼‰
import torch
import torch.nn as nn

class RetentionLayer(nn.Module):
    def __init__(self, d_model, num_heads=8):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        
        # Decay å‚æ•°
        self.gamma = nn.Parameter(torch.ones(num_heads) * 0.9)
        
    def parallel_forward(self, x):
        """å¹¶è¡Œæ¨¡å¼ï¼ˆè®­ç»ƒæ—¶ï¼‰"""
        batch, seq_len, _ = x.shape
        
        Q = self.q_proj(x).view(batch, seq_len, self.num_heads, self.head_dim)
        K = self.k_proj(x).view(batch, seq_len, self.num_heads, self.head_dim)
        V = self.v_proj(x).view(batch, seq_len, self.num_heads, self.head_dim)
        
        # Decay çŸ©é˜µ
        positions = torch.arange(seq_len, device=x.device)
        decay = self.gamma[:, None, None] ** (positions[None, :, None] - positions[None, None, :])
        decay = decay.tril()  # å› æœæ©ç 
        
        # Retention åˆ†æ•°
        scores = torch.einsum('bqhd,bkhd->bhqk', Q, K) * decay
        output = torch.einsum('bhqk,bkhd->bqhd', scores, V)
        
        output = output.reshape(batch, seq_len, self.d_model)
        return self.out_proj(output)
    
    def recurrent_forward(self, x_t, state):
        """å¾ªç¯æ¨¡å¼ï¼ˆæ¨ç†æ—¶ï¼‰"""
        # x_t: (batch, d_model) - å•ä¸ªæ—¶é—´æ­¥
        # state: (batch, num_heads, head_dim, head_dim)
        batch = x_t.shape[0]
        
        q_t = self.q_proj(x_t).view(batch, self.num_heads, self.head_dim)
        k_t = self.k_proj(x_t).view(batch, self.num_heads, self.head_dim)
        v_t = self.v_proj(x_t).view(batch, self.num_heads, self.head_dim)
        
        # æ›´æ–°çŠ¶æ€
        new_state = self.gamma[:, None, None] * state + \
                    torch.einsum('bhd,bhe->bhde', k_t, v_t)
        
        # è®¡ç®—è¾“å‡º
        output = torch.einsum('bhd,bhde->bhe', q_t, new_state)
        output = output.reshape(batch, self.d_model)
        
        return self.out_proj(output), new_state

# ä½¿ç”¨ç¤ºä¾‹
retention = RetentionLayer(d_model=512, num_heads=8)

# è®­ç»ƒæ¨¡å¼
x = torch.randn(2, 100, 512)
output_parallel = retention.parallel_forward(x)
print(output_parallel.shape)  # torch.Size([2, 100, 512])

# æ¨ç†æ¨¡å¼ï¼ˆé€ tokenï¼‰
state = torch.zeros(2, 8, 64, 64)  # (batch, num_heads, head_dim, head_dim)
for t in range(10):
    x_t = torch.randn(2, 512)
    output_t, state = retention.recurrent_forward(x_t, state)
    print(f"Step {t}: {output_t.shape}")  # torch.Size([2, 512])
```

### 28.2.4 RWKVï¼ˆRNN-like Transformerï¼‰

RWKV ä½¿ç”¨çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç° RNN çš„æ•ˆç‡å’Œ Transformer çš„å¹¶è¡Œæ€§ã€‚

```python
# RWKV ä½¿ç”¨ç¤ºä¾‹
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "RWKV/rwkv-4-169m-pile",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained("RWKV/rwkv-4-169m-pile")

prompt = "In a shocking finding, scientists discovered"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0]))
```

---

## 28.3 æ¨¡å‹åˆå¹¶ä¸ç»„åˆ

### 28.3.1 Model Mergingï¼ˆSLERPã€TIESï¼‰

æ¨¡å‹åˆå¹¶é€šè¿‡èåˆå¤šä¸ªæ¨¡å‹çš„æƒé‡ï¼Œåˆ›å»ºæ–°æ¨¡å‹ã€‚

**çº¿æ€§æ’å€¼ï¼ˆLinear Interpolationï¼‰**ï¼š

```python
import torch
from transformers import AutoModelForCausalLM

# åŠ è½½ä¸¤ä¸ªæ¨¡å‹
model_a = AutoModelForCausalLM.from_pretrained("gpt2")
model_b = AutoModelForCausalLM.from_pretrained("gpt2-medium")

# çº¿æ€§æ’å€¼ï¼ˆç®€åŒ–ç‰ˆ - éœ€è¦ç›¸åŒæ¶æ„ï¼‰
def linear_merge(model_a, model_b, alpha=0.5):
    """alpha=0.5 è¡¨ç¤º 50/50 æ··åˆ"""
    merged_state_dict = {}
    
    for key in model_a.state_dict().keys():
        if key in model_b.state_dict():
            merged_state_dict[key] = \
                alpha * model_a.state_dict()[key] + \
                (1 - alpha) * model_b.state_dict()[key]
    
    return merged_state_dict

# æ‰§è¡Œåˆå¹¶
merged_weights = linear_merge(model_a, model_b, alpha=0.5)

# åŠ è½½åˆ°æ–°æ¨¡å‹
merged_model = AutoModelForCausalLM.from_pretrained("gpt2")
merged_model.load_state_dict(merged_weights, strict=False)
```

**SLERPï¼ˆSpherical Linear Interpolationï¼‰**ï¼š

```python
def slerp_merge(model_a, model_b, alpha=0.5):
    """çƒé¢çº¿æ€§æ’å€¼"""
    import torch.nn.functional as F
    
    merged_state_dict = {}
    
    for key in model_a.state_dict().keys():
        if key in model_b.state_dict():
            w_a = model_a.state_dict()[key]
            w_b = model_b.state_dict()[key]
            
            # è®¡ç®—å¤¹è§’
            dot = (w_a * w_b).sum() / (w_a.norm() * w_b.norm())
            dot = torch.clamp(dot, -1.0, 1.0)
            theta = torch.acos(dot)
            
            # SLERP å…¬å¼
            if theta.abs() < 1e-6:
                # è¿‘ä¼¼çº¿æ€§
                merged_state_dict[key] = alpha * w_a + (1 - alpha) * w_b
            else:
                merged_state_dict[key] = \
                    (torch.sin((1 - alpha) * theta) / torch.sin(theta)) * w_a + \
                    (torch.sin(alpha * theta) / torch.sin(theta)) * w_b
    
    return merged_state_dict
```

**TIESï¼ˆTrim, Elect, and Mergeï¼‰**ï¼š

```python
def ties_merge(models, threshold=0.2):
    """
    TIES åˆå¹¶ç®—æ³•ï¼š
    1. Trim: ç§»é™¤å°æƒé‡
    2. Elect: é€‰æ‹©ç¬¦å·ä¸€è‡´çš„æƒé‡
    3. Merge: å¹³å‡åˆå¹¶
    """
    merged_state_dict = {}
    
    for key in models[0].state_dict().keys():
        weights = [model.state_dict()[key] for model in models]
        
        # 1. Trim - ç§»é™¤æ¥è¿‘é›¶çš„æƒé‡
        trimmed = [w * (w.abs() > threshold) for w in weights]
        
        # 2. Elect - é€‰æ‹©ç¬¦å·ä¸€è‡´çš„æƒé‡
        signs = torch.stack([w.sign() for w in trimmed])
        majority_sign = signs.sum(dim=0).sign()
        
        elected = [w * (w.sign() == majority_sign) for w in trimmed]
        
        # 3. Merge - å¹³å‡
        merged_state_dict[key] = sum(elected) / len(elected)
    
    return merged_state_dict
```

**ä½¿ç”¨ mergekit åº“**ï¼š

```bash
# å®‰è£…
pip install mergekit

# é…ç½®æ–‡ä»¶ merge_config.yaml
# models:
#   - model: model_a
#     weight: 0.5
#   - model: model_b
#     weight: 0.5
# merge_method: slerp

# æ‰§è¡Œåˆå¹¶
mergekit-yaml merge_config.yaml output_dir
```

### 28.3.2 LoRA é€‚é…å™¨ç»„åˆ

å¤šä¸ª LoRA é€‚é…å™¨å¯ä»¥åŠ¨æ€ç»„åˆï¼Œå®ç°å¤šä»»åŠ¡èƒ½åŠ›ã€‚

```python
from peft import PeftModel, LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM

# åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

# è®­ç»ƒå¤šä¸ª LoRA é€‚é…å™¨ï¼ˆç¤ºä¾‹ï¼‰
# adapter_1: æ•°å­¦ä»»åŠ¡
# adapter_2: ä»£ç ä»»åŠ¡
# adapter_3: å†™ä½œä»»åŠ¡

# åŠ è½½é€‚é…å™¨ 1
model_with_adapter1 = PeftModel.from_pretrained(base_model, "path/to/math_adapter")

# åˆ‡æ¢åˆ°é€‚é…å™¨ 2
model_with_adapter1.load_adapter("path/to/code_adapter", adapter_name="code")
model_with_adapter1.set_adapter("code")

# åŒæ—¶ä½¿ç”¨å¤šä¸ªé€‚é…å™¨ï¼ˆåŠ æƒç»„åˆï¼‰
model_with_adapter1.load_adapter("path/to/writing_adapter", adapter_name="writing")

# è®¾ç½®æƒé‡
model_with_adapter1.set_adapter(["code", "writing"])  # è‡ªåŠ¨å¹³å‡
# æˆ–æ‰‹åŠ¨è®¾ç½®æƒé‡
model_with_adapter1.set_adapter_weights({"code": 0.7, "writing": 0.3})

# ç”Ÿæˆ
inputs = tokenizer("Write a Python function", return_tensors="pt")
outputs = model_with_adapter1.generate(**inputs, max_new_tokens=100)
```

**åŠ¨æ€é€‚é…å™¨è·¯ç”±**ï¼š

```python
class AdapterRouter:
    def __init__(self, model, adapters):
        self.model = model
        self.adapters = adapters  # {"math": path, "code": path, ...}
        self.classifier = self.train_classifier()  # ä»»åŠ¡åˆ†ç±»å™¨
    
    def route_and_generate(self, prompt):
        # 1. åˆ†ç±»ä»»åŠ¡
        task = self.classify_task(prompt)
        
        # 2. åŠ è½½å¯¹åº”é€‚é…å™¨
        self.model.set_adapter(task)
        
        # 3. ç”Ÿæˆ
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = self.model.generate(**inputs, max_new_tokens=100)
        return tokenizer.decode(outputs[0])
    
    def classify_task(self, prompt):
        # ä½¿ç”¨å°å‹åˆ†ç±»å™¨åˆ¤æ–­ä»»åŠ¡ç±»å‹
        # è¿™é‡Œç®€åŒ–ä¸ºå…³é”®è¯åŒ¹é…
        if "code" in prompt.lower() or "python" in prompt.lower():
            return "code"
        elif "math" in prompt.lower() or "equation" in prompt.lower():
            return "math"
        else:
            return "writing"
```

### 28.3.3 Ensemble æ–¹æ³•

é›†æˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

# åŠ è½½å¤šä¸ªæ¨¡å‹
models = [
    AutoModelForSequenceClassification.from_pretrained("bert-base-uncased"),
    AutoModelForSequenceClassification.from_pretrained("roberta-base"),
    AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
]
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def ensemble_predict(text, models, method="voting"):
    """
    method: "voting" | "averaging" | "stacking"
    """
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
    all_logits = []
    for model in models:
        with torch.no_grad():
            outputs = model(**inputs)
            all_logits.append(outputs.logits)
    
    if method == "voting":
        # ç¡¬æŠ•ç¥¨
        predictions = [logits.argmax(dim=-1) for logits in all_logits]
        final_pred = torch.mode(torch.stack(predictions), dim=0).values
    
    elif method == "averaging":
        # æ¦‚ç‡å¹³å‡
        all_probs = [torch.softmax(logits, dim=-1) for logits in all_logits]
        avg_probs = torch.stack(all_probs).mean(dim=0)
        final_pred = avg_probs.argmax(dim=-1)
    
    elif method == "stacking":
        # å…ƒå­¦ä¹ å™¨ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºåŠ æƒå¹³å‡ï¼‰
        weights = torch.tensor([0.4, 0.35, 0.25])  # åŸºäºéªŒè¯é›†æ€§èƒ½
        weighted_logits = sum(w * logits for w, logits in zip(weights, all_logits))
        final_pred = weighted_logits.argmax(dim=-1)
    
    return final_pred

# æµ‹è¯•
text = "This movie is absolutely fantastic!"
prediction = ensemble_predict(text, models, method="averaging")
print(f"Ensemble prediction: {prediction}")
```

---

## 28.4 å¯è§£é‡Šæ€§ä¸åˆ†æ

### 28.4.1 Attention å¯è§†åŒ–ï¼ˆBertVizï¼‰

å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ï¼Œç†è§£æ¨¡å‹å…³æ³¨çš„å†…å®¹ã€‚

```python
# å®‰è£… bertviz
# pip install bertviz

from bertviz import head_view, model_view
from transformers import AutoTokenizer, AutoModel

# åŠ è½½æ¨¡å‹
model_name = "bert-base-uncased"
model = AutoModel.from_pretrained(model_name, output_attentions=True)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# å‡†å¤‡è¾“å…¥
text = "The cat sat on the mat"
inputs = tokenizer(text, return_tensors="pt")

# å‰å‘ä¼ æ’­
outputs = model(**inputs)
attention = outputs.attentions  # Tuple of (batch, num_heads, seq_len, seq_len)

# å¯è§†åŒ–æ³¨æ„åŠ›å¤´
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
head_view(attention, tokens)

# å¯è§†åŒ–æ‰€æœ‰å±‚
model_view(attention, tokens)
```

**è‡ªå®šä¹‰æ³¨æ„åŠ›åˆ†æ**ï¼š

```python
import torch
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention_heatmap(attention_weights, tokens, layer=0, head=0):
    """
    ç»˜åˆ¶æ³¨æ„åŠ›çƒ­åŠ›å›¾
    """
    # attention_weights: (num_layers, batch, num_heads, seq_len, seq_len)
    attn = attention_weights[layer][0, head].detach().cpu().numpy()
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        attn,
        xticklabels=tokens,
        yticklabels=tokens,
        cmap='viridis',
        cbar=True
    )
    plt.xlabel('Key Tokens')
    plt.ylabel('Query Tokens')
    plt.title(f'Attention Heatmap - Layer {layer}, Head {head}')
    plt.tight_layout()
    plt.show()

# ä½¿ç”¨
visualize_attention_heatmap(attention, tokens, layer=5, head=3)
```

<div data-component="AttentionPatternAnalyzer"></div>

### 28.4.2 æ¢é’ˆåˆ†ç±»ï¼ˆProbingï¼‰

é€šè¿‡æ¢é’ˆä»»åŠ¡è¯„ä¼°æ¨¡å‹çš„ä¸­é—´è¡¨ç¤ºç¼–ç äº†å“ªäº›ä¿¡æ¯ã€‚

```python
from transformers import AutoModel, AutoTokenizer
import torch
import torch.nn as nn
from sklearn.linear_model import LogisticRegression

# 1. æå–æ¨¡å‹è¡¨ç¤º
model = AutoModel.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def extract_representations(texts, layer=-1):
    """æå–æŒ‡å®šå±‚çš„è¡¨ç¤º"""
    representations = []
    
    for text in texts:
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        with torch.no_grad():
            outputs = model(**inputs, output_hidden_states=True)
            # å– [CLS] token çš„è¡¨ç¤º
            hidden_states = outputs.hidden_states[layer][:, 0, :]
            representations.append(hidden_states.squeeze().numpy())
    
    return torch.tensor(representations)

# 2. æ¢é’ˆä»»åŠ¡ï¼šå¥å­é•¿åº¦åˆ†ç±»
sentences = [
    "Short.",
    "A bit longer sentence.",
    "This is an even longer sentence with more words.",
    "Cat.",
    "The quick brown fox jumps over the lazy dog.",
]
labels = [0, 1, 2, 0, 2]  # 0: short, 1: medium, 2: long

# æå–è¡¨ç¤º
X = extract_representations(sentences, layer=6)  # æ¢æµ‹ç¬¬ 6 å±‚
y = torch.tensor(labels)

# è®­ç»ƒæ¢é’ˆåˆ†ç±»å™¨
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(
    X.numpy(), y.numpy(), test_size=0.3, random_state=42
)

probe = LogisticRegression(max_iter=1000)
probe.fit(X_train, y_train)

# è¯„ä¼°
y_pred = probe.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Probing accuracy: {accuracy:.2f}")

# 3. æ¢æµ‹ä¸åŒå±‚
layer_scores = []
for layer in range(model.config.num_hidden_layers):
    X_layer = extract_representations(sentences, layer=layer)
    probe.fit(X_layer[:-2].numpy(), y[:-2].numpy())  # ç®€åŒ– train/test split
    score = probe.score(X_layer[-2:].numpy(), y[-2:].numpy())
    layer_scores.append(score)

# ç»˜åˆ¶æ¯å±‚çš„æ¢æµ‹å‡†ç¡®ç‡
plt.plot(layer_scores)
plt.xlabel('Layer')
plt.ylabel('Probing Accuracy')
plt.title('Sentence Length Information Across Layers')
plt.show()
```

**å¸¸è§æ¢é’ˆä»»åŠ¡**ï¼š
- å¥æ³•ä¿¡æ¯ï¼ˆPOS taggingã€ä¾å­˜å…³ç³»ï¼‰
- è¯­ä¹‰ä¿¡æ¯ï¼ˆå‘½åå®ä½“ã€æƒ…æ„Ÿï¼‰
- å¥å­é•¿åº¦
- è¯åºä¿¡æ¯

### 28.4.3 æ¿€æ´»å€¼åˆ†æ

åˆ†æç¥ç»å…ƒçš„æ¿€æ´»æ¨¡å¼ã€‚

```python
def analyze_neuron_activation(model, texts, layer_idx=6, neuron_idx=100):
    """åˆ†æç‰¹å®šç¥ç»å…ƒå¯¹ä¸åŒè¾“å…¥çš„æ¿€æ´»"""
    activations = []
    
    for text in texts:
        inputs = tokenizer(text, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs, output_hidden_states=True)
            hidden_state = outputs.hidden_states[layer_idx]
            # å–ç‰¹å®šç¥ç»å…ƒçš„æ¿€æ´»ï¼ˆå¹³å‡æ‰€æœ‰ tokenï¼‰
            activation = hidden_state[0, :, neuron_idx].mean().item()
            activations.append(activation)
    
    return activations

# æµ‹è¯•ä¸åŒç±»å‹çš„å¥å­
positive_sentences = ["I love this!", "Amazing!", "Great work!"]
negative_sentences = ["I hate this.", "Terrible.", "Worst ever."]

pos_activations = analyze_neuron_activation(model, positive_sentences, layer_idx=6, neuron_idx=100)
neg_activations = analyze_neuron_activation(model, negative_sentences, layer_idx=6, neuron_idx=100)

print(f"Positive avg: {sum(pos_activations)/len(pos_activations):.3f}")
print(f"Negative avg: {sum(neg_activations)/len(neg_activations):.3f}")
```

### 28.4.4 å› æœå¹²é¢„å®éªŒ

é€šè¿‡å¹²é¢„æ¨¡å‹å†…éƒ¨è¡¨ç¤ºï¼Œæµ‹è¯•å› æœå…³ç³»ã€‚

```python
def causal_intervention(model, text, layer_idx, position, intervention_vector):
    """
    åœ¨æŒ‡å®šä½ç½®æ’å…¥å¹²é¢„å‘é‡
    """
    inputs = tokenizer(text, return_tensors="pt")
    
    # å®šä¹‰ hook å‡½æ•°
    def intervention_hook(module, input, output):
        # output: (batch, seq_len, hidden_size)
        output[0, position, :] = intervention_vector
        return output
    
    # æ³¨å†Œ hook
    layer = model.encoder.layer[layer_idx]
    handle = layer.register_forward_hook(intervention_hook)
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = model(**inputs)
    
    # ç§»é™¤ hook
    handle.remove()
    
    return outputs

# ç¤ºä¾‹ï¼šæ”¹å˜ç‰¹å®šä½ç½®çš„è¡¨ç¤ºï¼Œè§‚å¯Ÿå¯¹è¾“å‡ºçš„å½±å“
original_output = model(**tokenizer("The cat is happy", return_tensors="pt"))
intervened_output = causal_intervention(
    model,
    "The cat is happy",
    layer_idx=6,
    position=3,  # "happy" çš„ä½ç½®
    intervention_vector=torch.randn(768)  # éšæœºå¹²é¢„
)

print(f"Output difference: {(original_output.last_hidden_state - intervened_output.last_hidden_state).abs().mean()}")
```

---

## 28.5 å®‰å…¨æ€§ä¸å¯¹é½

### 28.5.1 å¯¹æŠ—æ”»å‡»ä¸é˜²å¾¡

**æ–‡æœ¬å¯¹æŠ—æ”»å‡»**ï¼ˆTextFoolerï¼‰ï¼š

```python
# å®‰è£… TextAttack
# pip install textattack

from textattack.models.wrappers import HuggingFaceModelWrapper
from textattack.attack_recipes import TextFoolerJin2019
from textattack.datasets import HuggingFaceDataset

# åŒ…è£…æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained("textattack/bert-base-uncased-SST-2")
tokenizer = AutoTokenizer.from_pretrained("textattack/bert-base-uncased-SST-2")
model_wrapper = HuggingFaceModelWrapper(model, tokenizer)

# åˆ›å»ºæ”»å‡»
attack = TextFoolerJin2019.build(model_wrapper)

# æµ‹è¯•
from textattack.datasets import Dataset

dataset = [("This movie is great!", 1)]  # (text, label)
attack_results = attack.attack_dataset(dataset)

for result in attack_results:
    print(f"Original: {result.original_text()}")
    print(f"Adversarial: {result.perturbed_text()}")
    print(f"Success: {result.goal_status}")
```

**å¯¹æŠ—è®­ç»ƒ**ï¼š

```python
from textattack.augmentation import Augmenter

# åˆ›å»ºå¢å¼ºå™¨ï¼ˆç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼‰
augmenter = Augmenter(
    transformation="word_swap_embedding",
    constraints=["repeat", "stopword"],
    pct_words_to_swap=0.1
)

# å¢å¼ºè®­ç»ƒæ•°æ®
original_text = "This movie is absolutely fantastic!"
augmented_texts = augmenter.augment(original_text, num_augmentations=5)

print("Original:", original_text)
for i, aug_text in enumerate(augmented_texts):
    print(f"Augmented {i+1}:", aug_text)

# å°†å¯¹æŠ—æ ·æœ¬åŠ å…¥è®­ç»ƒé›†
train_texts = [original_text] + augmented_texts
train_labels = [1] * len(train_texts)  # ä¿æŒç›¸åŒæ ‡ç­¾

# æ­£å¸¸è®­ç»ƒæµç¨‹
# trainer.train()
```

### 28.5.2 æœ‰å®³å†…å®¹æ£€æµ‹

```python
from transformers import pipeline

# ä½¿ç”¨ä¸“é—¨çš„æ¯’æ€§æ£€æµ‹æ¨¡å‹
toxicity_classifier = pipeline(
    "text-classification",
    model="unitary/toxic-bert"
)

texts = [
    "You are such a wonderful person!",
    "I hate you and wish you were dead.",  # æœ‰å®³å†…å®¹
    "This is a normal sentence."
]

results = toxicity_classifier(texts)
for text, result in zip(texts, results):
    print(f"Text: {text}")
    print(f"Label: {result['label']}, Score: {result['score']:.3f}\n")
```

**å†…å®¹è¿‡æ»¤å™¨**ï¼š

```python
class ContentFilter:
    def __init__(self, toxicity_threshold=0.7):
        self.detector = pipeline("text-classification", model="unitary/toxic-bert")
        self.threshold = toxicity_threshold
    
    def is_safe(self, text):
        result = self.detector(text)[0]
        is_toxic = result['label'] == 'toxic' and result['score'] > self.threshold
        return not is_toxic
    
    def filter_generation(self, model, tokenizer, prompt, **gen_kwargs):
        """ç”Ÿæˆå¹¶è¿‡æ»¤"""
        inputs = tokenizer(prompt, return_tensors="pt")
        
        # ç”Ÿæˆå¤šä¸ªå€™é€‰
        outputs = model.generate(
            **inputs,
            num_return_sequences=5,
            do_sample=True,
            **gen_kwargs
        )
        
        # è¿‡æ»¤å®‰å…¨çš„è¾“å‡º
        safe_outputs = []
        for output in outputs:
            text = tokenizer.decode(output, skip_special_tokens=True)
            if self.is_safe(text):
                safe_outputs.append(text)
        
        return safe_outputs if safe_outputs else ["[Content filtered]"]

# ä½¿ç”¨
filter = ContentFilter(toxicity_threshold=0.7)
safe_texts = filter.filter_generation(model, tokenizer, "Tell me about", max_new_tokens=50)
```

### 28.5.3 åè§è¯„ä¼°ï¼ˆBiasï¼‰

```python
# å®‰è£… lm-evaluation-harness
# pip install lm-eval

from lm_eval import evaluator

# è¯„ä¼°æ¨¡å‹åè§
results = evaluator.simple_evaluate(
    model="gpt2",
    tasks=["crows_pairs_english"],  # åè§è¯„ä¼°ä»»åŠ¡
    num_fewshot=0
)

print(results)
```

**è‡ªå®šä¹‰åè§æµ‹è¯•**ï¼š

```python
def bias_test(model, tokenizer, template_pairs):
    """
    æµ‹è¯•æ¨¡å‹å¯¹ä¸åŒç¾¤ä½“çš„åè§
    template_pairs: [("He is a {profession}", "She is a {profession}"), ...]
    """
    professions = ["doctor", "nurse", "engineer", "teacher"]
    
    results = []
    for male_template, female_template in template_pairs:
        for profession in professions:
            male_text = male_template.format(profession=profession)
            female_text = female_template.format(profession=profession)
            
            # è®¡ç®—å›°æƒ‘åº¦
            male_ppl = calculate_perplexity(model, tokenizer, male_text)
            female_ppl = calculate_perplexity(model, tokenizer, female_text)
            
            results.append({
                "profession": profession,
                "male_ppl": male_ppl,
                "female_ppl": female_ppl,
                "bias_score": abs(male_ppl - female_ppl)
            })
    
    return results

def calculate_perplexity(model, tokenizer, text):
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss
        perplexity = torch.exp(loss).item()
    return perplexity

# æµ‹è¯•
template_pairs = [
    ("He is a {profession}.", "She is a {profession}."),
    ("The {profession} finished his work.", "The {profession} finished her work.")
]

bias_results = bias_test(model, tokenizer, template_pairs)
for result in bias_results:
    print(f"{result['profession']}: Male PPL={result['male_ppl']:.2f}, "
          f"Female PPL={result['female_ppl']:.2f}, Bias={result['bias_score']:.2f}")
```

### 28.5.4 å¯æ§ç”Ÿæˆ

**PPLMï¼ˆPlug and Play Language Modelsï¼‰**ï¼š

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

class PPLMGenerator:
    def __init__(self, model_name="gpt2"):
        self.model = GPT2LMHeadModel.from_pretrained(model_name)
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        
        # å±æ€§åˆ†ç±»å™¨ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
        self.attribute_classifier = self.build_classifier()
    
    def build_classifier(self):
        # å®é™…åº”è¯¥æ˜¯è®­ç»ƒå¥½çš„åˆ†ç±»å™¨
        # è¿™é‡Œç®€åŒ–ä¸ºå…³é”®è¯åŒ¹é…
        return {
            "positive": ["happy", "great", "wonderful", "amazing"],
            "negative": ["sad", "terrible", "awful", "bad"]
        }
    
    def controlled_generate(self, prompt, attribute="positive", num_iterations=10, stepsize=0.01):
        """
        å—æ§ç”Ÿæˆ
        """
        inputs = self.tokenizer(prompt, return_tensors="pt")
        input_ids = inputs["input_ids"]
        
        # ç”Ÿæˆåˆå§‹å€™é€‰
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids,
                max_new_tokens=20,
                do_sample=True,
                output_scores=True,
                return_dict_in_generate=True
            )
        
        generated_ids = outputs.sequences[0]
        generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
        
        # è¿­ä»£ä¼˜åŒ–ï¼ˆç®€åŒ–ç‰ˆï¼‰
        for _ in range(num_iterations):
            # è®¡ç®—å±æ€§å¾—åˆ†
            score = self.compute_attribute_score(generated_text, attribute)
            
            # æ ¹æ®å¾—åˆ†è°ƒæ•´ï¼ˆå®é™… PPLM ä¼šåœ¨æ½œåœ¨ç©ºé—´æ“ä½œï¼‰
            if score < 0.5:
                # é‡æ–°ç”Ÿæˆ
                with torch.no_grad():
                    outputs = self.model.generate(
                        input_ids,
                        max_new_tokens=20,
                        do_sample=True,
                        temperature=1.2  # å¢åŠ å¤šæ ·æ€§
                    )
                generated_ids = outputs[0]
                generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
        
        return generated_text
    
    def compute_attribute_score(self, text, attribute):
        # ç®€åŒ–è¯„åˆ†
        keywords = self.attribute_classifier[attribute]
        score = sum(1 for kw in keywords if kw in text.lower()) / len(keywords)
        return score

# ä½¿ç”¨
generator = PPLMGenerator()
controlled_text = generator.controlled_generate(
    "The weather today is",
    attribute="positive"
)
print(controlled_text)
```

---

## 28.6 æœªæ¥å±•æœ›

### 28.6.1 å¤šæ¨¡æ€å¤§ä¸€ç»Ÿæ¨¡å‹

æœªæ¥è¶‹åŠ¿ï¼šå•ä¸€æ¨¡å‹å¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ã€‚

**ç¤ºä¾‹æ¨¡å‹**ï¼š
- GPT-4 Vision
- Gemini
- Llama 3.2 Vision

```python
# æœªæ¥çš„ç»Ÿä¸€ APIï¼ˆæ¦‚å¿µç¤ºä¾‹ï¼‰
from transformers import UniversalModel

model = UniversalModel.from_pretrained("future-model-v1")

# å¤šæ¨¡æ€è¾“å…¥
inputs = {
    "text": "Describe this image and video",
    "image": load_image("photo.jpg"),
    "video": load_video("clip.mp4"),
    "audio": load_audio("speech.wav")
}

# å¤šæ¨¡æ€è¾“å‡º
outputs = model.generate(**inputs, output_modalities=["text", "image"])

print(outputs["text"])  # æ–‡æœ¬æè¿°
display(outputs["image"])  # ç”Ÿæˆçš„å›¾åƒ
```

### 28.6.2 ç«¯åˆ°ç«¯è¯­éŸ³å¯¹è¯

**å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿ**ï¼š

```python
# æ¦‚å¿µç¤ºä¾‹ï¼šå®æ—¶è¯­éŸ³åˆ°è¯­éŸ³
from transformers import SpeechToSpeechPipeline

pipeline = SpeechToSpeechPipeline.from_pretrained("speech-llm-v1")

# æµå¼å¤„ç†
for audio_chunk in stream_microphone():
    response_audio = pipeline(
        audio_chunk,
        streaming=True,
        low_latency=True
    )
    play_audio(response_audio)
```

### 28.6.3 ä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelsï¼‰

æ¨¡å‹ä¸ä»…ç”Ÿæˆæ–‡æœ¬ï¼Œè¿˜èƒ½æ¨¡æ‹Ÿç‰©ç†ä¸–ç•Œã€‚

```python
# æ¦‚å¿µï¼šç‰©ç†æ„ŸçŸ¥çš„è¯­è¨€æ¨¡å‹
model = WorldModel.from_pretrained("physics-aware-llm")

query = "If I drop a ball from 10 meters, when will it hit the ground?"
response = model.generate(
    query,
    physical_simulation=True,
    gravity=9.8
)

print(response)  # "The ball will hit the ground in approximately 1.43 seconds."
print(response.simulation_trace)  # ç‰©ç†æ¨¡æ‹Ÿè½¨è¿¹
```

### 28.6.4 AGI è·¯å¾„æ¢è®¨

**å½“å‰æŒ‘æˆ˜**ï¼š
1. **å¸¸è¯†æ¨ç†**ï¼šæ¨¡å‹ä»ç¼ºä¹çœŸæ­£çš„å¸¸è¯†
2. **å› æœç†è§£**ï¼šå…³è” â‰  å› æœ
3. **æŒç»­å­¦ä¹ **ï¼šç¾éš¾æ€§é—å¿˜é—®é¢˜
4. **èƒ½æ•ˆæ¯”**ï¼šäººè„‘ 20W vs GPU æ•°ç™¾ W

**ç ”ç©¶æ–¹å‘**ï¼š
- ç¥ç»ç¬¦å·èåˆï¼ˆNeuro-Symbolic AIï¼‰
- å…ƒå­¦ä¹ ä¸å°‘æ ·æœ¬å­¦ä¹ 
- å¤šæ™ºèƒ½ä½“åä½œ
- å…·èº«æ™ºèƒ½ï¼ˆEmbodied AIï¼‰

---

## 28.7 å®æˆ˜ï¼šæ¢ç´¢å‰æ²¿æ¨¡å‹

### æ¡ˆä¾‹ 1ï¼šä½¿ç”¨ Mamba å¤„ç†è¶…é•¿åºåˆ—

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½ Mamba
model = AutoModelForCausalLM.from_pretrained(
    "state-spaces/mamba-2.8b",
    trust_remote_code=True,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("state-spaces/mamba-2.8b")

# è¶…é•¿æ–‡æœ¬ï¼ˆ10,000 tokensï¼‰
long_text = "The history of artificial intelligence. " * 2000
inputs = tokenizer(long_text, return_tensors="pt", truncation=False).to(model.device)

print(f"Input length: {inputs['input_ids'].shape[1]} tokens")

# Mamba å¯ä»¥é«˜æ•ˆå¤„ç†
import time
start = time.time()
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=100)
end = time.time()

print(f"Generation time: {end - start:.2f}s")
print(tokenizer.decode(outputs[0][-100:]))  # æœ€å 100 tokens
```

### æ¡ˆä¾‹ 2ï¼šæ¨¡å‹åˆå¹¶å®éªŒ

```python
from transformers import AutoModelForCausalLM

# åŠ è½½ä¸¤ä¸ªå¾®è°ƒæ¨¡å‹
model_math = AutoModelForCausalLM.from_pretrained("math-tuned-llama")
model_code = AutoModelForCausalLM.from_pretrained("code-tuned-llama")

# SLERP åˆå¹¶
merged_model = slerp_merge(model_math, model_code, alpha=0.5)

# ä¿å­˜åˆå¹¶æ¨¡å‹
merged_model.save_pretrained("merged-math-code-llama")

# æµ‹è¯•åˆå¹¶æ•ˆæœ
test_prompts = [
    "Solve: 2x + 5 = 15",  # æ•°å­¦
    "Write a Python function to sort a list",  # ä»£ç 
    "Explain bubble sort algorithm"  # ä¸¤è€…ç»“åˆ
]

for prompt in test_prompts:
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = merged_model.generate(**inputs, max_new_tokens=100)
    print(f"\nPrompt: {prompt}")
    print(f"Response: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")
```

### æ¡ˆä¾‹ 3ï¼šå¯è§£é‡Šæ€§åˆ†æ

```python
from bertviz import head_view
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained("bert-base-uncased", output_attentions=True)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# åˆ†æä¸€ä¸ªå¥å­
sentence = "The cat sat on the mat because it was comfortable"
inputs = tokenizer(sentence, return_tensors="pt")
outputs = model(**inputs)

# å¯è§†åŒ–æ³¨æ„åŠ›
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
attention = outputs.attentions

# æŸ¥çœ‹"because"å¦‚ä½•å…³æ³¨å…¶ä»–è¯
head_view(attention, tokens)

# åˆ†ææŒ‡ä»£æ¶ˆè§£
# "it" åº”è¯¥å…³æ³¨ "mat"
it_position = tokens.index('it')
mat_position = tokens.index('mat')

for layer in range(12):
    for head in range(12):
        attn_weight = attention[layer][0, head, it_position, mat_position].item()
        if attn_weight > 0.3:  # æ˜¾è‘—æ³¨æ„åŠ›
            print(f"Layer {layer}, Head {head}: it -> mat = {attn_weight:.3f}")
```

---

## 28.8 æœ€ä½³å®è·µä¸å»ºè®®

### è·Ÿè¸ªå‰æ²¿ç ”ç©¶

1. **è®ºæ–‡æ¥æº**ï¼š
   - arXivï¼ˆcs.CL, cs.LGï¼‰
   - NeurIPS, ICML, ICLR, ACL, EMNLP
   - Hugging Face Papersï¼ˆhttps://huggingface.co/papersï¼‰

2. **å¼€æºé¡¹ç›®**ï¼š
   - GitHub Trendingï¼ˆPython, Jupyter Notebookï¼‰
   - Hugging Face Spacesï¼ˆæ¼”ç¤ºé¡¹ç›®ï¼‰

3. **ç¤¾åŒºèµ„æº**ï¼š
   - Hugging Face Discord
   - Redditï¼ˆr/MachineLearningï¼‰
   - Twitter/Xï¼ˆå…³æ³¨ç ”ç©¶è€…ï¼‰

### å®éªŒæ–°æŠ€æœ¯

```python
# ä¿æŒä»£ç ç»“æ„çµæ´»ï¼Œä¾¿äºé›†æˆæ–°æ–¹æ³•
class ExperimentalPipeline:
    def __init__(self, model_type="transformer"):
        self.model_type = model_type
        self.model = self.load_model()
    
    def load_model(self):
        if self.model_type == "transformer":
            return AutoModelForCausalLM.from_pretrained("gpt2")
        elif self.model_type == "mamba":
            return AutoModelForCausalLM.from_pretrained("state-spaces/mamba-130m")
        elif self.model_type == "retnet":
            return self.load_retnet_model()  # è‡ªå®šä¹‰åŠ è½½
        else:
            raise ValueError(f"Unknown model type: {self.model_type}")
    
    def generate(self, prompt, **kwargs):
        # ç»Ÿä¸€ç”Ÿæˆæ¥å£
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.model.generate(**inputs, **kwargs)
        return self.tokenizer.decode(outputs[0])

# è½»æ¾åˆ‡æ¢ä¸åŒæ¶æ„
pipeline_transformer = ExperimentalPipeline("transformer")
pipeline_mamba = ExperimentalPipeline("mamba")

# å¯¹æ¯”æ€§èƒ½
import time

for pipeline, name in [(pipeline_transformer, "Transformer"), (pipeline_mamba, "Mamba")]:
    start = time.time()
    result = pipeline.generate("Once upon a time", max_new_tokens=100)
    end = time.time()
    print(f"{name}: {end - start:.2f}s")
```

### è´Ÿè´£ä»»çš„ AI å®è·µ

```python
# é›†æˆå®‰å…¨æ£€æŸ¥
class SafeGenerator:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.safety_filter = ContentFilter()
        self.bias_detector = BiasDetector()
    
    def safe_generate(self, prompt, **kwargs):
        # 1. æ£€æŸ¥è¾“å…¥
        if not self.safety_filter.is_safe(prompt):
            return "[Unsafe prompt detected]"
        
        # 2. ç”Ÿæˆ
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.model.generate(**inputs, **kwargs)
        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # 3. æ£€æŸ¥è¾“å‡º
        if not self.safety_filter.is_safe(text):
            return "[Output filtered for safety]"
        
        # 4. åè§æ£€æµ‹
        bias_score = self.bias_detector.detect(text)
        if bias_score > 0.8:
            return f"[Warning: High bias detected ({bias_score:.2f})] {text}"
        
        return text

# ä½¿ç”¨
safe_gen = SafeGenerator(model, tokenizer)
output = safe_gen.safe_generate("Tell me about", max_new_tokens=50)
```

---

## 28.9 ç« èŠ‚æ€»ç»“

æœ¬ç« æ¢ç´¢äº† Transformers é¢†åŸŸçš„å‰æ²¿ç ”ç©¶æ–¹å‘ï¼š

### æ ¸å¿ƒè¦ç‚¹

1. **é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡**ï¼š
   - ä½ç½®æ’å€¼ã€ALiBiã€RoPE æ‰©å±•
   - ç¨€ç–æ³¨æ„åŠ›ï¼ˆLongformerã€BigBirdï¼‰
   - RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰

2. **é«˜æ•ˆæ¶æ„**ï¼š
   - MoEï¼ˆä¸“å®¶æ··åˆï¼‰
   - State Space Modelsï¼ˆMambaã€S4ï¼‰
   - RetNetã€RWKV

3. **æ¨¡å‹åˆå¹¶ä¸ç»„åˆ**ï¼š
   - SLERPã€TIES åˆå¹¶ç®—æ³•
   - LoRA é€‚é…å™¨ç»„åˆ
   - Ensemble æ–¹æ³•

4. **å¯è§£é‡Šæ€§**ï¼š
   - æ³¨æ„åŠ›å¯è§†åŒ–
   - æ¢é’ˆåˆ†ç±»
   - å› æœå¹²é¢„

5. **å®‰å…¨æ€§ä¸å¯¹é½**ï¼š
   - å¯¹æŠ—æ”»å‡»é˜²å¾¡
   - æœ‰å®³å†…å®¹æ£€æµ‹
   - åè§è¯„ä¼°
   - å¯æ§ç”Ÿæˆ

### æœªæ¥è¶‹åŠ¿

- å¤šæ¨¡æ€å¤§ä¸€ç»Ÿ
- ç«¯åˆ°ç«¯è¯­éŸ³å¯¹è¯
- ä¸–ç•Œæ¨¡å‹
- AGI æ¢ç´¢

### å®è·µå»ºè®®

- æŒç»­å…³æ³¨å‰æ²¿ç ”ç©¶
- å®éªŒæ–°æŠ€æœ¯æ¶æ„
- è´Ÿè´£ä»»çš„ AI å¼€å‘
- å¹³è¡¡æ€§èƒ½ä¸å®‰å…¨

**æ­å–œå®Œæˆå…¨éƒ¨ 28 ç« æ ¸å¿ƒå†…å®¹ï¼** ğŸ‰

æ¥ä¸‹æ¥è¯·å‚è€ƒé™„å½•è·å–ï¼š
- å¸¸è§é”™è¯¯è°ƒè¯•æŒ‡å—
- æ€§èƒ½åŸºå‡†å¯¹æ¯”
- èµ„æºæ¸…å•
- API é€ŸæŸ¥è¡¨

---

**ä¸‹ä¸€æ­¥**ï¼š[Appendix A: å¸¸è§é”™è¯¯ä¸è°ƒè¯•](appendix-a-troubleshooting.md)

**ç›¸å…³ç« èŠ‚**ï¼š
- [Chapter 26: å¤šæ¨¡æ€æ¨¡å‹](26-multimodal.md)
- [Chapter 27: RLHF](27-rlhf.md)
