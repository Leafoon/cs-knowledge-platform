# é™„å½• (Appendices)

> **æœ¬é™„å½•æä¾›**ï¼šå¸¸è§é”™è¯¯è°ƒè¯•ã€æ€§èƒ½åŸºå‡†å¯¹æ¯”ã€èµ„æºæ¸…å•ã€API é€ŸæŸ¥è¡¨ç­‰å®ç”¨å‚è€ƒèµ„æ–™ã€‚

---

## Appendix A: å¸¸è§é”™è¯¯ä¸è°ƒè¯•

### A.1 CUDA Out of Memory (OOM)

**ç—‡çŠ¶**ï¼š
```
RuntimeError: CUDA out of memory. Tried to allocate XX MiB 
(GPU 0; XX GiB total capacity; XX GiB already allocated; ...)
```

**åŸå› **ï¼š
- æ‰¹æ¬¡å¤§å°ï¼ˆbatch sizeï¼‰è¿‡å¤§
- æ¨¡å‹å‚æ•°é‡è¿‡å¤§
- åºåˆ—é•¿åº¦è¿‡é•¿
- æ¢¯åº¦ç´¯ç§¯æœªæ¸…é™¤
- ç¼“å­˜æœªé‡Šæ”¾

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **å‡å°æ‰¹æ¬¡å¤§å°**ï¼š
```python
# ä»å¤§åˆ°å°é€æ­¥å°è¯•
training_args = TrainingArguments(
    per_device_train_batch_size=1,  # æœ€å°åŒ–æ‰¹æ¬¡
    gradient_accumulation_steps=16,  # ç´¯ç§¯æ¢¯åº¦æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡
)
```

2. **ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼š
```python
model.gradient_checkpointing_enable()

# æˆ–åœ¨ config ä¸­
model.config.use_cache = False  # ç¦ç”¨ KV cache
model.config.gradient_checkpointing = True
```

3. **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼š
```python
training_args = TrainingArguments(
    fp16=True,  # æˆ– bf16=True
)
```

4. **é‡åŒ–åŠ è½½æ¨¡å‹**ï¼š
```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=bnb_config,
    device_map="auto"
)
```

5. **æ¸…é™¤ç¼“å­˜**ï¼š
```python
import torch
import gc

# æ¸…é™¤æœªä½¿ç”¨çš„ç¼“å­˜
torch.cuda.empty_cache()
gc.collect()

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
for batch in dataloader:
    optimizer.zero_grad()
    outputs = model(**batch)
    loss = outputs.loss
    loss.backward()
    optimizer.step()
    
    # å®šæœŸæ¸…ç†
    if step % 100 == 0:
        torch.cuda.empty_cache()
```

6. **ä½¿ç”¨ CPU offload**ï¼š
```python
from accelerate import infer_auto_device_map

device_map = infer_auto_device_map(
    model,
    max_memory={0: "10GiB", "cpu": "30GiB"}
)
model = AutoModelForCausalLM.from_pretrained(
    "large-model",
    device_map=device_map
)
```

---

### A.2 Tokenizer ä¸åŒ¹é…

**ç—‡çŠ¶**ï¼š
```
Warning: Some weights of the model checkpoint were not used: ['lm_head.weight']
```

**åŸå› **ï¼š
- ä½¿ç”¨äº†ä¸åŒæ¨¡å‹çš„ tokenizer
- è¯æ±‡è¡¨å¤§å°ä¸ä¸€è‡´
- ç‰¹æ®Š token é…ç½®ä¸åŒ

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **ç¡®ä¿ tokenizer å’Œæ¨¡å‹åŒ¹é…**ï¼š
```python
# âœ— é”™è¯¯
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")  # ä¸åŒ¹é…ï¼

# âœ“ æ­£ç¡®
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

2. **æ£€æŸ¥è¯æ±‡è¡¨å¤§å°**ï¼š
```python
print(f"Model vocab size: {model.config.vocab_size}")
print(f"Tokenizer vocab size: {len(tokenizer)}")

# å¦‚æœä¸ä¸€è‡´ï¼Œè°ƒæ•´æ¨¡å‹
if model.config.vocab_size != len(tokenizer):
    model.resize_token_embeddings(len(tokenizer))
```

3. **æ·»åŠ ç‰¹æ®Š token**ï¼š
```python
# æ·»åŠ æ–° token
special_tokens_dict = {'additional_special_tokens': ['<NEW_TOKEN>']}
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)

# è°ƒæ•´æ¨¡å‹åµŒå…¥å±‚
model.resize_token_embeddings(len(tokenizer))
```

---

### A.3 æƒé‡åŠ è½½è­¦å‘Š

**ç—‡çŠ¶**ï¼š
```
Some weights of XxxForSequenceClassification were not initialized from the model checkpoint:
['classifier.weight', 'classifier.bias']
```

**åŸå› **ï¼š
- å¾®è°ƒä»»åŠ¡å¤´ï¼ˆclassification headï¼‰æœªé¢„è®­ç»ƒ
- æ¨¡å‹æ¶æ„å˜åŒ–
- æ­£å¸¸ç°è±¡ï¼ˆå¤šæ•°æƒ…å†µï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **æ­£å¸¸æƒ…å†µï¼ˆå¯å¿½ç•¥ï¼‰**ï¼š
```python
# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ç”¨äºåˆ†ç±»ä»»åŠ¡
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=3  # åˆ†ç±»å¤´ä¼šéšæœºåˆå§‹åŒ–
)
# è­¦å‘Šæ˜¯æ­£å¸¸çš„ï¼Œå› ä¸º BERT é¢„è®­ç»ƒæ—¶æ²¡æœ‰åˆ†ç±»å¤´
```

2. **ä»å¾®è°ƒæ£€æŸ¥ç‚¹åŠ è½½**ï¼š
```python
# å¦‚æœè¦åŠ è½½ä¹‹å‰å¾®è°ƒè¿‡çš„æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained(
    "./fine-tuned-model"  # æœ¬åœ°è·¯å¾„ï¼ŒåŒ…å«å®Œæ•´æƒé‡
)
```

3. **å¿½ç•¥ç‰¹å®šå±‚**ï¼š
```python
model = AutoModelForCausalLM.from_pretrained(
    "gpt2",
    ignore_mismatched_sizes=True  # å¿½ç•¥å¤§å°ä¸åŒ¹é…
)
```

---

### A.4 åˆ†å¸ƒå¼è®­ç»ƒå¡æ­»

**ç—‡çŠ¶**ï¼š
- ç¨‹åºå¯åŠ¨åå¡ä½ä¸åŠ¨
- å¤šè¿›ç¨‹æ— æ³•é€šä¿¡
- `torch.distributed.init_process_group()` è¶…æ—¶

**åŸå› **ï¼š
- ç¯å¢ƒå˜é‡æœªè®¾ç½®
- ç«¯å£è¢«å ç”¨
- ç½‘ç»œé…ç½®é—®é¢˜
- ä»£ç æœ‰æ­»é”

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **æ£€æŸ¥ç¯å¢ƒå˜é‡**ï¼š
```bash
# å•æœºå¤šå¡
export MASTER_ADDR=localhost
export MASTER_PORT=29500
export WORLD_SIZE=4
export RANK=0

# å¤šæœº
export MASTER_ADDR=ä¸»èŠ‚ç‚¹IP
export MASTER_PORT=29500
```

2. **ä½¿ç”¨ torchrun**ï¼ˆæ¨èï¼‰ï¼š
```bash
# å•æœº 4 å¡
torchrun --nproc_per_node=4 train.py

# å¤šæœºï¼ˆèŠ‚ç‚¹ 0ï¼‰
torchrun --nnodes=2 --nproc_per_node=4 \
         --node_rank=0 \
         --master_addr=192.168.1.1 \
         --master_port=29500 \
         train.py

# å¤šæœºï¼ˆèŠ‚ç‚¹ 1ï¼‰
torchrun --nnodes=2 --nproc_per_node=4 \
         --node_rank=1 \
         --master_addr=192.168.1.1 \
         --master_port=29500 \
         train.py
```

3. **ä½¿ç”¨ Accelerate**ï¼š
```bash
accelerate config  # é…ç½®
accelerate launch train.py  # å¯åŠ¨
```

4. **è°ƒè¯•æŠ€å·§**ï¼š
```python
import torch.distributed as dist

# æ·»åŠ è¶…æ—¶æ£€æµ‹
dist.init_process_group(
    backend='nccl',
    timeout=datetime.timedelta(seconds=30)  # 30ç§’è¶…æ—¶
)

# æ‰“å°è°ƒè¯•ä¿¡æ¯
import os
print(f"RANK: {os.environ.get('RANK', 'Not set')}")
print(f"WORLD_SIZE: {os.environ.get('WORLD_SIZE', 'Not set')}")
print(f"MASTER_ADDR: {os.environ.get('MASTER_ADDR', 'Not set')}")
```

5. **å¸¸è§æ­»é”åœºæ™¯**ï¼š
```python
# âœ— é”™è¯¯ï¼šä¸åŒè¿›ç¨‹æ‰§è¡Œä¸åŒä»£ç è·¯å¾„
if rank == 0:
    dist.barrier()  # åªæœ‰ rank 0 ç­‰å¾…ï¼Œå…¶ä»–è¿›ç¨‹ä¸ç­‰å¾… â†’ æ­»é”

# âœ“ æ­£ç¡®ï¼šæ‰€æœ‰è¿›ç¨‹æ‰§è¡Œç›¸åŒæ“ä½œ
dist.barrier()  # æ‰€æœ‰è¿›ç¨‹éƒ½ç­‰å¾…

# âœ— é”™è¯¯ï¼šæ¡ä»¶ä¸ä¸€è‡´
if condition_that_varies_by_rank:
    dist.all_reduce(tensor)  # åªæœ‰éƒ¨åˆ†è¿›ç¨‹å‚ä¸ â†’ æ­»é”

# âœ“ æ­£ç¡®ï¼šæ‰€æœ‰è¿›ç¨‹éƒ½å‚ä¸é›†åˆé€šä¿¡
dist.all_reduce(tensor)
```

---

### A.5 ç”Ÿæˆè´¨é‡å·®

**ç—‡çŠ¶**ï¼š
- ç”Ÿæˆé‡å¤å†…å®¹
- è¾“å‡ºä¸è¿è´¯
- ç”Ÿæˆåœä¸ä¸‹æ¥
- ç”Ÿæˆç»“æœå•è°ƒ

**åŸå› **ï¼š
- é‡‡æ ·ç­–ç•¥ä¸å½“
- æ¸©åº¦è®¾ç½®ä¸åˆç†
- æ²¡æœ‰è®¾ç½®åœæ­¢æ¡ä»¶
- æ¨¡å‹æœªå……åˆ†è®­ç»ƒ

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **è°ƒæ•´é‡‡æ ·å‚æ•°**ï¼š
```python
outputs = model.generate(
    **inputs,
    max_new_tokens=100,
    
    # æ–¹æ³• 1: Greedy (ç¡®å®šæ€§)
    do_sample=False,
    
    # æ–¹æ³• 2: Top-K é‡‡æ ·
    do_sample=True,
    top_k=50,
    temperature=0.7,
    
    # æ–¹æ³• 3: Top-P (Nucleus)
    do_sample=True,
    top_p=0.9,
    temperature=0.8,
    
    # æ–¹æ³• 4: Beam Search
    num_beams=5,
    early_stopping=True,
    
    # é˜²æ­¢é‡å¤
    repetition_penalty=1.2,
    no_repeat_ngram_size=3,
)
```

2. **è®¾ç½®åœæ­¢æ¡ä»¶**ï¼š
```python
# æ–¹æ³• 1: EOS token
outputs = model.generate(
    **inputs,
    max_new_tokens=200,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id
)

# æ–¹æ³• 2: è‡ªå®šä¹‰åœæ­¢åºåˆ—
from transformers import StoppingCriteria, StoppingCriteriaList

class CustomStoppingCriteria(StoppingCriteria):
    def __init__(self, stop_sequences, tokenizer):
        self.stop_sequences = stop_sequences
        self.tokenizer = tokenizer
    
    def __call__(self, input_ids, scores, **kwargs):
        decoded = self.tokenizer.decode(input_ids[0])
        return any(seq in decoded for seq in self.stop_sequences)

stopping_criteria = StoppingCriteriaList([
    CustomStoppingCriteria(["\n\n", "END"], tokenizer)
])

outputs = model.generate(
    **inputs,
    stopping_criteria=stopping_criteria
)
```

3. **æ¸©åº¦è°ƒä¼˜æŒ‡å—**ï¼š
```python
# temperature < 1.0: æ›´ä¿å®ˆã€ç¡®å®š
# temperature = 1.0: æ ‡å‡†é‡‡æ ·
# temperature > 1.0: æ›´éšæœºã€åˆ›é€ æ€§

# äº‹å®æ€§ä»»åŠ¡ï¼ˆQAã€æ‘˜è¦ï¼‰
temperature=0.3

# åˆ›é€ æ€§ä»»åŠ¡ï¼ˆæ•…äº‹ã€è¯—æ­Œï¼‰
temperature=1.0

# æåº¦éšæœºï¼ˆå¤´è„‘é£æš´ï¼‰
temperature=1.5
```

4. **å¯¹æ¯”ç”Ÿæˆé…ç½®**ï¼š
```python
# é…ç½® 1: äº‹å®æ€§ç”Ÿæˆ
generation_config_factual = GenerationConfig(
    do_sample=False,  # Greedy
    max_new_tokens=100,
    repetition_penalty=1.1
)

# é…ç½® 2: å¹³è¡¡ç”Ÿæˆ
generation_config_balanced = GenerationConfig(
    do_sample=True,
    top_p=0.9,
    temperature=0.7,
    max_new_tokens=150,
    repetition_penalty=1.2
)

# é…ç½® 3: åˆ›é€ æ€§ç”Ÿæˆ
generation_config_creative = GenerationConfig(
    do_sample=True,
    top_k=50,
    temperature=1.0,
    max_new_tokens=200,
    no_repeat_ngram_size=2
)

# ä½¿ç”¨
outputs = model.generate(**inputs, generation_config=generation_config_balanced)
```

---

## Appendix B: æ€§èƒ½åŸºå‡†æµ‹è¯•

### B.1 å¸¸è§æ¨¡å‹æ¨ç†é€Ÿåº¦å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | åºåˆ—é•¿åº¦ | ååé‡ (tokens/s) | å»¶è¿Ÿ (ms/token) | æ˜¾å­˜ (GB) |
|------|--------|----------|-------------------|-----------------|-----------|
| BERT-base | 110M | 512 | 1200 | 0.8 | 0.4 |
| RoBERTa-large | 355M | 512 | 450 | 2.2 | 1.4 |
| GPT-2 (small) | 124M | 1024 | 800 | 1.25 | 0.5 |
| GPT-2 (medium) | 355M | 1024 | 320 | 3.1 | 1.4 |
| GPT-2 (large) | 774M | 1024 | 150 | 6.7 | 3.1 |
| GPT-2 (xl) | 1.5B | 1024 | 75 | 13.3 | 6.0 |
| LLaMA-7B | 7B | 2048 | 30 | 33 | 28 |
| LLaMA-13B | 13B | 2048 | 16 | 62 | 52 |
| LLaMA-70B | 70B | 2048 | 3 | 333 | 280 |

**æµ‹è¯•ç¯å¢ƒ**: NVIDIA A100 40GB, batch_size=1, FP16

### B.2 è®­ç»ƒååé‡å¯¹æ¯”

| æ¨¡å‹ | æ‰¹æ¬¡å¤§å° | æ¢¯åº¦ç´¯ç§¯ | ååé‡ (samples/s) | GPU åˆ©ç”¨ç‡ |
|------|----------|----------|-------------------|------------|
| BERT-base | 32 | 1 | 120 | 85% |
| BERT-base | 8 | 4 | 115 | 82% |
| GPT-2 (medium) | 16 | 1 | 45 | 90% |
| GPT-2 (medium) | 4 | 4 | 42 | 88% |
| LLaMA-7B | 4 | 8 | 8 | 95% |
| LLaMA-7B (QLoRA) | 8 | 4 | 12 | 92% |

**æµ‹è¯•ç¯å¢ƒ**: NVIDIA A100 40GB, FP16/BF16

### B.3 æ˜¾å­˜å ç”¨å¯¹æ¯”è¡¨

| æ“ä½œ | BERT-base | GPT-2 | LLaMA-7B |
|------|-----------|-------|----------|
| æ¨ç† (FP32) | 1.2 GB | 2.4 GB | 28 GB |
| æ¨ç† (FP16) | 0.6 GB | 1.2 GB | 14 GB |
| æ¨ç† (INT8) | 0.3 GB | 0.6 GB | 7 GB |
| æ¨ç† (INT4) | 0.2 GB | 0.3 GB | 3.5 GB |
| è®­ç»ƒ (FP32) | 4.8 GB | 9.6 GB | 112 GB |
| è®­ç»ƒ (FP16 + AMP) | 2.8 GB | 5.2 GB | 56 GB |
| è®­ç»ƒ (LoRA) | 1.0 GB | 1.8 GB | 16 GB |
| è®­ç»ƒ (QLoRA 4-bit) | 0.6 GB | 1.0 GB | 9 GB |

### B.4 é‡åŒ–æ–¹æ³•å¯¹æ¯”çŸ©é˜µ

| é‡åŒ–æ–¹æ³• | ç²¾åº¦ | é€Ÿåº¦ | æ˜¾å­˜èŠ‚çœ | å‡†ç¡®åº¦ | æ˜“ç”¨æ€§ |
|----------|------|------|----------|--------|--------|
| FP16 | 16-bit | â­â­â­â­ | 50% | â­â­â­â­â­ | â­â­â­â­â­ |
| BF16 | 16-bit | â­â­â­â­ | 50% | â­â­â­â­â­ | â­â­â­â­â­ |
| INT8 (åŠ¨æ€) | 8-bit | â­â­â­â­â­ | 75% | â­â­â­â­ | â­â­â­â­ |
| INT8 (é™æ€) | 8-bit | â­â­â­â­â­ | 75% | â­â­â­â­â­ | â­â­â­ |
| GPTQ | 4-bit | â­â­â­â­ | 87.5% | â­â­â­â­ | â­â­â­ |
| AWQ | 4-bit | â­â­â­â­â­ | 87.5% | â­â­â­â­ | â­â­â­ |
| NF4 (QLoRA) | 4-bit | â­â­â­ | 87.5% | â­â­â­â­ | â­â­â­â­ |

---

## Appendix C: èµ„æºæ¸…å•

### C.1 å®˜æ–¹æ–‡æ¡£ä¸æ•™ç¨‹

**Hugging Face å®˜æ–¹èµ„æº**ï¼š
- ğŸ“– [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)
- ğŸ“– [Datasets æ–‡æ¡£](https://huggingface.co/docs/datasets)
- ğŸ“– [PEFT æ–‡æ¡£](https://huggingface.co/docs/peft)
- ğŸ“– [Accelerate æ–‡æ¡£](https://huggingface.co/docs/accelerate)
- ğŸ“– [Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum)
- ğŸ“– [TRL æ–‡æ¡£](https://huggingface.co/docs/trl)
- ğŸ“ [Hugging Face Course](https://huggingface.co/learn/nlp-course)
- ğŸ¥ [YouTube å®˜æ–¹é¢‘é“](https://www.youtube.com/@HuggingFace)

**PyTorch ç›¸å…³**ï¼š
- ğŸ“– [PyTorch å®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs)
- ğŸ“– [PyTorch Tutorials](https://pytorch.org/tutorials)
- ğŸ“– [DeepSpeed æ–‡æ¡£](https://www.deepspeed.ai/)
- ğŸ“– [FSDP æŒ‡å—](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)

### C.2 é‡è¦è®ºæ–‡åˆ—è¡¨

**åŸºç¡€æ¶æ„**ï¼š
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017) - Transformer åŸè®ºæ–‡
- [BERT](https://arxiv.org/abs/1810.04805) (2018) - é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
- [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (2019)
- [GPT-3](https://arxiv.org/abs/2005.14165) (2020) - å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹
- [T5](https://arxiv.org/abs/1910.10683) (2020) - Text-to-Text Transfer

**é«˜æ•ˆè®­ç»ƒ**ï¼š
- [LoRA](https://arxiv.org/abs/2106.09685) (2021) - ä½ç§©é€‚é…
- [QLoRA](https://arxiv.org/abs/2305.14314) (2023) - é‡åŒ– LoRA
- [FlashAttention](https://arxiv.org/abs/2205.14135) (2022) - IOä¼˜åŒ–æ³¨æ„åŠ›
- [FlashAttention-2](https://arxiv.org/abs/2307.08691) (2023)

**é‡åŒ–ä¸å‹ç¼©**ï¼š
- [GPTQ](https://arxiv.org/abs/2210.17323) (2022) - åè®­ç»ƒé‡åŒ–
- [AWQ](https://arxiv.org/abs/2306.00978) (2023) - æ¿€æ´»æ„ŸçŸ¥é‡åŒ–
- [SmoothQuant](https://arxiv.org/abs/2211.10438) (2022)

**é•¿ä¸Šä¸‹æ–‡**ï¼š
- [Longformer](https://arxiv.org/abs/2004.05150) (2020) - ç¨€ç–æ³¨æ„åŠ›
- [BigBird](https://arxiv.org/abs/2007.14062) (2020)
- [ALiBi](https://arxiv.org/abs/2108.12409) (2021) - çº¿æ€§åç½®
- [RoPE](https://arxiv.org/abs/2104.09864) (2021) - æ—‹è½¬ä½ç½®ç¼–ç 

**RLHF ä¸å¯¹é½**ï¼š
- [InstructGPT](https://arxiv.org/abs/2203.02155) (2022) - RLHF
- [DPO](https://arxiv.org/abs/2305.18290) (2023) - ç›´æ¥åå¥½ä¼˜åŒ–
- [Constitutional AI](https://arxiv.org/abs/2212.08073) (2022)

**å¤šæ¨¡æ€**ï¼š
- [CLIP](https://arxiv.org/abs/2103.00020) (2021)
- [Flamingo](https://arxiv.org/abs/2204.14198) (2022)
- [LLaVA](https://arxiv.org/abs/2304.08485) (2023)

### C.3 æ¨èå¼€æºé¡¹ç›®

**è®­ç»ƒæ¡†æ¶**ï¼š
- [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) - å¾®è°ƒå·¥å…·
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) - ä¸€ç«™å¼å¾®è°ƒ
- [Unsloth](https://github.com/unslothai/unsloth) - æé€Ÿå¾®è°ƒ

**æ¨ç†ä¼˜åŒ–**ï¼š
- [vLLM](https://github.com/vllm-project/vllm) - é«˜æ€§èƒ½æ¨ç†
- [Text Generation Inference](https://github.com/huggingface/text-generation-inference) - HF å®˜æ–¹
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - C++ æ¨ç†

**é‡åŒ–å·¥å…·**ï¼š
- [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
- [AutoAWQ](https://github.com/casper-hansen/AutoAWQ)
- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

**è¯„ä¼°æ¡†æ¶**ï¼š
- [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
- [OpenAI Evals](https://github.com/openai/evals)

### C.4 ç¤¾åŒºèµ„æº

**è®ºå›ä¸ç¤¾åŒº**ï¼š
- ğŸ’¬ [Hugging Face Discord](https://hf.co/join/discord)
- ğŸ’¬ [Hugging Face Forums](https://discuss.huggingface.co/)
- ğŸ¦ Twitter: [@huggingface](https://twitter.com/huggingface)
- ğŸ“§ [Newsletter](https://huggingface.co/subscribe)

**å­¦ä¹ èµ„æº**ï¼š
- [Papers with Code](https://paperswithcode.com/)
- [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)
- [Hugging Face Spaces](https://huggingface.co/spaces) - åœ¨çº¿æ¼”ç¤º

---

## Appendix D: API é€ŸæŸ¥è¡¨

### D.1 AutoModelForXXX ç±»åˆ—è¡¨

```python
from transformers import (
    # å› æœè¯­è¨€æ¨¡å‹ï¼ˆæ–‡æœ¬ç”Ÿæˆï¼‰
    AutoModelForCausalLM,
    
    # åºåˆ—åˆ°åºåˆ—ï¼ˆç¿»è¯‘ã€æ‘˜è¦ï¼‰
    AutoModelForSeq2SeqLM,
    
    # æ©ç è¯­è¨€æ¨¡å‹ï¼ˆå¡«ç©ºï¼‰
    AutoModelForMaskedLM,
    
    # åºåˆ—åˆ†ç±»ï¼ˆæƒ…æ„Ÿåˆ†æã€æ–‡æœ¬åˆ†ç±»ï¼‰
    AutoModelForSequenceClassification,
    
    # Token åˆ†ç±»ï¼ˆNERã€POSï¼‰
    AutoModelForTokenClassification,
    
    # é—®ç­”
    AutoModelForQuestionAnswering,
    
    # å¤šé€‰é¢˜
    AutoModelForMultipleChoice,
    
    # å›¾åƒåˆ†ç±»
    AutoModelForImageClassification,
    
    # è¯­éŸ³è¯†åˆ«
    AutoModelForSpeechSeq2Seq,
    
    # è§†è§‰é—®ç­”
    AutoModelForVisualQuestionAnswering,
)
```

### D.2 TrainingArguments å‚æ•°é€ŸæŸ¥

```python
from transformers import TrainingArguments

args = TrainingArguments(
    # åŸºç¡€å‚æ•°
    output_dir="./results",
    overwrite_output_dir=True,
    
    # è®­ç»ƒå‚æ•°
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=4,
    learning_rate=5e-5,
    weight_decay=0.01,
    warmup_steps=500,
    max_grad_norm=1.0,
    
    # è¯„ä¼°ä¸ä¿å­˜
    evaluation_strategy="steps",  # "no", "steps", "epoch"
    eval_steps=500,
    save_strategy="steps",
    save_steps=1000,
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    
    # æ—¥å¿—
    logging_dir="./logs",
    logging_steps=100,
    report_to=["tensorboard", "wandb"],
    
    # æ··åˆç²¾åº¦
    fp16=True,  # NVIDIA GPU
    bf16=False,  # TPU / Ampere GPU
    fp16_opt_level="O1",
    
    # åˆ†å¸ƒå¼è®­ç»ƒ
    ddp_backend="nccl",
    ddp_find_unused_parameters=False,
    
    # DeepSpeed
    deepspeed="ds_config.json",
    
    # å…¶ä»–
    seed=42,
    dataloader_num_workers=4,
    remove_unused_columns=True,
    push_to_hub=False,
)
```

### D.3 Generation Config å‚æ•°

```python
from transformers import GenerationConfig

config = GenerationConfig(
    # é•¿åº¦æ§åˆ¶
    max_length=100,
    max_new_tokens=50,
    min_length=0,
    min_new_tokens=0,
    
    # é‡‡æ ·ç­–ç•¥
    do_sample=True,
    temperature=0.7,
    top_k=50,
    top_p=0.9,
    
    # Beam Search
    num_beams=5,
    num_beam_groups=1,
    diversity_penalty=0.0,
    early_stopping=True,
    
    # é‡å¤æ§åˆ¶
    repetition_penalty=1.2,
    no_repeat_ngram_size=3,
    encoder_no_repeat_ngram_size=0,
    
    # åœæ­¢æ¡ä»¶
    eos_token_id=2,
    pad_token_id=0,
    forced_eos_token_id=None,
    
    # å¤šæ ·æ€§
    num_return_sequences=1,
    output_scores=False,
    output_attentions=False,
    output_hidden_states=False,
    return_dict_in_generate=False,
)

# ä½¿ç”¨
outputs = model.generate(**inputs, generation_config=config)
```

### D.4 PEFT é…ç½®å‚æ•°

**LoRA é…ç½®**ï¼š
```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,  # ç§©
    lora_alpha=32,  # ç¼©æ”¾å› å­
    target_modules=["q_proj", "v_proj"],  # ç›®æ ‡æ¨¡å—
    lora_dropout=0.1,
    bias="none",  # "none", "all", "lora_only"
    task_type="CAUSAL_LM",  # "SEQ_CLS", "SEQ_2_SEQ_LM", etc.
)

model = get_peft_model(model, lora_config)
```

**QLoRA é…ç½®**ï¼š
```python
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # "fp4", "nf4"
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,  # åµŒå¥—é‡åŒ–
)

model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    quantization_config=bnb_config
)
```

**Prefix Tuning é…ç½®**ï¼š
```python
from peft import PrefixTuningConfig

prefix_config = PrefixTuningConfig(
    task_type="CAUSAL_LM",
    num_virtual_tokens=20,
    prefix_projection=False,
)
```

**Prompt Tuning é…ç½®**ï¼š
```python
from peft import PromptTuningConfig

prompt_config = PromptTuningConfig(
    task_type="CAUSAL_LM",
    num_virtual_tokens=20,
    prompt_tuning_init="TEXT",  # "RANDOM", "TEXT"
    prompt_tuning_init_text="Classify if the tweet is positive, negative or neutral:",
    tokenizer_name_or_path="model_name",
)
```

---

## é™„å½•æ€»ç»“

æœ¬é™„å½•æä¾›äº†å®ç”¨çš„å‚è€ƒèµ„æ–™ï¼š

- **Appendix A**: 5 ä¸ªå¸¸è§é”™è¯¯çš„è¯Šæ–­ä¸è§£å†³æ–¹æ¡ˆ
- **Appendix B**: 4 å¼ æ€§èƒ½åŸºå‡†å¯¹æ¯”è¡¨
- **Appendix C**: å®˜æ–¹æ–‡æ¡£ã€è®ºæ–‡ã€é¡¹ç›®ã€ç¤¾åŒºèµ„æº
- **Appendix D**: 4 ç±» API é€ŸæŸ¥è¡¨

**å»ºè®®ä½¿ç”¨æ–¹å¼**ï¼š
1. é‡åˆ°é—®é¢˜æ—¶ï¼Œå…ˆæŸ¥ Appendix A å¸¸è§é”™è¯¯
2. æ€§èƒ½ä¼˜åŒ–æ—¶ï¼Œå‚è€ƒ Appendix B åŸºå‡†æ•°æ®
3. æ·±å…¥å­¦ä¹ æ—¶ï¼Œæµè§ˆ Appendix C è®ºæ–‡å’Œé¡¹ç›®
4. ç¼–ç æ—¶ï¼Œä½¿ç”¨ Appendix D ä½œä¸ºé€ŸæŸ¥æ‰‹å†Œ

---

**ğŸ‰ æ­å–œï¼æ‚¨å·²å®Œæˆ Hugging Face Transformers å®Œæ•´æ•™ç¨‹ï¼**

ä»é›¶åŸºç¡€çš„ Pipeline åˆ°å‰æ²¿çš„ MoEã€Mambaã€RLHFï¼Œæ‚¨ç°åœ¨å…·å¤‡äº†ï¼š
- âœ… ç³»ç»Ÿçš„ç†è®ºçŸ¥è¯†ï¼ˆ28 ç«  + é™„å½•ï¼‰
- âœ… ä¸°å¯Œçš„å®æˆ˜ç»éªŒï¼ˆ500+ ä»£ç ç¤ºä¾‹ï¼‰
- âœ… æ·±åº¦çš„åº•å±‚ç†è§£ï¼ˆ70+ äº¤äº’å¼ç»„ä»¶ï¼‰
- âœ… ç”Ÿäº§çº§çš„å·¥ç¨‹èƒ½åŠ›ï¼ˆåˆ†å¸ƒå¼ã€é‡åŒ–ã€éƒ¨ç½²ï¼‰

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®**ï¼š
1. é€‰æ‹©ä¸€ä¸ªæ„Ÿå…´è¶£çš„é¡¹ç›®åŠ¨æ‰‹å®è·µ
2. åŠ å…¥ Hugging Face ç¤¾åŒºäº¤æµ
3. å…³æ³¨æœ€æ–°è®ºæ–‡å’Œæ¨¡å‹å‘å¸ƒ
4. è´¡çŒ®å¼€æºé¡¹ç›®ï¼Œå›é¦ˆç¤¾åŒº

**ç»§ç»­å­¦ä¹ çš„æ–¹å‘**ï¼š
- å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆGPT-4Vã€Geminiï¼‰
- Agent ä¸å·¥å…·è°ƒç”¨
- é•¿ä¸Šä¸‹æ–‡å¤„ç†ï¼ˆ100K+ tokensï¼‰
- æ¨¡å‹å‹ç¼©æé™ä¼˜åŒ–

ç¥æ‚¨åœ¨ AI ä¹‹è·¯ä¸Šè¶Šèµ°è¶Šè¿œï¼ğŸš€
