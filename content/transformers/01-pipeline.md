---
title: "Chapter 1. Pipeline å¿«é€Ÿä¸Šæ‰‹"
description: "æ·±å…¥ç†è§£ Pipeline æ¶æ„ã€æŒæ¡å„ç±»ä»»åŠ¡çš„ Pipeline ä½¿ç”¨ä¸å‚æ•°è°ƒä¼˜"
updated: "2026-01-22"
---

> **Learning Objectives**
> * æ·±å…¥ç†è§£ Pipeline ä¸‰é˜¶æ®µæ¶æ„ï¼ˆTokenization â†’ Model â†’ Post-processingï¼‰
> * æŒæ¡ 5+ ç§æ ¸å¿ƒä»»åŠ¡çš„ Pipeline ä½¿ç”¨æ–¹æ³•
> * ç†Ÿç»ƒè°ƒèŠ‚ç”Ÿæˆå‚æ•°ï¼ˆtemperatureã€top_kã€top_pã€num_beamsï¼‰
> * è¯†åˆ« Pipeline çš„æ€§èƒ½ç“¶é¢ˆä¸é€‚ç”¨åœºæ™¯

---

## 1.1 Pipeline æ¶æ„è§£æ

### 1.1.1 ä¸‰é˜¶æ®µæµæ°´çº¿è¯¦è§£

Pipeline æ˜¯ Transformers åº“çš„**æœ€é«˜å±‚æŠ½è±¡**ï¼Œå®ƒå°†å¤æ‚çš„ NLP ä»»åŠ¡å°è£…ä¸ºä¸€ä¸ªç®€æ´çš„è°ƒç”¨æ¥å£ã€‚ç†è§£å…¶å†…éƒ¨æœºåˆ¶æ˜¯ä»"ä½¿ç”¨è€…"è¿›é˜¶åˆ°"å¼€å‘è€…"çš„å…³é”®ã€‚

<div data-component="PipelineFlowVisualizer"></div>

**å®Œæ•´æµç¨‹**ï¼š

```
åŸå§‹è¾“å…¥ (Raw Input)
    â†“
ã€é˜¶æ®µ 1: Tokenizationã€‘
    - æ–‡æœ¬ â†’ Token IDs
    - æ·»åŠ ç‰¹æ®Š token ([CLS], [SEP])
    - Padding & Truncation
    â†“
Tensor è¾“å…¥ (Model Input)
    â†“
ã€é˜¶æ®µ 2: Model Inferenceã€‘
    - Forward Pass
    - è®¡ç®— logits / embeddings
    â†“
æ¨¡å‹è¾“å‡º (Model Output)
    â†“
ã€é˜¶æ®µ 3: Post-processingã€‘
    - Logits â†’ Probabilities (softmax)
    - æå–æœ€ä½³ç»“æœ
    - æ ¼å¼åŒ–è¾“å‡º
    â†“
æœ€ç»ˆç»“æœ (Formatted Result)
```

**å®æˆ˜ï¼šæ‰‹åŠ¨å®ç°ä¸€ä¸ªæƒ…æ„Ÿåˆ†æ Pipeline**

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

class SimpleSentimentPipeline:
    def __init__(self, model_name="distilbert-base-uncased-finetuned-sst-2-english"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
    def __call__(self, texts):
        # é˜¶æ®µ 1: Tokenization
        inputs = self.tokenizer(
            texts,
            padding=True,       # è‡ªåŠ¨ padding åˆ°æœ€é•¿åºåˆ—
            truncation=True,    # æˆªæ–­è¶…é•¿åºåˆ—
            return_tensors="pt" # è¿”å› PyTorch å¼ é‡
        )
        print(f"[Tokenization] Input IDs shape: {inputs['input_ids'].shape}")
        
        # é˜¶æ®µ 2: Model Inference
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
        print(f"[Model] Logits shape: {logits.shape}")
        print(f"[Model] Raw logits: {logits[0].tolist()}")
        
        # é˜¶æ®µ 3: Post-processing
        probabilities = torch.softmax(logits, dim=-1)
        predictions = torch.argmax(probabilities, dim=-1)
        
        results = []
        for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
            label = self.model.config.id2label[pred.item()]
            score = prob[pred].item()
            results.append({"label": label, "score": score})
            print(f"[Post-processing] Text {i}: {label} ({score:.4f})")
        
        return results

# ä½¿ç”¨è‡ªå®šä¹‰ Pipeline
pipeline = SimpleSentimentPipeline()
texts = ["I love this!", "This is terrible."]
results = pipeline(texts)
```

**è¾“å‡º**ï¼š
```
[Tokenization] Input IDs shape: torch.Size([2, 6])
[Model] Logits shape: torch.Size([2, 2])
[Model] Raw logits: [-4.234, 4.562]
[Post-processing] Text 0: POSITIVE (0.9998)
[Post-processing] Text 1: NEGATIVE (0.9992)
```

> [!NOTE]
> **ä¸ºä»€ä¹ˆè¦åˆ†ä¸‰ä¸ªé˜¶æ®µï¼Ÿ**
> - **è§£è€¦**ï¼šæ¯ä¸ªé˜¶æ®µå¯ä»¥ç‹¬ç«‹ä¼˜åŒ–ï¼ˆå¦‚ä½¿ç”¨ Fast Tokenizerã€é‡åŒ–æ¨¡å‹ï¼‰
> - **å¤ç”¨**ï¼šTokenizer å’Œ Model å¯ä»¥å•ç‹¬ä½¿ç”¨
> - **çµæ´»**ï¼šå¯ä»¥æ’å…¥è‡ªå®šä¹‰ post-processing é€»è¾‘

### 1.1.2 è‡ªåŠ¨ä»»åŠ¡æ¨æ–­æœºåˆ¶

Pipeline å¦‚ä½•çŸ¥é“åŠ è½½å“ªä¸ªæ¨¡å‹ï¼Ÿ

```python
from transformers import pipeline

# æ–¹å¼ä¸€ï¼šä»…æŒ‡å®šä»»åŠ¡ï¼ˆä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼‰
classifier = pipeline("sentiment-analysis")
# ç­‰ä»·äºï¼š
# classifier = pipeline(
#     task="sentiment-analysis",
#     model="distilbert-base-uncased-finetuned-sst-2-english"
# )

# æ–¹å¼äºŒï¼šæŒ‡å®šæ¨¡å‹ï¼ˆè‡ªåŠ¨æ¨æ–­ä»»åŠ¡ï¼‰
generator = pipeline(model="gpt2")
# è‡ªåŠ¨æ£€æµ‹åˆ° gpt2 æ˜¯ CausalLM â†’ ä»»åŠ¡ä¸º text-generation

# æ–¹å¼ä¸‰ï¼šæ˜¾å¼æŒ‡å®šä»»åŠ¡å’Œæ¨¡å‹
qa_pipeline = pipeline(
    task="question-answering",
    model="distilbert-base-cased-distilled-squad"
)
```

**ä»»åŠ¡æ¨æ–­è§„åˆ™**ï¼š

<div data-component="TaskInferenceFlowchart"></div>

1. æ£€æŸ¥æ¨¡å‹é…ç½®ä¸­çš„ `architectures` å­—æ®µ
2. æ ¹æ®æ¶æ„ç±»åæ˜ å°„åˆ°ä»»åŠ¡
   - `BertForSequenceClassification` â†’ `text-classification`
   - `GPT2LMHeadModel` â†’ `text-generation`
   - `BertForQuestionAnswering` â†’ `question-answering`
3. å¦‚æœæ— æ³•æ¨æ–­ï¼Œè¦æ±‚ç”¨æˆ·æ˜¾å¼æŒ‡å®šä»»åŠ¡

### 1.1.3 è®¾å¤‡ç®¡ç†ï¼ˆCPUã€GPUã€å¤š GPUï¼‰

```python
import torch
from transformers import pipeline

# è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
device = 0 if torch.cuda.is_available() else -1
classifier = pipeline("sentiment-analysis", device=device)
# device=0    â†’ GPU 0
# device=1    â†’ GPU 1
# device=-1   â†’ CPU

# æ˜¾å¼æŒ‡å®šè®¾å¤‡
classifier = pipeline("sentiment-analysis", device="cuda:0")

# å¤š GPU å¹¶è¡Œï¼ˆè‡ªåŠ¨åˆ†ç‰‡ï¼‰
classifier = pipeline(
    "text-generation",
    model="meta-llama/Llama-2-7b-hf",
    device_map="auto"  # è‡ªåŠ¨åˆ†é…åˆ°å¤šå¼  GPU
)

# æŸ¥çœ‹å½“å‰è®¾å¤‡
print(f"Model device: {classifier.model.device}")
```

**è®¾å¤‡è¿ç§»**ï¼š
```python
# å°†å·²åˆ›å»ºçš„ Pipeline ç§»åˆ° GPU
classifier.model = classifier.model.to("cuda")
```

> [!TIP]
> **æ€§èƒ½å»ºè®®**ï¼š
> - å°æ¨¡å‹ï¼ˆ< 500M å‚æ•°ï¼‰ï¼šCPU è¶³å¤Ÿ
> - ä¸­ç­‰æ¨¡å‹ï¼ˆ500M - 3Bï¼‰ï¼šå• GPU
> - å¤§æ¨¡å‹ï¼ˆ7B+ï¼‰ï¼šå¤š GPU + `device_map="auto"`

---

## 1.2 æ–‡æœ¬åˆ†ç±» Pipeline

### 1.2.1 æƒ…æ„Ÿåˆ†æï¼ˆsentiment-analysisï¼‰

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")

# å•æ¡æ¨ç†
result = classifier("The movie was fantastic!")[0]
print(f"Label: {result['label']}, Score: {result['score']:.4f}")

# æ‰¹é‡æ¨ç†ï¼ˆæ›´é«˜æ•ˆï¼‰
texts = [
    "I absolutely loved it!",
    "Worst experience ever.",
    "It was okay, nothing special."
]
results = classifier(texts)

for text, result in zip(texts, results):
    print(f"{text:35} â†’ {result['label']:8} ({result['score']:.3f})")
```

**è¾“å‡º**ï¼š
```
Label: POSITIVE, Score: 0.9998

I absolutely loved it!              â†’ POSITIVE (0.999)
Worst experience ever.              â†’ NEGATIVE (0.999)
It was okay, nothing special.       â†’ POSITIVE (0.652)
```

**è‡ªå®šä¹‰æ¨¡å‹**ï¼ˆä¸­æ–‡æƒ…æ„Ÿåˆ†æï¼‰ï¼š

```python
# ä½¿ç”¨ä¸­æ–‡ BERT æ¨¡å‹
classifier_cn = pipeline(
    "sentiment-analysis",
    model="uer/roberta-base-finetuned-dianping-chinese"
)

result = classifier_cn("è¿™å®¶é¤å…å¤ªå¥½åƒäº†ï¼")
print(result)
# [{'label': 'positive', 'score': 0.9987}]
```

### 1.2.2 é›¶æ ·æœ¬åˆ†ç±»ï¼ˆzero-shot-classificationï¼‰

**æ— éœ€è®­ç»ƒå³å¯åˆ†ç±»ä»»æ„æ ‡ç­¾ï¼**

<div data-component="ZeroShotClassificationDemo"></div>

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")

text = "I have a problem with my iPhone battery draining too fast."
candidate_labels = ["technology", "politics", "sports", "health"]

result = classifier(text, candidate_labels)

print(f"Text: {text}\n")
for label, score in zip(result['labels'], result['scores']):
    print(f"{label:15} â†’ {score:.4f}")
```

**è¾“å‡º**ï¼š
```
Text: I have a problem with my iPhone battery draining too fast.

technology      â†’ 0.9542
health          â†’ 0.0234
sports          â†’ 0.0156
politics        â†’ 0.0068
```

**å·¥ä½œåŸç†**ï¼š
- ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¨ç† (NLI) æ¨¡å‹
- å°†åˆ†ç±»ä»»åŠ¡è½¬æ¢ä¸ºè•´å«å…³ç³»åˆ¤æ–­
- å‡è®¾ï¼š`text` è•´å« `"This text is about {label}"`

**å¤šæ ‡ç­¾åˆ†ç±»**ï¼š

```python
text = "Apple just released a new MacBook with M3 chip and improved battery life."
candidate_labels = ["technology", "business", "science"]

result = classifier(
    text,
    candidate_labels,
    multi_label=True  # å…è®¸å¤šä¸ªæ ‡ç­¾åŒæ—¶ä¸ºçœŸ
)

for label, score in zip(result['labels'], result['scores']):
    print(f"{label:15} â†’ {score:.4f}")
```

**è¾“å‡º**ï¼š
```
technology      â†’ 0.9823
business        â†’ 0.8934
science         â†’ 0.3421
```

### 1.2.3 è‡ªå®šä¹‰æ ‡ç­¾æ˜ å°„

æŸäº›æ¨¡å‹çš„æ ‡ç­¾æ˜¯æ•°å­—æˆ–ç¼©å†™ï¼Œå¯ä»¥è‡ªå®šä¹‰æ˜ å°„ï¼š

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

# ä¿®æ”¹æ ‡ç­¾æ˜ å°„
model.config.id2label = {0: "æ¶ˆæ", 1: "ç§¯æ"}

classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)
result = classifier("This is great!")
print(result)
# [{'label': 'ç§¯æ', 'score': 0.9998}]
```

---

## 1.3 æ–‡æœ¬ç”Ÿæˆ Pipeline

### 1.3.1 åŸºç¡€æ–‡æœ¬ç”Ÿæˆ

```python
from transformers import pipeline

generator = pipeline("text-generation", model="gpt2")

prompt = "Once upon a time"
outputs = generator(
    prompt,
    max_length=50,      # ç”Ÿæˆçš„æœ€å¤§æ€»é•¿åº¦ï¼ˆåŒ…æ‹¬ promptï¼‰
    num_return_sequences=1
)

print(outputs[0]['generated_text'])
```

**è¾“å‡º**ï¼ˆç¤ºä¾‹ï¼‰ï¼š
```
Once upon a time, there was a young girl named Lily who lived in a small village. 
She loved to explore the nearby forest and discover new things.
```

### 1.3.2 ç”Ÿæˆå‚æ•°è¯¦è§£

<div data-component="GenerationParametersExplorer"></div>

**æ ¸å¿ƒå‚æ•°å¯¹æ¯”è¡¨**ï¼š

| å‚æ•° | ä½œç”¨ | å…¸å‹å€¼ | æ•ˆæœ |
|------|------|--------|------|
| `max_length` | ç”Ÿæˆçš„æœ€å¤§ token æ•° | 50-512 | æ§åˆ¶è¾“å‡ºé•¿åº¦ |
| `max_new_tokens` | åœ¨ prompt åŸºç¡€ä¸Šæ–°ç”Ÿæˆçš„ token æ•° | 50-200 | æ›´ç²¾ç¡®çš„é•¿åº¦æ§åˆ¶ |
| `temperature` | é‡‡æ ·æ¸©åº¦ | 0.7-1.0 | è¶Šä½è¶Šç¡®å®šï¼Œè¶Šé«˜è¶Šéšæœº |
| `top_k` | åªä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ª token ä¸­é‡‡æ · | 50 | é™åˆ¶å€™é€‰é›† |
| `top_p` | æ ¸é‡‡æ ·ï¼Œç´¯è®¡æ¦‚ç‡è¾¾åˆ° p æ—¶åœæ­¢ | 0.9 | åŠ¨æ€å€™é€‰é›† |
| `num_beams` | æŸæœç´¢å®½åº¦ | 1ï¼ˆè´ªå©ªï¼‰æˆ– 4-10 | æé«˜è´¨é‡ä½†å˜æ…¢ |
| `do_sample` | æ˜¯å¦é‡‡æ ·ï¼ˆå¦åˆ™è´ªå©ªï¼‰ | True/False | æ§åˆ¶éšæœºæ€§ |

**å®éªŒï¼šæ¸©åº¦çš„å½±å“**

```python
generator = pipeline("text-generation", model="gpt2")

prompt = "The future of AI is"

for temp in [0.3, 0.7, 1.0, 1.5]:
    output = generator(
        prompt,
        max_new_tokens=30,
        temperature=temp,
        do_sample=True,
        num_return_sequences=1
    )[0]['generated_text']
    print(f"\n[Temperature={temp}]")
    print(output)
```

**è¾“å‡ºå¯¹æ¯”**ï¼š
```
[Temperature=0.3] (æ›´ç¡®å®šã€é‡å¤æ€§é«˜)
The future of AI is already here, and it's already being used in many different ways.
The most common use of AI is in the field of machine learning.

[Temperature=0.7] (å¹³è¡¡)
The future of AI is not about replacing humans, but augmenting them. We need systems
that can understand context, learn from experience, and collaborate with people.

[Temperature=1.0] (æ›´å¤šæ ·)
The future of AI is likely to be shaped by decentralized architectures where multiple
agents collaborate, similar to how biological neural networks operate in nature.

[Temperature=1.5] (æå…¶éšæœºã€å¯èƒ½ä¸è¿è´¯)
The future of AI is quantum blockchain synergy manifesting through holographic 
consciousness portals enabling telepathic cryptocurrency mining protocols.
```

**Top-K vs Top-P å¯è§†åŒ–**ï¼š

<div data-component="TopKTopPVisualizer"></div>

```python
# Top-K Sampling
output_topk = generator(
    "Once upon a time",
    max_new_tokens=50,
    do_sample=True,
    top_k=50,           # åªä»å‰ 50 ä¸ª token ä¸­é‡‡æ ·
    temperature=0.8
)

# Top-P (Nucleus) Sampling
output_topp = generator(
    "Once upon a time",
    max_new_tokens=50,
    do_sample=True,
    top_p=0.92,         # ç´¯è®¡æ¦‚ç‡è¾¾åˆ° 92% æ—¶åœæ­¢
    temperature=0.8
)

print("Top-K:", output_topk[0]['generated_text'])
print("\nTop-P:", output_topp[0]['generated_text'])
```

> [!TIP]
> **å‚æ•°ç»„åˆå»ºè®®**ï¼š
> - **åˆ›æ„å†™ä½œ**ï¼š`temperature=0.9, top_p=0.95, do_sample=True`
> - **äº‹å®æ€§æ–‡æœ¬**ï¼š`temperature=0.3, top_k=50, do_sample=True`
> - **ä»£ç ç”Ÿæˆ**ï¼š`temperature=0.2, num_beams=4` (æŸæœç´¢)
> - **èŠå¤©å¯¹è¯**ï¼š`temperature=0.7, top_p=0.9, repetition_penalty=1.2`

### 1.3.3 æ‰¹é‡ç”Ÿæˆä¸æµå¼è¾“å‡º

**æ‰¹é‡ç”Ÿæˆ**ï¼ˆåŒæ—¶ç”Ÿæˆå¤šä¸ªç»“æœï¼‰ï¼š

```python
generator = pipeline("text-generation", model="gpt2")

prompts = [
    "The capital of France is",
    "Python is a programming language that",
    "In the year 2050,"
]

outputs = generator(
    prompts,
    max_new_tokens=20,
    num_return_sequences=2,  # æ¯ä¸ª prompt ç”Ÿæˆ 2 ä¸ªç»“æœ
    batch_size=3             # æ‰¹å¤„ç†å¤§å°
)

for i, prompt in enumerate(prompts):
    print(f"\n=== Prompt: {prompt} ===")
    for j, output in enumerate(outputs[i*2:(i+1)*2]):
        print(f"[{j+1}] {output['generated_text']}")
```

**æµå¼è¾“å‡º**ï¼ˆé€ token ç”Ÿæˆï¼Œé€‚åˆèŠå¤©åº”ç”¨ï¼‰ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from threading import Thread

model_name = "gpt2"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "The best way to learn programming is"
inputs = tokenizer(prompt, return_tensors="pt")

# åˆ›å»ºæµå¼è¾“å‡ºå™¨
streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)

# åœ¨åå°çº¿ç¨‹ç”Ÿæˆ
generation_kwargs = dict(
    **inputs,
    max_new_tokens=50,
    streamer=streamer,
    do_sample=True,
    temperature=0.7
)
thread = Thread(target=model.generate, kwargs=generation_kwargs)
thread.start()

# å®æ—¶æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬
print(prompt, end="")
for new_text in streamer:
    print(new_text, end="", flush=True)
print()

thread.join()
```

**è¾“å‡º**ï¼ˆé€å­—æ˜¾ç¤ºï¼‰ï¼š
```
The best way to learn programming is to start with small projects and gradually
increase complexity. Practice regularly, read others' code, and don't be afraid
to make mistakes - they're the best teachers.
```

---

## 1.4 é—®ç­”ä¸æŠ½å– Pipeline

### 1.4.1 æŠ½å–å¼é—®ç­”ï¼ˆquestion-answeringï¼‰

æŠ½å–å¼é—®ç­”ä»ç»™å®šæ–‡æœ¬ä¸­**æå–ç­”æ¡ˆç‰‡æ®µ**ã€‚

<div data-component="QuestionAnsweringVisualizer"></div>

```python
from transformers import pipeline

qa_pipeline = pipeline("question-answering")

context = """
Hugging Face is a company founded in 2016 by ClÃ©ment Delangue, Julien Chaumond, 
and Thomas Wolf. The company is based in New York City and Paris. Hugging Face 
is known for its Transformers library, which provides state-of-the-art NLP models.
The company raised $40 million in Series B funding in 2021.
"""

questions = [
    "When was Hugging Face founded?",
    "Who are the founders?",
    "Where is the company based?",
    "How much funding did they raise in 2021?"
]

for question in questions:
    result = qa_pipeline(question=question, context=context)
    print(f"Q: {question}")
    print(f"A: {result['answer']} (score: {result['score']:.3f})\n")
```

**è¾“å‡º**ï¼š
```
Q: When was Hugging Face founded?
A: 2016 (score: 0.987)

Q: Who are the founders?
A: ClÃ©ment Delangue, Julien Chaumond, and Thomas Wolf (score: 0.953)

Q: Where is the company based?
A: New York City and Paris (score: 0.891)

Q: How much funding did they raise in 2021?
A: $40 million (score: 0.924)
```

**è¾“å‡ºç»“æ„è¯¦è§£**ï¼š
```python
{
    'score': 0.987,         # ç½®ä¿¡åº¦
    'start': 52,            # ç­”æ¡ˆèµ·å§‹ä½ç½®ï¼ˆå­—ç¬¦ç´¢å¼•ï¼‰
    'end': 56,              # ç­”æ¡ˆç»“æŸä½ç½®
    'answer': '2016'        # æå–çš„ç­”æ¡ˆæ–‡æœ¬
}
```

**è·å–å¤šä¸ªå€™é€‰ç­”æ¡ˆ**ï¼š

```python
result = qa_pipeline(
    question="Who founded Hugging Face?",
    context=context,
    top_k=3  # è¿”å›å‰ 3 ä¸ªå€™é€‰ç­”æ¡ˆ
)

for i, ans in enumerate(result, 1):
    print(f"{i}. {ans['answer']:40} (score: {ans['score']:.3f})")
```

**è¾“å‡º**ï¼š
```
1. ClÃ©ment Delangue, Julien Chaumond, and Thomas Wolf (score: 0.953)
2. ClÃ©ment Delangue                                    (score: 0.241)
3. Julien Chaumond                                     (score: 0.187)
```

### 1.4.2 è¡¨æ ¼é—®ç­”ï¼ˆtable-question-answeringï¼‰

å¯¹ç»“æ„åŒ–è¡¨æ ¼è¿›è¡Œé—®ç­”ï¼š

```python
from transformers import pipeline

tqa = pipeline("table-question-answering")

table = {
    "Model": ["BERT", "GPT-2", "T5", "LLaMA"],
    "Parameters": ["110M", "1.5B", "11B", "7B"],
    "Year": ["2018", "2019", "2020", "2023"]
}

questions = [
    "Which model has the most parameters?",
    "When was BERT released?",
    "What is the size of LLaMA?"
]

for question in questions:
    result = tqa(table=table, query=question)
    print(f"Q: {question}")
    print(f"A: {result['answer']}\n")
```

### 1.4.3 æ–‡æ¡£é—®ç­”ï¼ˆdocument-question-answeringï¼‰

ç»“åˆ OCR å’Œé—®ç­”ï¼Œå¤„ç†æ–‡æ¡£å›¾åƒï¼š

```python
from transformers import pipeline

doc_qa = pipeline("document-question-answering")

# æ”¯æŒå›¾åƒ URL æˆ–æœ¬åœ°æ–‡ä»¶
image_path = "invoice.png"
question = "What is the total amount?"

result = doc_qa(image=image_path, question=question)
print(f"Answer: {result['answer']}")
```

---

## 1.5 å…¶ä»–å¸¸ç”¨ Pipeline

### 1.5.1 å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰

```python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)

text = "Apple was founded by Steve Jobs in Cupertino, California in 1976."
entities = ner(text)

for entity in entities:
    print(f"{entity['word']:20} â†’ {entity['entity_group']:10} ({entity['score']:.3f})")
```

**è¾“å‡º**ï¼š
```
Apple                â†’ ORG        (0.998)
Steve Jobs           â†’ PER        (0.999)
Cupertino            â†’ LOC        (0.995)
California           â†’ LOC        (0.997)
1976                 â†’ DATE       (0.985)
```

<div data-component="NERVisualizer"></div>

### 1.5.2 æ‘˜è¦ç”Ÿæˆï¼ˆsummarizationï¼‰

```python
from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

article = """
The Transformer architecture, introduced in the 2017 paper "Attention Is All You Need,"
revolutionized natural language processing. Unlike previous sequence-to-sequence models
that relied on recurrent or convolutional layers, Transformers use self-attention
mechanisms to process input sequences in parallel. This allows for much faster training
and better capture of long-range dependencies. The architecture consists of an encoder
and decoder, both made up of stacked layers of multi-head attention and feed-forward
networks. Transformers have become the foundation for modern NLP models like BERT, GPT,
and T5, achieving state-of-the-art results across numerous tasks.
"""

summary = summarizer(article, max_length=50, min_length=25, do_sample=False)
print("Summary:", summary[0]['summary_text'])
```

**è¾“å‡º**ï¼š
```
Summary: The Transformer architecture revolutionized NLP by using self-attention 
mechanisms instead of recurrent layers. It enables parallel processing and has 
become the foundation for modern models like BERT and GPT.
```

### 1.5.3 ç¿»è¯‘ï¼ˆtranslationï¼‰

```python
from transformers import pipeline

# è‹±è¯‘æ³•
translator_en_fr = pipeline("translation_en_to_fr", model="Helsinki-NLP/opus-mt-en-fr")
result = translator_en_fr("Hello, how are you?")
print(f"FR: {result[0]['translation_text']}")

# ä¸­è¯‘è‹±
translator_zh_en = pipeline("translation_zh_to_en", model="Helsinki-NLP/opus-mt-zh-en")
result = translator_zh_en("ä½ å¥½ï¼Œä¸–ç•Œï¼")
print(f"EN: {result[0]['translation_text']}")
```

### 1.5.4 å¡«ç©ºï¼ˆfill-maskï¼‰

```python
from transformers import pipeline

unmasker = pipeline("fill-mask", model="bert-base-uncased")

sentence = "The capital of France is [MASK]."
results = unmasker(sentence, top_k=5)

for i, result in enumerate(results, 1):
    print(f"{i}. {result['token_str']:10} ({result['score']:.4f})")
```

**è¾“å‡º**ï¼š
```
1. paris      (0.8934)
2. lyon       (0.0234)
3. marseille  (0.0156)
4. nice       (0.0089)
5. toulouse   (0.0067)
```

### 1.5.5 ç‰¹å¾æå–ï¼ˆfeature-extractionï¼‰

è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤ºï¼ˆembeddingsï¼‰ï¼š

```python
from transformers import pipeline
import numpy as np

feature_extractor = pipeline("feature-extraction", model="sentence-transformers/all-MiniLM-L6-v2")

texts = [
    "The cat sits on the mat.",
    "A feline rests on a rug.",
    "The dog runs in the park."
]

# æå–ç‰¹å¾
embeddings = feature_extractor(texts)

# è½¬æ¢ä¸º numpy æ•°ç»„ï¼ˆå– [CLS] token çš„è¡¨ç¤ºï¼‰
vectors = np.array([emb[0] for emb in embeddings])
print(f"Embedding shape: {vectors.shape}")  # (3, 384)

# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity(vectors)

print("\nCosine Similarities:")
for i, text1 in enumerate(texts):
    for j, text2 in enumerate(texts):
        if i < j:
            print(f"{i}-{j}: {similarities[i][j]:.4f}")
```

**è¾“å‡º**ï¼š
```
Embedding shape: (3, 384)

Cosine Similarities:
0-1: 0.8234  # è¯­ä¹‰ç›¸ä¼¼ï¼ˆçŒ«/æ¯¯å­ï¼‰
0-2: 0.4521  # ä¸å¤ªç›¸å…³
1-2: 0.4312  # ä¸å¤ªç›¸å…³
```

---

## 1.6 Pipeline çš„é™åˆ¶ä¸ä½•æ—¶ä¸ç”¨

### 1.6.1 æ€§èƒ½ç“¶é¢ˆåˆ†æ

<div data-component="PipelinePerformanceAnalyzer"></div>

Pipeline çš„ä¸»è¦å¼€é”€ï¼š

1. **é‡å¤åŠ è½½æ¨¡å‹**ï¼ˆæ¯æ¬¡è°ƒç”¨éƒ½åˆå§‹åŒ–ï¼‰
2. **åŠ¨æ€ padding**ï¼ˆæ‰¹å†…åºåˆ—é•¿åº¦ä¸ä¸€è‡´ï¼‰
3. **å•æ ·æœ¬æ¨ç†**ï¼ˆæ— æ³•å……åˆ†åˆ©ç”¨ GPU å¹¶è¡Œï¼‰
4. **Python å¾ªç¯å¼€é”€**ï¼ˆè€Œéå‘é‡åŒ–ï¼‰

**æ€§èƒ½å¯¹æ¯”å®éªŒ**ï¼š

```python
import time
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import torch

texts = ["This is great!"] * 100

# æ–¹å¼ä¸€ï¼šPipelineï¼ˆä¾¿æ·ä½†æ…¢ï¼‰
print("=== Using Pipeline ===")
classifier = pipeline("sentiment-analysis")
start = time.time()
for text in texts:
    result = classifier(text)
time_pipeline = time.time() - start
print(f"Time: {time_pipeline:.2f}s")

# æ–¹å¼äºŒï¼šæ‰‹åŠ¨æ‰¹å¤„ç†ï¼ˆå¿«ï¼‰
print("\n=== Manual Batching ===")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model.eval()

start = time.time()
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)
time_batch = time.time() - start
print(f"Time: {time_batch:.2f}s")

print(f"\nâš¡ Speedup: {time_pipeline / time_batch:.1f}x")
```

**è¾“å‡º**ï¼š
```
=== Using Pipeline ===
Time: 12.34s

=== Manual Batching ===
Time: 0.89s

âš¡ Speedup: 13.9x
```

### 1.6.2 æ‰¹å¤„ç†çš„å¿…è¦æ€§

Pipeline æ”¯æŒæ‰¹å¤„ç†ï¼Œä½†éœ€è¦æ˜¾å¼æŒ‡å®šï¼š

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis", batch_size=32)

texts = ["Great!"] * 1000

# è‡ªåŠ¨åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹ 32 æ¡ï¼‰
results = classifier(texts)
```

### 1.6.3 è½¬å‘åº•å±‚ API çš„æ—¶æœº

**åº”è¯¥ä½¿ç”¨ Pipeline**ï¼š
- âœ… å¿«é€ŸåŸå‹å¼€å‘
- âœ… å•æ¬¡æ¨ç†æˆ–å°æ‰¹é‡
- âœ… æ¼”ç¤º / Jupyter Notebook
- âœ… ä¸åœ¨ä¹æ€§èƒ½ï¼ˆååé‡ < 10 QPSï¼‰

**åº”è¯¥ä½¿ç”¨åº•å±‚ API**ï¼š
- âœ… ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- âœ… éœ€è¦æ‰¹å¤„ç†ä¼˜åŒ–
- âœ… è‡ªå®šä¹‰ post-processing
- âœ… é«˜ååé‡éœ€æ±‚ï¼ˆ> 100 QPSï¼‰
- âœ… åˆ†å¸ƒå¼è®­ç»ƒ / æ¨ç†

---

## 1.7 æ€»ç»“ä¸å®æˆ˜ç»ƒä¹ 

### çŸ¥è¯†å›é¡¾

âœ… **æŒæ¡äº†**ï¼š
- Pipeline ä¸‰é˜¶æ®µæ¶æ„ï¼ˆTokenization â†’ Model â†’ Post-processingï¼‰
- 5+ ç§æ ¸å¿ƒä»»åŠ¡çš„ä½¿ç”¨æ–¹æ³•
- ç”Ÿæˆå‚æ•°è°ƒä¼˜ï¼ˆtemperatureã€top_kã€top_pã€num_beamsï¼‰
- æ€§èƒ½ä¼˜åŒ–æ–¹å‘ï¼ˆæ‰¹å¤„ç†ã€åº•å±‚ APIï¼‰

### å®æˆ˜ç»ƒä¹ 

**ç»ƒä¹  1ï¼šå¤šä»»åŠ¡ Pipeline æ•´åˆ**
ç¼–å†™ä¸€ä¸ªè„šæœ¬ï¼Œå¯¹åŒä¸€æ®µæ–°é—»æ–‡æœ¬ï¼š
1. æå–å‘½åå®ä½“ï¼ˆNERï¼‰
2. ç”Ÿæˆæ‘˜è¦
3. åˆ¤æ–­æƒ…æ„Ÿå€¾å‘
4. ç¿»è¯‘ä¸ºå¦ä¸€ç§è¯­è¨€

**ç»ƒä¹  2ï¼šç”Ÿæˆå‚æ•°å®éªŒ**
ä½¿ç”¨ GPT-2 ç”Ÿæˆæ•…äº‹å¼€å¤´ï¼Œå°è¯•è‡³å°‘ 5 ç§å‚æ•°ç»„åˆï¼Œå¯¹æ¯”è¾“å‡ºè´¨é‡ï¼š
- è´ªå©ªè§£ç 
- æŸæœç´¢ï¼ˆnum_beams=5ï¼‰
- é‡‡æ ·ï¼ˆtemperature=0.7ï¼‰
- Top-K é‡‡æ ·ï¼ˆtop_k=50ï¼‰
- Top-P é‡‡æ ·ï¼ˆtop_p=0.9ï¼‰

**ç»ƒä¹  3ï¼šæ€§èƒ½ä¼˜åŒ–**
å®ç°ä¸€ä¸ªæ‰¹å¤„ç†æƒ…æ„Ÿåˆ†æè„šæœ¬ï¼Œå¤„ç† 10,000 æ¡æ–‡æœ¬ï¼Œå¯¹æ¯”ï¼š
- Pipeline é€æ¡å¤„ç†
- Pipeline æ‰¹å¤„ç†ï¼ˆbatch_size=32ï¼‰
- æ‰‹åŠ¨æ‰¹å¤„ç†

### æ€è€ƒé¢˜

â“ **ä¸ºä»€ä¹ˆ temperature=0 ç­‰ä»·äºè´ªå©ªè§£ç ï¼Ÿ**  
ğŸ’¡ æç¤ºï¼šè§‚å¯Ÿ softmax åœ¨æ¸©åº¦è¶‹è¿‘ 0 æ—¶çš„è¡Œä¸º

â“ **Top-K å’Œ Top-P å¯ä»¥åŒæ—¶ä½¿ç”¨å—ï¼Ÿæ•ˆæœå¦‚ä½•ï¼Ÿ**  
ğŸ’¡ æç¤ºï¼šæŸ¥çœ‹æºç ä¸­çš„é‡‡æ ·é€»è¾‘

â“ **Pipeline çš„ `device_map="auto"` æ˜¯å¦‚ä½•åˆ†é…å±‚åˆ°ä¸åŒ GPU çš„ï¼Ÿ**  
ğŸ’¡ æç¤ºï¼šè€ƒè™‘æ¨¡å‹å¤§å°ã€GPU æ˜¾å­˜ã€å±‚é—´é€šä¿¡å¼€é”€

### æ‰©å±•é˜…è¯»

ğŸ“– **å®˜æ–¹æ–‡æ¡£**ï¼š
- [Pipeline å®Œæ•´ API æ–‡æ¡£](https://huggingface.co/docs/transformers/main_classes/pipelines)
- [ç”Ÿæˆç­–ç•¥è¯¦è§£](https://huggingface.co/docs/transformers/generation_strategies)
- [ä»»åŠ¡æŒ‡å—](https://huggingface.co/docs/transformers/task_summary)

ğŸ“„ **é‡è¦è®ºæ–‡**ï¼š
- The Curious Case of Neural Text Degeneration (Holtzman et al., 2019) - Top-P Sampling
- Hierarchical Neural Story Generation (Fan et al., 2018) - ç”Ÿæˆç­–ç•¥

ğŸ¥ **è§†é¢‘æ•™ç¨‹**ï¼š
- [Hugging Face Course - Pipelines](https://huggingface.co/learn/nlp-course/chapter1/3)

---

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼šChapter 2 å°†æ·±å…¥ Tokenization æœºåˆ¶ï¼Œå­¦ä¹  WordPieceã€BPEã€SentencePiece ç­‰ç®—æ³•ï¼Œç†è§£ Fast Tokenizer çš„ä¼˜åŠ¿ï¼ŒæŒæ¡å¤„ç†é•¿æ–‡æœ¬ã€å¤šè¯­è¨€ã€ç‰¹æ®Šåœºæ™¯çš„æŠ€å·§ã€‚

