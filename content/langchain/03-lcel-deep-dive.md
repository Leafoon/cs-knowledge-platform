> **æœ¬ç« ç›®æ ‡**ï¼šæ·±å…¥ç†è§£ LCEL çš„åº•å±‚æœºåˆ¶ï¼ŒæŒæ¡ Runnable é«˜çº§æ“ä½œã€é…ç½®åŒ–ã€Fallbackã€Retry ç­‰ä¼ä¸šçº§ç‰¹æ€§ã€‚

---

## æœ¬ç« å¯¼è§ˆ

æœ¬ç« æ·±å…¥ LCEL çš„é«˜çº§ç‰¹æ€§ä¸åº•å±‚åŸç†ï¼Œæå‡åº”ç”¨çš„å¥å£®æ€§ä¸æ€§èƒ½ï¼š

- **ç»„åˆæ•°å­¦**ï¼šç†è§£ Pipe æ“ä½œç¬¦èƒŒåçš„å‡½æ•°ç»„åˆåŸç† `fâ‚„(fâ‚ƒ(fâ‚‚(fâ‚(x))))`
- **é«˜çº§æ“ä½œ**ï¼šRunnablePassthroughã€RunnableLambdaã€RunnableBranch ç­‰çµæ´»ç»„åˆæŠ€å·§
- **é…ç½®åŒ–å¼€å‘**ï¼šé€šè¿‡ `configurable_fields` å’Œ `configurable_alternatives` å®ç°åŠ¨æ€é…ç½®
- **å®¹é”™æœºåˆ¶**ï¼šFallback é™çº§ã€Retry é‡è¯•ã€è¶…æ—¶æ§åˆ¶ç­‰ç”Ÿäº§ç¯å¢ƒå¿…å¤‡ç‰¹æ€§
- **æ€§èƒ½ä¼˜åŒ–**ï¼šå¹¶è¡Œæ‰§è¡Œã€æ‰¹å¤„ç†ã€ç¼“å­˜ç­‰æå‡ååé‡çš„å®æˆ˜æŠ€å·§

æŒæ¡è¿™äº›é«˜çº§æŠ€æœ¯ï¼Œä½ å°†èƒ½å¤Ÿæ„å»ºä¼ä¸šçº§çš„å¥å£® LLM åº”ç”¨ã€‚

---

## 3.1 Pipe ä¸ç»„åˆ

### 3.1.1 é“¾å¼è°ƒç”¨çš„æ•°å­¦åŸºç¡€

LCEL çš„ç®¡é“æ“ä½œç¬¦åŸºäº**å‡½æ•°ç»„åˆ**ï¼ˆFunction Compositionï¼‰çš„æ•°å­¦æ¦‚å¿µã€‚

**æ•°å­¦å®šä¹‰**:

$$
(f \circ g)(x) = f(g(x))
$$

åœ¨ LCEL ä¸­:

$$
\text{chain} = f_n \circ f_{n-1} \circ \cdots \circ f_2 \circ f_1
$$

```python
# æ•°å­¦è¡¨è¾¾å¼
# chain(x) = parser(model(prompt(x)))

# LCEL è¡¨è¾¾å¼
chain = prompt | model | parser

# ç­‰ä»·äº
def chain(x):
    return parser(model(prompt(x)))
```

<div data-component="RunnableCompositionFlow"></div>

### ç»„åˆæ¼”ç¤º

<div data-component="ParallelExecutionDemo"></div>

**ç»„åˆæ€§è´¨**:

1. **ç»“åˆå¾‹**: `(f | g) | h â‰¡ f | (g | h)`
2. **ç±»å‹å®‰å…¨**: `f: A â†’ B` å’Œ `g: B â†’ C` æ‰èƒ½ç»„åˆä¸º `f | g: A â†’ C`

```python
from langchain_core.runnables import Runnable

# ç±»å‹æ£€æŸ¥ç¤ºä¾‹
prompt: Runnable[dict, PromptValue] 
model: Runnable[PromptValue, AIMessage]
parser: Runnable[AIMessage, str]

# ç»„åˆåç±»å‹è‡ªåŠ¨æ¨æ–­
chain: Runnable[dict, str] = prompt | model | parser
```

### 3.1.2 ç±»å‹ä¼ é€’ä¸è‡ªåŠ¨æ¨æ–­

```python
from typing import TypedDict

class Input(TypedDict):
    text: str
    language: str

class Output(TypedDict):
    translation: str
    original: str

# æ˜¾å¼ç±»å‹æ ‡æ³¨
def create_typed_chain() -> Runnable[Input, Output]:
    prompt = ChatPromptTemplate.from_template("Translate to {language}: {text}")
    model = ChatOpenAI(model="gpt-4o-mini")
    
    # ä½¿ç”¨ RunnablePassthrough ä¿ç•™åŸæ–‡
    from langchain_core.runnables import RunnablePassthrough
    
    chain = (
        RunnablePassthrough.assign(
            translation=prompt | model | StrOutputParser()
        )
        | (lambda x: {"translation": x["translation"], "original": x["text"]})
    )
    
    return chain

# IDE è‡ªåŠ¨æç¤ºç±»å‹
typed_chain = create_typed_chain()
result: Output = typed_chain.invoke({"text": "Hello", "language": "French"})
```

### 3.1.3 RunnableSequence å†…éƒ¨å®ç°

```python
from langchain_core.runnables import RunnableSequence

# ç®¡é“æ“ä½œç¬¦çš„åº•å±‚å®ç°
class RunnableSequence(Runnable):
    def __init__(self, *steps: Runnable):
        self.steps = steps
    
    def invoke(self, input, config=None):
        result = input
        for step in self.steps:
            result = step.invoke(result, config)
        return result
    
    def stream(self, input, config=None):
        # åªæœ‰æœ€åä¸€ä¸ªç»„ä»¶æµå¼è¾“å‡º
        result = input
        for step in self.steps[:-1]:
            result = step.invoke(result, config)
        
        for chunk in self.steps[-1].stream(result, config):
            yield chunk

# ä½¿ç”¨
chain = RunnableSequence(prompt, model, parser)
# ç­‰ä»·äº
chain = prompt | model | parser
```

---

## 3.2 Runnable é«˜çº§æ“ä½œ

### 3.2.1 RunnablePassthroughï¼šé€ä¼ è¾“å…¥

**ç”¨é€”**: åœ¨é“¾ä¸­ä¿ç•™åŸå§‹è¾“å…¥ã€‚

```python
from langchain_core.runnables import RunnablePassthrough

# åŸºç¡€é€ä¼ 
passthrough = RunnablePassthrough()
print(passthrough.invoke({"key": "value"}))  # {"key": "value"}

# åœ¨é“¾ä¸­ä½¿ç”¨
chain = (
    {"original": RunnablePassthrough(), "processed": some_chain}
)

result = chain.invoke("input")
# {'original': 'input', 'processed': <å¤„ç†åçš„ç»“æœ>}
```

**å®é™…æ¡ˆä¾‹**:

```python
# ä¿ç•™åŸæ–‡çš„ç¿»è¯‘é“¾
translation_chain = (
    RunnablePassthrough.assign(
        translation=ChatPromptTemplate.from_template("Translate to French: {text}")
        | model
        | StrOutputParser()
    )
)

result = translation_chain.invoke({"text": "Hello"})
# {'text': 'Hello', 'translation': 'Bonjour'}
```

### 3.2.2 RunnableLambdaï¼šè‡ªå®šä¹‰å‡½æ•°åŒ…è£…

```python
from langchain_core.runnables import RunnableLambda

# åŒ…è£…æ™®é€šå‡½æ•°
def add_prefix(text: str) -> str:
    return f"[TRANSLATED] {text}"

prefix_runnable = RunnableLambda(add_prefix)

chain = prompt | model | StrOutputParser() | prefix_runnable

result = chain.invoke({"text": "Hello", "language": "French"})
# "[TRANSLATED] Bonjour"
```

**å¼‚æ­¥å‡½æ•°**:

```python
import asyncio

async def async_process(text: str) -> str:
    await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
    return text.upper()

async_runnable = RunnableLambda(async_process)

# æ”¯æŒå¼‚æ­¥è°ƒç”¨
result = await async_runnable.ainvoke("hello")  # "HELLO"
```

### 3.2.3 RunnableBranchï¼šæ¡ä»¶åˆ†æ”¯

```python
from langchain_core.runnables import RunnableBranch

# æ ¹æ®æ¡ä»¶é€‰æ‹©ä¸åŒé“¾
def is_long_text(x: dict) -> bool:
    return len(x.get("text", "")) > 100

summarize_chain = ChatPromptTemplate.from_template("Summarize: {text}") | model
direct_chain = RunnablePassthrough()

branch = RunnableBranch(
    (is_long_text, summarize_chain),    # æ¡ä»¶1: é•¿æ–‡æœ¬â†’æ‘˜è¦
    direct_chain                        # é»˜è®¤: ç›´æ¥é€ä¼ 
)

# çŸ­æ–‡æœ¬
result1 = branch.invoke({"text": "Hi"})  # é€ä¼ 

# é•¿æ–‡æœ¬
long_text = "a" * 150
result2 = branch.invoke({"text": long_text})  # æ‘˜è¦
```

### 3.2.4 RunnableParallelï¼šå¹¶è¡Œæ‰§è¡Œ

```python
from langchain_core.runnables import RunnableParallel

# å¹¶è¡Œæ‰§è¡Œå¤šä¸ªé“¾
parallel_chain = RunnableParallel(
    french=ChatPromptTemplate.from_template("Translate to French: {text}") | model,
    spanish=ChatPromptTemplate.from_template("Translate to Spanish: {text}") | model,
    german=ChatPromptTemplate.from_template("Translate to German: {text}") | model
)

result = parallel_chain.invoke({"text": "Hello"})
# {
#   'french': AIMessage(content='Bonjour'),
#   'spanish': AIMessage(content='Hola'),
#   'german': AIMessage(content='Hallo')
# }
```

**å­—å…¸è¯­æ³•ç³–**:

```python
# ä½¿ç”¨å­—å…¸ï¼ˆæ›´ç®€æ´ï¼‰
parallel_chain = {
    "french": prompt_fr | model,
    "spanish": prompt_es | model
}
```

### 3.2.5 RunnableMapï¼šå­—å…¸æ˜ å°„

```python
# æ˜ å°„è¾“å…¥åˆ°å¤šä¸ªé”®
from langchain_core.runnables import RunnableParallel

chain = RunnableParallel({
    "uppercase": RunnableLambda(lambda x: x.upper()),
    "lowercase": RunnableLambda(lambda x: x.lower()),
    "length": RunnableLambda(lambda x: len(x))
})

result = chain.invoke("Hello World")
# {'uppercase': 'HELLO WORLD', 'lowercase': 'hello world', 'length': 11}
```

---

## 3.3 é…ç½®åŒ–ï¼ˆConfigurableï¼‰

### 3.3.1 ConfigurableFieldï¼šåŠ¨æ€å‚æ•°

```python
from langchain_core.runnables import ConfigurableField

# å¯é…ç½®çš„æ¨¡å‹
model = ChatOpenAI(model="gpt-4o-mini").configurable_fields(
    temperature=ConfigurableField(
        id="llm_temperature",
        name="LLM Temperature",
        description="Controls randomness"
    ),
    model=ConfigurableField(
        id="llm_model",
        name="LLM Model"
    )
)

chain = prompt | model | parser

# è¿è¡Œæ—¶é…ç½®
result1 = chain.invoke(
    {"text": "Hello"},
    config={"configurable": {"llm_temperature": 0}}
)

result2 = chain.invoke(
    {"text": "Hello"},
    config={"configurable": {"llm_temperature": 1.5}}
)
```

### 3.3.2 ConfigurableAlternativesï¼šæ¨¡å‹åˆ‡æ¢

```python
from langchain_core.runnables import ConfigurableFieldAlternatives
from langchain_anthropic import ChatAnthropic

# å¯åˆ‡æ¢çš„æ¨¡å‹
model = ChatOpenAI(model="gpt-4o").configurable_alternatives(
    ConfigurableField(id="llm"),
    default_key="openai",
    anthropic=ChatAnthropic(model="claude-3-5-sonnet-20241022"),
    local=ChatOllama(model="llama3.2")
)

chain = prompt | model | parser

# ä½¿ç”¨ OpenAI (é»˜è®¤)
result1 = chain.invoke({"text": "Hello"})

# åˆ‡æ¢åˆ° Anthropic
result2 = chain.invoke(
    {"text": "Hello"},
    config={"configurable": {"llm": "anthropic"}}
)

# åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å‹
result3 = chain.invoke(
    {"text": "Hello"},
    config={"configurable": {"llm": "local"}}
)
```

### 3.3.3 è¿è¡Œæ—¶é…ç½®ï¼ˆRunnableConfigï¼‰

```python
from langchain_core.runnables import RunnableConfig

# åˆ›å»ºé…ç½®
config = RunnableConfig(
    tags=["translation", "production"],
    metadata={"user_id": "12345"},
    callbacks=[...],
    max_concurrency=5
)

result = chain.invoke({"text": "Hello"}, config=config)
```

### 3.3.4 with_config() æ–¹æ³•

```python
# é¢„é…ç½®é“¾
production_chain = chain.with_config({
    "tags": ["production"],
    "metadata": {"env": "prod"},
    "configurable": {"llm_temperature": 0}
})

# åç»­è°ƒç”¨è‡ªåŠ¨ä½¿ç”¨é…ç½®
result = production_chain.invoke({"text": "Hello"})
```

---

## 3.4 Fallback ä¸å®¹é”™

### 3.4.1 with_fallbacks()ï¼šå¤±è´¥é™çº§

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# ä¸»æ¨¡å‹
primary = ChatOpenAI(model="gpt-4o")

# å¤‡ç”¨æ¨¡å‹
fallbacks = [
    ChatOpenAI(model="gpt-4o-mini"),      # å¤‡ç”¨1: æ›´ä¾¿å®œçš„æ¨¡å‹
    ChatAnthropic(model="claude-3-5-sonnet-20241022"),  # å¤‡ç”¨2: ä¸åŒæä¾›å•†
    ChatOllama(model="llama3.2")          # å¤‡ç”¨3: æœ¬åœ°æ¨¡å‹
]

# å¸¦é™çº§çš„é“¾
chain = prompt | primary.with_fallbacks(fallbacks) | parser

# è‡ªåŠ¨é™çº§
try:
    result = chain.invoke({"text": "Hello"})
except Exception:
    # å¦‚æœæ‰€æœ‰é™çº§éƒ½å¤±è´¥æ‰æŠ›å‡ºå¼‚å¸¸
    pass
```

### 3.4.2 å¤šçº§ Fallback ç­–ç•¥

```python
# å®Œæ•´é“¾çš„é™çº§
primary_chain = prompt_complex | gpt4 | parser
fallback_chain = prompt_simple | gpt3 | parser
last_resort = RunnableLambda(lambda x: "Translation unavailable")

full_chain = primary_chain.with_fallbacks([
    fallback_chain,
    last_resort
])
```

### 3.4.3 å¼‚å¸¸å¤„ç†ä¸æ—¥å¿—è®°å½•

```python
from langchain.callbacks import StdOutCallbackHandler

class FallbackLogger(StdOutCallbackHandler):
    def on_chain_error(self, error, **kwargs):
        print(f"Primary chain failed: {error}")
        print("Trying fallback...")

chain = primary.with_fallbacks(
    fallbacks=[fallback],
    callbacks=[FallbackLogger()]
)
```

---

## 3.5 Retry é‡è¯•æœºåˆ¶

### 3.5.1 with_retry()ï¼šè‡ªåŠ¨é‡è¯•

```python
from langchain_core.runnables import Runnable

# è‡ªåŠ¨é‡è¯•ï¼ˆé»˜è®¤æœ€å¤š3æ¬¡ï¼‰
model_with_retry = model.with_retry()

chain = prompt | model_with_retry | parser

# é‡åˆ°ä¸´æ—¶é”™è¯¯è‡ªåŠ¨é‡è¯•
result = chain.invoke({"text": "Hello"})
```

### 3.5.2 æŒ‡æ•°é€€é¿ï¼ˆExponential Backoffï¼‰

```python
# è‡ªå®šä¹‰é‡è¯•ç­–ç•¥
model_with_retry = model.with_retry(
    stop_after_attempt=5,              # æœ€å¤š5æ¬¡
    wait_exponential_multiplier=1,     # åˆå§‹ç­‰å¾…1ç§’
    wait_exponential_max=60,           # æœ€å¤šç­‰å¾…60ç§’
    retry_if_exception_type=(RateLimitError,)  # åªé‡è¯•ç‰¹å®šé”™è¯¯
)

# é‡è¯•é—´éš”: 1s, 2s, 4s, 8s, 16s (ä¸Šé™60s)
```

### 3.5.3 é‡è¯•æ¡ä»¶è‡ªå®šä¹‰

```python
from openai import APIError

def should_retry(error: Exception) -> bool:
    """è‡ªå®šä¹‰é‡è¯•é€»è¾‘"""
    if isinstance(error, RateLimitError):
        return True
    if isinstance(error, APIError) and "timeout" in str(error):
        return True
    return False

model_with_custom_retry = model.with_retry(
    retry_if_exception=should_retry,
    stop_after_attempt=3
)
```

<InteractiveComponent name="FallbackPathSimulator" />
<InteractiveComponent name="RetryTimeline" />

---

## ğŸ¯ æœ¬ç« å°ç»“

**æ ¸å¿ƒè¦ç‚¹**:

1. **å‡½æ•°ç»„åˆ**: LCEL åŸºäºæ•°å­¦å‡½æ•°ç»„åˆ,å…·æœ‰ç»“åˆå¾‹å’Œç±»å‹å®‰å…¨
2. **é«˜çº§æ“ä½œ**: RunnablePassthroughã€RunnableLambdaã€RunnableBranchã€RunnableParallel
3. **é…ç½®åŒ–**: ConfigurableField å’Œ ConfigurableAlternatives å®ç°è¿è¡Œæ—¶é…ç½®
4. **å®¹é”™æœºåˆ¶**: with_fallbacks() å®ç°å¤šçº§é™çº§
5. **é‡è¯•ç­–ç•¥**: with_retry() æ”¯æŒæŒ‡æ•°é€€é¿å’Œè‡ªå®šä¹‰æ¡ä»¶

**æŒæ¡æ£€æŸ¥**:

- [ ] èƒ½è§£é‡Š LCEL çš„å‡½æ•°ç»„åˆæœ¬è´¨
- [ ] èƒ½ä½¿ç”¨ RunnableParallel å¹¶è¡Œæ‰§è¡Œä»»åŠ¡
- [ ] èƒ½ç”¨ ConfigurableAlternatives å®ç°æ¨¡å‹åˆ‡æ¢
- [ ] èƒ½é…ç½®å¤šçº§ Fallback ç­–ç•¥
- [ ] èƒ½è‡ªå®šä¹‰é‡è¯•æ¡ä»¶

**ç»ƒä¹ é¢˜**:

1. **å¹¶è¡Œç¿»è¯‘**: ç”¨ RunnableParallel åŒæ—¶ç¿»è¯‘åˆ°5ç§è¯­è¨€,æµ‹é‡æ€»è€—æ—¶
2. **æ™ºèƒ½è·¯ç”±**: æ ¹æ®æ–‡æœ¬é•¿åº¦é€‰æ‹©ä¸åŒæ¨¡å‹ï¼ˆ<100å­—ç”¨mini,>100å­—ç”¨gpt-4ï¼‰
3. **å®¹é”™é“¾**: å®ç°ä¸»æ¨¡å‹â†’å¤‡ç”¨æ¨¡å‹â†’æœ¬åœ°æ¨¡å‹çš„ä¸‰çº§é™çº§
4. **è‡ªå®šä¹‰é‡è¯•**: å¯¹ RateLimitError é‡è¯•5æ¬¡,å¯¹å…¶ä»–é”™è¯¯ç«‹å³å¤±è´¥

**ä¸‹ä¸€ç« é¢„å‘Š**:

Chapter 4 å°†å­¦ä¹ æµå¼å¤„ç†ä¸æ‰¹å¤„ç†,åŒ…æ‹¬ stream()ã€astream()ã€batch()ã€å¼‚æ­¥ç¼–ç¨‹ç­‰ã€‚

---

## ğŸ“š æ‰©å±•é˜…è¯»

- [LCEL Runnable æ¥å£](https://python.langchain.com/docs/concepts/runnables)
- [é…ç½®åŒ–æ–‡æ¡£](https://python.langchain.com/docs/how_to/configure)
- [Fallback æŒ‡å—](https://python.langchain.com/docs/how_to/fallbacks)
- [Retry ç­–ç•¥](https://python.langchain.com/docs/how_to/retry)
