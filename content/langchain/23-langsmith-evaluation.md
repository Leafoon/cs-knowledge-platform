# Chapter 23: LangSmith è¯„ä¼°ç³»ç»Ÿ

## æœ¬ç« æ¦‚è§ˆ

Tracing è®©ä½ çœ‹åˆ°**å‘ç”Ÿäº†ä»€ä¹ˆ**ï¼Œè€Œ Evaluationï¼ˆè¯„ä¼°ï¼‰å¸®ä½ åˆ¤æ–­**åšå¾—å¥½ä¸å¥½**ã€‚LangSmith çš„è¯„ä¼°ç³»ç»Ÿæä¾›å®Œæ•´çš„å·¥å…·é“¾ï¼šä»æ•°æ®é›†ç®¡ç†ã€ç¦»çº¿æ‰¹é‡è¯„ä¼°ã€å¤šç»´åº¦æŒ‡æ ‡è®¡ç®—ï¼Œåˆ°åœ¨çº¿ç”¨æˆ·åé¦ˆæ”¶é›†ä¸ A/B æµ‹è¯•ã€‚æœ¬ç« å°†æ·±å…¥å­¦ä¹ å¦‚ä½•æ„å»º LLM åº”ç”¨çš„è´¨é‡ä¿éšœä½“ç³»ï¼Œå®ç°å¯é‡åŒ–ã€å¯å¤ç°çš„æŒç»­æ”¹è¿›ã€‚

**æœ¬ç« é‡ç‚¹**ï¼š
- æ•°æ®é›†ï¼ˆDatasetï¼‰ç®¡ç†ä¸ç‰ˆæœ¬æ§åˆ¶
- ç¦»çº¿è¯„ä¼°ï¼ˆEvaluationï¼‰å®Œæ•´æµç¨‹
- è¯„ä¼°æŒ‡æ ‡ï¼ˆEvaluatorsï¼‰ï¼šLLM-as-Judgeã€è·ç¦»åº¦é‡ã€è‡ªå®šä¹‰
- A/B æµ‹è¯•ä¸å¯¹æ¯”å®éªŒ
- åœ¨çº¿åé¦ˆæ”¶é›†ä¸é—­ç¯ä¼˜åŒ–

---

## 23.1 æ•°æ®é›†ç®¡ç†

### 23.1.1 ä¸ºä»€ä¹ˆéœ€è¦æ•°æ®é›†ï¼Ÿ

**é—®é¢˜åœºæ™¯**ï¼šå¦‚ä½•éªŒè¯æç¤ºè¯æ”¹è¿›çœŸçš„æœ‰æ•ˆï¼Ÿ

```python
# æ”¹è¿›å‰çš„æç¤º
prompt_v1 = "Translate to French: {text}"

# æ”¹è¿›åçš„æç¤º
prompt_v2 = """You are a professional translator. 
Translate the following text to French while preserving tone and cultural nuances:

{text}"""

# â“ é—®é¢˜ï¼šå“ªä¸ªæ›´å¥½ï¼Ÿå¦‚ä½•è¯æ˜ï¼Ÿ
```

**æ²¡æœ‰æ•°æ®é›†çš„å›°å¢ƒ**ï¼š
- ğŸ¤” ä¾é ä¸»è§‚æ„Ÿè§‰ï¼ˆ"æ„Ÿè§‰ v2 æ›´å¥½"ï¼‰
- ğŸ¤” åªæµ‹è¯• 1-2 ä¸ªæ ·æœ¬ï¼ˆä¸å…·ä»£è¡¨æ€§ï¼‰
- ğŸ¤” æ— æ³•å¤ç°ï¼ˆä¸‹æ¬¡æµ‹è¯•æ—¶å¿˜è®°ç”¨ä»€ä¹ˆè¾“å…¥ï¼‰
- ğŸ¤” æ— æ³•é‡åŒ–æ”¹è¿›ï¼ˆåˆ°åº•å¥½äº†å¤šå°‘ï¼Ÿï¼‰

**æœ‰æ•°æ®é›†çš„ä¼˜åŠ¿**ï¼š
- âœ… å®¢è§‚è¯„ä¼°ï¼šç”¨ç›¸åŒæ•°æ®æµ‹è¯•æ‰€æœ‰ç‰ˆæœ¬
- âœ… ä»£è¡¨æ€§ï¼šè¦†ç›–å„ç§è¾¹ç•Œæƒ…å†µ
- âœ… å¯å¤ç°ï¼šéšæ—¶é‡æ–°è¯„ä¼°
- âœ… å¯é‡åŒ–ï¼šè®¡ç®—å‡†ç¡®ç‡ã€BLEU ç­‰æŒ‡æ ‡

### 23.1.2 åˆ›å»ºæ•°æ®é›†

**æ–¹æ³• 1ï¼šä»£ç åˆ›å»º**

```python
from langsmith import Client

client = Client()

# åˆ›å»ºæ•°æ®é›†
dataset_name = "translation-test-set"
dataset = client.create_dataset(
    dataset_name=dataset_name,
    description="æµ‹è¯•ç¿»è¯‘è´¨é‡çš„æ ‡å‡†æ•°æ®é›†"
)

# æ·»åŠ ç¤ºä¾‹
examples = [
    {
        "inputs": {"text": "Hello, world!"},
        "outputs": {"translation": "Bonjour, le monde !"}
    },
    {
        "inputs": {"text": "How are you?"},
        "outputs": {"translation": "Comment vas-tu ?"}
    },
    {
        "inputs": {"text": "The weather is nice today."},
        "outputs": {"translation": "Il fait beau aujourd'hui."}
    },
]

for example in examples:
    client.create_example(
        dataset_id=dataset.id,
        inputs=example["inputs"],
        outputs=example["outputs"]
    )

print(f"âœ… Created dataset '{dataset_name}' with {len(examples)} examples")
```

**æ–¹æ³• 2ï¼šä» CSV å¯¼å…¥**

```python
import pandas as pd

# å‡†å¤‡ CSV æ–‡ä»¶
data = {
    "input_text": ["Hello", "Goodbye", "Thank you"],
    "expected_translation": ["Bonjour", "Au revoir", "Merci"]
}
df = pd.DataFrame(data)
df.to_csv("translation_dataset.csv", index=False)

# ä» CSV åˆ›å»ºæ•°æ®é›†
dataset = client.create_dataset(dataset_name="translation-from-csv")

# è¯»å–å¹¶æ·»åŠ ç¤ºä¾‹
for _, row in df.iterrows():
    client.create_example(
        dataset_id=dataset.id,
        inputs={"text": row["input_text"]},
        outputs={"translation": row["expected_translation"]}
    )
```

**æ–¹æ³• 3ï¼šä» Trace åˆ›å»ºï¼ˆç”Ÿäº§æ•°æ®å¤ç”¨ï¼‰**

```python
# ä»æˆåŠŸçš„ Run åˆ›å»ºç¤ºä¾‹
from langsmith import Client

client = Client()

# æŸ¥è¯¢æˆåŠŸçš„ Runs
runs = client.list_runs(
    project_name="production-chatbot",
    filter='status="success" AND feedback.score > 0.8'  # é«˜åˆ† Run
)

# åˆ›å»ºæ•°æ®é›†
dataset = client.create_dataset(dataset_name="production-golden-set")

# æ·»åŠ é«˜è´¨é‡æ ·æœ¬
for run in runs[:50]:  # å–å‰ 50 ä¸ª
    client.create_example(
        dataset_id=dataset.id,
        inputs=run.inputs,
        outputs=run.outputs
    )
```

### 23.1.3 æ•°æ®é›†ç‰ˆæœ¬ç®¡ç†

```python
# åˆ›å»ºå¤šä¸ªç‰ˆæœ¬
dataset_v1 = client.create_dataset(
    dataset_name="qa-dataset-v1.0",
    description="åˆå§‹ç‰ˆæœ¬"
)

# åç»­åˆ›å»ºæ–°ç‰ˆæœ¬ï¼ˆä¸åŒåç§°ï¼‰
dataset_v2 = client.create_dataset(
    dataset_name="qa-dataset-v2.0",
    description="å¢åŠ è¾¹ç•Œæƒ…å†µæµ‹è¯•"
)

# å¤åˆ¶æ•°æ®é›†
def clone_dataset(old_name: str, new_name: str):
    old_dataset = client.read_dataset(dataset_name=old_name)
    new_dataset = client.create_dataset(dataset_name=new_name)
    
    # å¤åˆ¶æ‰€æœ‰ç¤ºä¾‹
    examples = client.list_examples(dataset_id=old_dataset.id)
    for example in examples:
        client.create_example(
            dataset_id=new_dataset.id,
            inputs=example.inputs,
            outputs=example.outputs
        )
    
    return new_dataset

# ä½¿ç”¨
clone_dataset("qa-dataset-v1.0", "qa-dataset-v2.0-candidate")
```

### 23.1.4 æ•°æ®é›†è´¨é‡æ ‡å‡†

**å¥½çš„è¯„ä¼°æ•°æ®é›†åº”å…·å¤‡**ï¼š

| æ ‡å‡† | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| **ä»£è¡¨æ€§** | è¦†ç›–çœŸå®ä½¿ç”¨åœºæ™¯ | åŒ…å«ç®€å•ã€ä¸­ç­‰ã€å¤æ‚é—®é¢˜ |
| **å¤šæ ·æ€§** | ä¸åŒç±»å‹è¾“å…¥ | çŸ­æ–‡æœ¬ã€é•¿æ–‡æœ¬ã€ç‰¹æ®Šå­—ç¬¦ |
| **è¾¹ç•Œæƒ…å†µ** | æç«¯è¾“å…¥ | ç©ºè¾“å…¥ã€è¶…é•¿è¾“å…¥ã€æ­§ä¹‰è¾“å…¥ |
| **æ ‡å‡†ç­”æ¡ˆ** | é«˜è´¨é‡å‚è€ƒè¾“å‡º | äººå·¥å®¡æ ¸çš„"é»„é‡‘æ ‡å‡†" |
| **è§„æ¨¡é€‚ä¸­** | 50-500 æ¡ | å¤ªå°‘ä¸ä»£è¡¨ï¼Œå¤ªå¤šæµªè´¹ |

**åä¾‹ï¼šç³Ÿç³•çš„æ•°æ®é›†**

```python
# âŒ ä¸å¥½çš„æ•°æ®é›†
bad_examples = [
    {"inputs": {"q": "hi"}, "outputs": {"a": "hello"}},  # å¤ªç®€å•
    {"inputs": {"q": "hi"}, "outputs": {"a": "hey"}},    # é‡å¤è¾“å…¥
    {"inputs": {"q": "hi"}, "outputs": {"a": "hi"}},     # é‡å¤è¾“å…¥
]
# é—®é¢˜ï¼šç¼ºä¹å¤šæ ·æ€§ï¼Œæ— æ³•åæ˜ çœŸå®åœºæ™¯
```

**æ­£ä¾‹ï¼šé«˜è´¨é‡æ•°æ®é›†**

```python
# âœ… å¥½çš„æ•°æ®é›†
good_examples = [
    # ç®€å•é—®é¢˜
    {
        "inputs": {"question": "What is the capital of France?"},
        "outputs": {"answer": "Paris"}
    },
    # éœ€è¦æ¨ç†
    {
        "inputs": {"question": "If a train leaves at 2pm and arrives at 5pm, how long is the journey?"},
        "outputs": {"answer": "3 hours"}
    },
    # æ­§ä¹‰é—®é¢˜ï¼ˆéœ€è¦æ¾„æ¸…ï¼‰
    {
        "inputs": {"question": "What is the best programming language?"},
        "outputs": {"answer": "It depends on your use case. For web development, JavaScript is popular. For data science, Python is widely used."}
    },
    # è¶…å‡ºçŸ¥è¯†èŒƒå›´
    {
        "inputs": {"question": "What will happen tomorrow?"},
        "outputs": {"answer": "I cannot predict future events."}
    },
]
```

---

## 23.2 ç¦»çº¿è¯„ä¼°ï¼ˆEvaluationï¼‰

### 23.2.1 evaluate() å‡½æ•°åŸºç¡€

<div data-component="EvaluationPipeline"></div>

**åŸºæœ¬è¯„ä¼°æµç¨‹**ï¼š

```python
from langsmith import Client
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# åˆå§‹åŒ–
client = Client()

# å®šä¹‰é“¾
prompt = ChatPromptTemplate.from_template("Translate to French: {text}")
llm = ChatOpenAI(model="gpt-4")
chain = prompt | llm

# è¿è¡Œè¯„ä¼°
results = client.evaluate(
    lambda inputs: chain.invoke(inputs),  # è¦è¯„ä¼°çš„å‡½æ•°
    data=dataset_name,                     # æ•°æ®é›†åç§°
    evaluators=[...],                      # è¯„ä¼°å™¨ï¼ˆä¸‹æ–‡è¯¦è§£ï¼‰
    experiment_prefix="translation-v1"     # å®éªŒåç§°å‰ç¼€
)

print(f"Evaluation completed: {results['experiment_name']}")
```

**evaluate() å·¥ä½œæµç¨‹**ï¼š

```
1. åŠ è½½æ•°æ®é›† (Dataset)
   â†“
2. å¯¹æ¯ä¸ªç¤ºä¾‹è°ƒç”¨é“¾ (Chain.invoke)
   â†“
3. åº”ç”¨æ‰€æœ‰ Evaluators
   â†“
4. èšåˆè¯„ä¼°ç»“æœ
   â†“
5. ä¿å­˜åˆ° LangSmithï¼ˆå¯è§†åŒ–æŸ¥çœ‹ï¼‰
```

### 23.2.2 å®Œæ•´è¯„ä¼°ç¤ºä¾‹

```python
from langsmith import Client
from langsmith.evaluation import evaluate
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.smith import RunEvalConfig

client = Client()

# åˆ›å»ºæµ‹è¯•é“¾
def create_translation_chain(model_name: str):
    prompt = ChatPromptTemplate.from_template(
        "Translate the following text to French:\n\n{text}"
    )
    llm = ChatOpenAI(model=model_name)
    return prompt | llm

# å®šä¹‰è¯„ä¼°å™¨ï¼ˆä¸‹ä¸€èŠ‚è¯¦è§£ï¼‰
eval_config = RunEvalConfig(
    evaluators=[
        "qa",  # å†…ç½® QA è¯„ä¼°å™¨
        "embedding_distance",  # åµŒå…¥è·ç¦»
    ]
)

# è¿è¡Œè¯„ä¼°
chain = create_translation_chain("gpt-4")

results = evaluate(
    lambda inputs: chain.invoke(inputs).content,  # æå–æ–‡æœ¬å†…å®¹
    data="translation-test-set",
    evaluators=eval_config.evaluators,
    experiment_prefix="gpt4-baseline",
)

# æŸ¥çœ‹ç»“æœ
print(f"âœ… Experiment: {results['experiment_name']}")
print(f"ğŸ“Š Results: {results['results']}")
```

### 23.2.3 æ‰¹é‡è¯„ä¼°å¹¶è¡ŒåŒ–

```python
from langsmith.evaluation import evaluate
from concurrent.futures import ThreadPoolExecutor

# æ–¹æ³• 1ï¼ševaluate() è‡ªåŠ¨å¹¶è¡Œ
results = evaluate(
    chain,
    data=dataset_name,
    evaluators=evaluators,
    max_concurrency=10,  # å¹¶è¡Œåº¦ï¼ˆé»˜è®¤å€¼ä¼šè‡ªåŠ¨è®¾ç½®ï¼‰
)

# æ–¹æ³• 2ï¼šæ‰‹åŠ¨æ§åˆ¶å¹¶è¡Œ
def evaluate_parallel(chains: list, dataset_name: str):
    """å¹¶è¡Œè¯„ä¼°å¤šä¸ªé“¾"""
    results_list = []
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = []
        for i, chain in enumerate(chains):
            future = executor.submit(
                evaluate,
                chain,
                data=dataset_name,
                evaluators=evaluators,
                experiment_prefix=f"chain-{i}"
            )
            futures.append(future)
        
        for future in futures:
            results_list.append(future.result())
    
    return results_list

# ä½¿ç”¨
chains = [
    create_translation_chain("gpt-4"),
    create_translation_chain("gpt-3.5-turbo"),
    create_translation_chain("claude-3-opus"),
]

all_results = evaluate_parallel(chains, "translation-test-set")
```

### 23.2.4 è¯„ä¼°ç»“æœæŸ¥çœ‹

**åœ¨ LangSmith UI ä¸­æŸ¥çœ‹**ï¼š

1. è®¿é—® [https://smith.langchain.com](https://smith.langchain.com)
2. è¿›å…¥ **Datasets** é¡µé¢
3. é€‰æ‹©æ•°æ®é›† â†’ **Experiments** æ ‡ç­¾
4. æŸ¥çœ‹æ¯ä¸ªå®éªŒçš„ï¼š
   - **æ•´ä½“åˆ†æ•°**ï¼ˆå¹³å‡å€¼ã€ä¸­ä½æ•°ï¼‰
   - **ç¤ºä¾‹çº§åˆ«ç»“æœ**ï¼ˆæ¯æ¡æ ·æœ¬çš„å¾—åˆ†ï¼‰
   - **å¯¹æ¯”è§†å›¾**ï¼ˆå¤šä¸ªå®éªŒå¯¹æ¯”ï¼‰

**ä»£ç ä¸­æŸ¥çœ‹ç»“æœ**ï¼š

```python
# è·å–å®éªŒè¯¦æƒ…
experiment = client.read_project(project_name=results['experiment_name'])

# è·å–æ‰€æœ‰ Runs
runs = list(client.list_runs(project_name=results['experiment_name']))

# ç»Ÿè®¡
total_runs = len(runs)
successful_runs = sum(1 for r in runs if r.status == "success")
failed_runs = total_runs - successful_runs

print(f"Total: {total_runs}, Success: {successful_runs}, Failed: {failed_runs}")

# è·å–è¯„ä¼°åˆ†æ•°
scores = []
for run in runs:
    if run.feedback_stats:
        for key, value in run.feedback_stats.items():
            if "score" in key.lower():
                scores.append(value.get("avg", 0))

avg_score = sum(scores) / len(scores) if scores else 0
print(f"Average Score: {avg_score:.2f}")
```

---

## 23.3 è¯„ä¼°æŒ‡æ ‡ï¼ˆEvaluatorsï¼‰

### 23.3.1 LLM-as-Judgeï¼šCriteria Evaluator

ä½¿ç”¨ LLM ä½œä¸ºè¯„åˆ¤è€…ï¼Œè¯„ä¼°è¾“å‡ºè´¨é‡ã€‚

**åŸºæœ¬ç”¨æ³•**ï¼š

```python
from langchain.evaluation import load_evaluator

# åˆ›å»º Criteria Evaluator
criteria_eval = load_evaluator("criteria", criteria="correctness")

# è¯„ä¼°å•ä¸ªæ ·æœ¬
result = criteria_eval.evaluate_strings(
    prediction="Paris is the capital of France.",
    reference="The capital of France is Paris.",
    input="What is the capital of France?"
)

print(result)
# {'reasoning': '...', 'value': 'Y', 'score': 1}
```

**è‡ªå®šä¹‰è¯„ä¼°æ ‡å‡†**ï¼š

```python
from langchain.evaluation import CriteriaEvalChain

# è‡ªå®šä¹‰æ ‡å‡†
custom_criteria = {
    "politeness": "Is the response polite and respectful?",
    "completeness": "Does the response fully answer the question?",
    "clarity": "Is the response clear and easy to understand?"
}

# åˆ›å»ºè¯„ä¼°é“¾
eval_chain = CriteriaEvalChain.from_llm(
    llm=ChatOpenAI(model="gpt-4"),
    criteria=custom_criteria
)

# è¯„ä¼°
result = eval_chain.evaluate_strings(
    prediction="I don't know.",
    input="What is the capital of France?",
    reference="Paris"
)
```

**å¤šç»´åº¦è¯„ä¼°**ï¼š

```python
from langsmith.evaluation import evaluate, EvaluatorType

# å®šä¹‰å¤šä¸ªè¯„ä¼°ç»´åº¦
evaluators = [
    # æ­£ç¡®æ€§
    {
        "type": EvaluatorType.CRITERIA,
        "criteria": "correctness",
        "llm": ChatOpenAI(model="gpt-4")
    },
    # ç®€æ´æ€§
    {
        "type": EvaluatorType.CRITERIA,
        "criteria": "conciseness",
        "llm": ChatOpenAI(model="gpt-4")
    },
    # ä¸“ä¸šæ€§
    {
        "type": EvaluatorType.CRITERIA,
        "criteria": {
            "professionalism": "Is the response professional and appropriate for a business setting?"
        },
        "llm": ChatOpenAI(model="gpt-4")
    },
]

# è¯„ä¼°
results = evaluate(
    chain,
    data=dataset_name,
    evaluators=evaluators,
)
```

### 23.3.2 Embedding Distance

é€šè¿‡åµŒå…¥å‘é‡çš„è·ç¦»åº¦é‡è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚

```python
from langchain.evaluation import load_evaluator
from langchain_openai import OpenAIEmbeddings

# åˆ›å»º Embedding Distance Evaluator
embedding_eval = load_evaluator(
    "embedding_distance",
    embeddings=OpenAIEmbeddings(),
    distance_metric="cosine"  # æˆ– "euclidean", "manhattan"
)

# è¯„ä¼°
result = embedding_eval.evaluate_strings(
    prediction="Paris is the capital of France.",
    reference="The capital of France is Paris."
)

print(result)
# {'score': 0.95}  # åˆ†æ•°è¶Šé«˜è¶Šç›¸ä¼¼ï¼ˆcosine similarityï¼‰
```

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­ï¼ˆæ”¹å†™ã€æ‘˜è¦ï¼‰
- âœ… å¤šè¯­è¨€è¯„ä¼°ï¼ˆåµŒå…¥ç©ºé—´å¯¹é½ï¼‰
- âŒ ç²¾ç¡®åŒ¹é…è¦æ±‚ï¼ˆå¦‚ä»£ç ç”Ÿæˆï¼‰

### 23.3.3 String Distanceï¼ˆç¼–è¾‘è·ç¦»ã€BLEUï¼‰

**ç¼–è¾‘è·ç¦»ï¼ˆLevenshtein Distanceï¼‰**ï¼š

```python
from langchain.evaluation import load_evaluator

string_eval = load_evaluator("string_distance", distance="levenshtein")

result = string_eval.evaluate_strings(
    prediction="Bonjour le monde",
    reference="Bonjour, le monde!"
)

print(result)
# {'score': 2}  # ç¼–è¾‘è·ç¦»ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
```

**BLEU Scoreï¼ˆç¿»è¯‘è¯„ä¼°ï¼‰**ï¼š

```python
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def bleu_evaluator(prediction: str, reference: str) -> dict:
    """BLEU è¯„ä¼°å™¨"""
    # Tokenize
    pred_tokens = prediction.split()
    ref_tokens = [reference.split()]  # BLEU éœ€è¦åˆ—è¡¨çš„åˆ—è¡¨
    
    # è®¡ç®— BLEU
    smoothing = SmoothingFunction().method1
    score = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoothing)
    
    return {"score": score}

# ä½¿ç”¨
result = bleu_evaluator(
    prediction="Bonjour le monde",
    reference="Bonjour, le monde!"
)
print(result)  # {'score': 0.7071...}
```

### 23.3.4 Regex Evaluator

```python
import re

def regex_evaluator(pattern: str):
    """æ­£åˆ™è¡¨è¾¾å¼è¯„ä¼°å™¨"""
    def evaluate(prediction: str, **kwargs) -> dict:
        match = re.search(pattern, prediction)
        return {
            "score": 1 if match else 0,
            "reasoning": f"Pattern '{pattern}' {'found' if match else 'not found'}"
        }
    return evaluate

# ç¤ºä¾‹ï¼šæ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«æ•°å­—
number_eval = regex_evaluator(r'\d+')

result = number_eval(prediction="The answer is 42")
print(result)  # {'score': 1, 'reasoning': "Pattern '\\d+' found"}
```

### 23.3.5 è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°

```python
from langsmith.evaluation import EvaluationResult

def custom_length_evaluator(max_length: int):
    """è‡ªå®šä¹‰è¯„ä¼°å™¨ï¼šæ£€æŸ¥è¾“å‡ºé•¿åº¦"""
    def evaluate(run, example):
        prediction = run.outputs.get("output", "")
        length = len(prediction)
        
        # è¿”å› EvaluationResult
        return EvaluationResult(
            key="length_check",
            score=1 if length <= max_length else 0,
            comment=f"Length: {length} ({'PASS' if length <= max_length else 'FAIL'})"
        )
    
    return evaluate

# ä½¿ç”¨
evaluators = [
    custom_length_evaluator(max_length=200)
]

results = evaluate(
    chain,
    data=dataset_name,
    evaluators=evaluators,
)
```

**å¤æ‚è‡ªå®šä¹‰è¯„ä¼°å™¨ç¤ºä¾‹**ï¼š

```python
def fact_checker_evaluator(llm):
    """äº‹å®å‡†ç¡®æ€§æ£€æŸ¥å™¨"""
    def evaluate(run, example):
        prediction = run.outputs.get("output", "")
        reference = example.outputs.get("answer", "")
        
        # ä½¿ç”¨ LLM æ£€æŸ¥äº‹å®å‡†ç¡®æ€§
        prompt = f"""Compare the following two statements and check if the PREDICTION contains factual errors compared to the REFERENCE.

REFERENCE: {reference}
PREDICTION: {prediction}

Is the PREDICTION factually correct? Answer with:
- "CORRECT" if factually accurate
- "INCORRECT" if contains factual errors
- "PARTIALLY_CORRECT" if mostly correct with minor issues

Answer: """
        
        result = llm.invoke(prompt).content.strip()
        
        score_map = {
            "CORRECT": 1.0,
            "PARTIALLY_CORRECT": 0.5,
            "INCORRECT": 0.0
        }
        
        return EvaluationResult(
            key="fact_check",
            score=score_map.get(result, 0.0),
            comment=f"Fact check result: {result}"
        )
    
    return evaluate

# ä½¿ç”¨
evaluators = [
    fact_checker_evaluator(ChatOpenAI(model="gpt-4"))
]
```

---

## 23.4 A/B æµ‹è¯•

### 23.4.1 å¯¹æ¯”ä¸åŒæç¤ºç‰ˆæœ¬

<div data-component="ABTestComparison"></div>

```python
from langsmith import Client
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

client = Client()

# ç‰ˆæœ¬ Aï¼šç®€å•æç¤º
prompt_v1 = ChatPromptTemplate.from_template("Translate to French: {text}")

# ç‰ˆæœ¬ Bï¼šè¯¦ç»†æç¤º
prompt_v2 = ChatPromptTemplate.from_template(
    """You are a professional translator with expertise in French.
    
Translate the following text to French while:
- Preserving the original tone
- Using appropriate cultural references
- Maintaining grammatical accuracy

Text: {text}"""
)

# åˆ›å»ºä¸¤ä¸ªé“¾
llm = ChatOpenAI(model="gpt-4")
chain_v1 = prompt_v1 | llm
chain_v2 = prompt_v2 | llm

# è¯„ä¼°ä¸¤ä¸ªç‰ˆæœ¬
results_v1 = evaluate(
    chain_v1,
    data="translation-test-set",
    evaluators=[...],
    experiment_prefix="prompt-v1-simple"
)

results_v2 = evaluate(
    chain_v2,
    data="translation-test-set",
    evaluators=[...],
    experiment_prefix="prompt-v2-detailed"
)

# å¯¹æ¯”ç»“æœ
print(f"V1 Average Score: {results_v1['avg_score']}")
print(f"V2 Average Score: {results_v2['avg_score']}")
```

### 23.4.2 å¯¹æ¯”ä¸åŒæ¨¡å‹

```python
models = ["gpt-4", "gpt-3.5-turbo", "claude-3-opus-20240229"]

results_dict = {}

for model_name in models:
    chain = prompt | ChatOpenAI(model=model_name)
    
    results = evaluate(
        chain,
        data="translation-test-set",
        evaluators=evaluators,
        experiment_prefix=f"model-{model_name}"
    )
    
    results_dict[model_name] = results

# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
print("\nğŸ“Š Model Comparison Report")
print("="*60)
for model, results in results_dict.items():
    print(f"{model:30} Score: {results['avg_score']:.3f}")
```

### 23.4.3 ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ

```python
from scipy import stats

def compare_experiments(exp1_scores: list, exp2_scores: list):
    """æ¯”è¾ƒä¸¤ä¸ªå®éªŒçš„ç»Ÿè®¡æ˜¾è‘—æ€§"""
    # T-test
    t_stat, p_value = stats.ttest_ind(exp1_scores, exp2_scores)
    
    # æ•ˆåº”é‡ï¼ˆCohen's dï¼‰
    mean1, mean2 = np.mean(exp1_scores), np.mean(exp2_scores)
    std = np.sqrt((np.std(exp1_scores)**2 + np.std(exp2_scores)**2) / 2)
    cohen_d = (mean2 - mean1) / std
    
    return {
        "p_value": p_value,
        "is_significant": p_value < 0.05,
        "cohen_d": cohen_d,
        "effect_size": "small" if abs(cohen_d) < 0.5 else "medium" if abs(cohen_d) < 0.8 else "large"
    }

# ä½¿ç”¨
exp1_scores = [0.8, 0.85, 0.9, 0.75, 0.88]
exp2_scores = [0.92, 0.95, 0.93, 0.89, 0.94]

comparison = compare_experiments(exp1_scores, exp2_scores)
print(comparison)
# {
#   'p_value': 0.012,
#   'is_significant': True,
#   'cohen_d': 1.8,
#   'effect_size': 'large'
# }
```

---

## 23.5 åœ¨çº¿è¯„ä¼°ä¸åé¦ˆ

### 23.5.1 ç”¨æˆ·åé¦ˆæ”¶é›†ï¼ˆFeedbackï¼‰

<div data-component="FeedbackDashboard"></div>

**æ”¶é›† Thumbs Up/Down**ï¼š

```python
from langsmith import Client

client = Client()

# ç”¨æˆ·ç»™äº†å¥½è¯„
client.create_feedback(
    run_id="run-abc123",  # ä» Trace ä¸­è·å–
    key="user_rating",
    score=1,  # 1 = Thumbs Up, 0 = Thumbs Down
    comment="Great response!"
)

# ç”¨æˆ·ç»™äº†å·®è¯„
client.create_feedback(
    run_id="run-def456",
    key="user_rating",
    score=0,
    comment="Incorrect answer"
)
```

**é›†æˆåˆ°åº”ç”¨ä¸­**ï¼š

```python
from langchain_openai import ChatOpenAI
from langsmith import Client
from langsmith.run_helpers import traceable

client = Client()
llm = ChatOpenAI(model="gpt-4")

@traceable
def chatbot(question: str) -> dict:
    """èŠå¤©æœºå™¨äºº"""
    response = llm.invoke(question)
    return {"answer": response.content}

# åœ¨ä½ çš„ Web åº”ç”¨ä¸­
def handle_user_query(question: str, session_id: str):
    # è°ƒç”¨èŠå¤©æœºå™¨äºº
    result = chatbot(question)
    
    # è¿”å›ç­”æ¡ˆå’Œ run_idï¼ˆç”¨äºåé¦ˆï¼‰
    return {
        "answer": result["answer"],
        "run_id": result["__run"].id  # è·å– run_id
    }

# ç”¨æˆ·åé¦ˆç«¯ç‚¹
def submit_feedback(run_id: str, thumbs_up: bool, comment: str = ""):
    client.create_feedback(
        run_id=run_id,
        key="user_rating",
        score=1 if thumbs_up else 0,
        comment=comment
    )
```

### 23.5.2 è‡ªå®šä¹‰åé¦ˆ Schema

```python
# å¤šç»´åº¦åé¦ˆ
client.create_feedback(
    run_id="run-abc123",
    key="detailed_feedback",
    score=0.8,  # æ€»ä½“åˆ†æ•°
    value={
        "accuracy": 0.9,
        "relevance": 0.8,
        "completeness": 0.7,
        "clarity": 0.85
    },
    comment="Good answer but missing some details"
)

# åˆ†ç±»åé¦ˆ
client.create_feedback(
    run_id="run-def456",
    key="issue_type",
    value="factual_error",  # æˆ– "off_topic", "incomplete", etc.
    comment="Stated Paris is in Germany"
)
```

### 23.5.3 åé¦ˆæ•°æ®å¯¼å…¥è¯„ä¼°

```python
# è·å–æœ‰é«˜åˆ†åé¦ˆçš„ Runs
high_rated_runs = client.list_runs(
    project_name="production-chatbot",
    filter='feedback.user_rating.score > 0.8'
)

# åˆ›å»ºæ•°æ®é›†
dataset = client.create_dataset(dataset_name="high-quality-prod-samples")

# æ·»åŠ åˆ°æ•°æ®é›†
for run in high_rated_runs[:100]:
    client.create_example(
        dataset_id=dataset.id,
        inputs=run.inputs,
        outputs=run.outputs
    )

# ä½¿ç”¨æ­¤æ•°æ®é›†è¯„ä¼°æ–°ç‰ˆæœ¬
results = evaluate(
    new_chain,
    data="high-quality-prod-samples",
    evaluators=evaluators,
)
```

### 23.5.4 åé¦ˆé©±åŠ¨çš„æŒç»­æ”¹è¿›

**å®Œæ•´é—­ç¯å·¥ä½œæµ**ï¼š

```
1. ç”Ÿäº§ç¯å¢ƒè¿è¡Œ
   â†“ (è‡ªåŠ¨è¿½è¸ª)
2. æ”¶é›†ç”¨æˆ·åé¦ˆ
   â†“ (ç­›é€‰é«˜è´¨é‡æ ·æœ¬)
3. æ„å»ºè¯„ä¼°æ•°æ®é›†
   â†“ (ç¦»çº¿è¯„ä¼°)
4. æµ‹è¯•æ”¹è¿›ç‰ˆæœ¬
   â†“ (A/B æµ‹è¯•)
5. éƒ¨ç½²è·èƒœç‰ˆæœ¬
   â†“ (å¾ªç¯)
å›åˆ°ç¬¬ 1 æ­¥
```

**å®ç°ç¤ºä¾‹**ï¼š

```python
import schedule
import time

def weekly_improvement_cycle():
    """æ¯å‘¨è‡ªåŠ¨æ”¹è¿›æµç¨‹"""
    # 1. æ”¶é›†ä¸Šå‘¨é«˜åˆ†æ ·æœ¬
    last_week = datetime.now() - timedelta(days=7)
    high_rated = client.list_runs(
        project_name="production",
        filter=f'feedback.user_rating.score > 0.8 AND start_time > "{last_week.isoformat()}"'
    )
    
    # 2. æ›´æ–°æ•°æ®é›†
    dataset = client.read_dataset(dataset_name="golden-set")
    for run in high_rated[:20]:  # æ¯å‘¨æ·»åŠ  20 ä¸ª
        client.create_example(
            dataset_id=dataset.id,
            inputs=run.inputs,
            outputs=run.outputs
        )
    
    # 3. é‡æ–°è¯„ä¼°å½“å‰ç‰ˆæœ¬
    current_results = evaluate(
        current_chain,
        data="golden-set",
        evaluators=evaluators,
        experiment_prefix="weekly-baseline"
    )
    
    # 4. è¯„ä¼°å®éªŒç‰ˆæœ¬
    experimental_results = evaluate(
        experimental_chain,
        data="golden-set",
        evaluators=evaluators,
        experiment_prefix="weekly-experiment"
    )
    
    # 5. å†³å®šæ˜¯å¦éƒ¨ç½²
    if experimental_results['avg_score'] > current_results['avg_score'] * 1.05:
        print("ğŸ‰ Experimental version is 5% better! Deploying...")
        deploy_new_version(experimental_chain)
    else:
        print("â¸ï¸ No significant improvement. Keeping current version.")

# å®šæ—¶æ‰§è¡Œ
schedule.every().monday.at("02:00").do(weekly_improvement_cycle)

while True:
    schedule.run_pending()
    time.sleep(3600)
```

---

## 23.6 æœ€ä½³å®è·µ

### 23.6.1 æ•°æ®é›†æ„å»ºç­–ç•¥

```python
# âœ… å¥½çš„ç­–ç•¥ï¼šåˆ†å±‚é‡‡æ ·
def build_balanced_dataset():
    """æ„å»ºå¹³è¡¡çš„æ•°æ®é›†"""
    dataset = client.create_dataset(dataset_name="balanced-qa-set")
    
    categories = {
        "simple": 20,      # 20% ç®€å•é—®é¢˜
        "medium": 50,      # 50% ä¸­ç­‰é—®é¢˜
        "complex": 20,     # 20% å¤æ‚é—®é¢˜
        "edge_case": 10    # 10% è¾¹ç•Œæƒ…å†µ
    }
    
    for category, count in categories.items():
        examples = load_examples_by_category(category, count)
        for example in examples:
            client.create_example(
                dataset_id=dataset.id,
                inputs=example["inputs"],
                outputs=example["outputs"],
                metadata={"category": category}
            )
```

### 23.6.2 è¯„ä¼°å™¨é€‰æ‹©æŒ‡å—

| ä»»åŠ¡ç±»å‹ | æ¨èè¯„ä¼°å™¨ | åŸå›  |
|---------|-----------|------|
| ç¿»è¯‘ | BLEU + Embedding Distance | å…¼é¡¾ç²¾ç¡®ä¸è¯­ä¹‰ |
| æ‘˜è¦ | ROUGE + LLM-as-Judge | è¦†ç›–ç‡ + è´¨é‡ |
| QA | Exact Match + Criteria | å‡†ç¡®æ€§ + å®Œæ•´æ€§ |
| å¯¹è¯ | LLM-as-Judge (å¤šç»´åº¦) | éœ€è¦ä¸»è§‚åˆ¤æ–­ |
| ä»£ç ç”Ÿæˆ | Execution + Unit Tests | åŠŸèƒ½æ­£ç¡®æ€§ |

### 23.6.3 è¯„ä¼°æˆæœ¬æ§åˆ¶

```python
# ç­–ç•¥ 1ï¼šä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹è¯„ä¼°
cheap_evaluator = load_evaluator(
    "criteria",
    criteria="correctness",
    llm=ChatOpenAI(model="gpt-3.5-turbo")  # è€Œé GPT-4
)

# ç­–ç•¥ 2ï¼šç¼“å­˜è¯„ä¼°ç»“æœ
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_evaluate(prediction: str, reference: str) -> float:
    return evaluator.evaluate_strings(
        prediction=prediction,
        reference=reference
    )["score"]

# ç­–ç•¥ 3ï¼šé‡‡æ ·è¯„ä¼°
def sample_evaluate(chain, dataset_name: str, sample_rate: float = 0.2):
    """ä»…è¯„ä¼° 20% çš„æ ·æœ¬"""
    examples = list(client.list_examples(dataset_name=dataset_name))
    sampled = random.sample(examples, int(len(examples) * sample_rate))
    
    # åªè¯„ä¼°é‡‡æ ·çš„ç¤ºä¾‹
    # ...
```

---

## æœ¬ç« æ€»ç»“

**æ ¸å¿ƒæ”¶è·**ï¼š

1. âœ… **æ•°æ®é›†æ˜¯è´¨é‡ä¿éšœçš„åŸºçŸ³**
   - ä»£è¡¨æ€§ã€å¤šæ ·æ€§ã€è¾¹ç•Œæƒ…å†µ
   - ç‰ˆæœ¬ç®¡ç†ä¸æŒç»­æ›´æ–°
   - ç”Ÿäº§æ•°æ®å¤ç”¨

2. âœ… **ç¦»çº¿è¯„ä¼°æµç¨‹æ ‡å‡†åŒ–**
   - evaluate() ä¸€ç«™å¼è¯„ä¼°
   - å¤šç»´åº¦è¯„ä¼°å™¨ç»„åˆ
   - æ‰¹é‡å¹¶è¡ŒåŠ é€Ÿ

3. âœ… **è¯„ä¼°å™¨ç”Ÿæ€ä¸°å¯Œ**
   - LLM-as-Judgeï¼šçµæ´»ä½†æˆæœ¬é«˜
   - è·ç¦»åº¦é‡ï¼šå¿«é€Ÿä¸”ä¾¿å®œ
   - è‡ªå®šä¹‰ï¼šé€‚é…ç‰¹å®šéœ€æ±‚

4. âœ… **A/B æµ‹è¯•é©±åŠ¨è¿­ä»£**
   - å¯¹æ¯”ä¸åŒç‰ˆæœ¬
   - ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ
   - æ•°æ®é©±åŠ¨å†³ç­–

5. âœ… **åœ¨çº¿åé¦ˆé—­ç¯ä¼˜åŒ–**
   - æ”¶é›†ç”¨æˆ·è¯„ä»·
   - æ„å»ºé»„é‡‘æ•°æ®é›†
   - æŒç»­æ”¹è¿›æµç¨‹

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š
Chapter 24 å°†å­¦ä¹  **LangSmith ç”Ÿäº§ç›‘æ§**ï¼ŒæŒæ¡å®æ—¶ Dashboardã€å‘Šè­¦é…ç½®ã€Playground ä½¿ç”¨ã€æˆæœ¬åˆ†æç­‰ç”Ÿäº§ç¯å¢ƒå¿…å¤‡æŠ€èƒ½ã€‚

---

## ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

1. **åˆ›å»ºæ•°æ®é›†**ï¼šä¸ºä½ çš„èŠå¤©æœºå™¨äººåˆ›å»ºä¸€ä¸ªåŒ…å« 10 æ¡æµ‹è¯•æ ·æœ¬çš„æ•°æ®é›†ã€‚

2. **åŸºç¡€è¯„ä¼°**ï¼šä½¿ç”¨ `evaluate()` å‡½æ•°è¯„ä¼°ä¸€ä¸ªç®€å•çš„ç¿»è¯‘é“¾ã€‚

3. **å¤šè¯„ä¼°å™¨**ï¼šç»„åˆä½¿ç”¨ Embedding Distance å’Œ String Distance è¯„ä¼°åŒä¸€ä¸ªä»»åŠ¡ã€‚

### è¿›é˜¶ç»ƒä¹ 

4. **è‡ªå®šä¹‰è¯„ä¼°å™¨**ï¼šå®ç°ä¸€ä¸ªæ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«ç‰¹å®šå…³é”®è¯çš„è¯„ä¼°å™¨ã€‚

5. **A/B æµ‹è¯•**ï¼šå¯¹æ¯”ä¸¤ä¸ªä¸åŒçš„æç¤ºæ¨¡æ¿ï¼Œåˆ¤æ–­å“ªä¸ªæ•ˆæœæ›´å¥½ã€‚

6. **åé¦ˆæ”¶é›†**ï¼šä¸ºä½ çš„åº”ç”¨æ·»åŠ ç”¨æˆ·åé¦ˆåŠŸèƒ½ï¼ˆThumbs Up/Downï¼‰ã€‚

### æŒ‘æˆ˜ç»ƒä¹ 

7. **ç»Ÿè®¡åˆ†æ**ï¼šå®ç°ä¸€ä¸ªå‡½æ•°ï¼Œè®¡ç®—ä¸¤ä¸ªå®éªŒä¹‹é—´çš„ç»Ÿè®¡æ˜¾è‘—æ€§ï¼ˆp-valueï¼‰ã€‚

8. **æŒç»­æ”¹è¿›æµç¨‹**ï¼šè®¾è®¡ä¸€ä¸ªè‡ªåŠ¨åŒ–è„šæœ¬ï¼Œæ¯å‘¨ä»ç”Ÿäº§æ•°æ®ä¸­æå–é«˜åˆ†æ ·æœ¬æ›´æ–°æ•°æ®é›†ã€‚

9. **æˆæœ¬ä¼˜åŒ–è¯„ä¼°**ï¼šå®ç°ä¸€ä¸ªè¯„ä¼°ç­–ç•¥ï¼Œåœ¨ä¿æŒå‡†ç¡®æ€§çš„å‰æä¸‹å°†è¯„ä¼°æˆæœ¬é™ä½ 50%ã€‚

---

## æ‰©å±•é˜…è¯»

- [LangSmith Evaluation Guide](https://docs.smith.langchain.com/evaluation)
- [LangChain Evaluators](https://python.langchain.com/docs/guides/evaluation/)
- [Building Quality Datasets for LLM Evaluation](https://blog.langchain.dev/building-quality-datasets/)
- [A/B Testing for LLM Applications](https://blog.langchain.dev/ab-testing-llm-apps/)
