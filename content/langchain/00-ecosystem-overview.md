> **æœ¬ç« ç›®æ ‡**ï¼šå…¨é¢äº†è§£ LangChain ç”Ÿæ€ç³»ç»Ÿçš„è®¾è®¡å“²å­¦ã€æ ¸å¿ƒç»„ä»¶åŠå…¶åœ¨ AI åº”ç”¨å¼€å‘ä¸­çš„å®šä½ï¼Œé€šè¿‡ç¬¬ä¸€ä¸ªèŠå¤©æœºå™¨äººåº”ç”¨å¿«é€Ÿä¸Šæ‰‹ã€‚

---

## æœ¬ç« å¯¼è§ˆ

æœ¬ç« å°†å¸¦ä½ ç³»ç»Ÿæ€§åœ°è®¤è¯† LangChain ç”Ÿæ€ä½“ç³»ï¼Œå†…å®¹åŒ…æ‹¬ï¼š

- **æ ¸å¿ƒç†å¿µ**ï¼šç†è§£"ç»„åˆä¼˜äºé…ç½®"çš„è®¾è®¡å“²å­¦åŠå…¶ä¸ä¼ ç»Ÿæ¡†æ¶çš„æœ¬è´¨åŒºåˆ«
- **ç”Ÿæ€ç»„ä»¶**ï¼šæŒæ¡ LangChainã€LangGraphã€LangSmithã€LangServe å››å¤§æ ¸å¿ƒæ¨¡å—çš„å®šä½ä¸åä½œå…³ç³»
- **æŠ€æœ¯æ¶æ„**ï¼šäº†è§£ä»ç®€å•é“¾åˆ°å¤æ‚ Agent çš„æŠ€æœ¯æ¼”è¿›è·¯å¾„
- **å¿«é€Ÿå®è·µ**ï¼šé€šè¿‡ Hello World ç¤ºä¾‹å®Œæˆç¬¬ä¸€ä¸ª LangChain åº”ç”¨çš„æ­å»º
- **ç¤¾åŒºèµ„æº**ï¼šç†Ÿæ‚‰å®˜æ–¹æ–‡æ¡£ã€Hubã€æ¨¡æ¿ç­‰å…³é”®å­¦ä¹ èµ„æºçš„ä½¿ç”¨æ–¹æ³•

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ å°†å»ºç«‹å¯¹ LangChain ç”Ÿæ€çš„æ•´ä½“è®¤çŸ¥æ¡†æ¶ï¼Œä¸ºåç»­æ·±å…¥å­¦ä¹ æ‰“ä¸‹åšå®åŸºç¡€ã€‚

---

## 0.1 ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ

LangChain æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨ç¨‹åºå¼€å‘ã€‚å®ƒäº 2022 å¹´ 10 æœˆç”± Harrison Chase åˆ›å»ºï¼Œè¿…é€Ÿæˆä¸º AI åº”ç”¨å¼€å‘é¢†åŸŸæœ€å—æ¬¢è¿çš„å·¥å…·ä¹‹ä¸€ã€‚

### 0.1.1 è®¾è®¡å“²å­¦ï¼šComposition over Configuration

LangChain çš„æ ¸å¿ƒè®¾è®¡å“²å­¦æ˜¯**ç»„åˆä¼˜äºé…ç½®**ï¼ˆComposition over Configurationï¼‰ã€‚ä¸ä¼ ç»Ÿçš„é…ç½®é©±åŠ¨æ¡†æ¶ä¸åŒï¼ŒLangChain æä¾›äº†ä¸€ç³»åˆ—å¯ç»„åˆçš„æ¨¡å—åŒ–ç»„ä»¶ï¼Œå¼€å‘è€…å¯ä»¥åƒæ­ç§¯æœ¨ä¸€æ ·å°†å®ƒä»¬ç»„åˆæˆå¤æ‚çš„åº”ç”¨ã€‚

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š

1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¯ä¸ªç»„ä»¶èŒè´£å•ä¸€ï¼Œå¯ç‹¬ç«‹æµ‹è¯•å’Œæ›¿æ¢
2. **çµæ´»ç»„åˆ**ï¼šé€šè¿‡ç®¡é“ï¼ˆpipeï¼‰æ“ä½œç¬¦å°†ç»„ä»¶ä¸²è”
3. **æ¸è¿›å¼å­¦ä¹ **ï¼šä»ç®€å•é“¾åˆ°å¤æ‚ Agent é€æ­¥é€’è¿›
4. **ç”Ÿæ€å¼€æ”¾**ï¼šæ”¯æŒ 100+ ç§é›†æˆï¼ˆæ¨¡å‹ã€å‘é‡åº“ã€å·¥å…·ç­‰ï¼‰

**è®¾è®¡åŸåˆ™**ï¼š

```python
# ä¼ ç»Ÿé…ç½®é©±åŠ¨æ–¹å¼ï¼ˆä¼ªä»£ç ï¼‰
config = {
    "model": "gpt-4",
    "temperature": 0.7,
    "prompt_template": "...",
    "output_parser": "json"
}
app = App(config)

# LangChain ç»„åˆæ–¹å¼
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

model = ChatOpenAI(model="gpt-4", temperature=0.7)
prompt = ChatPromptTemplate.from_template("Translate to French: {text}")
parser = StrOutputParser()

# é€šè¿‡ç®¡é“ç»„åˆ
chain = prompt | model | parser
```

### 0.1.2 ä¸å…¶ä»–æ¡†æ¶å¯¹æ¯”

| ç‰¹æ€§ | LangChain | LlamaIndex | Haystack | Semantic Kernel |
|------|-----------|------------|----------|-----------------|
| **ä¸»è¦å®šä½** | é€šç”¨ LLM åº”ç”¨æ¡†æ¶ | RAG/æ–‡æ¡£æ£€ç´¢ä¸“å®¶ | æœç´¢å¼•æ“ä¼˜å…ˆ | å¾®è½¯ç”Ÿæ€é›†æˆ |
| **å­¦ä¹ æ›²çº¿** | ä¸­ç­‰ | ä½ï¼ˆä¸“æ³¨ RAGï¼‰ | ä¸­ç­‰ | ä½ |
| **Agent æ”¯æŒ** | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜… | â˜…â˜…â˜… | â˜…â˜…â˜…â˜… |
| **çŠ¶æ€ç®¡ç†** | LangGraphï¼ˆå¤æ‚ï¼‰ | ç®€å• | ç®€å• | Memory Stores |
| **å¯è§‚æµ‹æ€§** | LangSmithï¼ˆå®Œå–„ï¼‰ | LlamaTrace | Haystack UI | å†…ç½®æ—¥å¿— |
| **ç”Ÿæ€é›†æˆ** | 100+ | 50+ | 40+ | Azure ä¼˜å…ˆ |
| **ç”Ÿäº§éƒ¨ç½²** | LangServe | FastAPI | REST API | Semantic Kernel Service |

**é€‰æ‹©å»ºè®®**ï¼š
- **LangChain**ï¼šéœ€è¦å¤æ‚ Agentã€çŠ¶æ€ç®¡ç†ã€å¤šæ­¥éª¤ç¼–æ’
- **LlamaIndex**ï¼šä¸“æ³¨æ–‡æ¡£æ£€ç´¢ä¸ RAG
- **Haystack**ï¼šæœç´¢å¼•æ“èƒŒæ™¯å›¢é˜Ÿï¼Œéœ€è¦ä¼ä¸šçº§æœç´¢
- **Semantic Kernel**ï¼šæ·±åº¦é›†æˆ Azure/Microsoft ç”Ÿæ€

### 0.1.3 æ ¸å¿ƒä»·å€¼ä¸»å¼ 

**ä¸ºä»€ä¹ˆé€‰æ‹© LangChainï¼Ÿ**

1. **å®Œæ•´çš„æŠ½è±¡å±‚çº§**
   - ä½çº§ï¼šRunnable åè®®ã€æ¶ˆæ¯æ ¼å¼
   - ä¸­çº§ï¼šé“¾ã€æ£€ç´¢å™¨ã€å·¥å…·
   - é«˜çº§ï¼šAgentã€å¤š Agent ç³»ç»Ÿ

2. **ç”Ÿäº§å°±ç»ª**
   - LangSmithï¼šè¿½è¸ªã€è°ƒè¯•ã€è¯„ä¼°
   - LangServeï¼šä¸€é”®éƒ¨ç½² REST API
   - ä¼ä¸šçº§é”™è¯¯å¤„ç†ï¼šé‡è¯•ã€é™çº§ã€è¶…æ—¶

3. **æ´»è·ƒçš„ç¤¾åŒºä¸ç”Ÿæ€**
   - GitHub 80k+ stars
   - æ¯æœˆ 1000+ æ¬¡è´¡çŒ®
   - å®˜æ–¹æ¨¡æ¿åº“ï¼šlangchain-ai/langchain/templates

---

## 0.2 ç”Ÿæ€ç»„ä»¶å…¨æ™¯å›¾

<div data-component="LangChainEcosystemMap"></div>

### 0.2.0 æ¶æ„åˆ†å±‚è§†è§’

<div data-component="LangChainArchitectureFlow"></div>

LangChain ç”Ÿæ€ç”±ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶æ„æˆï¼š

### 0.2.1 LangChain Coreï¼šåŸºç¡€æŠ½è±¡ä¸ LCEL

**langchain-core** æ˜¯æ‰€æœ‰ç»„ä»¶çš„åŸºçŸ³ï¼Œå®šä¹‰äº†ç»Ÿä¸€çš„æ¥å£å’ŒæŠ½è±¡ã€‚

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š

```python
from langchain_core.runnables import Runnable

class Runnable(ABC):
    """æ‰€æœ‰å¯æ‰§è¡Œç»„ä»¶çš„åŸºç±»"""
    def invoke(self, input):       # åŒæ­¥è°ƒç”¨
        pass
    def ainvoke(self, input):      # å¼‚æ­¥è°ƒç”¨
        pass
    def stream(self, input):       # æµå¼è¾“å‡º
        pass
    def batch(self, inputs):       # æ‰¹é‡å¤„ç†
        pass
```

**LCELï¼ˆLangChain Expression Languageï¼‰**ï¼š

```python
# LCEL ä½¿ç”¨ç®¡é“æ“ä½œç¬¦ç»„åˆç»„ä»¶
chain = prompt | model | parser

# ç­‰ä»·äºå‡½æ•°ç»„åˆ
def chain(input):
    return parser(model(prompt(input)))
```

**æ•°å­¦è¡¨ç¤º**ï¼š

$$
\text{Chain} = f_3 \circ f_2 \circ f_1
$$

å…¶ä¸­ $f_1$ æ˜¯ promptï¼Œ$f_2$ æ˜¯ modelï¼Œ$f_3$ æ˜¯ parserã€‚

### 0.2.2 LangChain Communityï¼šç¬¬ä¸‰æ–¹é›†æˆ

**langchain-community** æä¾›äº†ä¸å¤–éƒ¨æœåŠ¡çš„é›†æˆã€‚

**ä¸»è¦é›†æˆåˆ†ç±»**ï¼š

1. **æ¨¡å‹æä¾›å•†**ï¼šOpenAIã€Anthropicã€Cohereã€HuggingFaceã€æœ¬åœ°æ¨¡å‹ï¼ˆOllamaã€LM Studioï¼‰
2. **å‘é‡æ•°æ®åº“**ï¼šPineconeã€Weaviateã€Chromaã€FAISSã€Qdrantã€Milvus
3. **æ–‡æ¡£åŠ è½½å™¨**ï¼šPDFã€ç½‘é¡µã€æ•°æ®åº“ã€APIã€æ–‡ä»¶ç³»ç»Ÿ
4. **å·¥å…·**ï¼šæœç´¢ï¼ˆGoogleã€Bingï¼‰ã€è®¡ç®—å™¨ã€æ•°æ®åº“æŸ¥è¯¢ã€Shell å‘½ä»¤

**å®‰è£…ç­–ç•¥**ï¼š

```bash
# æœ€å°åŒ–å®‰è£…
pip install langchain-core

# æ ¸å¿ƒåŠŸèƒ½
pip install langchain

# ç‰¹å®šé›†æˆ
pip install langchain-openai      # OpenAI é›†æˆ
pip install langchain-anthropic   # Anthropic é›†æˆ
pip install langchain-community   # ç¤¾åŒºé›†æˆåŒ…
```

### 0.2.3 LangGraphï¼šçŠ¶æ€å›¾ä¸å¤æ‚æ§åˆ¶æµ

**LangGraph** ç”¨äºæ„å»ºå…·æœ‰å¾ªç¯ã€æ¡ä»¶åˆ†æ”¯å’ŒæŒä¹…åŒ–çŠ¶æ€çš„å¤æ‚åº”ç”¨ã€‚

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š

```python
from langgraph.graph import StateGraph

# å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    messages: list[BaseMessage]
    next_action: str

# æ„å»ºçŠ¶æ€å›¾
graph = StateGraph(AgentState)
graph.add_node("agent", agent_node)
graph.add_node("tools", tool_node)
graph.add_conditional_edges("agent", should_continue)
graph.add_edge("tools", "agent")

app = graph.compile()
```

**çŠ¶æ€æœºè¡¨ç¤º**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Start  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  éœ€è¦å·¥å…·  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agent  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Tools  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚                      â”‚
     â”‚ å®Œæˆ                  â”‚
     â–¼                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   End   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 0.2.4 LangSmithï¼šè¿½è¸ªã€è¯„ä¼°ã€ç›‘æ§

**LangSmith** æ˜¯ LangChain çš„å¯è§‚æµ‹æ€§å¹³å°ï¼Œæä¾›ç”Ÿäº§çº§ç›‘æ§èƒ½åŠ›ã€‚

**æ ¸å¿ƒåŠŸèƒ½**ï¼š

1. **è¿½è¸ªï¼ˆTracingï¼‰**ï¼šè®°å½•æ¯æ¬¡è°ƒç”¨çš„è¾“å…¥ã€è¾“å‡ºã€å»¶è¿Ÿã€token æ¶ˆè€—
2. **æ•°æ®é›†**ï¼šç®¡ç†è¯„ä¼°æ•°æ®é›†
3. **è¯„ä¼°**ï¼šè‡ªåŠ¨åŒ–æµ‹è¯•ä¸æŒ‡æ ‡è®¡ç®—
4. **ç›‘æ§**ï¼šå®æ—¶æ€§èƒ½ç›‘æ§ä¸å‘Šè­¦
5. **åé¦ˆ**ï¼šæ”¶é›†ç”¨æˆ·åé¦ˆå¹¶å…³è”è¿½è¸ª

**é…ç½®ç¤ºä¾‹**ï¼š

```python
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"
os.environ["LANGCHAIN_PROJECT"] = "my-project"

# æ­¤åæ‰€æœ‰è°ƒç”¨è‡ªåŠ¨è¿½è¸ª
result = chain.invoke({"text": "Hello"})
# åœ¨ LangSmith ä»ªè¡¨æ¿æŸ¥çœ‹è¿½è¸ªï¼šhttps://smith.langchain.com
```

### 0.2.5 LangServeï¼šé“¾/å›¾çš„ REST API éƒ¨ç½²

**LangServe** å°† LangChain åº”ç”¨ä¸€é”®éƒ¨ç½²ä¸º REST APIã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š

```python
from fastapi import FastAPI
from langserve import add_routes

app = FastAPI()

# æ·»åŠ é“¾çš„è·¯ç”±
add_routes(
    app,
    chain,
    path="/translate",
    enable_feedback_endpoint=True,
    enable_public_trace_link_endpoint=True,
)

# è‡ªåŠ¨ç”Ÿæˆï¼š
# - POST /translate/invoke - åŒæ­¥è°ƒç”¨
# - POST /translate/batch - æ‰¹é‡å¤„ç†
# - POST /translate/stream - æµå¼è¾“å‡º
# - GET /translate/playground - äº¤äº’å¼æµ‹è¯•ç•Œé¢
```

**éƒ¨ç½²æ¶æ„**ï¼š

```
Client
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI + ASGI â”‚
â”‚   (Uvicorn)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LangServe     â”‚
â”‚   Middleware    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LangChain App  â”‚
â”‚  (Chain/Graph)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 0.2.6 LangChain Hubï¼šæç¤ºæ¨¡æ¿ä»“åº“

**LangChain Hub** æ˜¯ç¤¾åŒºé©±åŠ¨çš„æç¤ºæ¨¡æ¿ä»“åº“ã€‚

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
from langchain import hub

# æ‹‰å–å…¬å¼€æç¤º
prompt = hub.pull("rlm/rag-prompt")

# æ¨é€è‡ªå·±çš„æç¤º
hub.push("my-org/my-prompt", prompt)

# ç‰ˆæœ¬ç®¡ç†
prompt_v2 = hub.pull("my-org/my-prompt:v2")
```

**æµè§ˆå™¨è®¿é—®**ï¼šhttps://smith.langchain.com/hub

---

## 0.3 ç¯å¢ƒå‡†å¤‡ä¸å®‰è£…

### 0.3.1 å®‰è£…ç­–ç•¥

**æ¨èå®‰è£…æ–¹å¼**ï¼ˆæŒ‰éœ€å®‰è£…ï¼‰ï¼š

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv langchain-env
source langchain-env/bin/activate  # Linux/Mac
# langchain-env\Scripts\activate   # Windows

# 2. å®‰è£…æ ¸å¿ƒåŒ…
pip install langchain langchain-openai

# 3. å¯é€‰ï¼šLangGraphï¼ˆçŠ¶æ€å›¾ï¼‰
pip install langgraph

# 4. å¯é€‰ï¼šå…¶ä»–é›†æˆ
pip install langchain-anthropic     # Anthropic Claude
pip install langchain-community     # ç¤¾åŒºé›†æˆ
pip install langchain-chroma        # Chroma å‘é‡åº“
pip install langchain-experimental  # å®éªŒæ€§åŠŸèƒ½
```

**å®Œæ•´å®‰è£…**ï¼ˆä¸æ¨èï¼ŒåŒ…ä½“ç§¯å¤§ï¼‰ï¼š

```bash
pip install langchain[all]
```

### 0.3.2 æä¾›å•†é›†æˆ

**OpenAI é…ç½®**ï¼š

```python
import os
from langchain_openai import ChatOpenAI

os.environ["OPENAI_API_KEY"] = "sk-..."

model = ChatOpenAI(
    model="gpt-4o",           # æ¨¡å‹åç§°
    temperature=0.7,          # æ¸©åº¦å‚æ•°
    max_tokens=1000,          # æœ€å¤§ token æ•°
    timeout=30,               # è¶…æ—¶æ—¶é—´
    max_retries=2,            # é‡è¯•æ¬¡æ•°
)
```

**Anthropic é…ç½®**ï¼š

```python
from langchain_anthropic import ChatAnthropic

os.environ["ANTHROPIC_API_KEY"] = "sk-ant-..."

model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0.7,
)
```

**æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰**ï¼š

```python
from langchain_community.llms import Ollama

# éœ€è¦å…ˆå¯åŠ¨ Ollama æœåŠ¡
model = Ollama(model="llama3.2")
```

### 0.3.3 ç¯å¢ƒå˜é‡é…ç½®

**åˆ›å»º `.env` æ–‡ä»¶**ï¼š

```bash
# API Keys
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...

# LangSmith è¿½è¸ª
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=ls__...
LANGCHAIN_PROJECT=my-first-project

# å¯é€‰é…ç½®
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
```

**åŠ è½½ç¯å¢ƒå˜é‡**ï¼š

```python
from dotenv import load_dotenv
load_dotenv()  # è‡ªåŠ¨åŠ è½½ .env æ–‡ä»¶

# æˆ–æ‰‹åŠ¨è®¾ç½®
import os
os.environ["OPENAI_API_KEY"] = "sk-..."
```

### 0.3.4 éªŒè¯å®‰è£…ï¼šHello World ç¤ºä¾‹

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# åˆ›å»ºæ¨¡å‹
model = ChatOpenAI(model="gpt-4o-mini")

# å‘é€æ¶ˆæ¯
response = model.invoke([
    HumanMessage(content="Say 'Hello, LangChain!' in French.")
])

print(response.content)
# é¢„æœŸè¾“å‡º: Bonjour, LangChain!
```

**éªŒè¯æ£€æŸ¥æ¸…å•**ï¼š
- âœ… æ¨¡å‹æ­£å¸¸å“åº”
- âœ… æ—  API Key é”™è¯¯
- âœ… è¾“å‡ºç¬¦åˆé¢„æœŸ
- âœ… LangSmith ä»ªè¡¨æ¿æ˜¾ç¤ºè¿½è¸ªï¼ˆå¦‚å¯ç”¨ï¼‰

---

## 0.4 ç¬¬ä¸€ä¸ªåº”ç”¨ï¼šèŠå¤©æœºå™¨äºº

### 0.4.1 é›¶ä»£ç ä½“éªŒï¼šChatOpenAI + PromptTemplate

**éœ€æ±‚**ï¼šæ„å»ºä¸€ä¸ªæ”¯æŒè§’è‰²æ‰®æ¼”çš„èŠå¤©æœºå™¨äººã€‚

**å®Œæ•´ä»£ç **ï¼š

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. å®šä¹‰æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant that speaks like a {persona}."),
    ("human", "{input}")
])

# 2. åˆ›å»ºæ¨¡å‹
model = ChatOpenAI(model="gpt-4o-mini", temperature=0.8)

# 3. åˆ›å»ºè¾“å‡ºè§£æå™¨
parser = StrOutputParser()

# 4. ç»„åˆæˆé“¾
chain = prompt | model | parser

# 5. è°ƒç”¨
response = chain.invoke({
    "persona": "pirate",
    "input": "Tell me about LangChain."
})

print(response)
```

**é¢„æœŸè¾“å‡º**ï¼š

```
Ahoy, matey! LangChain be a fine treasure o' a framework fer buildin' 
applications with them fancy large language models, arr! It helps ye 
chain together different components like a sturdy ship's rigging...
```

**ä»£ç è§£æ**ï¼š

1. **ChatPromptTemplate.from_messages**ï¼šå®šä¹‰å¯¹è¯æ¨¡æ¿ï¼Œæ”¯æŒ systemã€humanã€ai ä¸‰ç§è§’è‰²
2. **å˜é‡æ³¨å…¥**ï¼š`{persona}` å’Œ `{input}` åœ¨è¿è¡Œæ—¶æ›¿æ¢
3. **ç®¡é“ç»„åˆ**ï¼š`|` æ“ä½œç¬¦å°†ä¸‰ä¸ªç»„ä»¶ä¸²è”
4. **ç±»å‹å®‰å…¨**ï¼šè¾“å…¥æ˜¯å­—å…¸ï¼Œè¾“å‡ºæ˜¯å­—ç¬¦ä¸²

### 0.4.2 æµå¼è¾“å‡º

**éœ€æ±‚**ï¼šå®ç°é€å­—æ‰“å°æ•ˆæœï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

```python
# ä½¿ç”¨ stream() æ–¹æ³•
for chunk in chain.stream({
    "persona": "poet",
    "input": "Describe a sunset."
}):
    print(chunk, end="", flush=True)

# è¾“å‡ºç±»ä¼¼æ‰“å­—æœºæ•ˆæœ
```

**å¼‚æ­¥æµå¼**ï¼ˆæ¨èç”¨äº Web åº”ç”¨ï¼‰ï¼š

```python
import asyncio

async def stream_response():
    async for chunk in chain.astream({
        "persona": "scientist",
        "input": "Explain quantum computing."
    }):
        print(chunk, end="", flush=True)
        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿæ‰“å­—å»¶è¿Ÿ

asyncio.run(stream_response())
```

### 0.4.3 å¯¹è¯å†å²ç®¡ç†

**é—®é¢˜**ï¼šå¦‚ä½•è®©æœºå™¨äººè®°ä½ä¹‹å‰çš„å¯¹è¯ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨æ‰‹åŠ¨ç®¡ç†ã€‚

```python
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# åˆå§‹åŒ–å¯¹è¯å†å²
messages = [
    SystemMessage(content="You are a helpful coding assistant.")
]

# å¯¹è¯å¾ªç¯
while True:
    user_input = input("You: ")
    if user_input.lower() == "exit":
        break
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    messages.append(HumanMessage(content=user_input))
    
    # è°ƒç”¨æ¨¡å‹
    response = model.invoke(messages)
    
    # æ·»åŠ  AI å“åº”
    messages.append(response)
    
    print(f"Assistant: {response.content}")
```

**å¯¹è¯ç¤ºä¾‹**ï¼š

```
You: My name is Alice.
Assistant: Nice to meet you, Alice! How can I help you today?

You: What's my name?
Assistant: Your name is Alice.
```

**å†…å­˜ç®¡ç†**ï¼ˆè‡ªåŠ¨åŒ–æ–¹å¼å°†åœ¨ Chapter 9 è¯¦è§£ï¼‰ï¼š

```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(return_messages=True)
memory.save_context({"input": "Hi"}, {"output": "Hello!"})
```

### 0.4.4 éƒ¨ç½²åˆ° Streamlit

**éœ€æ±‚**ï¼šåˆ›å»ºä¸€ä¸ª Web ç•Œé¢ã€‚

**å®‰è£…ä¾èµ–**ï¼š

```bash
pip install streamlit
```

**å®Œæ•´åº”ç”¨**ï¼ˆ`app.py`ï¼‰ï¼š

```python
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

st.title("ğŸ¦œ LangChain Chatbot")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    persona = st.selectbox(
        "Select Persona",
        ["helpful assistant", "pirate", "poet", "scientist"]
    )
    temperature = st.slider("Temperature", 0.0, 1.0, 0.7)

# åˆå§‹åŒ–é“¾
@st.cache_resource
def create_chain(temp):
    prompt = ChatPromptTemplate.from_messages([
        ("system", f"You are a {persona}."),
        ("human", "{input}")
    ])
    model = ChatOpenAI(model="gpt-4o-mini", temperature=temp)
    parser = StrOutputParser()
    return prompt | model | parser

chain = create_chain(temperature)

# ç”¨æˆ·è¾“å…¥
user_input = st.text_input("You:", key="input")

if user_input:
    with st.spinner("Thinking..."):
        response = chain.invoke({"input": user_input})
        st.write(f"**Assistant:** {response}")
```

**è¿è¡Œ**ï¼š

```bash
streamlit run app.py
```

**å¢å¼ºç‰ˆ**ï¼ˆæµå¼è¾“å‡ºï¼‰ï¼š

```python
if user_input:
    with st.chat_message("user"):
        st.write(user_input)
    
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
        
        for chunk in chain.stream({"input": user_input}):
            full_response += chunk
            response_placeholder.markdown(full_response + "â–Œ")
        
        response_placeholder.markdown(full_response)
```

---

## ğŸ¯ æœ¬ç« å°ç»“

**æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **LangChain ç”Ÿæ€**ï¼šCoreã€Communityã€LangGraphã€LangSmithã€LangServeã€Hub
2. **è®¾è®¡å“²å­¦**ï¼šç»„åˆä¼˜äºé…ç½®ï¼Œæ¨¡å—åŒ–ç»„ä»¶ï¼ŒLCEL ç®¡é“
3. **ç¯å¢ƒå‡†å¤‡**ï¼šæŒ‰éœ€å®‰è£…ã€API Key é…ç½®ã€ç¯å¢ƒå˜é‡
4. **ç¬¬ä¸€ä¸ªåº”ç”¨**ï¼šæç¤ºæ¨¡æ¿ã€æ¨¡å‹ã€è§£æå™¨çš„ç»„åˆ

**æŒæ¡æ£€æŸ¥**ï¼š

- [ ] èƒ½è¯´å‡º LangChain ä¸ LlamaIndex çš„æ ¸å¿ƒå·®å¼‚
- [ ] ç†è§£ Runnable åè®®çš„å››ä¸ªæ ¸å¿ƒæ–¹æ³•
- [ ] èƒ½ç”¨ LCEL æ„å»ºç®€å•çš„ç¿»è¯‘é“¾
- [ ] èƒ½é…ç½® LangSmith è¿½è¸ª
- [ ] èƒ½éƒ¨ç½²ä¸€ä¸ª Streamlit èŠå¤©åº”ç”¨

**ç»ƒä¹ é¢˜**ï¼š

1. **ä¿®æ”¹ Persona**ï¼šå°†èŠå¤©æœºå™¨äººæ”¹ä¸º"èå£«æ¯”äºšé£æ ¼"ï¼Œæµ‹è¯•è¾“å‡ºæ•ˆæœ
2. **æ¸©åº¦å®éªŒ**ï¼šå¯¹æ¯” temperature=0 å’Œ temperature=1 çš„è¾“å‡ºå·®å¼‚
3. **é”™è¯¯å¤„ç†**ï¼šæ•…æ„è¾“å…¥é”™è¯¯çš„ API Keyï¼Œè§‚å¯Ÿé”™è¯¯ä¿¡æ¯
4. **æ€§èƒ½æµ‹è¯•**ï¼šä½¿ç”¨ `chain.batch()` æ‰¹é‡å¤„ç† 10 æ¡æ¶ˆæ¯ï¼Œå¯¹æ¯”å•æ¬¡è°ƒç”¨çš„è€—æ—¶

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š

Chapter 1 å°†æ·±å…¥ Runnable åè®®ã€Language Modelsã€Prompt Templates ç­‰æ ¸å¿ƒæŠ½è±¡ï¼ŒæŒæ¡ LangChain çš„åº•å±‚æœºåˆ¶ã€‚

---

## ğŸ“š æ‰©å±•é˜…è¯»

- [LangChain å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/get_started/introduction)
- [LCEL æ¦‚å¿µæŒ‡å—](https://python.langchain.com/docs/concepts/lcel)
- [LangSmith å¿«é€Ÿå¼€å§‹](https://docs.smith.langchain.com/)
- [LangChain Templates](https://github.com/langchain-ai/langchain/tree/master/templates)
- [LangChain Hub](https://smith.langchain.com/hub)
