> **æœ¬ç« ç›®æ ‡**ï¼šæ·±å…¥ç†è§£ LangChain çš„åº•å±‚æŠ½è±¡æœºåˆ¶ï¼ŒæŒæ¡ Runnable åè®®ã€Language Modelsã€Prompt Templatesã€Output Parsers ç­‰æ ¸å¿ƒç»„ä»¶çš„ä½¿ç”¨æ–¹æ³•ã€‚

---

## æœ¬ç« å¯¼è§ˆ

æœ¬ç« æ·±å…¥å‰–æ LangChain çš„æ ¸å¿ƒæŠ½è±¡å±‚ï¼Œè¿™äº›æ¦‚å¿µæ˜¯ç†è§£æ•´ä¸ªæ¡†æ¶çš„å…³é”®ï¼š

- **Runnable åè®®**ï¼šç»Ÿä¸€çš„æ¥å£æ ‡å‡†ï¼Œæ”¯æŒåŒæ­¥/å¼‚æ­¥è°ƒç”¨ã€æµå¼å¤„ç†ã€æ‰¹å¤„ç†ç­‰å¤šç§æ‰§è¡Œæ¨¡å¼
- **è¯­è¨€æ¨¡å‹é›†æˆ**ï¼šæŒæ¡ Chat Models ä¸ LLMs çš„åŒºåˆ«ï¼Œå­¦ä¹ æ¨¡å‹åˆ‡æ¢ä¸é…ç½®æœ€ä½³å®è·µ
- **æç¤ºå·¥ç¨‹**ï¼šä»ç®€å•æ¨¡æ¿åˆ° Few-Shot å­¦ä¹ ï¼Œç³»ç»ŸåŒ–ç®¡ç†æç¤ºè¯èµ„äº§
- **è¾“å‡ºè§£æ**ï¼šç»“æ„åŒ–æå– LLM å“åº”ï¼Œå®ç°ç±»å‹å®‰å…¨çš„æ•°æ®å¤„ç†
- **æ¶ˆæ¯æŠ½è±¡**ï¼šç†è§£ SystemMessageã€HumanMessageã€AIMessage çš„è®¾è®¡ä¸åº”ç”¨åœºæ™¯

è¿™äº›åŸºç¡€ç»„ä»¶æ˜¯æ„å»ºæ‰€æœ‰ LangChain åº”ç”¨çš„åŸºçŸ³ï¼ŒåŠ¡å¿…æ‰å®æŒæ¡ã€‚

---

## 1.1 Runnable åè®®

Runnable æ˜¯ LangChain ä¸­æ‰€æœ‰å¯æ‰§è¡Œç»„ä»¶çš„ç»Ÿä¸€æ¥å£ï¼Œå®ƒå®šä¹‰äº†ä¸€å¥—æ ‡å‡†åŒ–çš„è°ƒç”¨æ–¹æ³•ï¼Œä½¿å¾—ä¸åŒç»„ä»¶å¯ä»¥æ— ç¼ç»„åˆã€‚

### 1.1.1 ç»Ÿä¸€æ¥å£ï¼šinvoke()ã€stream()ã€batch()ã€astream()

<div data-component="RunnableProtocolVisualizer"></div>

**Runnable åè®®çš„æ ¸å¿ƒæ–¹æ³•**ï¼š

```python
from langchain_core.runnables import Runnable
from typing import Any, Iterator, AsyncIterator

class Runnable(ABC):
    """æ‰€æœ‰å¯æ‰§è¡Œç»„ä»¶çš„åŸºç±»"""
    
    def invoke(self, input: Any, config: RunnableConfig = None) -> Any:
        """åŒæ­¥è°ƒç”¨ï¼Œé˜»å¡ç›´åˆ°ç»“æœè¿”å›"""
        pass
    
    async def ainvoke(self, input: Any, config: RunnableConfig = None) -> Any:
        """å¼‚æ­¥è°ƒç”¨ï¼Œéé˜»å¡"""
        pass
    
    def stream(self, input: Any, config: RunnableConfig = None) -> Iterator[Any]:
        """åŒæ­¥æµå¼è¾“å‡ºï¼Œé€å—è¿”å›"""
        pass
    
    async def astream(self, input: Any, config: RunnableConfig = None) -> AsyncIterator[Any]:
        """å¼‚æ­¥æµå¼è¾“å‡º"""
        pass
    
    def batch(self, inputs: list[Any], config: RunnableConfig = None) -> list[Any]:
        """æ‰¹é‡å¤„ç†"""
        pass
    
    async def abatch(self, inputs: list[Any], config: RunnableConfig = None) -> list[Any]:
        """å¼‚æ­¥æ‰¹é‡å¤„ç†"""
        pass
```

**å®é™…ç¤ºä¾‹**ï¼š

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

model = ChatOpenAI(model="gpt-4o-mini")
message = HumanMessage(content="Count from 1 to 5.")

# 1. åŒæ­¥è°ƒç”¨
response = model.invoke([message])
print(response.content)
# è¾“å‡º: 1, 2, 3, 4, 5

# 2. æµå¼è¾“å‡º
for chunk in model.stream([message]):
    print(chunk.content, end="", flush=True)
# è¾“å‡º: 1, 2, 3, 4, 5 (é€å­—æ˜¾ç¤º)

# 3. æ‰¹é‡å¤„ç†
messages_batch = [
    [HumanMessage(content="Say 'Hi'")],
    [HumanMessage(content="Say 'Hello'")],
    [HumanMessage(content="Say 'Hey'")]
]
responses = model.batch(messages_batch)
for resp in responses:
    print(resp.content)
# è¾“å‡º: Hi / Hello / Hey

# 4. å¼‚æ­¥è°ƒç”¨
import asyncio

async def async_example():
    response = await model.ainvoke([message])
    print(response.content)

asyncio.run(async_example())
```

**æ–¹æ³•é€‰æ‹©æŒ‡å—**ï¼š

| æ–¹æ³• | ä½¿ç”¨åœºæ™¯ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|------|----------|------|------|
| `invoke()` | ç®€å•è„šæœ¬ã€å•æ¬¡è°ƒç”¨ | ä»£ç ç®€æ´ | é˜»å¡ä¸»çº¿ç¨‹ |
| `ainvoke()` | Web åç«¯ã€å¹¶å‘åœºæ™¯ | é«˜æ•ˆå¹¶å‘ | éœ€è¦å¼‚æ­¥ä¸Šä¸‹æ–‡ |
| `stream()` | èŠå¤©ç•Œé¢ã€å®æ—¶åé¦ˆ | ç”¨æˆ·ä½“éªŒå¥½ | å¤„ç†å¤æ‚ |
| `astream()` | å¼‚æ­¥æµå¼åœºæ™¯ | é«˜æ€§èƒ½æµå¼ | æœ€å¤æ‚ |
| `batch()` | æ•°æ®å¤„ç†ã€æ‰¹é‡ä»»åŠ¡ | èŠ‚çœè¯·æ±‚æ¬¡æ•° | å†…å­˜å ç”¨å¤§ |

### 1.1.2 Runnable å®ç°ç±»

**å¸¸ç”¨ Runnable å®ç°**ï¼š

```python
from langchain_core.runnables import (
    RunnableLambda,      # åŒ…è£…ä»»æ„å‡½æ•°
    RunnablePassthrough, # é€ä¼ è¾“å…¥
    RunnableParallel,    # å¹¶è¡Œæ‰§è¡Œ
    RunnableBranch,      # æ¡ä»¶åˆ†æ”¯
)

# 1. RunnableLambdaï¼šåŒ…è£…æ™®é€šå‡½æ•°
def add_prefix(text: str) -> str:
    return f"Translated: {text}"

prefix_runnable = RunnableLambda(add_prefix)
result = prefix_runnable.invoke("Bonjour")
print(result)  # "Translated: Bonjour"

# 2. RunnablePassthroughï¼šé€ä¼ è¾“å…¥ï¼ˆå¸¸ç”¨äºè°ƒè¯•ï¼‰
from langchain_core.runnables import RunnablePassthrough

passthrough = RunnablePassthrough()
print(passthrough.invoke({"key": "value"}))  # {"key": "value"}

# 3. RunnableParallelï¼šå¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡
from langchain_core.prompts import ChatPromptTemplate

chain1 = ChatPromptTemplate.from_template("Translate to French: {text}") | model
chain2 = ChatPromptTemplate.from_template("Translate to Spanish: {text}") | model

parallel = RunnableParallel(
    french=chain1,
    spanish=chain2
)

result = parallel.invoke({"text": "Hello"})
# {'french': AIMessage(content='Bonjour'), 
#  'spanish': AIMessage(content='Hola')}
```

**ç»„åˆæ¨¡å¼**ï¼š

```python
# ç®¡é“ç»„åˆï¼ˆé¡ºåºæ‰§è¡Œï¼‰
chain = prompt | model | parser

# ç­‰ä»·äº
from langchain_core.runnables import RunnableSequence
chain = RunnableSequence(prompt, model, parser)

# æ•°å­¦è¡¨ç¤º
# f(x) = parser(model(prompt(x)))
```

### 1.1.3 ä¸ Python ç”Ÿæ€çš„äº’æ“ä½œæ€§

**Runnable å¯ä»¥ç›´æ¥ä¸ Python å‡½æ•°äº’æ“ä½œ**ï¼š

```python
# Python å‡½æ•°è‡ªåŠ¨è½¬ä¸º Runnable
def uppercase(text: str) -> str:
    return text.upper()

# ç›´æ¥ç”¨äºé“¾ä¸­
chain = prompt | model | uppercase | parser

# æˆ–æ˜¾å¼åŒ…è£…
chain = prompt | model | RunnableLambda(uppercase) | parser
```

**ç±»å‹æ ‡æ³¨ä¸ IDE æ”¯æŒ**ï¼š

```python
from langchain_core.runnables import Runnable

def create_chain() -> Runnable[dict, str]:
    """è¿”å›ç±»å‹æ ‡æ³¨ï¼šè¾“å…¥ dictï¼Œè¾“å‡º str"""
    return prompt | model | parser

# IDE ä¼šè‡ªåŠ¨æ¨æ–­ç±»å‹
chain = create_chain()
result: str = chain.invoke({"text": "test"})  # ç±»å‹æ£€æŸ¥é€šè¿‡
```

---

## 1.2 Prompt Templates

<div data-component="PromptTemplateBuilder"></div>

### 1.2.1 åŸºç¡€æ¨¡æ¿ç”¨æ³•

PromptTemplate æ˜¯ LangChain ä¸­ç”¨äºç®¡ç†å’Œå¤ç”¨æç¤ºçš„æ ¸å¿ƒç»„ä»¶ã€‚

**ä¸¤ç§æ¨¡å‹æ¥å£**ï¼š

| ç‰¹æ€§ | LLM | ChatModel |
|------|-----|-----------|
| **è¾“å…¥æ ¼å¼** | å­—ç¬¦ä¸² | æ¶ˆæ¯åˆ—è¡¨ |
| **è¾“å‡ºæ ¼å¼** | å­—ç¬¦ä¸² | AIMessage |
| **é€‚ç”¨æ¨¡å‹** | æ—§å¼æ¨¡å‹ï¼ˆGPT-3ï¼‰ | ç°ä»£å¯¹è¯æ¨¡å‹ï¼ˆGPT-4ã€Claudeï¼‰ |
| **æ¨èä½¿ç”¨** | âŒ å·²è¿‡æ—¶ | âœ… ä¼˜å…ˆä½¿ç”¨ |

**LLM ç¤ºä¾‹**ï¼ˆä¸æ¨èï¼‰ï¼š

```python
from langchain_openai import OpenAI

llm = OpenAI(model="gpt-3.5-turbo-instruct")
result = llm.invoke("Translate 'Hello' to French:")
print(result)  # "Bonjour"
```

**ChatModel ç¤ºä¾‹**ï¼ˆæ¨èï¼‰ï¼š

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

chat = ChatOpenAI(model="gpt-4o")

messages = [
    SystemMessage(content="You are a translator."),
    HumanMessage(content="Translate 'Hello' to French.")
]

result = chat.invoke(messages)
print(result.content)  # "Bonjour"
print(type(result))    # <class 'langchain_core.messages.ai.AIMessage'>
```

**ä¸ºä»€ä¹ˆä¼˜å…ˆä½¿ç”¨ ChatModelï¼Ÿ**

1. **æ›´å¥½çš„ä¸Šä¸‹æ–‡ç®¡ç†**ï¼šç³»ç»Ÿæ¶ˆæ¯ã€ç”¨æˆ·æ¶ˆæ¯ã€åŠ©æ‰‹æ¶ˆæ¯åˆ†ç¦»
2. **æ”¯æŒå¤šè½®å¯¹è¯**ï¼šæ¶ˆæ¯åˆ—è¡¨å¤©ç„¶æ”¯æŒå¯¹è¯å†å²
3. **æ¨¡å‹å¯¹é½**ï¼šç°ä»£ LLM éƒ½æ˜¯å¯¹è¯æ¨¡å‹è®­ç»ƒ
4. **åŠŸèƒ½å®Œæ•´**ï¼šæ”¯æŒ Function Callingã€JSON Mode ç­‰

### 1.2.2 æ¨¡å‹æä¾›å•†åˆ‡æ¢

**OpenAI**ï¼š

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-4o",
    temperature=0.7,
    max_tokens=1000,
    timeout=30,
    max_retries=2,
    api_key="sk-...",  # æˆ–ä»ç¯å¢ƒå˜é‡è¯»å–
    base_url="https://api.openai.com/v1"  # æ”¯æŒä»£ç†
)
```

**Anthropic Claude**ï¼š

```python
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0.7,
    max_tokens=4096,
    api_key="sk-ant-..."
)
```

**æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰**ï¼š

```python
from langchain_community.chat_models import ChatOllama

model = ChatOllama(
    model="llama3.2",
    temperature=0.7,
    base_url="http://localhost:11434"
)

# éœ€è¦å…ˆå¯åŠ¨ Ollamaï¼šollama serve
```

**ç»Ÿä¸€æ¥å£**ï¼ˆæ¨èï¼‰ï¼š

```python
def get_model(provider: str = "openai"):
    """å·¥å‚æ¨¡å¼åˆ›å»ºæ¨¡å‹"""
    if provider == "openai":
        return ChatOpenAI(model="gpt-4o")
    elif provider == "anthropic":
        return ChatAnthropic(model="claude-3-5-sonnet-20241022")
    elif provider == "ollama":
        return ChatOllama(model="llama3.2")
    else:
        raise ValueError(f"Unknown provider: {provider}")

# ä½¿ç”¨
model = get_model("anthropic")
chain = prompt | model | parser
```

### 1.2.3 æ¨¡å‹å‚æ•°è¯¦è§£

**æ ¸å¿ƒå‚æ•°**ï¼š

```python
model = ChatOpenAI(
    # å¿…é€‰å‚æ•°
    model="gpt-4o",                    # æ¨¡å‹åç§°
    
    # ç”Ÿæˆå‚æ•°
    temperature=0.7,                   # æ¸©åº¦ï¼š0-2ï¼Œè¶Šé«˜è¶Šéšæœº
    top_p=1.0,                         # æ ¸é‡‡æ ·ï¼š0-1
    frequency_penalty=0.0,             # é¢‘ç‡æƒ©ç½šï¼š-2 to 2
    presence_penalty=0.0,              # å­˜åœ¨æƒ©ç½šï¼š-2 to 2
    max_tokens=1000,                   # æœ€å¤§ç”Ÿæˆ token æ•°
    
    # è¿æ¥å‚æ•°
    timeout=30,                        # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    max_retries=2,                     # é‡è¯•æ¬¡æ•°
    request_timeout=60,                # å•æ¬¡è¯·æ±‚è¶…æ—¶
    
    # æµå¼å‚æ•°
    streaming=True,                    # å¯ç”¨æµå¼
    
    # å…¶ä»–
    model_kwargs={                     # é¢å¤–å‚æ•°
        "seed": 42,                    # éšæœºç§å­
        "response_format": {"type": "json_object"}  # JSON æ¨¡å¼
    }
)
```

**å‚æ•°æ•ˆæœå¯¹æ¯”**ï¼š

| å‚æ•° | ä½å€¼ | é«˜å€¼ | ä½¿ç”¨åœºæ™¯ |
|------|------|------|----------|
| `temperature` | 0 (ç¡®å®šæ€§) | 2 (åˆ›æ„æ€§) | 0: ç¿»è¯‘ã€æ‘˜è¦ï¼›1: åˆ›ä½œã€å¯¹è¯ |
| `top_p` | 0.1 (ä¿å®ˆ) | 1.0 (å¤šæ ·) | ä¸ temperature é…åˆ |
| `frequency_penalty` | 0 | 2 | å‡å°‘é‡å¤è¯æ±‡ |
| `presence_penalty` | 0 | 2 | é¼“åŠ±æ–°è¯é¢˜ |

**å®éªŒç¤ºä¾‹**ï¼š

```python
# Temperature å¯¹æ¯”
prompt_text = "Write a creative story about a robot."

# ä½æ¸©åº¦ï¼ˆç¡®å®šæ€§ï¼‰
model_deterministic = ChatOpenAI(model="gpt-4o", temperature=0)
result1 = model_deterministic.invoke([HumanMessage(content=prompt_text)])

# é«˜æ¸©åº¦ï¼ˆåˆ›æ„æ€§ï¼‰
model_creative = ChatOpenAI(model="gpt-4o", temperature=1.5)
result2 = model_creative.invoke([HumanMessage(content=prompt_text)])

# å¤šæ¬¡è¿è¡Œ result1 å‡ ä¹ä¸€è‡´ï¼Œresult2 æ¯æ¬¡ä¸åŒ
```

### 1.2.4 Callbacks ä¸æ—¥å¿—

**Callbacks æœºåˆ¶**ï¼š

```python
from langchain.callbacks import StdOutCallbackHandler

model = ChatOpenAI(
    model="gpt-4o",
    callbacks=[StdOutCallbackHandler()],  # æ ‡å‡†è¾“å‡ºå›è°ƒ
    verbose=True
)

result = model.invoke([HumanMessage(content="Hello")])

# è¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼š
# > Entering new ChatOpenAI chain...
# > Prompt: [HumanMessage(content='Hello')]
# > Response: AIMessage(content='Hi there!')
# > Finished chain.
```

**è‡ªå®šä¹‰ Callback**ï¼š

```python
from langchain.callbacks.base import BaseCallbackHandler

class TokenCounterCallback(BaseCallbackHandler):
    """ç»Ÿè®¡ token ä½¿ç”¨é‡"""
    
    def __init__(self):
        self.prompt_tokens = 0
        self.completion_tokens = 0
    
    def on_llm_start(self, serialized, prompts, **kwargs):
        print(f"ğŸš€ Starting LLM with {len(prompts)} prompts")
    
    def on_llm_end(self, response, **kwargs):
        # æå– token ä½¿ç”¨ä¿¡æ¯
        usage = response.llm_output.get("token_usage", {})
        self.prompt_tokens += usage.get("prompt_tokens", 0)
        self.completion_tokens += usage.get("completion_tokens", 0)
        print(f"ğŸ“Š Tokens: {self.prompt_tokens} prompt + {self.completion_tokens} completion")

# ä½¿ç”¨
counter = TokenCounterCallback()
model = ChatOpenAI(model="gpt-4o", callbacks=[counter])
model.invoke([HumanMessage(content="Hello")])
```

---

## 1.3 Prompt Templates

Prompt Templates ç”¨äºæ„å»ºç»“æ„åŒ–ã€å¯å¤ç”¨çš„æç¤ºæ–‡æœ¬ã€‚

### 1.3.1 PromptTemplate åŸºç¡€

**åŸºç¡€ç”¨æ³•**ï¼š

```python
from langchain_core.prompts import PromptTemplate

# æ–¹å¼1ï¼šfrom_templateï¼ˆæ¨èï¼‰
template = PromptTemplate.from_template(
    "Translate the following text to {language}: {text}"
)

# æ–¹å¼2ï¼šæ„é€ å‡½æ•°
template = PromptTemplate(
    input_variables=["language", "text"],
    template="Translate the following text to {language}: {text}"
)

# æ ¼å¼åŒ–
prompt = template.format(language="French", text="Hello")
print(prompt)
# "Translate the following text to French: Hello"

# ç›´æ¥ä½œä¸º Runnable ä½¿ç”¨
result = template.invoke({"language": "Spanish", "text": "Goodbye"})
print(result)
# PromptValue(text="Translate the following text to Spanish: Goodbye")
```

**éƒ¨åˆ†å¡«å……ï¼ˆPartialï¼‰**ï¼š

```python
# é¢„å¡«å……æŸäº›å˜é‡
template = PromptTemplate.from_template(
    "You are a {role}. {instruction}"
)

# å›ºå®šè§’è‰²
assistant_template = template.partial(role="helpful assistant")

# åç»­åªéœ€æä¾› instruction
result = assistant_template.invoke({"instruction": "Explain quantum physics."})
```

### 1.3.2 ChatPromptTemplate ä¸æ¶ˆæ¯æ ¼å¼

**ChatPromptTemplate** æ˜¯ä¸ºå¯¹è¯æ¨¡å‹è®¾è®¡çš„æ¨¡æ¿ã€‚

```python
from langchain_core.prompts import ChatPromptTemplate

# å®šä¹‰å¤šè§’è‰²æ¨¡æ¿
template = ChatPromptTemplate.from_messages([
    ("system", "You are a {role}."),
    ("human", "{user_input}"),
    ("ai", "I understand you want to know about {topic}."),
    ("human", "Yes, please explain.")
])

# æ ¼å¼åŒ–
messages = template.invoke({
    "role": "science teacher",
    "user_input": "Tell me about photosynthesis",
    "topic": "photosynthesis"
})

print(messages)
# [
#   SystemMessage(content='You are a science teacher.'),
#   HumanMessage(content='Tell me about photosynthesis'),
#   AIMessage(content='I understand you want to know about photosynthesis.'),
#   HumanMessage(content='Yes, please explain.')
# ]
```

**æ¶ˆæ¯ç±»å‹**ï¼š

```python
from langchain_core.messages import (
    SystemMessage,    # ç³»ç»ŸæŒ‡ä»¤
    HumanMessage,     # ç”¨æˆ·è¾“å…¥
    AIMessage,        # AI å›å¤
    FunctionMessage,  # å‡½æ•°è°ƒç”¨ç»“æœï¼ˆå·²åºŸå¼ƒï¼‰
    ToolMessage       # å·¥å…·è°ƒç”¨ç»“æœï¼ˆæ¨èï¼‰
)

# ç›´æ¥æ„é€ 
messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="What is 2+2?"),
    AIMessage(content="2+2 equals 4."),
    HumanMessage(content="Thanks!")
]
```

### 1.3.3 å˜é‡æ³¨å…¥ä¸éƒ¨åˆ†å¡«å……

**åŠ¨æ€å˜é‡**ï¼š

```python
template = ChatPromptTemplate.from_messages([
    ("system", "Current date: {date}. You are a {role}."),
    ("human", "{input}")
])

# ä½¿ç”¨ partial å¡«å……æ—¥æœŸ
from datetime import datetime

template_with_date = template.partial(
    date=lambda: datetime.now().strftime("%Y-%m-%d")
)

# æ¯æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨è·å–å½“å‰æ—¥æœŸ
result = template_with_date.invoke({
    "role": "assistant",
    "input": "What's the weather?"
})
```

**æ¡ä»¶å˜é‡**ï¼š

```python
def get_system_prompt(user_level: str) -> str:
    """æ ¹æ®ç”¨æˆ·çº§åˆ«è¿”å›ä¸åŒçš„ç³»ç»Ÿæç¤º"""
    prompts = {
        "beginner": "Explain in simple terms.",
        "expert": "Use technical terminology."
    }
    return prompts.get(user_level, prompts["beginner"])

template = ChatPromptTemplate.from_messages([
    ("system", "{system_instruction}"),
    ("human", "{question}")
])

# åŠ¨æ€ç³»ç»Ÿæç¤º
result = template.invoke({
    "system_instruction": get_system_prompt("expert"),
    "question": "How does TCP work?"
})
```

### 1.3.4 æ¨¡æ¿ç»„åˆ

**PipelinePromptTemplate**ï¼ˆå¤šé˜¶æ®µæç¤ºï¼‰ï¼š

```python
from langchain_core.prompts import PipelinePromptTemplate

# å­æ¨¡æ¿
intro_template = PromptTemplate.from_template(
    "You are an expert in {domain}."
)

task_template = PromptTemplate.from_template(
    "{intro}\nTask: {task}"
)

# ç»„åˆ
full_template = PipelinePromptTemplate(
    final_prompt=task_template,
    pipeline_prompts=[
        ("intro", intro_template)
    ]
)

result = full_template.invoke({
    "domain": "machine learning",
    "task": "Explain gradient descent"
})
```

---

## 1.4 Output Parsers

Output Parsers å°† LLM çš„æ–‡æœ¬è¾“å‡ºè§£æä¸ºç»“æ„åŒ–æ•°æ®ã€‚

### 1.4.1 StrOutputParserï¼šåŸºç¡€æ–‡æœ¬è§£æ

```python
from langchain_core.output_parsers import StrOutputParser

parser = StrOutputParser()

# ä» AIMessage æå– content
ai_message = AIMessage(content="Hello, world!")
result = parser.invoke(ai_message)
print(result)  # "Hello, world!"

# åœ¨é“¾ä¸­ä½¿ç”¨
chain = prompt | model | StrOutputParser()
result = chain.invoke({"input": "Say hi"})
print(type(result))  # <class 'str'>
```

### 1.4.2 JsonOutputParserï¼šç»“æ„åŒ–è¾“å‡º

```python
from langchain_core.output_parsers import JsonOutputParser

parser = JsonOutputParser()

# æç¤ºæ¨¡å‹è¾“å‡º JSON
template = ChatPromptTemplate.from_messages([
    ("system", "Output your response as JSON."),
    ("human", "List 3 colors with their hex codes.")
])

chain = template | model | parser

result = chain.invoke({})
print(result)
# {'colors': [
#   {'name': 'red', 'hex': '#FF0000'},
#   {'name': 'green', 'hex': '#00FF00'},
#   {'name': 'blue', 'hex': '#0000FF'}
# ]}
```

### 1.4.3 PydanticOutputParserï¼šç±»å‹å®‰å…¨

```python
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# å®šä¹‰æ•°æ®æ¨¡å‹
class Person(BaseModel):
    name: str = Field(description="Person's name")
    age: int = Field(description="Person's age")
    occupation: str = Field(description="Person's job")

parser = PydanticOutputParser(pydantic_object=Person)

# è·å–æ ¼å¼è¯´æ˜
format_instructions = parser.get_format_instructions()
print(format_instructions)
# Output your response as JSON matching this schema:
# {"name": "string", "age": "integer", "occupation": "string"}

# åœ¨æç¤ºä¸­ä½¿ç”¨
template = ChatPromptTemplate.from_messages([
    ("system", "Extract person information.\n{format_instructions}"),
    ("human", "{input}")
])

chain = template | model | parser

result = chain.invoke({
    "format_instructions": format_instructions,
    "input": "John is a 30-year-old engineer."
})

print(result)
# Person(name='John', age=30, occupation='engineer')
print(type(result))  # <class '__main__.Person'>
```

### 1.4.4 CommaSeparatedListOutputParserï¼šåˆ—è¡¨è§£æ

```python
from langchain_core.output_parsers import CommaSeparatedListOutputParser

parser = CommaSeparatedListOutputParser()

template = ChatPromptTemplate.from_messages([
    ("system", "Output a comma-separated list."),
    ("human", "List 5 programming languages.")
])

chain = template | model | parser

result = chain.invoke({})
print(result)
# ['Python', 'JavaScript', 'Java', 'C++', 'Go']
print(type(result))  # <class 'list'>
```

---

## 1.5 Message ä¸ Conversation

### 1.5.1 æ¶ˆæ¯ç±»å‹

<div data-component="MessageFlowDiagram"></div>

**å®Œæ•´æ¶ˆæ¯ç±»å‹**ï¼š

```python
from langchain_core.messages import (
    BaseMessage,
    SystemMessage,
    HumanMessage,
    AIMessage,
    ToolMessage,
    ChatMessage
)

# 1. SystemMessageï¼šç³»ç»ŸæŒ‡ä»¤
sys_msg = SystemMessage(content="You are a helpful assistant.")

# 2. HumanMessageï¼šç”¨æˆ·è¾“å…¥
human_msg = HumanMessage(content="What is LangChain?")

# 3. AIMessageï¼šAI å›å¤
ai_msg = AIMessage(
    content="LangChain is a framework...",
    additional_kwargs={"model": "gpt-4o"}
)

# 4. ToolMessageï¼šå·¥å…·è°ƒç”¨ç»“æœ
tool_msg = ToolMessage(
    content="Search result: ...",
    tool_call_id="call_123"
)

# 5. ChatMessageï¼šè‡ªå®šä¹‰è§’è‰²
custom_msg = ChatMessage(
    content="...",
    role="custom_role"
)
```

### 1.5.2 æ¶ˆæ¯å†å²ç®¡ç†

```python
from langchain_core.messages import BaseMessage

class SimpleConversation:
    """ç®€å•å¯¹è¯ç®¡ç†"""
    
    def __init__(self, system_message: str):
        self.messages: list[BaseMessage] = [
            SystemMessage(content=system_message)
        ]
    
    def add_user_message(self, content: str):
        self.messages.append(HumanMessage(content=content))
    
    def add_ai_message(self, content: str):
        self.messages.append(AIMessage(content=content))
    
    def get_messages(self) -> list[BaseMessage]:
        return self.messages
    
    def clear(self):
        self.messages = self.messages[:1]  # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯

# ä½¿ç”¨
conv = SimpleConversation("You are a coding assistant.")
conv.add_user_message("How do I sort a list in Python?")

# è°ƒç”¨æ¨¡å‹
response = model.invoke(conv.get_messages())
conv.add_ai_message(response.content)

conv.add_user_message("What about in reverse order?")
response = model.invoke(conv.get_messages())
```

### 1.5.3 æ¶ˆæ¯è½¬æ¢ä¸è¿‡æ»¤

**é™åˆ¶æ¶ˆæ¯å†å²é•¿åº¦**ï¼š

```python
def trim_messages(messages: list[BaseMessage], max_tokens: int = 2000) -> list[BaseMessage]:
    """ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯ï¼Œç¡®ä¿ä¸è¶…è¿‡ token é™åˆ¶"""
    from langchain_openai import ChatOpenAI
    
    # ç®€åŒ–ç‰ˆï¼šä¿ç•™æœ€å N æ¡æ¶ˆæ¯
    max_messages = 10
    if len(messages) > max_messages:
        return [messages[0]] + messages[-max_messages:]  # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
    return messages

# ä½¿ç”¨
trimmed = trim_messages(conv.get_messages())
```

**æ¶ˆæ¯æ ¼å¼è½¬æ¢**ï¼š

```python
def messages_to_dict(messages: list[BaseMessage]) -> list[dict]:
    """è½¬æ¢ä¸º OpenAI API æ ¼å¼"""
    return [
        {
            "role": msg.type,
            "content": msg.content
        }
        for msg in messages
    ]

# ä½¿ç”¨
api_format = messages_to_dict(conv.get_messages())
```

---

## ğŸ¯ æœ¬ç« å°ç»“

**æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **Runnable åè®®**ï¼šinvokeã€streamã€batchã€ainvoke å››ç§è°ƒç”¨æ–¹å¼
2. **Language Models**ï¼šä¼˜å…ˆä½¿ç”¨ ChatModelï¼Œæ”¯æŒå¤šæä¾›å•†åˆ‡æ¢
3. **Prompt Templates**ï¼šChatPromptTemplate ç”¨äºå¯¹è¯ï¼Œæ”¯æŒå˜é‡æ³¨å…¥
4. **Output Parsers**ï¼šStrOutputParserã€JsonOutputParserã€PydanticOutputParser
5. **Message ç®¡ç†**ï¼šSystemMessageã€HumanMessageã€AIMessageï¼Œæ‰‹åŠ¨ç®¡ç†å†å²

**æŒæ¡æ£€æŸ¥**ï¼š

- [ ] èƒ½è§£é‡Š Runnable åè®®çš„è®¾è®¡æ„ä¹‰
- [ ] èƒ½åˆ‡æ¢ä¸åŒæ¨¡å‹æä¾›å•†ï¼ˆOpenAIã€Anthropicã€Ollamaï¼‰
- [ ] èƒ½ä½¿ç”¨ ChatPromptTemplate æ„å»ºå¤šè½®å¯¹è¯
- [ ] èƒ½ç”¨ PydanticOutputParser è§£æç»“æ„åŒ–è¾“å‡º
- [ ] èƒ½å®ç°ç®€å•çš„å¯¹è¯å†å²ç®¡ç†

**ç»ƒä¹ é¢˜**ï¼š

1. **æ€§èƒ½å¯¹æ¯”**ï¼šå¯¹æ¯” `invoke()` å’Œ `batch()` å¤„ç† 100 æ¡æ¶ˆæ¯çš„è€—æ—¶
2. **æ¨¡å‹åˆ‡æ¢**ï¼šå®ç°ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®è¾“å…¥é•¿åº¦è‡ªåŠ¨é€‰æ‹©æ¨¡å‹ï¼ˆçŸ­æ–‡æœ¬ç”¨ gpt-4o-miniï¼Œé•¿æ–‡æœ¬ç”¨ gpt-4oï¼‰
3. **ç»“æ„åŒ–æå–**ï¼šç”¨ PydanticOutputParser ä»æ–‡æœ¬ä¸­æå–ä¹¦ç±ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€å‡ºç‰ˆå¹´ä»½ï¼‰
4. **å¯¹è¯è®°å¿†**ï¼šæ‰©å±• SimpleConversation ç±»ï¼Œæ·»åŠ  `save_to_file()` å’Œ `load_from_file()` æ–¹æ³•

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š

Chapter 2 å°†å­¦ä¹ å¦‚ä½•ç”¨ LCEL æ„å»ºç®€å•é“¾ï¼ŒåŒ…æ‹¬ç¿»è¯‘é“¾ã€æ‘˜è¦é“¾ã€é—®ç­”é“¾ç­‰å¸¸è§æ¨¡å¼ã€‚

---

## ğŸ“š æ‰©å±•é˜…è¯»

- [Runnable æ¥å£æ–‡æ¡£](https://python.langchain.com/docs/concepts/runnables)
- [Chat Models å¯¹æ¯”](https://python.langchain.com/docs/integrations/chat/)
- [Prompt Templates æŒ‡å—](https://python.langchain.com/docs/concepts/prompt_templates)
- [Output Parsers è¯¦è§£](https://python.langchain.com/docs/concepts/output_parsers)
- [Message ç±»å‹å‚è€ƒ](https://python.langchain.com/api_reference/core/messages.html)
