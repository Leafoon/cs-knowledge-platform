# Chapter 1: æ ¸å¿ƒæŠ½è±¡ä¸åŸºç¡€ç»„ä»¶

> **æœ¬ç« ç›®æ ‡**ï¼šæ·±å…¥ç†è§£ LangChain çš„åº•å±‚æŠ½è±¡æœºåˆ¶ï¼ŒæŒæ¡ Runnable åè®®ã€Language Modelsã€Prompt Templatesã€Output Parsers ç­‰æ ¸å¿ƒç»„ä»¶çš„ä½¿ç”¨æ–¹æ³•ï¼Œå»ºç«‹æ„å»º AI åº”ç”¨çš„åšå®åŸºç¡€ã€‚

---

## ğŸ“– æœ¬ç« å¯¼è§ˆ

æœ¬ç« æ·±å…¥å‰–æ LangChain çš„æ ¸å¿ƒæŠ½è±¡å±‚ï¼Œè¿™äº›æ¦‚å¿µæ˜¯ç†è§£æ•´ä¸ªæ¡†æ¶çš„å…³é”®ã€‚

### ğŸ¯ å­¦ä¹ è·¯çº¿å›¾

```
Runnable åè®® â†’ Language Models â†’ Prompt Templates â†’ Output Parsers â†’ Message ç³»ç»Ÿ â†’ å®Œæ•´åº”ç”¨
    â†“               â†“                  â†“                   â†“              â†“
 ç»Ÿä¸€æ¥å£       æ¨¡å‹è°ƒç”¨          æç¤ºç®¡ç†            ç»“æ„åŒ–è¾“å‡º      å¯¹è¯ç®¡ç†
```

### ğŸ”‘ æ ¸å¿ƒçŸ¥è¯†ç‚¹æ¦‚è§ˆ

| ç»„ä»¶ | æ ¸å¿ƒä»·å€¼ | éš¾åº¦ | é‡è¦æ€§ | é¢„è®¡å­¦ä¹ æ—¶é—´ |
|------|----------|------|--------|------------|
| **Runnable åè®®** | ç»Ÿä¸€æ¥å£ï¼Œç»„åˆåŸºç¡€ | â­â­â­ | â­â­â­â­â­ | 15 åˆ†é’Ÿ |
| **Language Models** | LLM è°ƒç”¨ä¸é…ç½® | â­â­ | â­â­â­â­â­ | 20 åˆ†é’Ÿ |
| **Prompt Templates** | æç¤ºå·¥ç¨‹æ ‡å‡†åŒ– | â­â­ | â­â­â­â­ | 20 åˆ†é’Ÿ |
| **Output Parsers** | ç»“æ„åŒ–è¾“å‡º | â­â­â­ | â­â­â­â­ | 15 åˆ†é’Ÿ |
| **Message ç³»ç»Ÿ** | å¯¹è¯ç®¡ç† | â­â­ | â­â­â­ | 10 åˆ†é’Ÿ |

### ğŸ“š æœ¬ç« ç»“æ„

1. **1.1 Runnable åè®®** - ç»Ÿä¸€æ¥å£è®¾è®¡ä¸ç»„åˆæ¨¡å¼
2. **1.2 Language Models** - Chat Modelsã€LLMs ä¸å¤šæä¾›å•†é›†æˆ
3. **1.3 Prompt Templates** - æ¨¡æ¿ç³»ç»Ÿä¸æç¤ºå·¥ç¨‹
4. **1.4 Output Parsers** - ç»“æ„åŒ–è¾“å‡ºè§£æ
5. **1.5 Message ä¸ Conversation** - æ¶ˆæ¯ç³»ç»Ÿä¸å¯¹è¯ç®¡ç†
6. **1.6 é«˜çº§ä¸»é¢˜** - RunnableConfigã€è‡ªå®šä¹‰ç»„ä»¶ä¸æ€§èƒ½ä¼˜åŒ–

---

## 1.1 Runnable åè®®

> **æ ¸å¿ƒç†å¿µ**ï¼šLangChain é€šè¿‡ Runnable åè®®ç»Ÿä¸€æ‰€æœ‰ç»„ä»¶çš„è°ƒç”¨æ¥å£ï¼Œå®ç°çµæ´»çš„ç»„åˆä¸ç¼–æ’ã€‚

### 1.1.1 è®¾è®¡åŠ¨æœºï¼šä¸ºä»€ä¹ˆéœ€è¦ Runnableï¼Ÿ

**é—®é¢˜èƒŒæ™¯**ï¼š

åœ¨ LangChain æ—©æœŸç‰ˆæœ¬ä¸­ï¼Œä¸åŒç»„ä»¶æœ‰ä¸åŒçš„è°ƒç”¨æ–¹å¼ï¼š
- PromptTemplate ä½¿ç”¨ `.format()`
- LLM ä½¿ç”¨ `.predict()` æˆ– `__call__()`
- Chain ä½¿ç”¨ `.run()` æˆ– `.apply()`

è¿™å¯¼è‡´ï¼š
- âŒ ç»„åˆå›°éš¾ï¼šä¸åŒç»„ä»¶éš¾ä»¥æ— ç¼è¿æ¥
- âŒ å­¦ä¹ æ›²çº¿é™¡å³­ï¼šéœ€è¦è®°ä½å¤šç§è°ƒç”¨æ–¹å¼
- âŒ ä»£ç ä¸ä¸€è‡´ï¼šåŒä¸€æ“ä½œæœ‰å¤šç§å†™æ³•

**è§£å†³æ–¹æ¡ˆï¼šRunnable åè®®**

Runnable æ˜¯ä¸€ä¸ªæŠ½è±¡åŸºç±»ï¼Œå®šä¹‰äº†ç»Ÿä¸€çš„æ¥å£æ ‡å‡†ï¼š

```python
from abc import ABC, abstractmethod
from typing import Any, Iterator, AsyncIterator, Optional

class Runnable(ABC):
    """æ‰€æœ‰å¯æ‰§è¡Œç»„ä»¶çš„åŸºç±»"""
    
    @abstractmethod
    def invoke(self, input: Any, config: Optional[RunnableConfig] = None) -> Any:
        """åŒæ­¥è°ƒç”¨ï¼šé˜»å¡ç›´åˆ°ç»“æœè¿”å›"""
        pass
    
    @abstractmethod
    async def ainvoke(self, input: Any, config: Optional[RunnableConfig] = None) -> Any:
        """å¼‚æ­¥è°ƒç”¨ï¼šéé˜»å¡ï¼Œé€‚åˆé«˜å¹¶å‘åœºæ™¯"""
        pass
    
    @abstractmethod
    def stream(self, input: Any, config: Optional[RunnableConfig] = None) -> Iterator[Any]:
        """åŒæ­¥æµå¼è¾“å‡ºï¼šé€å—è¿”å›ç»“æœ"""
        pass
    
    @abstractmethod
    async def astream(self, input: Any, config: Optional[RunnableConfig] = None) -> AsyncIterator[Any]:
        """å¼‚æ­¥æµå¼è¾“å‡ºï¼šç»“åˆå¼‚æ­¥ä¸æµå¼çš„ä¼˜åŠ¿"""
        pass
    
    @abstractmethod
    def batch(self, inputs: list[Any], config: Optional[RunnableConfig] = None) -> list[Any]:
        """æ‰¹é‡å¤„ç†ï¼šä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªè¾“å…¥"""
        pass
    
    @abstractmethod
    async def abatch(self, inputs: list[Any], config: Optional[RunnableConfig] = None) -> list[Any]:
        """å¼‚æ­¥æ‰¹é‡å¤„ç†ï¼šé«˜æ•ˆå¤„ç†å¤§æ‰¹é‡ä»»åŠ¡"""
        pass
```

**ä¼˜åŠ¿**ï¼š
- âœ… ç»Ÿä¸€æ¥å£ï¼šæ‰€æœ‰ç»„ä»¶ä½¿ç”¨ç›¸åŒçš„è°ƒç”¨æ–¹å¼
- âœ… çµæ´»ç»„åˆï¼šé€šè¿‡ `|` æ“ä½œç¬¦è¿æ¥ç»„ä»¶
- âœ… æ€§èƒ½ä¼˜åŒ–ï¼šæ”¯æŒæµå¼ã€æ‰¹é‡ã€å¼‚æ­¥ç­‰å¤šç§æ‰§è¡Œæ¨¡å¼
- âœ… é…ç½®ä¼ é€’ï¼šRunnableConfig åœ¨é“¾ä¸­è‡ªåŠ¨ä¼ é€’

### 1.1.2 æ ¸å¿ƒæ–¹æ³•è¯¦è§£

<div data-component="RunnableProtocolVisualizer"></div>

#### æ–¹æ³• 1ï¼šinvoke() - åŒæ­¥å•æ¬¡è°ƒç”¨

**é€‚ç”¨åœºæ™¯**ï¼šç®€å•è„šæœ¬ã€å•æ¬¡è¯·æ±‚ã€æµ‹è¯•ä»£ç 

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = ChatOpenAI(
    model="gpt-4o-mini",      # æ¨¡å‹åç§°
    temperature=0.7,          # æ¸©åº¦å‚æ•°ï¼ˆ0-2ï¼‰
    timeout=30,               # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    max_retries=2             # é‡è¯•æ¬¡æ•°
)

# åŒæ­¥è°ƒç”¨
message = HumanMessage(content="What is 2+2?")
response = model.invoke([message])

print(response.content)       # "4"
print(type(response))         # <class 'langchain_core.messages.ai.AIMessage'>
```

**å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | ç±»å‹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `model` | `str` | æ¨¡å‹æ ‡è¯†ç¬¦ï¼ˆå¦‚ `gpt-4o`, `gpt-4o-mini`ï¼‰ | å¿…å¡« |
| `temperature` | `float` | æ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼ˆ0=ç¡®å®šæ€§ï¼Œ2=é«˜åˆ›æ„ï¼‰ | `0.7` |
| `timeout` | `int` | è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ | `None` |
| `max_retries` | `int` | å¤±è´¥é‡è¯•æ¬¡æ•° | `2` |
| `max_tokens` | `int` | æœ€å¤§ç”Ÿæˆtokenæ•° | `None`ï¼ˆæ¨¡å‹é»˜è®¤å€¼ï¼‰ |
| `streaming` | `bool` | æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º | `False` |

**æ‰§è¡Œæµç¨‹**ï¼š

```
è¾“å…¥ â†’ invoke() â†’ ç½‘ç»œè¯·æ±‚ â†’ ç­‰å¾…å“åº” â†’ è¿”å›å®Œæ•´ç»“æœ
         â†“
      é˜»å¡ä¸»çº¿ç¨‹ï¼ˆåŒæ­¥ï¼‰
```

#### æ–¹æ³• 2ï¼šainvoke() - å¼‚æ­¥å•æ¬¡è°ƒç”¨

**é€‚ç”¨åœºæ™¯**ï¼šWeb åç«¯ï¼ˆFastAPIã€Djangoï¼‰ã€é«˜å¹¶å‘åº”ç”¨ã€éœ€è¦åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚

```python
import asyncio
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

async def async_example():
    model = ChatOpenAI(model="gpt-4o-mini")
    
    # å¼‚æ­¥è°ƒç”¨
    response = await model.ainvoke([
        HumanMessage(content="What is the capital of France?")
    ])
    
    print(response.content)  # "Paris"

# è¿è¡Œå¼‚æ­¥å‡½æ•°
asyncio.run(async_example())
```

**å¹¶å‘ä¼˜åŠ¿å¯¹æ¯”**ï¼š

```python
import time

# åŒæ­¥ç‰ˆæœ¬ï¼šæ€»è€—æ—¶ = å•æ¬¡è€—æ—¶ Ã— è¯·æ±‚æ•°
def sync_version():
    model = ChatOpenAI(model="gpt-4o-mini")
    questions = ["What is 1+1?", "What is 2+2?", "What is 3+3?"]
    
    start = time.time()
    for q in questions:
        model.invoke([HumanMessage(content=q)])
    
    print(f"åŒæ­¥è€—æ—¶: {time.time() - start:.2f}ç§’")  # çº¦ 3-6 ç§’

# å¼‚æ­¥ç‰ˆæœ¬ï¼šæ€»è€—æ—¶ â‰ˆ å•æ¬¡è€—æ—¶ï¼ˆå¹¶å‘æ‰§è¡Œï¼‰
async def async_version():
    model = ChatOpenAI(model="gpt-4o-mini")
    questions = ["What is 1+1?", "What is 2+2?", "What is 3+3?"]
    
    start = time.time()
    tasks = [
        model.ainvoke([HumanMessage(content=q)])
        for q in questions
    ]
    await asyncio.gather(*tasks)  # å¹¶å‘æ‰§è¡Œ
    
    print(f"å¼‚æ­¥è€—æ—¶: {time.time() - start:.2f}ç§’")  # çº¦ 1-2 ç§’

asyncio.run(async_version())
```

**æ€§èƒ½æå‡å…¬å¼**ï¼š

$$
\text{åŠ é€Ÿæ¯”} = \frac{\text{ä¸²è¡Œæ€»æ—¶é—´}}{\text{å¹¶è¡Œæ€»æ—¶é—´}} = \frac{n \times t}{t + \text{overhead}} \approx n
$$

å…¶ä¸­ $n$ ä¸ºä»»åŠ¡æ•°é‡ï¼Œ$t$ ä¸ºå•ä»»åŠ¡è€—æ—¶ã€‚

#### æ–¹æ³• 3ï¼šstream() - åŒæ­¥æµå¼è¾“å‡º

**é€‚ç”¨åœºæ™¯**ï¼šèŠå¤©ç•Œé¢ã€å®æ—¶åé¦ˆã€æ¸è¿›å¼æ˜¾ç¤º

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

model = ChatOpenAI(model="gpt-4o", streaming=True)

# æµå¼è¾“å‡º
for chunk in model.stream([HumanMessage(content="Count from 1 to 10.")]):
    print(chunk.content, end="", flush=True)
    # è¾“å‡º: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 (é€ä¸ªæ˜¾ç¤º)
```

**ç”¨æˆ·ä½“éªŒå¯¹æ¯”**ï¼š

```python
# éæµå¼ï¼šç”¨æˆ·ç­‰å¾… 5 ç§’åä¸€æ¬¡æ€§çœ‹åˆ°å®Œæ•´å›å¤
response = model.invoke([HumanMessage(content="Write a story.")])
print(response.content)  # ä¸€æ¬¡æ€§æ˜¾ç¤ºå…¨éƒ¨å†…å®¹

# æµå¼ï¼šç”¨æˆ·ç«‹å³çœ‹åˆ°å¼€å§‹ï¼Œé€å­—æ˜¾ç¤ºï¼ˆç±»ä¼¼ ChatGPTï¼‰
for chunk in model.stream([HumanMessage(content="Write a story.")]):
    print(chunk.content, end="", flush=True)
    time.sleep(0.05)  # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
```

**å®ç°åŸç†**ï¼š

```
LLM ç”Ÿæˆ â†’ æœåŠ¡å™¨åˆ†å—å‘é€ â†’ å®¢æˆ·ç«¯é€å—æ¥æ”¶ â†’ å®æ—¶æ˜¾ç¤º
             â†“                    â†“
         SSE (Server-Sent     Iterator/Generator
            Events)           (Python yield)
```

#### æ–¹æ³• 4ï¼šastream() - å¼‚æ­¥æµå¼è¾“å‡º

**é€‚ç”¨åœºæ™¯**ï¼šå¼‚æ­¥ Web æ¡†æ¶ï¼ˆFastAPIï¼‰ã€WebSocketã€é«˜æ€§èƒ½å®æ—¶åº”ç”¨

```python
import asyncio
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

async def async_stream_example():
    model = ChatOpenAI(model="gpt-4o", streaming=True)
    
    async for chunk in model.astream([HumanMessage(content="Explain AI.")]):
        print(chunk.content, end="", flush=True)

asyncio.run(async_stream_example())
```

**FastAPI é›†æˆç¤ºä¾‹**ï¼š

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

app = FastAPI()

@app.get("/stream")
async def stream_response(question: str):
    model = ChatOpenAI(model="gpt-4o", streaming=True)
    
    async def generate():
        async for chunk in model.astream([HumanMessage(content=question)]):
            yield chunk.content
    
    return StreamingResponse(generate(), media_type="text/plain")

# è®¿é—® http://localhost:8000/stream?question=What+is+AI
```

#### æ–¹æ³• 5ï¼šbatch() - æ‰¹é‡å¤„ç†

**é€‚ç”¨åœºæ™¯**ï¼šæ•°æ®å¤„ç†ã€æ‰¹é‡ç¿»è¯‘ã€æ‰¹é‡æ‘˜è¦ã€æ‰¹é‡è¯„ä¼°

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

model = ChatOpenAI(model="gpt-4o-mini")

# æ‰¹é‡å¤„ç†å¤šä¸ªè¾“å…¥
messages_batch = [
    [HumanMessage(content="Translate 'Hello' to French")],
    [HumanMessage(content="Translate 'Goodbye' to Spanish")],
    [HumanMessage(content="Translate 'Thank you' to German")]
]

responses = model.batch(messages_batch)

for resp in responses:
    print(resp.content)
# è¾“å‡º:
# Bonjour
# AdiÃ³s
# Danke
```

**æ€§èƒ½ä¼˜åŠ¿**ï¼š

```python
import time

# é€ä¸ªè°ƒç”¨ï¼ˆä¸æ¨èï¼‰
start = time.time()
results = []
for msg in messages_batch:
    results.append(model.invoke(msg))
print(f"é€ä¸ªè°ƒç”¨è€—æ—¶: {time.time() - start:.2f}ç§’")  # çº¦ 6 ç§’

# æ‰¹é‡è°ƒç”¨ï¼ˆæ¨èï¼‰
start = time.time()
results = model.batch(messages_batch)
print(f"æ‰¹é‡è°ƒç”¨è€—æ—¶: {time.time() - start:.2f}ç§’")  # çº¦ 2 ç§’
```

**æ‰¹é‡è°ƒç”¨ä¼˜åŒ–åŸç†**ï¼š

1. **è¯·æ±‚åˆå¹¶**ï¼šå¤šä¸ªè¯·æ±‚åˆå¹¶ä¸ºä¸€ä¸ª HTTP è¯·æ±‚
2. **å¹¶è¡Œå¤„ç†**ï¼šæœåŠ¡å™¨ç«¯å¹¶è¡Œå¤„ç†å¤šä¸ªè¾“å…¥
3. **è¿æ¥å¤ç”¨**ï¼šå‡å°‘ TCP è¿æ¥å»ºç«‹å¼€é”€

**æœ€ä½³å®è·µ**ï¼š

```python
# æ‰¹é‡å¤§å°å»ºè®®ï¼š10-50
batch_size = 20

def process_in_batches(inputs, batch_size=20):
    results = []
    for i in range(0, len(inputs), batch_size):
        batch = inputs[i:i+batch_size]
        results.extend(model.batch(batch))
    return results

# å¤„ç† 1000 æ¡æ•°æ®
large_dataset = [[HumanMessage(content=f"Translate {i}")] for i in range(1000)]
results = process_in_batches(large_dataset)
```

#### æ–¹æ³• 6ï¼šabatch() - å¼‚æ­¥æ‰¹é‡å¤„ç†

**é€‚ç”¨åœºæ™¯**ï¼šå¤§è§„æ¨¡æ•°æ®å¤„ç†ã€é«˜å¹¶å‘æ‰¹é‡ä»»åŠ¡

```python
import asyncio
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

async def async_batch_example():
    model = ChatOpenAI(model="gpt-4o-mini")
    
    messages_batch = [
        [HumanMessage(content=f"What is {i} + {i}?")]
        for i in range(1, 6)
    ]
    
    results = await model.abatch(messages_batch)
    
    for i, resp in enumerate(results, 1):
        print(f"{i} + {i} = {resp.content}")

asyncio.run(async_batch_example())
```

### 1.1.3 æ–¹æ³•é€‰æ‹©å†³ç­–æ ‘

```
æ˜¯å¦éœ€è¦å®æ—¶åé¦ˆï¼Ÿ
â”œâ”€ æ˜¯ â†’ æ˜¯å¦å¼‚æ­¥ç¯å¢ƒï¼Ÿ
â”‚        â”œâ”€ æ˜¯ â†’ astream()
â”‚        â””â”€ å¦ â†’ stream()
â””â”€ å¦ â†’ æ˜¯å¦æ‰¹é‡å¤„ç†ï¼Ÿ
         â”œâ”€ æ˜¯ â†’ æ˜¯å¦å¼‚æ­¥ç¯å¢ƒï¼Ÿ
         â”‚        â”œâ”€ æ˜¯ â†’ abatch()
         â”‚        â””â”€ å¦ â†’ batch()
         â””â”€ å¦ â†’ æ˜¯å¦å¼‚æ­¥ç¯å¢ƒï¼Ÿ
                  â”œâ”€ æ˜¯ â†’ ainvoke()
                  â””â”€ å¦ â†’ invoke()
```

**å®Œæ•´å¯¹æ¯”è¡¨**ï¼š

| æ–¹æ³• | åŒæ­¥/å¼‚æ­¥ | æµå¼ | æ‰¹é‡ | é€‚ç”¨åœºæ™¯ | æ€§èƒ½ | å¤æ‚åº¦ |
|------|-----------|------|------|----------|------|--------|
| `invoke()` | åŒæ­¥ | âŒ | âŒ | ç®€å•è„šæœ¬ã€æµ‹è¯• | â­â­ | â­ |
| `ainvoke()` | å¼‚æ­¥ | âŒ | âŒ | Web åç«¯ã€å¹¶å‘ | â­â­â­â­ | â­â­ |
| `stream()` | åŒæ­¥ | âœ… | âŒ | èŠå¤©ç•Œé¢ | â­â­â­ | â­â­ |
| `astream()` | å¼‚æ­¥ | âœ… | âŒ | å¼‚æ­¥èŠå¤© | â­â­â­â­â­ | â­â­â­ |
| `batch()` | åŒæ­¥ | âŒ | âœ… | æ‰¹é‡æ•°æ®å¤„ç† | â­â­â­â­ | â­â­ |
| `abatch()` | å¼‚æ­¥ | âŒ | âœ… | å¤§è§„æ¨¡æ•°æ® | â­â­â­â­â­ | â­â­â­ |

### 1.1.4 Runnable ç»„åˆæ¨¡å¼

#### ç®¡é“æ“ä½œç¬¦ï¼ˆ|ï¼‰

**æ ¸å¿ƒè¯­æ³•**ï¼š

```python
# ä½¿ç”¨ | æ“ä½œç¬¦ç»„åˆå¤šä¸ª Runnable
chain = component1 | component2 | component3

# ç­‰ä»·äº
from langchain_core.runnables import RunnableSequence
chain = RunnableSequence(component1, component2, component3)
```

**å®Œæ•´ç¤ºä¾‹**ï¼š

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. åˆ›å»º Prompt Template (Runnable)
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a translator."),
    ("human", "Translate '{text}' to {language}.")
])

# 2. åˆ›å»º Model (Runnable)
model = ChatOpenAI(model="gpt-4o-mini")

# 3. åˆ›å»º Output Parser (Runnable)
parser = StrOutputParser()

# 4. ç»„åˆæˆé“¾
chain = prompt | model | parser

# 5. æ‰§è¡Œ
result = chain.invoke({"text": "Hello", "language": "French"})
print(result)  # "Bonjour"
```

**æ‰§è¡Œæµç¨‹å¯è§†åŒ–**ï¼š

```
è¾“å…¥: {"text": "Hello", "language": "French"}
  â†“
prompt.invoke() â†’ ç”Ÿæˆæ¶ˆæ¯åˆ—è¡¨
  â†“
[SystemMessage("You are a translator."),
 HumanMessage("Translate 'Hello' to French.")]
  â†“
model.invoke() â†’ è°ƒç”¨ LLM
  â†“
AIMessage(content="Bonjour")
  â†“
parser.invoke() â†’ æå–æ–‡æœ¬
  â†“
è¾“å‡º: "Bonjour"
```

#### RunnablePassthroughï¼šé€ä¼ ä¸è°ƒè¯•

**åŸºæœ¬ç”¨æ³•**ï¼š

```python
from langchain_core.runnables import RunnablePassthrough

# ç›´æ¥é€ä¼ è¾“å…¥
passthrough = RunnablePassthrough()
result = passthrough.invoke({"key": "value"})
print(result)  # {"key": "value"}
```

**è°ƒè¯•é“¾**ï¼š

```python
# åœ¨é“¾ä¸­æ’å…¥ passthrough æŸ¥çœ‹ä¸­é—´ç»“æœ
chain = (
    prompt
    | RunnablePassthrough()  # æŸ¥çœ‹ prompt è¾“å‡º
    | model
    | RunnablePassthrough()  # æŸ¥çœ‹ model è¾“å‡º
    | parser
)
```

**æ·»åŠ é¢å¤–å­—æ®µ**ï¼š

```python
chain = (
    {"input": RunnablePassthrough()}  # ä¿ç•™åŸå§‹è¾“å…¥
    | prompt
    | model
    | {"output": parser, "raw": RunnablePassthrough()}  # åŒæ—¶è¿”å›è§£æç»“æœå’ŒåŸå§‹æ¶ˆæ¯
)

result = chain.invoke({"text": "Hello"})
# {
#   "output": "Bonjour",
#   "raw": AIMessage(content="Bonjour")
# }
```

#### RunnableParallelï¼šå¹¶è¡Œæ‰§è¡Œ

**åŸºæœ¬ç”¨æ³•**ï¼š

```python
from langchain_core.runnables import RunnableParallel

# å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡
parallel = RunnableParallel(
    french=chain_french,
    spanish=chain_spanish,
    german=chain_german
)

result = parallel.invoke({"text": "Hello"})
# {
#   "french": "Bonjour",
#   "spanish": "Hola",
#   "german": "Guten Tag"
# }
```

**å®é™…æ¡ˆä¾‹ï¼šå¤šè§’åº¦åˆ†æ**ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

model = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

# å®šä¹‰å¤šä¸ªåˆ†æé“¾
sentiment_chain = (
    ChatPromptTemplate.from_template("Analyze the sentiment of: {text}")
    | model
    | parser
)

topic_chain = (
    ChatPromptTemplate.from_template("Extract the main topic of: {text}")
    | model
    | parser
)

summary_chain = (
    ChatPromptTemplate.from_template("Summarize in one sentence: {text}")
    | model
    | parser
)

# å¹¶è¡Œæ‰§è¡Œ
analysis_pipeline = RunnableParallel(
    sentiment=sentiment_chain,
    topic=topic_chain,
    summary=summary_chain
)

result = analysis_pipeline.invoke({
    "text": "LangChain is an amazing framework for building LLM applications!"
})

print(result)
# {
#   "sentiment": "Positive",
#   "topic": "LangChain framework",
#   "summary": "LangChain is a great tool for developing LLM-based apps."
# }
```

#### RunnableLambdaï¼šåŒ…è£…ä»»æ„å‡½æ•°

**åŸºæœ¬ç”¨æ³•**ï¼š

```python
from langchain_core.runnables import RunnableLambda

def uppercase(text: str) -> str:
    return text.upper()

def add_prefix(text: str) -> str:
    return f"[TRANSLATED] {text}"

# åŒ…è£…ä¸º Runnable
chain = (
    prompt
    | model
    | parser
    | RunnableLambda(uppercase)
    | RunnableLambda(add_prefix)
)

result = chain.invoke({"text": "Hello", "language": "French"})
print(result)  # "[TRANSLATED] BONJOUR"
```

**å¤æ‚æ•°æ®å¤„ç†**ï¼š

```python
def extract_and_format(ai_message):
    """ä» AIMessage æå–å†…å®¹å¹¶æ ¼å¼åŒ–"""
    content = ai_message.content
    return {
        "text": content,
        "length": len(content),
        "word_count": len(content.split())
    }

chain = (
    prompt
    | model
    | RunnableLambda(extract_and_format)
)

result = chain.invoke({"text": "Hello", "language": "French"})
# {
#   "text": "Bonjour",
#   "length": 7,
#   "word_count": 1
# }
```

#### RunnableBranchï¼šæ¡ä»¶åˆ†æ”¯

**åŸºæœ¬ç”¨æ³•**ï¼š

```python
from langchain_core.runnables import RunnableBranch

def route_by_language(input_dict):
    """æ ¹æ®è¯­è¨€é€‰æ‹©ä¸åŒçš„é“¾"""
    language = input_dict.get("language", "").lower()
    
    if language == "french":
        return chain_french
    elif language == "spanish":
        return chain_spanish
    else:
        return chain_default

# åˆ›å»ºåˆ†æ”¯
branch = RunnableBranch(
    (lambda x: x["language"] == "french", chain_french),
    (lambda x: x["language"] == "spanish", chain_spanish),
    chain_default  # é»˜è®¤åˆ†æ”¯
)

result = branch.invoke({"text": "Hello", "language": "french"})
```

### 1.1.5 RunnableConfigï¼šé…ç½®ä¼ é€’

**RunnableConfig ç»“æ„**ï¼š

```python
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(
    # å›è°ƒç®¡ç†
    callbacks=[StdOutCallbackHandler()],
    
    # æ ‡ç­¾ä¸å…ƒæ•°æ®
    tags=["production", "translation"],
    metadata={"user_id": "12345", "session_id": "abc"},
    
    # è¿è¡Œæ—¶é…ç½®
    run_name="translation_task",
    max_concurrency=5,
    
    # é€’å½’é™åˆ¶
    recursion_limit=25
)
```

**åœ¨é“¾ä¸­ä¼ é€’é…ç½®**ï¼š

```python
# é…ç½®ä¼šè‡ªåŠ¨ä¼ é€’ç»™é“¾ä¸­çš„æ‰€æœ‰ç»„ä»¶
result = chain.invoke(
    {"text": "Hello", "language": "French"},
    config=config
)
```

**åŠ¨æ€é…ç½®**ï¼š

```python
def get_config_for_user(user_id: str) -> RunnableConfig:
    """æ ¹æ®ç”¨æˆ·IDç”Ÿæˆé…ç½®"""
    return RunnableConfig(
        tags=[f"user:{user_id}"],
        metadata={"user_id": user_id}
    )

# ä½¿ç”¨
user_config = get_config_for_user("user_123")
result = chain.invoke({"text": "Hello"}, config=user_config)
```

---

## 1.2 Language Models é›†æˆ

> **æ ¸å¿ƒæ¦‚å¿µ**ï¼šLangChain æ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹æä¾›å•†ï¼Œé€šè¿‡ç»Ÿä¸€æ¥å£å®ç°æ¨¡å‹æ— ç¼åˆ‡æ¢ã€‚

### 1.2.1 Chat Models vs LLMs

**æ¶æ„å¯¹æ¯”**ï¼š

| ç‰¹æ€§ | LLM | Chat Model |
|------|-----|------------|
| **è¾“å…¥æ ¼å¼** | å­—ç¬¦ä¸² | æ¶ˆæ¯åˆ—è¡¨ï¼ˆList[BaseMessage]ï¼‰ |
| **è¾“å‡ºæ ¼å¼** | å­—ç¬¦ä¸² | AIMessage |
| **å…¸å‹æ¨¡å‹** | GPT-3 text-davinci-003 | GPT-4, Claude-3, Llama-3 |
| **ä¸Šä¸‹æ–‡ç®¡ç†** | éœ€æ‰‹åŠ¨æ‹¼æ¥ | åŸç”Ÿæ”¯æŒè§’è‰²åŒºåˆ† |
| **Function Calling** | âŒ ä¸æ”¯æŒ | âœ… æ”¯æŒ |
| **æ¨èä½¿ç”¨** | âŒ å·²åºŸå¼ƒ | âœ… ä¼˜å…ˆä½¿ç”¨ |

**LLM ç¤ºä¾‹**ï¼ˆä¸æ¨èï¼Œä»…ä¾›ç†è§£ï¼‰ï¼š

```python
from langchain_openai import OpenAI

llm = OpenAI(
    model="gpt-3.5-turbo-instruct",  # æ—§å¼æ¨¡å‹
    temperature=0.7
)

# è¾“å…¥ï¼šçº¯æ–‡æœ¬å­—ç¬¦ä¸²
prompt = "Translate 'Hello' to French:"
response = llm.invoke(prompt)

print(response)  # "Bonjour"ï¼ˆå­—ç¬¦ä¸²ï¼‰
print(type(response))  # <class 'str'>
```

**Chat Model ç¤ºä¾‹**ï¼ˆæ¨èï¼‰ï¼š

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

chat = ChatOpenAI(
    model="gpt-4o",  # ç°ä»£å¯¹è¯æ¨¡å‹
    temperature=0.7
)

# è¾“å…¥ï¼šæ¶ˆæ¯åˆ—è¡¨
messages = [
    SystemMessage(content="You are a professional translator."),
    HumanMessage(content="Translate 'Hello' to French.")
]

response = chat.invoke(messages)

print(response.content)  # "Bonjour"
print(type(response))    # <class 'langchain_core.messages.ai.AIMessage'>
print(response.response_metadata)  # {'model': 'gpt-4o', 'usage': {...}}
```

**ä¸ºä»€ä¹ˆå¿…é¡»ä½¿ç”¨ Chat Modelï¼Ÿ**

1. **è§’è‰²åˆ†ç¦»**ï¼šSystemã€Humanã€AI æ¶ˆæ¯æ¸…æ™°åŒºåˆ†
2. **å¯¹è¯å†å²**ï¼šæ¶ˆæ¯åˆ—è¡¨å¤©ç„¶æ”¯æŒå¤šè½®å¯¹è¯
3. **å…ƒæ•°æ®ä¸°å¯Œ**ï¼šåŒ…å« token ä½¿ç”¨é‡ã€æ¨¡å‹ç‰ˆæœ¬ç­‰ä¿¡æ¯
4. **åŠŸèƒ½å®Œæ•´**ï¼šæ”¯æŒ Function/Tool Callingã€JSON Modeã€Streaming
5. **è¡Œä¸šæ ‡å‡†**ï¼šæ‰€æœ‰ç°ä»£ LLM éƒ½æ˜¯å¯¹è¯æ¨¡å‹è®­ç»ƒ

### 1.2.2 æ¨¡å‹æä¾›å•†é›†æˆ

#### OpenAIï¼ˆæ¨èï¼‰

**å®‰è£…**ï¼š

```bash
# å®‰è£… LangChain OpenAI é›†æˆ
pip install langchain-openai

# ç¯å¢ƒå˜é‡é…ç½®
export OPENAI_API_KEY="sk-..."
```

**åŸºç¡€ç”¨æ³•**ï¼š

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    # å¿…å¡«å‚æ•°
    model="gpt-4o",                    # æ¨¡å‹åç§°
    
    # API é…ç½®
    api_key="sk-...",                  # API å¯†é’¥ï¼ˆæˆ–ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    base_url="https://api.openai.com/v1",  # API ç«¯ç‚¹
    organization="org-...",            # ç»„ç»‡ IDï¼ˆå¯é€‰ï¼‰
    
    # ç”Ÿæˆå‚æ•°
    temperature=0.7,                   # æ¸©åº¦ï¼š0-2
    max_tokens=1000,                   # æœ€å¤§ç”Ÿæˆ token æ•°
    top_p=1.0,                         # æ ¸é‡‡æ ·ï¼š0-1
    frequency_penalty=0.0,             # é¢‘ç‡æƒ©ç½šï¼š-2 to 2
    presence_penalty=0.0,              # å­˜åœ¨æƒ©ç½šï¼š-2 to 2
    n=1,                               # ç”Ÿæˆç»“æœæ•°é‡
    
    # è¿æ¥å‚æ•°
    timeout=30,                        # æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    max_retries=2,                     # æœ€å¤§é‡è¯•æ¬¡æ•°
    request_timeout=60,                # å•æ¬¡è¯·æ±‚è¶…æ—¶
    
    # æµå¼å‚æ•°
    streaming=True,                    # å¯ç”¨æµå¼è¾“å‡º
    
    # é¢å¤–å‚æ•°
    model_kwargs={                     
        "seed": 42,                    # éšæœºç§å­ï¼ˆå¯å¤ç°ï¼‰
        "response_format": {"type": "json_object"}  # JSON æ¨¡å¼
    }
)
```

**å‚æ•°è¯¦è§£**ï¼š

##### temperatureï¼ˆæ¸©åº¦ï¼‰

æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§å’Œåˆ›é€ æ€§ã€‚

$$
\text{probability}(token_i) = \frac{\exp(logit_i / T)}{\sum_j \exp(logit_j / T)}
$$

å…¶ä¸­ $T$ ä¸ºæ¸©åº¦ã€‚

| Temperature | æ•ˆæœ | é€‚ç”¨åœºæ™¯ | ç¤ºä¾‹ |
|-------------|------|----------|------|
| `0` | å®Œå…¨ç¡®å®šæ€§ | ç¿»è¯‘ã€æ‘˜è¦ã€æ•°æ®æå– | "Translate 'cat' to French" â†’ æ€»æ˜¯ "chat" |
| `0.3-0.5` | è½»å¾®éšæœºæ€§ | é—®ç­”ã€åˆ†ç±» | å›ç­”ç¨æœ‰å˜åŒ–ä½†æ ¸å¿ƒä¸€è‡´ |
| `0.7-0.9` | å¹³è¡¡ | å¯¹è¯ã€å†…å®¹ç”Ÿæˆ | ChatGPT é»˜è®¤å€¼ |
| `1.0-1.5` | é«˜åˆ›é€ æ€§ | åˆ›æ„å†™ä½œã€å¤´è„‘é£æš´ | æ¯æ¬¡ç”Ÿæˆä¸åŒçš„æ•…äº‹ |
| `1.5-2.0` | æé«˜éšæœºæ€§ | æ¢ç´¢æ€§å®éªŒ | è¾“å‡ºå¯èƒ½ä¸è¿è´¯ |

**å®éªŒå¯¹æ¯”**ï¼š

```python
prompts = [HumanMessage(content="Write a 3-word slogan for a coffee shop.")]

# ä½æ¸©åº¦
model_det = ChatOpenAI(model="gpt-4o-mini", temperature=0)
print("T=0:", model_det.invoke(prompts).content)
# å¤šæ¬¡è¿è¡Œå‡ ä¹ç›¸åŒï¼š
# "Fresh Coffee Daily"
# "Fresh Coffee Daily"
# "Fresh Coffee Daily"

# é«˜æ¸©åº¦
model_creative = ChatOpenAI(model="gpt-4o-mini", temperature=1.5)
print("T=1.5:", model_creative.invoke(prompts).content)
# æ¯æ¬¡ä¸åŒï¼š
# "Brewed to Perfection"
# "Sip, Savor, Smile"
# "Awaken Your Senses"
```

##### top_pï¼ˆæ ¸é‡‡æ ·ï¼‰

åªè€ƒè™‘ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° $p$ çš„ token é›†åˆã€‚

$$
\text{tokens}_\text{considered} = \{t : \sum_{i=1}^{t} P(token_i) \leq p\}
$$

| Top P | æ•ˆæœ | ä¸ Temperature é…åˆ |
|-------|------|---------------------|
| `0.1` | åªè€ƒè™‘é«˜æ¦‚ç‡è¯ | é€‚åˆä½æ¸©åº¦ï¼Œç¡®ä¿è´¨é‡ |
| `0.5` | ä¸­ç­‰èŒƒå›´ | å¹³è¡¡å¤šæ ·æ€§å’Œè´¨é‡ |
| `0.9-1.0` | è€ƒè™‘å¤§éƒ¨åˆ†è¯ | é«˜æ¸©åº¦ï¼Œæœ€å¤§åŒ–åˆ›é€ æ€§ |

**æœ€ä½³å®è·µ**ï¼š
- ç¿»è¯‘/æ‘˜è¦ï¼š`temperature=0, top_p=1`
- å¯¹è¯ï¼š`temperature=0.7, top_p=0.9`
- åˆ›ä½œï¼š`temperature=1.0, top_p=0.95`

##### frequency_penalty ä¸ presence_penalty

**frequency_penalty**ï¼šæ ¹æ®è¯é¢‘é™ä½é‡å¤è¯çš„æ¦‚ç‡ã€‚

$$
\text{penalty} = \alpha \times \text{count}(token)
$$

**presence_penalty**ï¼šå¦‚æœè¯å·²å‡ºç°ï¼Œé™ä½å…¶æ¦‚ç‡ï¼ˆä¸è€ƒè™‘æ¬¡æ•°ï¼‰ã€‚

$$
\text{penalty} = \alpha \times \mathbb{I}(token \text{ appeared})
$$

| å‚æ•° | èŒƒå›´ | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| `frequency_penalty` | -2 to 2 | å‡å°‘é‡å¤è¯æ±‡ | ç”Ÿæˆå¤šæ ·åŒ–æ–‡æœ¬ |
| `presence_penalty` | -2 to 2 | é¼“åŠ±æ–°è¯é¢˜ | é¿å…åç¦»ä¸»é¢˜ |

**ç¤ºä¾‹**ï¼š

```python
# æ— æƒ©ç½š
model_none = ChatOpenAI(model="gpt-4o-mini", frequency_penalty=0)
# å¯èƒ½è¾“å‡ºï¼š
# "The cat is cute. The cat is fluffy. The cat is playful."

# é¢‘ç‡æƒ©ç½š
model_freq = ChatOpenAI(model="gpt-4o-mini", frequency_penalty=1.0)
# è¾“å‡ºï¼š
# "The cat is cute. It's fluffy. This feline is playful."
```

##### max_tokens

é™åˆ¶ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡ã€‚

**Token ä¼°ç®—**ï¼š
- è‹±æ–‡ï¼š1 token â‰ˆ 4 characters â‰ˆ 0.75 words
- ä¸­æ–‡ï¼š1 token â‰ˆ 1-2 characters

```python
# æ§åˆ¶è¾“å‡ºé•¿åº¦
short_model = ChatOpenAI(model="gpt-4o", max_tokens=50)
long_model = ChatOpenAI(model="gpt-4o", max_tokens=500)

prompt = [HumanMessage(content="Explain quantum physics.")]

print("Short:", short_model.invoke(prompt).content)
# çº¦ 50 tokensï¼Œç®€çŸ­å›ç­”

print("Long:", long_model.invoke(prompt).content)
# çº¦ 500 tokensï¼Œè¯¦ç»†è§£é‡Š
```

#### Anthropic Claude

**å®‰è£…**ï¼š

```bash
pip install langchain-anthropic
export ANTHROPIC_API_KEY="sk-ant-..."
```

**ç”¨æ³•**ï¼š

```python
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",  # æ¨èæ¨¡å‹
    temperature=0.7,
    max_tokens=4096,
    timeout=30,
    max_retries=2,
    api_key="sk-ant-..."
)

# ä½¿ç”¨æ–¹å¼ä¸ OpenAI å®Œå…¨ç›¸åŒ
response = model.invoke([HumanMessage(content="Hello")])
```

**Claude æ¨¡å‹é€‰æ‹©**ï¼š

| æ¨¡å‹ | ä¸Šä¸‹æ–‡çª—å£ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|-----------|------|----------|
| `claude-3-5-sonnet-20241022` | 200K | æœ€å¼ºèƒ½åŠ›ï¼Œæœ€æ–°ç‰ˆæœ¬ | å¤æ‚æ¨ç†ã€é•¿æ–‡æ¡£ |
| `claude-3-opus-20240229` | 200K | æœ€é«˜è´¨é‡ | éœ€è¦æœ€ä½³æ€§èƒ½ |
| `claude-3-haiku-20240307` | 200K | æœ€å¿«é€Ÿåº¦ï¼Œä½æˆæœ¬ | ç®€å•ä»»åŠ¡ã€é«˜å¹¶å‘ |

#### æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰

**å®‰è£…ä¸å¯åŠ¨**ï¼š

```bash
# å®‰è£… Ollama
curl -fsSL https://ollama.com/install.sh | sh

# ä¸‹è½½æ¨¡å‹
ollama pull llama3.2
ollama pull mistral

# å¯åŠ¨æœåŠ¡
ollama serve
```

**LangChain é›†æˆ**ï¼š

```python
from langchain_community.chat_models import ChatOllama

model = ChatOllama(
    model="llama3.2",                  # æœ¬åœ°æ¨¡å‹åç§°
    temperature=0.7,
    base_url="http://localhost:11434"  # Ollama API ç«¯ç‚¹
)

response = model.invoke([HumanMessage(content="Hello")])
```

**ä¼˜åŠ¿ä¸é™åˆ¶**ï¼š

| ç‰¹æ€§ | Ollama | OpenAI/Anthropic |
|------|--------|------------------|
| **æˆæœ¬** | âœ… å…è´¹ | âŒ æŒ‰ token è®¡è´¹ |
| **éšç§** | âœ… æœ¬åœ°éƒ¨ç½² | âŒ æ•°æ®ä¸Šä¼ äº‘ç«¯ |
| **æ€§èƒ½** | âš ï¸ å–å†³äºç¡¬ä»¶ | âœ… é«˜æ€§èƒ½ |
| **èƒ½åŠ›** | âš ï¸ è¾ƒå¼± | âœ… æœ€å¼º |
| **ç»´æŠ¤** | âŒ éœ€è‡ªè¡Œç®¡ç† | âœ… æ— éœ€ç»´æŠ¤ |

### 1.2.3 ç»Ÿä¸€æ¨¡å‹æ¥å£ï¼ˆå·¥å‚æ¨¡å¼ï¼‰

**é—®é¢˜**ï¼šå¦‚ä½•åœ¨ä¸åŒæä¾›å•†ä¹‹é—´æ— ç¼åˆ‡æ¢ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼šå·¥å‚å‡½æ•°

```python
from typing import Literal
from langchain_core.language_models import BaseChatModel
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_community.chat_models import ChatOllama

def get_model(
    provider: Literal["openai", "anthropic", "ollama"] = "openai",
    model_name: str | None = None,
    **kwargs
) -> BaseChatModel:
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®æä¾›å•†åˆ›å»ºæ¨¡å‹
    
    Args:
        provider: æ¨¡å‹æä¾›å•†ï¼ˆopenai/anthropic/ollamaï¼‰
        model_name: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼‰
        **kwargs: é¢å¤–å‚æ•°ï¼ˆtemperatureã€max_tokensç­‰ï¼‰
    
    Returns:
        BaseChatModel: ç»Ÿä¸€æ¥å£çš„èŠå¤©æ¨¡å‹
    
    Examples:
        >>> model = get_model("openai", temperature=0.5)
        >>> model = get_model("anthropic", model_name="claude-3-opus-20240229")
        >>> model = get_model("ollama", model_name="llama3.2")
    """
    # é»˜è®¤æ¨¡å‹æ˜ å°„
    default_models = {
        "openai": "gpt-4o",
        "anthropic": "claude-3-5-sonnet-20241022",
        "ollama": "llama3.2"
    }
    
    # ä½¿ç”¨æŒ‡å®šæ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
    model = model_name or default_models[provider]
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    if provider == "openai":
        return ChatOpenAI(model=model, **kwargs)
    elif provider == "anthropic":
        return ChatAnthropic(model=model, **kwargs)
    elif provider == "ollama":
        return ChatOllama(model=model, **kwargs)
    else:
        raise ValueError(f"Unknown provider: {provider}")

# ä½¿ç”¨ç¤ºä¾‹
model = get_model("anthropic", temperature=0.7)

# æ— éœ€ä¿®æ”¹å…¶ä»–ä»£ç 
chain = prompt | model | parser
result = chain.invoke({"text": "Hello", "language": "French"})
```

**ç¯å¢ƒå˜é‡é…ç½®**ï¼š

```python
import os

def get_model_from_env(**kwargs) -> BaseChatModel:
    """ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®"""
    provider = os.getenv("LLM_PROVIDER", "openai")
    model_name = os.getenv("LLM_MODEL")
    
    return get_model(provider, model_name, **kwargs)

# .env æ–‡ä»¶
# LLM_PROVIDER=anthropic
# LLM_MODEL=claude-3-5-sonnet-20241022

model = get_model_from_env(temperature=0.7)
```

### 1.2.4 Callbacks ä¸ç›‘æ§

<div data-component="CallbackFlow"></div>

#### æ ‡å‡†è¾“å‡º Callback

```python
from langchain.callbacks import StdOutCallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

model = ChatOpenAI(
    model="gpt-4o-mini",
    callbacks=[StdOutCallbackHandler()],
    verbose=True
)

response = model.invoke([HumanMessage(content="Hello")])

# è¾“å‡ºï¼š
# > Entering new ChatOpenAI chain...
# > Prompt: [HumanMessage(content='Hello')]
# > Response: AIMessage(content='Hi there! How can I assist you today?')
# > Finished chain.
```

#### è‡ªå®šä¹‰ Callbackï¼šToken è®¡æ•°å™¨

```python
from langchain.callbacks.base import BaseCallbackHandler
from langchain_core.outputs import LLMResult

class TokenCounterCallback(BaseCallbackHandler):
    """ç»Ÿè®¡ token ä½¿ç”¨é‡"""
    
    def __init__(self):
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.total_tokens = 0
        self.total_cost = 0.0
    
    def on_llm_start(self, serialized: dict, prompts: list[str], **kwargs):
        """LLM å¼€å§‹æ—¶è§¦å‘"""
        print(f"ğŸš€ Starting LLM with {len(prompts)} prompts")
    
    def on_llm_end(self, response: LLMResult, **kwargs):
        """LLM ç»“æŸæ—¶è§¦å‘"""
        # æå– token ä½¿ç”¨ä¿¡æ¯
        if response.llm_output and "token_usage" in response.llm_output:
            usage = response.llm_output["token_usage"]
            prompt_tokens = usage.get("prompt_tokens", 0)
            completion_tokens = usage.get("completion_tokens", 0)
            total_tokens = usage.get("total_tokens", 0)
            
            self.prompt_tokens += prompt_tokens
            self.completion_tokens += completion_tokens
            self.total_tokens += total_tokens
            
            # æˆæœ¬è®¡ç®—ï¼ˆGPT-4o ä»·æ ¼ï¼‰
            cost = (prompt_tokens * 0.0025 + completion_tokens * 0.01) / 1000
            self.total_cost += cost
            
            print(f"ğŸ“Š Tokens: {prompt_tokens} prompt + {completion_tokens} completion = {total_tokens} total")
            print(f"ğŸ’° Cost: ${cost:.6f}")
    
    def on_llm_error(self, error: Exception, **kwargs):
        """LLM å‡ºé”™æ—¶è§¦å‘"""
        print(f"âŒ Error: {error}")
    
    def reset(self):
        """é‡ç½®è®¡æ•°å™¨"""
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.total_tokens = 0
        self.total_cost = 0.0

# ä½¿ç”¨
counter = TokenCounterCallback()
model = ChatOpenAI(model="gpt-4o", callbacks=[counter])

model.invoke([HumanMessage(content="Hello")])
# è¾“å‡º:
# ğŸš€ Starting LLM with 1 prompts
# ğŸ“Š Tokens: 8 prompt + 12 completion = 20 total
# ğŸ’° Cost: $0.000140

# æŸ¥çœ‹ç´¯è®¡ç»Ÿè®¡
print(f"Total tokens: {counter.total_tokens}")
print(f"Total cost: ${counter.total_cost:.6f}")
```

#### è‡ªå®šä¹‰ Callbackï¼šå»¶è¿Ÿç›‘æ§

```python
import time
from langchain.callbacks.base import BaseCallbackHandler

class LatencyMonitorCallback(BaseCallbackHandler):
    """ç›‘æ§ LLM è°ƒç”¨å»¶è¿Ÿ"""
    
    def __init__(self):
        self.start_time = None
        self.latencies = []
    
    def on_llm_start(self, serialized, prompts, **kwargs):
        """è®°å½•å¼€å§‹æ—¶é—´"""
        self.start_time = time.time()
    
    def on_llm_end(self, response, **kwargs):
        """è®¡ç®—å»¶è¿Ÿ"""
        if self.start_time:
            latency = time.time() - self.start_time
            self.latencies.append(latency)
            print(f"â±ï¸  Latency: {latency:.2f}s")
            self.start_time = None
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.latencies:
            return {"count": 0}
        
        return {
            "count": len(self.latencies),
            "avg": sum(self.latencies) / len(self.latencies),
            "min": min(self.latencies),
            "max": max(self.latencies)
        }

# ä½¿ç”¨
latency_monitor = LatencyMonitorCallback()
model = ChatOpenAI(model="gpt-4o-mini", callbacks=[latency_monitor])

for i in range(5):
    model.invoke([HumanMessage(content=f"Say {i}")])

print("\nç»Ÿè®¡ä¿¡æ¯ï¼š", latency_monitor.get_stats())
# {
#   "count": 5,
#   "avg": 1.23,
#   "min": 0.98,
#   "max": 1.56
# }
```

#### ç»„åˆå¤šä¸ª Callbacks

```python
# åŒæ—¶ä½¿ç”¨å¤šä¸ª callback
model = ChatOpenAI(
    model="gpt-4o-mini",
    callbacks=[
        StdOutCallbackHandler(),
        TokenCounterCallback(),
        LatencyMonitorCallback()
    ]
)
```

---

## 1.3 Prompt Templates

> **æ ¸å¿ƒä»·å€¼**ï¼šPrompt Templates å°†æç¤ºè¯ä»ä»£ç ä¸­è§£è€¦ï¼Œå®ç°å¤ç”¨ã€ç‰ˆæœ¬ç®¡ç†å’Œåä½œã€‚

### 1.3.1 PromptTemplate åŸºç¡€

#### åˆ›å»ºæ–¹å¼

**æ–¹å¼ 1ï¼šfrom_template()ï¼ˆæ¨èï¼‰**

```python
from langchain_core.prompts import PromptTemplate

template = PromptTemplate.from_template(
    "Translate the following text to {language}: {text}"
)

# æ ¼å¼åŒ–
prompt = template.format(language="French", text="Hello")
print(prompt)
# "Translate the following text to French: Hello"

# ä½œä¸º Runnable ä½¿ç”¨
result = template.invoke({"language": "Spanish", "text": "Goodbye"})
print(result.to_string())
# "Translate the following text to Spanish: Goodbye"
```

**æ–¹å¼ 2ï¼šæ„é€ å‡½æ•°**

```python
template = PromptTemplate(
    input_variables=["language", "text"],
    template="Translate the following text to {language}: {text}"
)

# éªŒè¯å˜é‡
print(template.input_variables)  # ['language', 'text']
```

**æ–¹å¼ 3ï¼šfrom_file()ï¼ˆå¤§å‹æç¤ºï¼‰**

```python
# prompts/translate.txt:
# Translate the following text to {language}:
#
# Text: {text}
#
# Translation:

template = PromptTemplate.from_file(
    "prompts/translate.txt",
    input_variables=["language", "text"]
)
```

#### å˜é‡ç±»å‹

**å•å˜é‡**ï¼š

```python
template = PromptTemplate.from_template("Say {word}")
result = template.invoke({"word": "hello"})
```

**å¤šå˜é‡**ï¼š

```python
template = PromptTemplate.from_template(
    "Translate '{text}' from {source_lang} to {target_lang}"
)

result = template.invoke({
    "text": "Bonjour",
    "source_lang": "French",
    "target_lang": "English"
})
```

**å¯é€‰å˜é‡**ï¼ˆä½¿ç”¨ partialï¼‰ï¼š

```python
template = PromptTemplate.from_template(
    "You are a {role}. {instruction}"
)

# å›ºå®šè§’è‰²
assistant_template = template.partial(role="helpful assistant")

# åç»­åªéœ€æä¾› instruction
result = assistant_template.invoke({"instruction": "Explain AI."})
```

#### éƒ¨åˆ†å¡«å……ï¼ˆPartialï¼‰

**é™æ€éƒ¨åˆ†å¡«å……**ï¼š

```python
from datetime import datetime

template = PromptTemplate.from_template(
    "Today is {date}. {question}"
)

# å›ºå®šæ—¥æœŸ
dated_template = template.partial(date="2024-01-29")

# æ¯æ¬¡åªéœ€æä¾›é—®é¢˜
result = dated_template.invoke({"question": "What is the weather?"})
```

**åŠ¨æ€éƒ¨åˆ†å¡«å……ï¼ˆå‡½æ•°ï¼‰**ï¼š

```python
def get_current_date():
    """æ¯æ¬¡è°ƒç”¨æ—¶è·å–å½“å‰æ—¥æœŸ"""
    return datetime.now().strftime("%Y-%m-%d")

template = PromptTemplate.from_template(
    "Current date: {date}. {question}"
)

# ä½¿ç”¨å‡½æ•°åŠ¨æ€å¡«å……
dynamic_template = template.partial(date=get_current_date)

# æ¯æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨è·å–æœ€æ–°æ—¥æœŸ
result1 = dynamic_template.invoke({"question": "What day is it?"})
# "Current date: 2024-01-29. What day is it?"

# ä¸€å¤©åè°ƒç”¨
result2 = dynamic_template.invoke({"question": "What day is it?"})
# "Current date: 2024-01-30. What day is it?"
```

### 1.3.2 ChatPromptTemplateï¼šå¯¹è¯æ¨¡æ¿

<div data-component="PromptComposer"></div>

#### åŸºç¡€ç”¨æ³•

```python
from langchain_core.prompts import ChatPromptTemplate

# æ–¹å¼1ï¼šfrom_messagesï¼ˆæ¨èï¼‰
template = ChatPromptTemplate.from_messages([
    ("system", "You are a {role}."),
    ("human", "{user_input}")
])

# æ ¼å¼åŒ–
messages = template.invoke({
    "role": "translator",
    "user_input": "Translate 'Hello' to French"
})

print(messages)
# [
#   SystemMessage(content='You are a translator.'),
#   HumanMessage(content="Translate 'Hello' to French")
# ]
```

**æ–¹å¼2ï¼šä½¿ç”¨æ¶ˆæ¯ç±»**

```python
from langchain_core.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)

template = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template("You are a {role}."),
    HumanMessagePromptTemplate.from_template("{user_input}")
])
```

#### æ¶ˆæ¯è§’è‰²

**æ”¯æŒçš„è§’è‰²ç±»å‹**ï¼š

| è§’è‰² | å­—ç¬¦ä¸²è¡¨ç¤º | ç±» | ç”¨é€” |
|------|-----------|-----|------|
| System | `"system"` | `SystemMessage` | ç³»ç»ŸæŒ‡ä»¤ã€è§’è‰²è®¾å®š |
| Human | `"human"`, `"user"` | `HumanMessage` | ç”¨æˆ·è¾“å…¥ |
| AI | `"ai"`, `"assistant"` | `AIMessage` | AI å›å¤ï¼ˆå†å²ï¼‰ |
| Tool | `"tool"` | `ToolMessage` | å·¥å…·è°ƒç”¨ç»“æœ |

**å®Œæ•´ç¤ºä¾‹**ï¼š

```python
template = ChatPromptTemplate.from_messages([
    ("system", "You are an AI assistant."),
    ("human", "What is {topic}?"),
    ("ai", "Let me explain {topic} in detail."),
    ("human", "Can you summarize?")
])

messages = template.invoke({"topic": "quantum physics"})
# [
#   SystemMessage(content='You are an AI assistant.'),
#   HumanMessage(content='What is quantum physics?'),
#   AIMessage(content='Let me explain quantum physics in detail.'),
#   HumanMessage(content='Can you summarize?')
# ]
```

#### å¤šè½®å¯¹è¯æ¨¡æ¿

**åœºæ™¯ï¼šFew-Shot ç¤ºä¾‹**

```python
template = ChatPromptTemplate.from_messages([
    ("system", "You are a sentiment analyzer. Classify text as Positive, Negative, or Neutral."),
    ("human", "I love this product!"),
    ("ai", "Positive"),
    ("human", "This is terrible."),
    ("ai", "Negative"),
    ("human", "It's okay."),
    ("ai", "Neutral"),
    ("human", "{text}")  # å®é™…å¾…åˆ†ç±»æ–‡æœ¬
])

result = template.invoke({"text": "Amazing experience!"})
```

### 1.3.3 Few-Shot Prompting

#### FewShotPromptTemplateï¼ˆæ—§å¼ï¼Œä¸æ¨èï¼‰

```python
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

# å®šä¹‰ç¤ºä¾‹
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "hot", "output": "cold"}
]

# ç¤ºä¾‹æ¨¡æ¿
example_template = PromptTemplate.from_template(
    "Input: {input}\nOutput: {output}"
)

# Few-Shot æ¨¡æ¿
few_shot_template = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_template,
    prefix="Give the opposite of the word.",
    suffix="Input: {word}\nOutput:",
    input_variables=["word"]
)

prompt = few_shot_template.format(word="big")
print(prompt)
# Give the opposite of the word.
# Input: happy
# Output: sad
# Input: tall
# Output: short
# Input: hot
# Output: cold
# Input: big
# Output:
```

#### FewShotChatMessagePromptTemplateï¼ˆæ¨èï¼‰

```python
from langchain_core.prompts import (
    ChatPromptTemplate,
    FewShotChatMessagePromptTemplate
)

# å®šä¹‰ç¤ºä¾‹ï¼ˆä½¿ç”¨æ¶ˆæ¯æ ¼å¼ï¼‰
examples = [
    {"input": "2+2", "output": "4"},
    {"input": "3+5", "output": "8"}
]

# ç¤ºä¾‹æç¤ºæ¨¡æ¿
example_prompt = ChatPromptTemplate.from_messages([
    ("human", "{input}"),
    ("ai", "{output}")
])

# Few-Shot æ¨¡æ¿
few_shot_prompt = FewShotChatMessagePromptTemplate(
    examples=examples,
    example_prompt=example_prompt
)

# æœ€ç»ˆæ¨¡æ¿
final_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a math calculator."),
    few_shot_prompt,
    ("human", "{input}")
])

# ä½¿ç”¨
messages = final_prompt.invoke({"input": "5+7"})
# [
#   SystemMessage(content='You are a math calculator.'),
#   HumanMessage(content='2+2'),
#   AIMessage(content='4'),
#   HumanMessage(content='3+5'),
#   AIMessage(content='8'),
#   HumanMessage(content='5+7')
# ]
```

#### åŠ¨æ€ç¤ºä¾‹é€‰æ‹©ï¼ˆExampleSelectorï¼‰

**åœºæ™¯**ï¼šæ ¹æ®è¾“å…¥é€‰æ‹©æœ€ç›¸å…³çš„ç¤ºä¾‹

```python
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

# å®šä¹‰å¤§é‡ç¤ºä¾‹
examples = [
    {"input": "happy", "output": "ğŸ˜Š"},
    {"input": "sad", "output": "ğŸ˜¢"},
    {"input": "angry", "output": "ğŸ˜ "},
    {"input": "excited", "output": "ğŸ‰"},
    {"input": "tired", "output": "ğŸ˜´"}
]

# åˆ›å»ºç¤ºä¾‹é€‰æ‹©å™¨
example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples,
    OpenAIEmbeddings(),  # ä½¿ç”¨ embeddings è®¡ç®—ç›¸ä¼¼åº¦
    FAISS,               # å‘é‡æ•°æ®åº“
    k=2                  # é€‰æ‹©æœ€ç›¸å…³çš„ 2 ä¸ªç¤ºä¾‹
)

# Few-Shot æ¨¡æ¿
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_selector=example_selector,  # ä½¿ç”¨é€‰æ‹©å™¨
    example_prompt=ChatPromptTemplate.from_messages([
        ("human", "{input}"),
        ("ai", "{output}")
    ])
)

# ä½¿ç”¨
final_prompt = ChatPromptTemplate.from_messages([
    ("system", "Convert words to emojis."),
    few_shot_prompt,
    ("human", "{input}")
])

# è¾“å…¥ "joyful" ä¼šè‡ªåŠ¨é€‰æ‹© "happy" å’Œ "excited" ä½œä¸ºç¤ºä¾‹
messages = final_prompt.invoke({"input": "joyful"})
```

### 1.3.4 LangChain Hub é›†æˆ

**LangChain Hub** æ˜¯ä¸€ä¸ªæç¤ºè¯ç®¡ç†å¹³å°ï¼Œç±»ä¼¼ GitHub for Promptsã€‚

#### å®‰è£…ä¸é…ç½®

```bash
pip install langchainhub
export LANGCHAIN_API_KEY="ls__..."
```

#### æ‹‰å–å…¬å¼€æç¤º

```python
from langchain import hub

# æ‹‰å–çƒ­é—¨æç¤º
prompt = hub.pull("rlm/rag-prompt")

# æŸ¥çœ‹å†…å®¹
print(prompt.template)

# ä½¿ç”¨
chain = prompt | model | parser
result = chain.invoke({
    "context": "LangChain is a framework...",
    "question": "What is LangChain?"
})
```

#### æ¨é€è‡ªå®šä¹‰æç¤º

```python
from langchain_core.prompts import ChatPromptTemplate

# åˆ›å»ºè‡ªå®šä¹‰æç¤º
my_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are an expert {domain} consultant."),
    ("human", "{question}")
])

# æ¨é€åˆ° Hubï¼ˆéœ€è¦ç™»å½•ï¼‰
hub.push("my-username/expert-consultant", my_prompt)

# ä»–äººå¯ä»¥æ‹‰å–
prompt = hub.pull("my-username/expert-consultant")
```

#### ç‰ˆæœ¬ç®¡ç†

```python
# æ‹‰å–ç‰¹å®šç‰ˆæœ¬
prompt_v1 = hub.pull("rlm/rag-prompt:v1")
prompt_v2 = hub.pull("rlm/rag-prompt:v2")

# æ‹‰å–æœ€æ–°ç‰ˆæœ¬
prompt_latest = hub.pull("rlm/rag-prompt")
```

---

## 1.4 Output Parsers

> **æ ¸å¿ƒä»·å€¼**ï¼šå°† LLM çš„æ–‡æœ¬è¾“å‡ºè½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®ï¼ˆJSONã€Python å¯¹è±¡ç­‰ï¼‰ï¼Œå®ç°ç±»å‹å®‰å…¨ã€‚

### 1.4.1 StrOutputParserï¼šæ–‡æœ¬æå–

**æœ€ç®€å•çš„è§£æå™¨**ï¼šä» AIMessage æå– `.content` å­—æ®µã€‚

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

model = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

# ä¸ä½¿ç”¨ parser
response = model.invoke([HumanMessage(content="Say hi")])
print(type(response))  # <class 'langchain_core.messages.ai.AIMessage'>
print(response.content)  # "Hi!"

# ä½¿ç”¨ parser
chain = model | parser
result = chain.invoke([HumanMessage(content="Say hi")])
print(type(result))  # <class 'str'>
print(result)  # "Hi!"
```

**é€‚ç”¨åœºæ™¯**ï¼š
- ç®€å•æ–‡æœ¬ç”Ÿæˆ
- ä¸éœ€è¦ç»“æ„åŒ–è¾“å‡º
- å¿«é€ŸåŸå‹å¼€å‘

### 1.4.2 JsonOutputParserï¼šJSON è§£æ

**åŸºæœ¬ç”¨æ³•**ï¼š

```python
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

parser = JsonOutputParser()

prompt = ChatPromptTemplate.from_messages([
    ("system", "Output your response as valid JSON."),
    ("human", "List 3 colors with their hex codes.")
])

chain = prompt | model | parser

result = chain.invoke({})
print(type(result))  # <class 'dict'>
print(result)
# {
#   "colors": [
#     {"name": "red", "hex": "#FF0000"},
#     {"name": "green", "hex": "#00FF00"},
#     {"name": "blue", "hex": "#0000FF"}
#   ]
# }

# å¯ä»¥ç›´æ¥è®¿é—®
print(result["colors"][0]["name"])  # "red"
```

**é”™è¯¯å¤„ç†**ï¼š

```python
try:
    result = chain.invoke({})
except Exception as e:
    print(f"Parsing failed: {e}")
    # å¯ä»¥é‡è¯•æˆ–ä½¿ç”¨é»˜è®¤å€¼
```

### 1.4.3 PydanticOutputParserï¼šç±»å‹å®‰å…¨

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
- âœ… ç±»å‹æ£€æŸ¥ï¼šIDE è‡ªåŠ¨è¡¥å…¨å’Œç±»å‹æç¤º
- âœ… æ•°æ®éªŒè¯ï¼šè‡ªåŠ¨éªŒè¯å­—æ®µç±»å‹å’Œçº¦æŸ
- âœ… æ–‡æ¡£ç”Ÿæˆï¼šè‡ªåŠ¨ç”Ÿæˆ schema è¯´æ˜

**åŸºç¡€ç”¨æ³•**ï¼š

```python
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field, field_validator

# 1. å®šä¹‰æ•°æ®æ¨¡å‹
class Person(BaseModel):
    """Person information"""
    
    name: str = Field(description="Person's full name")
    age: int = Field(description="Person's age in years", ge=0, le=150)
    occupation: str = Field(description="Person's current job")
    email: str | None = Field(default=None, description="Email address")
    
    @field_validator("email")
    def validate_email(cls, v):
        """éªŒè¯é‚®ç®±æ ¼å¼"""
        if v and "@" not in v:
            raise ValueError("Invalid email")
        return v

# 2. åˆ›å»ºè§£æå™¨
parser = PydanticOutputParser(pydantic_object=Person)

# 3. è·å–æ ¼å¼è¯´æ˜
format_instructions = parser.get_format_instructions()
print(format_instructions)
# The output should be formatted as a JSON instance that conforms to the JSON schema below.
# 
# {"properties": {"name": {"description": "Person's full name", "title": "Name", "type": "string"}, ...}}
# 
# Here is the output schema:
# ```
# {"name": "string", "age": "integer", "occupation": "string", "email": "string"}
# ```

# 4. åœ¨æç¤ºä¸­ä½¿ç”¨
prompt = ChatPromptTemplate.from_messages([
    ("system", "Extract person information from the text.\n{format_instructions}"),
    ("human", "{input}")
])

chain = prompt | model | parser

# 5. è°ƒç”¨
result = chain.invoke({
    "format_instructions": format_instructions,
    "input": "John Doe is a 30-year-old software engineer at Google. His email is john@gmail.com."
})

print(type(result))  # <class '__main__.Person'>
print(result)
# Person(name='John Doe', age=30, occupation='software engineer', email='john@gmail.com')

# 6. è®¿é—®å±æ€§ï¼ˆç±»å‹å®‰å…¨ï¼‰
print(result.name)  # "John Doe" (IDE æœ‰è‡ªåŠ¨è¡¥å…¨)
print(result.age + 5)  # 35 (ç±»å‹æ£€æŸ¥é€šè¿‡)
```

**å¤æ‚åµŒå¥—ç»“æ„**ï¼š

```python
from typing import List
from pydantic import BaseModel, Field

class Address(BaseModel):
    """Address information"""
    street: str
    city: str
    country: str
    postal_code: str

class Company(BaseModel):
    """Company information"""
    name: str
    industry: str
    employees: int = Field(ge=1)

class Person(BaseModel):
    """Complete person profile"""
    name: str
    age: int
    addresses: List[Address] = Field(default_factory=list)
    company: Company | None = None

parser = PydanticOutputParser(pydantic_object=Person)

prompt = ChatPromptTemplate.from_messages([
    ("system", "Extract detailed person information.\n{format_instructions}"),
    ("human", "{text}")
])

chain = prompt | model | parser

result = chain.invoke({
    "format_instructions": parser.get_format_instructions(),
    "text": """
    Alice Johnson, 28, works at TechCorp (a software company with 500 employees).
    She lives at 123 Main St, San Francisco, CA 94101, USA.
    She also has a vacation home at 456 Beach Rd, Miami, FL 33101, USA.
    """
})

print(result.name)  # "Alice Johnson"
print(result.company.name)  # "TechCorp"
print(len(result.addresses))  # 2
print(result.addresses[0].city)  # "San Francisco"
```

**éªŒè¯ä¸é”™è¯¯å¤„ç†**ï¼š

```python
from pydantic import ValidationError

try:
    result = chain.invoke({
        "format_instructions": parser.get_format_instructions(),
        "input": "Invalid data"
    })
except ValidationError as e:
    print("Validation failed:")
    print(e.json())
    # [
    #   {
    #     "loc": ["age"],
    #     "msg": "field required",
    #     "type": "value_error.missing"
    #   }
    # ]
```

### 1.4.4 CommaSeparatedListOutputParserï¼šåˆ—è¡¨è§£æ

```python
from langchain_core.output_parsers import CommaSeparatedListOutputParser

parser = CommaSeparatedListOutputParser()

prompt = ChatPromptTemplate.from_messages([
    ("system", "Output a comma-separated list."),
    ("human", "List 5 {category}.")
])

chain = prompt | model | parser

result = chain.invoke({"category": "programming languages"})
print(type(result))  # <class 'list'>
print(result)  # ['Python', 'JavaScript', 'Java', 'C++', 'Go']

# ç›´æ¥ä½¿ç”¨
for lang in result:
    print(f"- {lang}")
```

### 1.4.5 è‡ªå®šä¹‰ Output Parser

**åœºæ™¯**ï¼šè§£æç‰¹æ®Šæ ¼å¼è¾“å‡º

```python
from langchain_core.output_parsers import BaseOutputParser
from typing import List
import re

class BulletPointParser(BaseOutputParser[List[str]]):
    """è§£æé¡¹ç›®ç¬¦å·åˆ—è¡¨"""
    
    def parse(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–é¡¹ç›®ç¬¦å·åˆ—è¡¨"""
        # åŒ¹é… "- item" æˆ– "* item" æ ¼å¼
        pattern = r'^[\-\*]\s+(.+)$'
        lines = text.split('\n')
        items = []
        
        for line in lines:
            match = re.match(pattern, line.strip())
            if match:
                items.append(match.group(1))
        
        return items
    
    @property
    def _type(self) -> str:
        return "bullet_point_parser"

# ä½¿ç”¨
parser = BulletPointParser()

prompt = ChatPromptTemplate.from_messages([
    ("system", "Output a bullet-point list using '-' or '*'."),
    ("human", "List benefits of exercise.")
])

chain = prompt | model | parser

result = chain.invoke({})
print(result)
# ['Improves cardiovascular health', 'Reduces stress', 'Increases energy']
```

**å¤„ç†è§£æå¤±è´¥**ï¼š

```python
from langchain_core.output_parsers import OutputParserException

class SafeBulletPointParser(BaseOutputParser[List[str]]):
    """å¸¦é”™è¯¯å¤„ç†çš„è§£æå™¨"""
    
    def parse(self, text: str) -> List[str]:
        items = []
        for line in text.split('\n'):
            match = re.match(r'^[\-\*]\s+(.+)$', line.strip())
            if match:
                items.append(match.group(1))
        
        if not items:
            raise OutputParserException(
                f"No bullet points found in output: {text}",
                llm_output=text
            )
        
        return items

# ä½¿ç”¨
try:
    result = chain.invoke({})
except OutputParserException as e:
    print(f"Parsing failed: {e}")
    # å¯ä»¥é‡è¯•æˆ–ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
```

---

## 1.5 Message ä¸ Conversation

> **æ ¸å¿ƒæ¦‚å¿µ**ï¼šMessage æ˜¯ LangChain å¯¹è¯ç³»ç»Ÿçš„åŸºç¡€ï¼Œç†è§£æ¶ˆæ¯ç±»å‹å¯¹äºæ„å»ºå¤šè½®å¯¹è¯è‡³å…³é‡è¦ã€‚

### 1.5.1 æ¶ˆæ¯ç±»å‹ç³»ç»Ÿ

<div data-component="MessageFlowDiagram"></div>

#### å®Œæ•´æ¶ˆæ¯ç±»å‹

```python
from langchain_core.messages import (
    BaseMessage,       # æŠ½è±¡åŸºç±»
    SystemMessage,     # ç³»ç»ŸæŒ‡ä»¤
    HumanMessage,      # ç”¨æˆ·è¾“å…¥
    AIMessage,         # AI å›å¤
    ToolMessage,       # å·¥å…·è°ƒç”¨ç»“æœ
    ChatMessage,       # è‡ªå®šä¹‰è§’è‰²
    FunctionMessage    # å·²åºŸå¼ƒï¼Œä½¿ç”¨ ToolMessage
)

# 1. SystemMessageï¼šè®¾å®š AI è¡Œä¸º
sys_msg = SystemMessage(content="You are a helpful assistant.")

# 2. HumanMessageï¼šç”¨æˆ·è¾“å…¥
human_msg = HumanMessage(content="What is LangChain?")

# 3. AIMessageï¼šAI å›å¤ï¼ˆå¸¦å…ƒæ•°æ®ï¼‰
ai_msg = AIMessage(
    content="LangChain is a framework for building LLM applications.",
    additional_kwargs={"model": "gpt-4o", "finish_reason": "stop"}
)

# 4. ToolMessageï¼šå·¥å…·è°ƒç”¨ç»“æœ
tool_msg = ToolMessage(
    content="Temperature: 72Â°F",
    tool_call_id="call_abc123"
)

# 5. ChatMessageï¼šè‡ªå®šä¹‰è§’è‰²
custom_msg = ChatMessage(
    content="I am a custom role.",
    role="narrator"
)
```

#### æ¶ˆæ¯å±æ€§

```python
# æ‰€æœ‰æ¶ˆæ¯å…±æœ‰å±æ€§
msg = HumanMessage(content="Hello")

print(msg.content)       # "Hello"
print(msg.type)          # "human"
print(msg.additional_kwargs)  # {}

# AIMessage ç‰¹æœ‰å±æ€§
ai_msg = AIMessage(
    content="Hi there!",
    response_metadata={
        "token_usage": {"total_tokens": 20},
        "model_name": "gpt-4o"
    }
)

print(ai_msg.response_metadata)
```

### 1.5.2 å¯¹è¯å†å²ç®¡ç†

#### ç®€å•å¯¹è¯ç±»

```python
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage
from langchain_openai import ChatOpenAI

class SimpleConversation:
    """ç®€å•å¯¹è¯ç®¡ç†å™¨"""
    
    def __init__(self, system_message: str, model_name: str = "gpt-4o-mini"):
        self.messages: List[BaseMessage] = [
            SystemMessage(content=system_message)
        ]
        self.model = ChatOpenAI(model=model_name)
    
    def add_user_message(self, content: str):
        """æ·»åŠ ç”¨æˆ·æ¶ˆæ¯"""
        self.messages.append(HumanMessage(content=content))
    
    def add_ai_message(self, content: str):
        """æ·»åŠ  AI æ¶ˆæ¯"""
        self.messages.append(AIMessage(content=content))
    
    def get_messages(self) -> List[BaseMessage]:
        """è·å–æ‰€æœ‰æ¶ˆæ¯"""
        return self.messages
    
    def chat(self, user_input: str) -> str:
        """å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤"""
        self.add_user_message(user_input)
        response = self.model.invoke(self.messages)
        self.add_ai_message(response.content)
        return response.content
    
    def clear(self, keep_system: bool = True):
        """æ¸…é™¤å†å²ï¼ˆå¯é€‰ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼‰"""
        if keep_system:
            self.messages = self.messages[:1]
        else:
            self.messages = []
    
    def get_history(self) -> str:
        """æ ¼å¼åŒ–æ˜¾ç¤ºå¯¹è¯å†å²"""
        lines = []
        for msg in self.messages:
            role = msg.type.capitalize()
            lines.append(f"{role}: {msg.content}")
        return "\n".join(lines)

# ä½¿ç”¨
conv = SimpleConversation("You are a Python tutor.")

response1 = conv.chat("How do I sort a list?")
print(response1)
# "You can use the sorted() function or the .sort() method..."

response2 = conv.chat("What's the difference?")
print(response2)
# "sorted() returns a new list, while .sort() modifies in-place..."

print("\n--- History ---")
print(conv.get_history())
# System: You are a Python tutor.
# Human: How do I sort a list?
# AI: You can use the sorted() function or the .sort() method...
# Human: What's the difference?
# AI: sorted() returns a new list, while .sort() modifies in-place...
```

### 1.5.3 æ¶ˆæ¯è¿‡æ»¤ä¸è½¬æ¢

#### é™åˆ¶æ¶ˆæ¯å†å²é•¿åº¦

```python
def trim_messages(
    messages: List[BaseMessage],
    max_tokens: int = 2000,
    keep_system: bool = True
) -> List[BaseMessage]:
    """
    ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯ï¼Œç¡®ä¿ä¸è¶…è¿‡ token é™åˆ¶
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        max_tokens: æœ€å¤§ token æ•°
        keep_system: æ˜¯å¦ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
    
    Returns:
        ä¿®å‰ªåçš„æ¶ˆæ¯åˆ—è¡¨
    """
    from langchain.text_splitter import TokenTextSplitter
    
    # ç®€åŒ–ç‰ˆï¼šä¿ç•™æœ€å N æ¡æ¶ˆæ¯
    max_messages = 20
    
    if keep_system and len(messages) > 0 and isinstance(messages[0], SystemMessage):
        system_msg = [messages[0]]
        other_messages = messages[1:]
        
        if len(other_messages) > max_messages:
            return system_msg + other_messages[-max_messages:]
        else:
            return messages
    else:
        if len(messages) > max_messages:
            return messages[-max_messages:]
        else:
            return messages

# ä½¿ç”¨
class ConversationWithTrim(SimpleConversation):
    def chat(self, user_input: str) -> str:
        self.add_user_message(user_input)
        
        # ä¿®å‰ªå†å²
        self.messages = trim_messages(self.messages)
        
        response = self.model.invoke(self.messages)
        self.add_ai_message(response.content)
        return response.content
```

#### æ¶ˆæ¯æ ¼å¼è½¬æ¢

```python
def messages_to_openai_format(messages: List[BaseMessage]) -> List[dict]:
    """è½¬æ¢ä¸º OpenAI API æ ¼å¼"""
    return [
        {
            "role": msg.type if msg.type != "ai" else "assistant",
            "content": msg.content
        }
        for msg in messages
    ]

# ä½¿ç”¨
api_messages = messages_to_openai_format(conv.get_messages())
print(api_messages)
# [
#   {"role": "system", "content": "You are a Python tutor."},
#   {"role": "user", "content": "How do I sort a list?"},
#   {"role": "assistant", "content": "You can use sorted()..."}
# ]
```

---

## 1.6 é«˜çº§ä¸»é¢˜

### 1.6.1 RunnableConfig æ·±åº¦è§£æ

**å®Œæ•´é…ç½®ç»“æ„**ï¼š

```python
from langchain_core.runnables import RunnableConfig
from langchain.callbacks import StdOutCallbackHandler

config = RunnableConfig(
    # Callbacksï¼šç›‘æ§ä¸æ—¥å¿—
    callbacks=[StdOutCallbackHandler()],
    
    # Tagsï¼šä»»åŠ¡åˆ†ç±»
    tags=["production", "translation", "urgent"],
    
    # Metadataï¼šè‡ªå®šä¹‰å…ƒæ•°æ®
    metadata={
        "user_id": "user_123",
        "session_id": "session_abc",
        "environment": "production",
        "version": "1.0.0"
    },
    
    # Run Nameï¼šè¿è¡Œæ ‡è¯†
    run_name="translate_hello_to_french",
    
    # Concurrencyï¼šå¹¶å‘æ§åˆ¶
    max_concurrency=5,  # æœ€å¤šåŒæ—¶5ä¸ªè¯·æ±‚
    
    # Recursion Limitï¼šé€’å½’æ·±åº¦
    recursion_limit=25,
    
    # Configurableï¼šåŠ¨æ€é…ç½®
    configurable={
        "model": "gpt-4o",
        "temperature": 0.7
    }
)
```

**åœ¨é“¾ä¸­ä¼ é€’**ï¼š

```python
# é…ç½®è‡ªåŠ¨ä¼ é€’ç»™æ‰€æœ‰ç»„ä»¶
result = chain.invoke(
    {"text": "Hello"},
    config=config
)

# æ¯ä¸ªç»„ä»¶éƒ½ä¼šæ”¶åˆ°ç›¸åŒçš„ config
```

### 1.6.2 è‡ªå®šä¹‰ Runnable

**åœºæ™¯**ï¼šå®ç°å¤æ‚çš„è‡ªå®šä¹‰é€»è¾‘

```python
from langchain_core.runnables import Runnable, RunnableConfig
from typing import Any, Iterator

class RetryableRunnable(Runnable):
    """å¸¦é‡è¯•æœºåˆ¶çš„ Runnable"""
    
    def __init__(self, runnable: Runnable, max_retries: int = 3):
        self.runnable = runnable
        self.max_retries = max_retries
    
    def invoke(self, input: Any, config: RunnableConfig = None) -> Any:
        """åŒæ­¥è°ƒç”¨ï¼ˆå¸¦é‡è¯•ï¼‰"""
        for attempt in range(self.max_retries):
            try:
                return self.runnable.invoke(input, config)
            except Exception as e:
                if attempt == self.max_retries - 1:
                    raise
                print(f"Attempt {attempt + 1} failed: {e}. Retrying...")
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
    
    async def ainvoke(self, input: Any, config: RunnableConfig = None) -> Any:
        """å¼‚æ­¥è°ƒç”¨ï¼ˆå¸¦é‡è¯•ï¼‰"""
        import asyncio
        for attempt in range(self.max_retries):
            try:
                return await self.runnable.ainvoke(input, config)
            except Exception as e:
                if attempt == self.max_retries - 1:
                    raise
                print(f"Attempt {attempt + 1} failed: {e}. Retrying...")
                await asyncio.sleep(2 ** attempt)

# ä½¿ç”¨
model = ChatOpenAI(model="gpt-4o-mini")
retryable_model = RetryableRunnable(model, max_retries=3)

chain = prompt | retryable_model | parser
```

### 1.6.3 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

#### 1. æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
# âŒ ä¸æ¨èï¼šé€ä¸ªè°ƒç”¨
results = []
for item in large_dataset:
    results.append(chain.invoke(item))

# âœ… æ¨èï¼šæ‰¹é‡è°ƒç”¨
batch_size = 20
results = []
for i in range(0, len(large_dataset), batch_size):
    batch = large_dataset[i:i+batch_size]
    results.extend(chain.batch(batch))
```

#### 2. å¼‚æ­¥å¹¶å‘

```python
import asyncio

# âœ… æ¨èï¼šå¼‚æ­¥å¹¶å‘
async def process_all(items):
    tasks = [chain.ainvoke(item) for item in items]
    return await asyncio.gather(*tasks)

results = asyncio.run(process_all(large_dataset))
```

#### 3. ç¼“å­˜ç­–ç•¥

```python
from langchain.cache import InMemoryCache, SQLiteCache
from langchain.globals import set_llm_cache

# å†…å­˜ç¼“å­˜
set_llm_cache(InMemoryCache())

# æŒä¹…åŒ–ç¼“å­˜
set_llm_cache(SQLiteCache(database_path=".langchain.db"))

# ç›¸åŒè¾“å…¥ä¼šç›´æ¥è¿”å›ç¼“å­˜ç»“æœ
model = ChatOpenAI(model="gpt-4o-mini", cache=True)
```

---

## ğŸ¯ æœ¬ç« å°ç»“

### æ ¸å¿ƒè¦ç‚¹å›é¡¾

| ç»„ä»¶ | æ ¸å¿ƒæ¦‚å¿µ | å…³é”®æ–¹æ³•/ç±» |
|------|----------|-------------|
| **Runnable** | ç»Ÿä¸€æ¥å£ | invoke, ainvoke, stream, astream, batch, abatch |
| **Language Models** | æ¨¡å‹è°ƒç”¨ | ChatOpenAI, ChatAnthropic, ChatOllama |
| **Prompt Templates** | æç¤ºç®¡ç† | PromptTemplate, ChatPromptTemplate, FewShotChatMessagePromptTemplate |
| **Output Parsers** | ç»“æ„åŒ–è¾“å‡º | StrOutputParser, JsonOutputParser, PydanticOutputParser |
| **Message** | æ¶ˆæ¯ç³»ç»Ÿ | SystemMessage, HumanMessage, AIMessage |

### æŒæ¡æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] **Runnable åè®®**
  - [ ] è§£é‡Š Runnable è®¾è®¡çš„åŠ¨æœº
  - [ ] é€‰æ‹©åˆé€‚çš„è°ƒç”¨æ–¹æ³•ï¼ˆinvoke/stream/batch/asyncï¼‰
  - [ ] ä½¿ç”¨ `|` æ“ä½œç¬¦ç»„åˆå¤šä¸ª Runnable
  - [ ] ç†è§£ RunnableConfig çš„ä½œç”¨

- [ ] **Language Models**
  - [ ] åŒºåˆ† Chat Model å’Œ LLM
  - [ ] åœ¨ OpenAIã€Anthropicã€Ollama ä¹‹é—´åˆ‡æ¢
  - [ ] é…ç½® temperatureã€max_tokens ç­‰å‚æ•°
  - [ ] å®ç°è‡ªå®šä¹‰ Callback

- [ ] **Prompt Templates**
  - [ ] åˆ›å»º PromptTemplate å’Œ ChatPromptTemplate
  - [ ] ä½¿ç”¨å˜é‡æ³¨å…¥å’Œéƒ¨åˆ†å¡«å……
  - [ ] å®ç° Few-Shot æç¤º
  - [ ] ä» LangChain Hub æ‹‰å–æç¤º

- [ ] **Output Parsers**
  - [ ] ä½¿ç”¨ PydanticOutputParser å®ç°ç±»å‹å®‰å…¨
  - [ ] å¤„ç† JSON è¾“å‡º
  - [ ] è‡ªå®šä¹‰è§£æå™¨

- [ ] **Message ç®¡ç†**
  - [ ] ç†è§£ä¸åŒæ¶ˆæ¯ç±»å‹çš„ç”¨é€”
  - [ ] å®ç°ç®€å•å¯¹è¯å†å²ç®¡ç†
  - [ ] ä¿®å‰ªå’Œè½¬æ¢æ¶ˆæ¯åˆ—è¡¨

### ç»ƒä¹ é¢˜

#### ç»ƒä¹  1ï¼šæ€§èƒ½å¯¹æ¯”å®éªŒ

å¯¹æ¯” `invoke()` å’Œ `batch()` å¤„ç† 100 æ¡æ¶ˆæ¯çš„è€—æ—¶ã€‚

```python
import time
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

model = ChatOpenAI(model="gpt-4o-mini")
messages = [[HumanMessage(content=f"Say {i}")] for i in range(100)]

# TODO: å®ç°å¯¹æ¯”å®éªŒ
```

#### ç»ƒä¹  2ï¼šæ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨

å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®è¾“å…¥é•¿åº¦è‡ªåŠ¨é€‰æ‹©æ¨¡å‹ã€‚

```python
def smart_model_selector(text: str) -> ChatOpenAI:
    """
    æ ¹æ®æ–‡æœ¬é•¿åº¦é€‰æ‹©æ¨¡å‹ï¼š
    - çŸ­æ–‡æœ¬ï¼ˆ<100 å­—ç¬¦ï¼‰ï¼šgpt-4o-mini
    - é•¿æ–‡æœ¬ï¼ˆ>=100 å­—ç¬¦ï¼‰ï¼šgpt-4o
    """
    # TODO: å®ç°é€»è¾‘
    pass
```

#### ç»ƒä¹  3ï¼šç»“æ„åŒ–æ•°æ®æå–

ä½¿ç”¨ PydanticOutputParser ä»æ–‡æœ¬ä¸­æå–ä¹¦ç±ä¿¡æ¯ã€‚

```python
from pydantic import BaseModel, Field

class Book(BaseModel):
    title: str = Field(description="Book title")
    author: str = Field(description="Author name")
    year: int = Field(description="Publication year")
    genre: str = Field(description="Book genre")

# TODO: å®ç°æå–é“¾
text = "1984 by George Orwell, published in 1949, is a dystopian novel."
```

#### ç»ƒä¹  4ï¼šå¯¹è¯æŒä¹…åŒ–

æ‰©å±• `SimpleConversation` ç±»ï¼Œæ·»åŠ ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½ã€‚

```python
class PersistentConversation(SimpleConversation):
    def save_to_file(self, filepath: str):
        """ä¿å­˜å¯¹è¯å†å²åˆ°æ–‡ä»¶"""
        # TODO: å®ç°
        pass
    
    @classmethod
    def load_from_file(cls, filepath: str) -> "PersistentConversation":
        """ä»æ–‡ä»¶åŠ è½½å¯¹è¯å†å²"""
        # TODO: å®ç°
        pass
```

### ä¸‹ä¸€ç« é¢„å‘Š

**Chapter 2: ç®€å•é“¾æ„å»ºå…¥é—¨**

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ LCELï¼ˆLangChain Expression Languageï¼‰æ„å»ºå®ç”¨çš„åº”ç”¨ï¼š
- ç¿»è¯‘é“¾ï¼šå¤šè¯­è¨€ç¿»è¯‘ç³»ç»Ÿ
- æ‘˜è¦é“¾ï¼šæ™ºèƒ½æ–‡æ¡£æ‘˜è¦
- é—®ç­”é“¾ï¼šåŸºäºä¸Šä¸‹æ–‡çš„é—®ç­”
- é”™è¯¯å¤„ç†ï¼šé‡è¯•ã€é™çº§ä¸æ—¥å¿—

---

## ğŸ“š æ‰©å±•é˜…è¯»

### å®˜æ–¹æ–‡æ¡£

- [Runnable æ¥å£æ–‡æ¡£](https://python.langchain.com/docs/concepts/runnables) - å®˜æ–¹ Runnable åè®®è¯¦è§£
- [Chat Models é›†æˆ](https://python.langchain.com/docs/integrations/chat/) - æ”¯æŒçš„æ¨¡å‹æä¾›å•†åˆ—è¡¨
- [Prompt Templates æŒ‡å—](https://python.langchain.com/docs/concepts/prompt_templates) - æç¤ºæ¨¡æ¿å®Œæ•´æ•™ç¨‹
- [Output Parsers è¯¦è§£](https://python.langchain.com/docs/concepts/output_parsers) - è¾“å‡ºè§£æå™¨å‚è€ƒ
- [Message ç±»å‹å‚è€ƒ](https://python.langchain.com/api_reference/core/messages.html) - æ¶ˆæ¯APIæ–‡æ¡£

### è¿›é˜¶èµ„æº

- [LangChain Hub](https://smith.langchain.com/hub) - æç¤ºè¯ç®¡ç†å¹³å°
- [LangSmith æ–‡æ¡£](https://docs.smith.langchain.com/) - å¯è§‚æµ‹æ€§ä¸è¯„ä¼°
- [Pydantic æ–‡æ¡£](https://docs.pydantic.dev/) - æ•°æ®éªŒè¯åº“
- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs/api-reference) - OpenAI å®˜æ–¹API
- [Anthropic Claude æ–‡æ¡£](https://docs.anthropic.com/) - Claude API æ–‡æ¡£

### ç¤¾åŒºèµ„æº

- [LangChain GitHub Discussions](https://github.com/langchain-ai/langchain/discussions) - ç¤¾åŒºè®¨è®º
- [LangChain Discord](https://discord.gg/langchain) - å®æ—¶äº¤æµ
- [LangChain Blog](https://blog.langchain.dev/) - å®˜æ–¹åšå®¢

---

