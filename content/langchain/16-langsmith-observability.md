# Chapter 16: LangSmith å¯è§‚æµ‹æ€§ä¸è°ƒè¯•

## æœ¬ç« æ¦‚è§ˆ

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡Œ LLM åº”ç”¨æ—¶ï¼Œå¯è§‚æµ‹æ€§ï¼ˆObservabilityï¼‰æ˜¯ç¡®ä¿ç³»ç»Ÿå¯é æ€§ã€æ€§èƒ½å’Œè´¨é‡çš„å…³é”®ã€‚LangSmith æ˜¯ LangChain å®˜æ–¹æä¾›çš„ç«¯åˆ°ç«¯å¼€å‘è€…å¹³å°ï¼Œä¸“æ³¨äº LLM åº”ç”¨çš„**è¿½è¸ªï¼ˆTracingï¼‰**ã€**è°ƒè¯•ï¼ˆDebuggingï¼‰**ã€**è¯„ä¼°ï¼ˆEvaluationï¼‰**å’Œ**ç›‘æ§ï¼ˆMonitoringï¼‰**ã€‚

æœ¬ç« å°†æ·±å…¥æ¢è®¨ï¼š
- LangSmith æ ¸å¿ƒæ¦‚å¿µä¸æ¶æ„
- Tracing æœºåˆ¶ä¸ Span å±‚çº§ç»“æ„
- è°ƒè¯•å·¥å…·ä¸æç¤ºä¼˜åŒ–
- æ•°æ®é›†ç®¡ç†ä¸ç¦»çº¿è¯„ä¼°
- åœ¨çº¿ç›‘æ§ä¸åé¦ˆå¾ªç¯

---

## 16.1 ä¸ºä»€ä¹ˆéœ€è¦ LangSmithï¼Ÿ

### 16.1.1 LLM åº”ç”¨çš„å¯è§‚æµ‹æ€§æŒ‘æˆ˜

ä¼ ç»Ÿè½¯ä»¶å¼€å‘ä¸­ï¼Œå¯è§‚æµ‹æ€§ä¸‰å¤§æ”¯æŸ±æ˜¯**æ—¥å¿—ï¼ˆLogsï¼‰**ã€**æŒ‡æ ‡ï¼ˆMetricsï¼‰**å’Œ**è¿½è¸ªï¼ˆTracesï¼‰**ã€‚ä½† LLM åº”ç”¨å…·æœ‰ç‹¬ç‰¹æŒ‘æˆ˜ï¼š

1. **éç¡®å®šæ€§è¾“å‡º**ï¼šåŒæ ·çš„è¾“å…¥å¯èƒ½äº§ç”Ÿä¸åŒè¾“å‡º
2. **å¤æ‚é“¾è·¯**ï¼šå¤šä¸ª LLM è°ƒç”¨ã€æ£€ç´¢ã€å·¥å…·æ‰§è¡Œç»„æˆçš„å¤šæ­¥æµç¨‹
3. **é«˜å»¶è¿Ÿä¸æˆæœ¬**ï¼šæ¯æ¬¡ LLM è°ƒç”¨è€—æ—¶é•¿ä¸”æ¶ˆè€— Token
4. **è´¨é‡éš¾é‡åŒ–**ï¼šè¾“å‡ºè´¨é‡æ²¡æœ‰æ˜ç¡®çš„"æ­£ç¡®ç­”æ¡ˆ"
5. **ä¸Šä¸‹æ–‡ä¾èµ–**ï¼šéœ€è¦è¿½è¸ªå®Œæ•´çš„å¯¹è¯å†å²å’ŒçŠ¶æ€å˜åŒ–

### 16.1.2 LangSmith çš„æ ¸å¿ƒä»·å€¼

<Callout type="success">
**LangSmith è§£å†³çš„æ ¸å¿ƒé—®é¢˜**

- **é€æ˜åŒ–æ‰§è¡Œè¿‡ç¨‹**ï¼šè®°å½•æ¯ä¸€æ­¥çš„è¾“å…¥ã€è¾“å‡ºã€å»¶è¿Ÿã€Token æ¶ˆè€—
- **å¿«é€Ÿå®šä½é—®é¢˜**ï¼šå¯è§†åŒ– Traceï¼Œå®šä½å¤±è´¥èŠ‚ç‚¹ã€æ€§èƒ½ç“¶é¢ˆ
- **ç³»ç»ŸåŒ–è¯„ä¼°**ï¼šé€šè¿‡æ•°æ®é›†å’Œè¯„ä¼°å™¨é‡åŒ–åº”ç”¨è´¨é‡
- **æŒç»­ä¼˜åŒ–**ï¼šæ”¶é›†åé¦ˆï¼Œè¿­ä»£æç¤ºã€æ¨¡å‹ã€æ£€ç´¢ç­–ç•¥
- **ç”Ÿäº§ç›‘æ§**ï¼šå®æ—¶ç›‘æ§é”™è¯¯ç‡ã€å»¶è¿Ÿã€æˆæœ¬ï¼Œè®¾ç½®è­¦æŠ¥
</Callout>

### 16.1.3 LangSmith vs å…¶ä»–å·¥å…·

| ç‰¹æ€§ | LangSmith | LangFuse | Weights & Biases | Arize Phoenix |
|------|-----------|----------|------------------|---------------|
| **LangChain é›†æˆ** | åŸç”Ÿæ— ç¼ | éœ€é…ç½® | éœ€é…ç½® | éœ€é…ç½® |
| **Trace å¯è§†åŒ–** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| **æ•°æ®é›†ç®¡ç†** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­ |
| **åœ¨çº¿è¯„ä¼°** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ |
| **Prompt Playground** | â­â­â­â­â­ | â­â­â­ | â­â­ | â­â­â­ |
| **è‡ªæ‰˜ç®¡** | âŒ (äº‘ç«¯) | âœ… | âœ… | âœ… |
| **å®šä»·** | å…è´¹å±‚ + ä¼ä¸š | å¼€æº + ä¼ä¸š | ä»˜è´¹ | å¼€æº |

---

## 16.2 LangSmith å¿«é€Ÿä¸Šæ‰‹

### 16.2.1 ç¯å¢ƒé…ç½®

#### æ­¥éª¤ 1ï¼šè·å– API Key

è®¿é—® [https://smith.langchain.com](https://smith.langchain.com) æ³¨å†Œè´¦å·ï¼Œåœ¨è®¾ç½®ä¸­è·å– API Keyã€‚

#### æ­¥éª¤ 2ï¼šé…ç½®ç¯å¢ƒå˜é‡

```bash
# .env æ–‡ä»¶
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=lsv2_pt_xxx...  # ä½ çš„ API Key
LANGCHAIN_PROJECT=my-first-project  # é¡¹ç›®åç§°
```

#### æ­¥éª¤ 3ï¼šå®‰è£…ä¾èµ–

```bash
pip install langchain langchain-openai langsmith
```

### 16.2.2 ç¬¬ä¸€ä¸ª Traced åº”ç”¨

```python
import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ç¯å¢ƒå˜é‡å·²è®¾ç½®ï¼ŒLangSmith è‡ªåŠ¨å¯ç”¨
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "lsv2_pt_xxx"
os.environ["LANGCHAIN_PROJECT"] = "demo-simple-chain"

# æ„å»ºç®€å•é“¾
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{role}ã€‚"),
    ("user", "{input}")
])

llm = ChatOpenAI(model="gpt-4", temperature=0.7)
chain = prompt | llm | StrOutputParser()

# æ‰§è¡Œé“¾ï¼ˆè‡ªåŠ¨è®°å½•åˆ° LangSmithï¼‰
result = chain.invoke({
    "role": "Python å¯¼å¸ˆ",
    "input": "è§£é‡Šä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"
})

print(result)
```

**é¢„æœŸè¾“å‡ºï¼š**
```
è£…é¥°å™¨ï¼ˆDecoratorï¼‰æ˜¯ Python ä¸­çš„ä¸€ç§è®¾è®¡æ¨¡å¼ï¼Œç”¨äºåœ¨ä¸ä¿®æ”¹åŸå‡½æ•°ä»£ç çš„æƒ…å†µä¸‹ï¼ŒåŠ¨æ€åœ°å¢å¼ºå‡½æ•°çš„åŠŸèƒ½...

âœ… è‡ªåŠ¨è®°å½•åˆ° LangSmithï¼š
   - Trace ID: a7f3e9d2-...
   - é“¾æ¥: https://smith.langchain.com/o/.../projects/p/.../runs/r/...
```

åœ¨ LangSmith UI ä¸­å¯ä»¥çœ‹åˆ°ï¼š
- **Trace æ ‘**ï¼šPrompt æ ¼å¼åŒ– â†’ LLM è°ƒç”¨ â†’ è¾“å‡ºè§£æ
- **æ¯ä¸ªæ­¥éª¤çš„è¾“å…¥/è¾“å‡º**
- **Token æ¶ˆè€—**ï¼šPrompt Tokens: 28, Completion Tokens: 156
- **å»¶è¿Ÿåˆ†å¸ƒ**ï¼šTotal: 2.3s, LLM: 2.1s

<div data-component="LangSmithTraceVisualization"></div>

---

## 16.3 Tracing æ·±åº¦è§£æ

### 16.3.1 Trace ä¸ Span å±‚çº§ç»“æ„

LangSmith çš„è¿½è¸ªæœºåˆ¶åŸºäº **OpenTelemetry** æ ‡å‡†ï¼Œæ¯æ¬¡æ‰§è¡Œäº§ç”Ÿä¸€ä¸ª **Trace**ï¼Œå†…éƒ¨åŒ…å«å¤šä¸ª **Span**ã€‚

#### Trace å±‚çº§ç¤ºä¾‹

```
Root Trace (Chain Execution)
â”œâ”€ Span 1: PromptTemplate.format()
â”‚  â”œâ”€ Input: {role: "Python å¯¼å¸ˆ", input: "è§£é‡Šè£…é¥°å™¨"}
â”‚  â””â”€ Output: [SystemMessage, HumanMessage]
â”‚
â”œâ”€ Span 2: ChatOpenAI.invoke()
â”‚  â”œâ”€ Input: [SystemMessage, HumanMessage]
â”‚  â”œâ”€ Metadata: {model: "gpt-4", temperature: 0.7}
â”‚  â”œâ”€ Token ä½¿ç”¨: {prompt: 28, completion: 156, total: 184}
â”‚  â””â”€ Output: AIMessage(content="è£…é¥°å™¨æ˜¯...")
â”‚
â””â”€ Span 3: StrOutputParser.parse()
   â”œâ”€ Input: AIMessage(content="è£…é¥°å™¨æ˜¯...")
   â””â”€ Output: "è£…é¥°å™¨æ˜¯..."
```

### 16.3.2 è‡ªåŠ¨ vs æ‰‹åŠ¨ Tracing

#### è‡ªåŠ¨ Tracing

æ‰€æœ‰ LangChain ç»„ä»¶ï¼ˆRunnableï¼‰é»˜è®¤æ”¯æŒè‡ªåŠ¨è¿½è¸ªï¼š

```python
from langchain_core.runnables import RunnableLambda

# è‡ªå®šä¹‰å‡½æ•°ä¹Ÿä¼šè¢«è¿½è¸ª
def custom_processor(text: str) -> str:
    return text.upper()

# åŒ…è£…ä¸º Runnable åè‡ªåŠ¨è¿½è¸ª
runnable_processor = RunnableLambda(custom_processor)

chain = prompt | llm | runnable_processor
chain.invoke({"role": "åŠ©æ‰‹", "input": "hello"})
```

#### æ‰‹åŠ¨ Tracingï¼ˆè‡ªå®šä¹‰ Spanï¼‰

å¯¹äºå¤æ‚ä¸šåŠ¡é€»è¾‘ï¼Œå¯æ‰‹åŠ¨åˆ›å»º Spanï¼š

```python
from langsmith import trace

@trace(name="æ•°æ®é¢„å¤„ç†", run_type="tool")
def preprocess_data(data: dict) -> dict:
    """è‡ªå®šä¹‰ Span è¿½è¸ªæ•°æ®å¤„ç†è¿‡ç¨‹"""
    # å¤æ‚å¤„ç†é€»è¾‘
    processed = {k: v.strip().lower() for k, v in data.items()}
    return processed

@trace(name="å®Œæ•´æµç¨‹", run_type="chain")
def full_pipeline(user_input: str):
    data = {"input": user_input}
    data = preprocess_data(data)  # åˆ›å»ºå­ Span
    
    result = chain.invoke(data)
    return result

# æ‰§è¡Œæ—¶ä¼šåˆ›å»ºåµŒå¥— Trace
full_pipeline("  HELLO WORLD  ")
```

**Trace ç»“æ„ï¼š**
```
å®Œæ•´æµç¨‹ (Chain)
â”œâ”€ æ•°æ®é¢„å¤„ç† (Tool)
â”‚  â””â”€ è¾“å…¥: {"input": "  HELLO WORLD  "}
â”‚  â””â”€ è¾“å‡º: {"input": "hello world"}
â””â”€ Chain Execution
   â”œâ”€ PromptTemplate
   â”œâ”€ ChatOpenAI
   â””â”€ StrOutputParser
```

### 16.3.3 Trace å…ƒæ•°æ®ä¸æ ‡ç­¾

ä¸º Trace æ·»åŠ å…ƒæ•°æ®å’Œæ ‡ç­¾ä¾¿äºåç»­ç­›é€‰å’Œåˆ†æï¼š

```python
from langsmith import Client

client = Client()

# æ–¹å¼ 1ï¼šé€šè¿‡é…ç½®æ·»åŠ å…ƒæ•°æ®
chain.invoke(
    {"role": "åŠ©æ‰‹", "input": "ä½ å¥½"},
    config={
        "metadata": {
            "user_id": "user_12345",
            "session_id": "sess_abc",
            "environment": "production"
        },
        "tags": ["customer-support", "greeting"]
    }
)

# æ–¹å¼ 2ï¼šé€šè¿‡è£…é¥°å™¨æ·»åŠ 
@trace(
    name="ç”¨æˆ·æŸ¥è¯¢å¤„ç†",
    metadata={"department": "sales"},
    tags=["high-priority"]
)
def handle_query(query: str):
    return chain.invoke({"role": "é”€å”®ä¸“å®¶", "input": query})
```

åœ¨ LangSmith UI ä¸­å¯ä»¥é€šè¿‡æ ‡ç­¾å’Œå…ƒæ•°æ®è¿‡æ»¤ï¼š
- æŸ¥çœ‹ç‰¹å®šç”¨æˆ·çš„æ‰€æœ‰ Trace
- åˆ†æç”Ÿäº§ç¯å¢ƒ vs æµ‹è¯•ç¯å¢ƒçš„æ€§èƒ½å·®å¼‚
- ç»Ÿè®¡é«˜ä¼˜å…ˆçº§æŸ¥è¯¢çš„æˆåŠŸç‡

---

## 16.4 è°ƒè¯•ä¸ Prompt ä¼˜åŒ–

### 16.4.1 Playgroundï¼šäº¤äº’å¼æç¤ºè°ƒè¯•

LangSmith Playground å…è®¸ä½ åœ¨æµè§ˆå™¨ä¸­ç›´æ¥ä¿®æ”¹æç¤ºã€å‚æ•°ï¼Œå®æ—¶å¯¹æ¯”æ•ˆæœã€‚

#### ä½¿ç”¨æµç¨‹

1. **é€‰æ‹© Trace**ï¼šåœ¨ Trace åˆ—è¡¨ä¸­ç‚¹å‡»ä¸€ä¸ª LLM è°ƒç”¨
2. **æ‰“å¼€ Playground**ï¼šç‚¹å‡» "Open in Playground"
3. **ä¿®æ”¹æç¤º**ï¼š
   - è°ƒæ•´ System Message
   - ä¿®æ”¹ Temperatureã€Top-p
   - åˆ‡æ¢æ¨¡å‹ï¼ˆGPT-4 â†’ GPT-3.5 â†’ Claudeï¼‰
4. **å¯¹æ¯”æµ‹è¯•**ï¼š
   - å¹¶æ’å¯¹æ¯”å¤šä¸ªç‰ˆæœ¬
   - æŸ¥çœ‹ Token æ¶ˆè€—å·®å¼‚
5. **ä¿å­˜ä¼˜åŒ–ç‰ˆæœ¬**ï¼šå°†ä¼˜åŒ–åçš„æç¤ºä¿å­˜åˆ° Hub

#### ç¤ºä¾‹ï¼šä¼˜åŒ–ç¿»è¯‘æç¤º

**åŸå§‹æç¤ºï¼š**
```
Translate to French: Hello world
```

**ä¼˜åŒ–åï¼š**
```
You are a professional translator specializing in English to French translation.
Translate the following text to French, maintaining the tone and style:

Text: Hello world

Translation:
```

**å¯¹æ¯”ç»“æœï¼š**
| ç‰ˆæœ¬ | è¾“å‡º | Token | è´¨é‡è¯„åˆ† |
|------|------|-------|----------|
| åŸå§‹ | "Bonjour le monde" | 15 | 7/10 |
| ä¼˜åŒ– | "Bonjour le monde" (with explanation) | 45 | 9/10 |

### 16.4.2 å¤±è´¥ Trace è‡ªåŠ¨æ ‡è®°

LangSmith è‡ªåŠ¨æ ‡è®°å¤±è´¥çš„ Traceï¼ˆæŠ›å‡ºå¼‚å¸¸æˆ–è¶…æ—¶ï¼‰ï¼š

```python
from langchain_core.runnables import RunnableLambda

def risky_operation(x: int) -> int:
    if x < 0:
        raise ValueError("ä¸æ”¯æŒè´Ÿæ•°")
    return x * 2

chain = RunnableLambda(risky_operation)

try:
    chain.invoke(-5)
except ValueError as e:
    print(f"æ•è·å¼‚å¸¸: {e}")
```

åœ¨ LangSmith ä¸­ä¼šçœ‹åˆ°ï¼š
- âŒ **Status: Error**
- **Error Type**: `ValueError`
- **Error Message**: "ä¸æ”¯æŒè´Ÿæ•°"
- **Stack Trace**: å®Œæ•´å †æ ˆä¿¡æ¯

### 16.4.3 æˆæœ¬ä¸å»¶è¿Ÿåˆ†æ

LangSmith è‡ªåŠ¨è®¡ç®—æ¯æ¬¡æ‰§è¡Œçš„æˆæœ¬å’Œå»¶è¿Ÿï¼š

```python
# æ‰§è¡Œå¤šæ¬¡ä»¥æ”¶é›†ç»Ÿè®¡æ•°æ®
for i in range(10):
    chain.invoke({
        "role": "åŠ©æ‰‹",
        "input": f"ç¬¬ {i+1} ä¸ªé—®é¢˜"
    })
```

**åˆ†æè§†å›¾ï¼š**
- **å»¶è¿Ÿåˆ†å¸ƒå›¾**ï¼šP50: 1.2s, P95: 2.8s, P99: 4.1s
- **æˆæœ¬ç»Ÿè®¡**ï¼š
  - å¹³å‡æ¯æ¬¡è°ƒç”¨: $0.0045
  - æ€»æˆæœ¬: $0.045
- **Token åˆ†å¸ƒ**ï¼š
  - Prompt Tokens: å¹³å‡ 32 (èŒƒå›´ 28-35)
  - Completion Tokens: å¹³å‡ 150 (èŒƒå›´ 120-180)

---

## 16.5 æ•°æ®é›†ç®¡ç†ä¸ç¦»çº¿è¯„ä¼°

### 16.5.1 åˆ›å»ºæ•°æ®é›†

æ•°æ®é›†æ˜¯è¯„ä¼°çš„åŸºç¡€ï¼ŒåŒ…å«**è¾“å…¥**å’Œ**é¢„æœŸè¾“å‡º**ï¼ˆå¯é€‰ï¼‰ã€‚

#### é€šè¿‡ä»£ç åˆ›å»ºæ•°æ®é›†

```python
from langsmith import Client

client = Client()

# åˆ›å»ºæ•°æ®é›†
dataset_name = "customer-support-qa"
dataset = client.create_dataset(
    dataset_name=dataset_name,
    description="å®¢æœå¸¸è§é—®ç­”æ•°æ®é›†"
)

# æ·»åŠ æ ·æœ¬
examples = [
    {
        "inputs": {"question": "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ"},
        "outputs": {"answer": "ç‚¹å‡»ç™»å½•é¡µé¢çš„'å¿˜è®°å¯†ç 'é“¾æ¥ï¼ŒæŒ‰ç…§é‚®ä»¶æŒ‡å¼•æ“ä½œã€‚"}
    },
    {
        "inputs": {"question": "æ”¯æŒå“ªäº›æ”¯ä»˜æ–¹å¼ï¼Ÿ"},
        "outputs": {"answer": "æ”¯æŒä¿¡ç”¨å¡ã€PayPalã€æ”¯ä»˜å®å’Œå¾®ä¿¡æ”¯ä»˜ã€‚"}
    },
    {
        "inputs": {"question": "é€€è´§æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ"},
        "outputs": {"answer": "30 å¤©å†…æ— ç†ç”±é€€è´§ï¼Œéœ€ä¿æŒå•†å“å®Œå¥½ã€‚"}
    }
]

for example in examples:
    client.create_example(
        dataset_id=dataset.id,
        inputs=example["inputs"],
        outputs=example["outputs"]
    )

print(f"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆï¼š{dataset_name}")
```

#### ä» Trace åˆ›å»ºæ•°æ®é›†

åœ¨ LangSmith UI ä¸­ï¼š
1. é€‰æ‹©é«˜è´¨é‡çš„ Trace
2. ç‚¹å‡» "Add to Dataset"
3. é€‰æ‹©ç›®æ ‡æ•°æ®é›†æˆ–åˆ›å»ºæ–°æ•°æ®é›†

### 16.5.2 è¿è¡Œè¯„ä¼°

#### å®šä¹‰è¯„ä¼°å™¨

LangSmith æ”¯æŒå¤šç§è¯„ä¼°å™¨ï¼š

**1. ç²¾ç¡®åŒ¹é…ï¼ˆExact Matchï¼‰**

```python
from langsmith.evaluation import EvaluationResult

def exact_match_evaluator(run, example):
    """æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸é¢„æœŸå®Œå…¨ä¸€è‡´"""
    prediction = run.outputs.get("output", "")
    reference = example.outputs.get("answer", "")
    
    return EvaluationResult(
        key="exact_match",
        score=1.0 if prediction == reference else 0.0
    )
```

**2. LLM-as-Judgeï¼ˆä½¿ç”¨ LLM è¯„ä¼°è´¨é‡ï¼‰**

```python
from langchain_openai import ChatOpenAI
from langsmith.evaluation import LangChainStringEvaluator

# ä½¿ç”¨ GPT-4 è¯„ä¼°ç­”æ¡ˆè´¨é‡
evaluator = LangChainStringEvaluator(
    "qa",  # QA è¯„ä¼°æ¨¡å¼
    config={
        "llm": ChatOpenAI(model="gpt-4", temperature=0),
        "criteria": {
            "accuracy": "ç­”æ¡ˆæ˜¯å¦å‡†ç¡®å›ç­”äº†é—®é¢˜ï¼Ÿ",
            "completeness": "ç­”æ¡ˆæ˜¯å¦å®Œæ•´ï¼Ÿ",
            "clarity": "ç­”æ¡ˆæ˜¯å¦æ¸…æ™°æ˜“æ‡‚ï¼Ÿ"
        }
    }
)
```

**3. è‡ªå®šä¹‰è¯„ä¼°å™¨**

```python
def custom_evaluator(run, example):
    """è‡ªå®šä¹‰è¯„ä¼°é€»è¾‘"""
    prediction = run.outputs.get("output", "")
    reference = example.outputs.get("answer", "")
    
    # æ£€æŸ¥å…³é”®è¯æ˜¯å¦å‡ºç°
    keywords = ["å¯†ç ", "é‡ç½®", "é‚®ä»¶"]
    keyword_match = sum(kw in prediction for kw in keywords) / len(keywords)
    
    # æ£€æŸ¥é•¿åº¦åˆç†æ€§
    length_ok = 20 < len(prediction) < 200
    
    return EvaluationResult(
        key="custom_quality",
        score=(keyword_match + (1.0 if length_ok else 0.0)) / 2,
        comment=f"å…³é”®è¯åŒ¹é…: {keyword_match:.2f}, é•¿åº¦åˆç†: {length_ok}"
    )
```

#### æ‰§è¡Œè¯„ä¼°

```python
from langsmith.evaluation import evaluate

# å®šä¹‰å¾…è¯„ä¼°çš„é“¾
def predict(inputs: dict) -> dict:
    question = inputs["question"]
    result = chain.invoke({"role": "å®¢æœ", "input": question})
    return {"output": result}

# è¿è¡Œè¯„ä¼°
results = evaluate(
    predict,
    data=dataset_name,
    evaluators=[
        exact_match_evaluator,
        custom_evaluator
    ],
    experiment_prefix="customer-support-v1",
    metadata={
        "model": "gpt-4",
        "version": "1.0.0"
    }
)

print(results)
```

**è¯„ä¼°æŠ¥å‘Šï¼š**
```
ğŸ“Š è¯„ä¼°ç»“æœï¼šcustomer-support-v1-20240115-123045

æ€»æ ·æœ¬æ•°: 3
å¹³å‡åˆ†æ•°:
  - exact_match: 0.33 (1/3 å®Œå…¨åŒ¹é…)
  - custom_quality: 0.78 (è´¨é‡è‰¯å¥½)

è¯¦ç»†ç»“æœ:
1. âœ… å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ
   - exact_match: 0.0
   - custom_quality: 0.85
   - è¯„è®º: å…³é”®è¯åŒ¹é…è‰¯å¥½ï¼Œä½†è¡¨è¿°ä¸æ ‡å‡†ç­”æ¡ˆä¸åŒ

2. âœ… æ”¯æŒå“ªäº›æ”¯ä»˜æ–¹å¼ï¼Ÿ
   - exact_match: 1.0
   - custom_quality: 1.0
   
3. âŒ é€€è´§æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ
   - exact_match: 0.0
   - custom_quality: 0.50
   - è¯„è®º: ç¼ºå°‘å…³é”®ä¿¡æ¯"ä¿æŒå•†å“å®Œå¥½"
```

<div data-component="EvaluationDashboard"></div>

### 16.5.3 å¯¹æ¯”å®éªŒï¼ˆA/B Testingï¼‰

æ¯”è¾ƒä¸åŒæç¤ºã€æ¨¡å‹æˆ–æ£€ç´¢ç­–ç•¥çš„æ•ˆæœï¼š

```python
# å®éªŒ Aï¼šGPT-3.5 + ç®€å•æç¤º
chain_a = ChatPromptTemplate.from_template("å›ç­”ï¼š{question}") | \
          ChatOpenAI(model="gpt-3.5-turbo") | \
          StrOutputParser()

# å®éªŒ Bï¼šGPT-4 + è¯¦ç»†æç¤º
chain_b = ChatPromptTemplate.from_template(
    "ä½ æ˜¯ä¸“ä¸šå®¢æœï¼Œè¯·ç”¨å‹å¥½ã€å‡†ç¡®çš„è¯­æ°”å›ç­”ï¼š{question}"
) | ChatOpenAI(model="gpt-4") | StrOutputParser()

# åˆ†åˆ«è¯„ä¼°
results_a = evaluate(
    lambda x: {"output": chain_a.invoke(x)},
    data=dataset_name,
    evaluators=[custom_evaluator],
    experiment_prefix="experiment-A-gpt35"
)

results_b = evaluate(
    lambda x: {"output": chain_b.invoke(x)},
    data=dataset_name,
    evaluators=[custom_evaluator],
    experiment_prefix="experiment-B-gpt4"
)
```

**å¯¹æ¯”ç»“æœï¼š**

| æŒ‡æ ‡ | Experiment A (GPT-3.5) | Experiment B (GPT-4) | æ”¹è¿› |
|------|------------------------|----------------------|------|
| **å¹³å‡è´¨é‡åˆ†** | 0.65 | 0.82 | +26% |
| **å¹³å‡å»¶è¿Ÿ** | 1.2s | 2.8s | +133% |
| **å¹³å‡æˆæœ¬** | $0.0008 | $0.0045 | +463% |
| **æˆåŠŸç‡** | 66% | 100% | +34% |

**ç»“è®º**ï¼šGPT-4 è´¨é‡æ˜¾è‘—æ›´é«˜ï¼Œä½†æˆæœ¬å’Œå»¶è¿Ÿä¹Ÿæ˜æ˜¾å¢åŠ ã€‚å¯è€ƒè™‘æ··åˆç­–ç•¥ï¼šç®€å•é—®é¢˜ç”¨ GPT-3.5ï¼Œå¤æ‚é—®é¢˜ç”¨ GPT-4ã€‚

---

## 16.6 ç”Ÿäº§ç›‘æ§ä¸åé¦ˆå¾ªç¯

### 16.6.1 å®æ—¶ç›‘æ§ä»ªè¡¨æ¿

LangSmith æä¾›å®æ—¶ç›‘æ§é¢æ¿ï¼Œå±•ç¤ºå…³é”®æŒ‡æ ‡ï¼š

#### æ ¸å¿ƒç›‘æ§æŒ‡æ ‡

1. **ååé‡ï¼ˆThroughputï¼‰**
   - æ¯åˆ†é’Ÿè¯·æ±‚æ•°ï¼ˆRPMï¼‰
   - æ¯å°æ—¶ Token æ¶ˆè€—

2. **å»¶è¿Ÿï¼ˆLatencyï¼‰**
   - P50ã€P95ã€P99 å»¶è¿Ÿ
   - LLM è°ƒç”¨å»¶è¿Ÿ vs æ€»å»¶è¿Ÿ

3. **é”™è¯¯ç‡ï¼ˆError Rateï¼‰**
   - æŒ‰é”™è¯¯ç±»å‹åˆ†ç±»ï¼ˆTimeout, RateLimitError, ValidationErrorï¼‰
   - é”™è¯¯ Trace å æ¯”

4. **æˆæœ¬ï¼ˆCostï¼‰**
   - æ¯æ—¥æˆæœ¬è¶‹åŠ¿
   - æŒ‰æ¨¡å‹/ç”¨æˆ·/åŠŸèƒ½åˆ†ç»„çš„æˆæœ¬

5. **è´¨é‡æŒ‡æ ‡ï¼ˆQuality Metricsï¼‰**
   - ç”¨æˆ·åé¦ˆè¯„åˆ†
   - è‡ªåŠ¨è¯„ä¼°å™¨åˆ†æ•°

#### è®¾ç½®è­¦æŠ¥

```python
# é€šè¿‡ LangSmith API è®¾ç½®è­¦æŠ¥ï¼ˆä¼ªä»£ç ï¼Œå®é™…éœ€åœ¨ UI é…ç½®ï¼‰
alert_config = {
    "name": "é«˜é”™è¯¯ç‡è­¦æŠ¥",
    "condition": "error_rate > 0.05",  # é”™è¯¯ç‡è¶…è¿‡ 5%
    "window": "5m",  # 5 åˆ†é’Ÿçª—å£
    "actions": [
        {"type": "email", "recipients": ["team@example.com"]},
        {"type": "slack", "webhook": "https://hooks.slack.com/..."}
    ]
}
```

### 16.6.2 ç”¨æˆ·åé¦ˆæ”¶é›†

#### åœ¨çº¿æ”¶é›†åé¦ˆ

```python
from langsmith import Client

client = Client()

# æ‰§è¡Œé“¾
result = chain.invoke({"role": "åŠ©æ‰‹", "input": "ä½ å¥½"})

# å‡è®¾ç”¨æˆ·ç»™å‡ºåé¦ˆï¼ˆåœ¨ UI ä¸­æ”¶é›†ï¼‰
run_id = "run_abc123"  # ä» Trace ä¸­è·å–

# è®°å½•ç”¨æˆ·åé¦ˆ
client.create_feedback(
    run_id=run_id,
    key="user_rating",
    score=0.8,  # 0-1 ä¹‹é—´
    comment="å›ç­”å‡†ç¡®ä½†ç•¥æ˜¾å†—é•¿"
)

client.create_feedback(
    run_id=run_id,
    key="user_thumbs",
    score=1.0,  # 1 = ğŸ‘, 0 = ğŸ‘
)
```

#### åé¦ˆé©±åŠ¨çš„è¿­ä»£

1. **åˆ†æä½åˆ† Trace**ï¼šæ‰¾å‡ºç”¨æˆ·è¯„åˆ†ä½çš„å…±æ€§é—®é¢˜
2. **åˆ›å»ºæ”¹è¿›æ•°æ®é›†**ï¼šå°†ä½åˆ†æ ·æœ¬åŠ å…¥æ•°æ®é›†
3. **è°ƒæ•´æç¤º/æ¨¡å‹**ï¼šé’ˆå¯¹æ€§ä¼˜åŒ–
4. **A/B æµ‹è¯•éªŒè¯**ï¼šå¯¹æ¯”æ–°æ—§ç‰ˆæœ¬
5. **ç°åº¦å‘å¸ƒ**ï¼šé€æ­¥æ¨å¹¿ä¼˜åŒ–ç‰ˆæœ¬

### 16.6.3 æŒç»­è¯„ä¼°ï¼ˆOnline Evaluationï¼‰

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æŒç»­è¿è¡Œè¯„ä¼°å™¨ï¼š

```python
from langsmith.evaluation import EvaluationResult

def online_evaluator(run):
    """ç”Ÿäº§ç¯å¢ƒå®æ—¶è¯„ä¼°"""
    output = run.outputs.get("output", "")
    
    # æ£€æŸ¥è¾“å‡ºé•¿åº¦
    length_ok = 50 < len(output) < 500
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸å½“å†…å®¹ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
    inappropriate = any(word in output for word in ["è„è¯", "ä¾®è¾±"])
    
    return EvaluationResult(
        key="production_quality",
        score=1.0 if (length_ok and not inappropriate) else 0.0,
        comment=f"é•¿åº¦: {len(output)}, å†…å®¹åˆè§„: {not inappropriate}"
    )

# é…ç½®åœ¨çº¿è¯„ä¼°ï¼ˆè‡ªåŠ¨åº”ç”¨åˆ°æ‰€æœ‰æ–° Traceï¼‰
# å®é™…éœ€åœ¨ LangSmith UI ä¸­é…ç½®
```

<div data-component="MonitoringDashboard"></div>

---

## 16.7 é«˜çº§ç‰¹æ€§ä¸æœ€ä½³å®è·µ

### 16.7.1 è‡ªå®šä¹‰ Run æ”¶é›†å™¨

å¯¹äºé LangChain åº”ç”¨ï¼Œå¯æ‰‹åŠ¨å‘é€ Traceï¼š

```python
from langsmith import Client
from datetime import datetime

client = Client()

# æ‰‹åŠ¨åˆ›å»º Run
run_id = client.create_run(
    name="è‡ªå®šä¹‰æ¨èç³»ç»Ÿ",
    run_type="chain",
    inputs={"user_id": "user_123", "context": "æµè§ˆå†å²"},
    start_time=datetime.now()
)

try:
    # æ‰§è¡Œä¸šåŠ¡é€»è¾‘
    recommendations = my_custom_algorithm()
    
    # è®°å½•æˆåŠŸ
    client.update_run(
        run_id=run_id,
        outputs={"recommendations": recommendations},
        end_time=datetime.now()
    )
except Exception as e:
    # è®°å½•å¤±è´¥
    client.update_run(
        run_id=run_id,
        error=str(e),
        end_time=datetime.now()
    )
```

### 16.7.2 æ‰¹é‡å¯¼å‡º Trace æ•°æ®

ç”¨äºç¦»çº¿åˆ†ææˆ–æ•°æ®ç§‘å­¦å·¥ä½œæµï¼š

```python
from langsmith import Client
import pandas as pd

client = Client()

# æŸ¥è¯¢æŒ‡å®šæ—¶é—´èŒƒå›´çš„ Trace
runs = client.list_runs(
    project_name="production-app",
    start_time="2024-01-01",
    end_time="2024-01-31",
    filter='eq(status, "success")'  # åªå¯¼å‡ºæˆåŠŸçš„
)

# è½¬æ¢ä¸º DataFrame
data = []
for run in runs:
    data.append({
        "run_id": run.id,
        "name": run.name,
        "latency": run.latency,
        "total_tokens": run.total_tokens,
        "cost": run.prompt_tokens * 0.00003 + run.completion_tokens * 0.00006,
        "created_at": run.start_time
    })

df = pd.DataFrame(data)
print(df.describe())
```

### 16.7.3 Privacy & Compliance

#### è„±æ•å¤„ç†

```python
import re
from langsmith.run_helpers import traceable

def redact_pii(text: str) -> str:
    """ç§»é™¤ä¸ªäººèº«ä»½ä¿¡æ¯"""
    # ç§»é™¤é‚®ç®±
    text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', text)
    # ç§»é™¤ç”µè¯å·ç 
    text = re.sub(r'\b\d{3}-\d{3}-\d{4}\b', '[PHONE]', text)
    return text

@traceable(
    name="è„±æ•æŸ¥è¯¢å¤„ç†",
    process_inputs=lambda x: {"query": redact_pii(x["query"])},
    process_outputs=lambda x: {"result": redact_pii(x["result"])}
)
def handle_query(query: str):
    result = chain.invoke({"input": query})
    return {"result": result}
```

#### æ•°æ®ä¿ç•™ç­–ç•¥

åœ¨ LangSmith è®¾ç½®ä¸­é…ç½®ï¼š
- **è‡ªåŠ¨åˆ é™¤**ï¼š30 å¤©ååˆ é™¤ Trace
- **é‡‡æ ·ç­–ç•¥**ï¼šåªä¿ç•™ 10% çš„æˆåŠŸ Traceï¼Œä¿ç•™ 100% å¤±è´¥ Trace
- **åœ°åŸŸé™åˆ¶**ï¼šç¡®ä¿æ•°æ®å­˜å‚¨åœ¨åˆè§„åœ°åŒºï¼ˆEU/USï¼‰

---

## 16.8 å®æˆ˜æ¡ˆä¾‹ï¼šä¼˜åŒ–å®¢æœ Agent

### 16.8.1 åˆå§‹ç‰ˆæœ¬

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("ä½œä¸ºå®¢æœå›ç­”ï¼š{question}")
chain = prompt | ChatOpenAI(model="gpt-3.5-turbo") | StrOutputParser()
```

**åˆå§‹æŒ‡æ ‡ï¼ˆè¿è¡Œ 1 å‘¨ï¼‰ï¼š**
- å¹³å‡å»¶è¿Ÿï¼š1.5s
- ç”¨æˆ·æ»¡æ„åº¦ï¼š65%
- æˆæœ¬/æŸ¥è¯¢ï¼š$0.0012

### 16.8.2 é—®é¢˜è¯Šæ–­

é€šè¿‡ LangSmith å‘ç°ï¼š
1. **20% çš„æŸ¥è¯¢è¶…æ—¶**ï¼ˆ> 5sï¼‰
2. **ä½åˆ† Trace å…±æ€§**ï¼šå›ç­”è¿‡äºç®€çŸ­æˆ–ä¸ç›¸å…³
3. **é«˜æˆæœ¬ Trace**ï¼šå¤æ‚æŸ¥è¯¢é‡å¤è°ƒç”¨ LLM

### 16.8.3 ä¼˜åŒ–æªæ–½

#### ä¼˜åŒ– 1ï¼šæ”¹è¿›æç¤º

```python
improved_prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸“ä¸šå®¢æœä»£è¡¨ï¼Œéµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. å‹å¥½ã€è€å¿ƒã€ä¸“ä¸š
2. æä¾›å…·ä½“ã€å¯æ“ä½œçš„å»ºè®®
3. å¦‚æœä¸ç¡®å®šï¼Œè¯šå®å‘ŠçŸ¥å¹¶å»ºè®®è”ç³»äººå·¥å®¢æœ

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›ç­”ï¼š
""")
```

#### ä¼˜åŒ– 2ï¼šæ·»åŠ ç¼“å­˜

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())  # ç›¸åŒé—®é¢˜ç›´æ¥è¿”å›ç¼“å­˜
```

#### ä¼˜åŒ– 3ï¼šæ·»åŠ  Fallback

```python
primary_chain = improved_prompt | ChatOpenAI(model="gpt-4", timeout=3)
fallback_chain = improved_prompt | ChatOpenAI(model="gpt-3.5-turbo", timeout=5)

robust_chain = primary_chain.with_fallbacks([fallback_chain])
```

### 16.8.4 A/B æµ‹è¯•éªŒè¯

```python
# å¯¹æ¯”æ–°æ—§ç‰ˆæœ¬
results_old = evaluate(
    lambda x: {"output": chain.invoke(x)},
    data="customer-qa-v1",
    experiment_prefix="baseline"
)

results_new = evaluate(
    lambda x: {"output": robust_chain.invoke(x)},
    data="customer-qa-v1",
    experiment_prefix="optimized"
)
```

**ç»“æœå¯¹æ¯”ï¼š**

| æŒ‡æ ‡ | åŸºçº¿ç‰ˆæœ¬ | ä¼˜åŒ–ç‰ˆæœ¬ | æ”¹è¿› |
|------|----------|----------|------|
| ç”¨æˆ·æ»¡æ„åº¦ | 65% | 85% | +31% |
| å¹³å‡å»¶è¿Ÿ | 1.5s | 1.2s | -20% |
| è¶…æ—¶ç‡ | 20% | 2% | -90% |
| æˆæœ¬/æŸ¥è¯¢ | $0.0012 | $0.0018 | +50% |

**å†³ç­–**ï¼šè™½ç„¶æˆæœ¬å¢åŠ ï¼Œä½†æ»¡æ„åº¦å’Œç¨³å®šæ€§å¤§å¹…æå‡ï¼Œå†³å®šå…¨é‡ä¸Šçº¿ã€‚

---

## 16.9 å¸¸è§é—®é¢˜ä¸é™·é˜±

### 16.9.1 Trace æ•°æ®é‡è¿‡å¤§

**é—®é¢˜**ï¼šé«˜æµé‡åº”ç”¨æ¯å¤©äº§ç”Ÿæ•°ç™¾ä¸‡ Traceï¼Œæˆæœ¬å’Œå­˜å‚¨å‹åŠ›å¤§ã€‚

**è§£å†³æ–¹æ¡ˆï¼š**

1. **é‡‡æ ·ç­–ç•¥**
   ```python
   import random
   
   # åªè¿½è¸ª 10% çš„è¯·æ±‚
   if random.random() < 0.1:
       os.environ["LANGCHAIN_TRACING_V2"] = "true"
   else:
       os.environ["LANGCHAIN_TRACING_V2"] = "false"
   
   chain.invoke(...)
   ```

2. **æŒ‰æ¡ä»¶è¿½è¸ª**
   ```python
   # åªè¿½è¸ªå¤±è´¥æˆ–é«˜ä»·å€¼ç”¨æˆ·
   should_trace = (user.is_premium or has_error)
   
   with trace(enabled=should_trace):
       chain.invoke(...)
   ```

### 16.9.2 è¯„ä¼°å™¨ä¸å®é™…ç”¨æˆ·ä½“éªŒä¸ä¸€è‡´

**é—®é¢˜**ï¼šLLM-as-Judge è¯„åˆ†é«˜ï¼Œä½†ç”¨æˆ·åé¦ˆå·®ã€‚

**è§£å†³æ–¹æ¡ˆï¼š**
- ç»“åˆ**çœŸå®ç”¨æˆ·åé¦ˆ**æ ¡å‡†è¯„ä¼°å™¨æƒé‡
- ä½¿ç”¨**å¤šæ ·åŒ–è¯„ä¼°å™¨**ï¼ˆè¯­æ³•ã€äº‹å®ã€ç”¨æˆ·æ„å›¾ï¼‰
- å®šæœŸ**äººå·¥æŠ½æŸ¥**è¯„ä¼°ç»“æœ

### 16.9.3 éšç§ä¸åˆè§„é—®é¢˜

**é—®é¢˜**ï¼šTrace åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼ˆPIIï¼‰ï¼Œè¿å GDPRã€‚

**è§£å†³æ–¹æ¡ˆï¼š**
- **è„±æ•å¤„ç†**ï¼šåœ¨å‘é€å‰ç§»é™¤ PII
- **æœ¬åœ°éƒ¨ç½²**ï¼šè€ƒè™‘è‡ªæ‰˜ç®¡ LangFuse ç­‰å¼€æºæ–¹æ¡ˆ
- **æ•°æ®ä¿ç•™ç­–ç•¥**ï¼šè‡ªåŠ¨åˆ é™¤æ—§ Trace

---

## 16.10 æ‰©å±•é˜…è¯»ä¸èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [LangSmith æ–‡æ¡£](https://docs.smith.langchain.com/)
- [LangSmith Python SDK](https://github.com/langchain-ai/langsmith-sdk)
- [LangSmith Cookbook](https://github.com/langchain-ai/langsmith-cookbook)

### æœ€ä½³å®è·µæŒ‡å—
- [Tracing Best Practices](https://docs.smith.langchain.com/tracing/best-practices)
- [Evaluation Strategies](https://docs.smith.langchain.com/evaluation/strategies)
- [Production Monitoring](https://docs.smith.langchain.com/monitoring)

### è§†é¢‘æ•™ç¨‹
- [LangSmith å¿«é€Ÿä¸Šæ‰‹](https://www.youtube.com/watch?v=xxx) (å®˜æ–¹)
- [ç”Ÿäº§çº§ LLM åº”ç”¨ç›‘æ§](https://www.youtube.com/watch?v=yyy)

---

## æœ¬ç« å°ç»“

æœ¬ç« æ·±å…¥æ¢è®¨äº† LangSmith çš„æ ¸å¿ƒåŠŸèƒ½ï¼š

âœ… **Tracing**ï¼šè‡ªåŠ¨è®°å½•æ¯ä¸€æ­¥æ‰§è¡Œè¿‡ç¨‹ï¼Œæ”¯æŒåµŒå¥— Span å’Œè‡ªå®šä¹‰å…ƒæ•°æ®  
âœ… **Debugging**ï¼šPlayground äº¤äº’å¼è°ƒè¯•ï¼Œå¿«é€Ÿå®šä½å¤±è´¥ Trace  
âœ… **Evaluation**ï¼šæ•°æ®é›†ç®¡ç† + å¤šæ ·åŒ–è¯„ä¼°å™¨ + A/B æµ‹è¯•  
âœ… **Monitoring**ï¼šå®æ—¶ç›‘æ§å»¶è¿Ÿã€æˆæœ¬ã€é”™è¯¯ç‡ï¼Œè®¾ç½®è­¦æŠ¥  
âœ… **Feedback Loop**ï¼šæ”¶é›†ç”¨æˆ·åé¦ˆï¼ŒæŒç»­ä¼˜åŒ–æç¤ºå’Œæ¨¡å‹  

**å…³é”®è¦ç‚¹ï¼š**
1. LangSmith æ˜¯ LangChain ç”Ÿæ€çš„å¯è§‚æµ‹æ€§åŸºçŸ³ï¼Œç”Ÿäº§å¿…å¤‡
2. Trace æä¾›å®Œæ•´çš„æ‰§è¡Œé“¾è·¯é€æ˜åº¦ï¼Œä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–
3. æ•°æ®é›† + è¯„ä¼°å™¨å®ç°ç³»ç»ŸåŒ–è´¨é‡æ§åˆ¶
4. åœ¨çº¿ç›‘æ§ + ç”¨æˆ·åé¦ˆå½¢æˆæŒç»­æ”¹è¿›é—­ç¯
5. æ³¨æ„éšç§åˆè§„ï¼Œåˆç†é‡‡æ ·æ§åˆ¶æˆæœ¬

ä¸‹ä¸€ç« å°†å­¦ä¹  **LangServe**ï¼Œå°†ä¼˜åŒ–åçš„é“¾éƒ¨ç½²ä¸ºç”Ÿäº§çº§ API æœåŠ¡ã€‚

---

**æ€è€ƒé¢˜ï¼š**
1. å¦‚ä½•è®¾è®¡ä¸€ä¸ªè¯„ä¼°å™¨æ¥è¡¡é‡èŠå¤©æœºå™¨äººçš„"å‹å¥½åº¦"ï¼Ÿ
2. åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹åº”è¯¥ä½¿ç”¨ GPT-4 vs GPT-3.5ï¼Ÿå¦‚ä½•é€šè¿‡ LangSmith æ•°æ®æ”¯æŒå†³ç­–ï¼Ÿ
3. å¦‚æœ Trace æ˜¾ç¤º 80% çš„å»¶è¿Ÿæ¥è‡ªå‘é‡æ£€ç´¢ï¼Œä½ ä¼šé‡‡å–å“ªäº›ä¼˜åŒ–æªæ–½ï¼Ÿ
