> **æœ¬ç« ç›®æ ‡**ï¼šæŒæ¡ LCELï¼ˆLangChain Expression Languageï¼‰çš„åŸºæœ¬ç”¨æ³•ï¼Œå­¦ä¼šæ„å»ºç¿»è¯‘é“¾ã€æ‘˜è¦é“¾ã€é—®ç­”é“¾ç­‰å¸¸è§æ¨¡å¼ï¼Œå¹¶æŒæ¡é“¾çš„è°ƒè¯•ä¸é”™è¯¯å¤„ç†æŠ€å·§ã€‚

---

## æœ¬ç« å¯¼è§ˆ

æœ¬ç« ä»å®æˆ˜è§’åº¦å‡ºå‘ï¼Œæ•™ä½ ä½¿ç”¨ LCEL æ„å»ºç”Ÿäº§çº§åº”ç”¨é“¾ï¼š

- **LCEL vs Legacy**ï¼šå¯¹æ¯”æ–°æ—§å†™æ³•ï¼Œç†è§£ä¸ºä½•å®˜æ–¹å¼ºçƒˆæ¨è LCEL ä½œä¸ºæ ‡å‡†å¼€å‘èŒƒå¼
- **ç»å…¸é“¾æ¨¡å¼**ï¼šç¿»è¯‘é“¾ã€æ‘˜è¦é“¾ã€é—®ç­”é“¾ç­‰é«˜é¢‘åœºæ™¯çš„æ ‡å‡†å®ç°æ¨¡æ¿
- **è°ƒè¯•æŠ€å·§**ï¼šä½¿ç”¨ LangSmithã€verbose æ¨¡å¼ã€æ—¥å¿—ç­‰å·¥å…·å¿«é€Ÿå®šä½é—®é¢˜
- **é”™è¯¯å¤„ç†**ï¼šRetryã€Fallbackã€è¶…æ—¶æ§åˆ¶ç­‰ä¼ä¸šçº§å¯é æ€§ä¿éšœæœºåˆ¶
- **å¯è§†åŒ–è°ƒè¯•**ï¼šé€šè¿‡ LangSmith Trace ç†è§£é“¾çš„æ‰§è¡Œæµç¨‹ä¸æ€§èƒ½ç“¶é¢ˆ

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ å°†èƒ½å¤Ÿç‹¬ç«‹æ„å»ºç¨³å®šå¯é çš„ LLM åº”ç”¨é“¾ã€‚

---

## 2.1 Legacy Chain vs LCEL

åœ¨ LangChain æ—©æœŸç‰ˆæœ¬ä¸­ï¼Œå¼€å‘è€…ä½¿ç”¨ `LLMChain`ã€`SequentialChain` ç­‰ç±»æ¥æ„å»ºåº”ç”¨ã€‚ä» v0.1.0 å¼€å§‹ï¼Œå®˜æ–¹æ¨èä½¿ç”¨ **LCELï¼ˆLangChain Expression Languageï¼‰** æ›¿ä»£è¿™äº›æ—§å¼ Chainã€‚

### 2.1.1 LLMChainï¼ˆå·²åºŸå¼ƒï¼‰å›é¡¾

**æ—§å¼å†™æ³•**ï¼ˆä¸æ¨èï¼‰ï¼š

```python
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# æ—§å¼ LLMChain
prompt = ChatPromptTemplate.from_template("Translate to {language}: {text}")
model = ChatOpenAI(model="gpt-4o-mini")

chain = LLMChain(
    llm=model,
    prompt=prompt,
    verbose=True
)

result = chain.run(language="French", text="Hello")
# è­¦å‘Š: LLMChain is deprecated, use LCEL instead
```

**é—®é¢˜**ï¼š
1. éœ€è¦è®°å¿†ä¸åŒ Chain ç±»çš„ APIï¼ˆLLMChainã€SequentialChainã€TransformChain...ï¼‰
2. ç»„åˆå¤æ‚é“¾æ—¶ä»£ç å†—é•¿
3. ç±»å‹æ¨æ–­å›°éš¾
4. æ€§èƒ½ä¼˜åŒ–å—é™

### 2.1.2 ä¸ºä»€ä¹ˆè¿ç§»åˆ° LCELï¼Ÿ

**LCEL çš„ä¼˜åŠ¿**ï¼š

| ç‰¹æ€§ | Legacy Chain | LCEL |
|------|--------------|------|
| **è¯­æ³•** | ç±»å®ä¾‹åŒ– | ç®¡é“æ“ä½œç¬¦ `\|` |
| **ç±»å‹æ¨æ–­** | âŒ å¼± | âœ… å¼ºï¼ˆIDE æ”¯æŒï¼‰ |
| **æµå¼æ”¯æŒ** | âŒ éƒ¨åˆ†æ”¯æŒ | âœ… åŸç”Ÿæ”¯æŒ |
| **å¹¶è¡Œæ‰§è¡Œ** | âŒ éœ€æ‰‹åŠ¨ | âœ… RunnableParallel |
| **è°ƒè¯•** | verbose=True | get_graph()ã€LangSmith |
| **æ€§èƒ½** | âš ï¸ ä¸€èˆ¬ | âœ… ä¼˜åŒ–çš„æ‰§è¡Œå¼•æ“ |

**è®¾è®¡å“²å­¦**ï¼š

$$
\text{LCEL} = \text{Functional Programming} + \text{Runnable Protocol}
$$

LCEL å°†é“¾è§†ä¸ºå‡½æ•°çš„ç»„åˆï¼Œæ¯ä¸ªç»„ä»¶éƒ½å®ç° Runnable æ¥å£ï¼Œé€šè¿‡ç®¡é“æ“ä½œç¬¦ä¸²è”ã€‚

### 2.1.3 è¿ç§»æŒ‡å—ä¸å¯¹æ¯”ç¤ºä¾‹

<div data-component="LegacyVsLCELComparison"></div>

**è¿ç§»å¯¹æ¯”**ï¼š

```python
# âŒ æ—§å¼å†™æ³•
from langchain.chains import LLMChain

chain = LLMChain(llm=model, prompt=prompt)
result = chain.run(text="Hello", language="French")

# âœ… LCEL å†™æ³•
chain = prompt | model | StrOutputParser()
result = chain.invoke({"text": "Hello", "language": "French"})
```

**å¤æ‚é“¾è¿ç§»**ï¼š

```python
# âŒ æ—§å¼ SequentialChain
from langchain.chains import SequentialChain

chain1 = LLMChain(llm=model, prompt=prompt1, output_key="translation")
chain2 = LLMChain(llm=model, prompt=prompt2, output_key="summary")

sequential = SequentialChain(
    chains=[chain1, chain2],
    input_variables=["text"],
    output_variables=["translation", "summary"]
)

# âœ… LCEL å†™æ³•
from langchain_core.runnables import RunnablePassthrough

chain = (
    {"text": RunnablePassthrough()}
    | prompt1 | model | StrOutputParser()
    | (lambda x: {"translation": x})
    | RunnablePassthrough.assign(
        summary=prompt2 | model | StrOutputParser()
    )
)
```

---

## 2.2 ç¬¬ä¸€æ¡ LCEL é“¾

### 2.2.1 Pipe æ“ä½œç¬¦ï¼ˆ|ï¼‰çš„é­”åŠ›

**ç®¡é“æ“ä½œç¬¦** `|` æ˜¯ LCEL çš„æ ¸å¿ƒè¯­æ³•ï¼Œå®ƒè¿æ¥ä¸åŒçš„ Runnable ç»„ä»¶ã€‚

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# å®šä¹‰ç»„ä»¶
prompt = ChatPromptTemplate.from_template("Translate to French: {text}")
model = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

# ç”¨ç®¡é“è¿æ¥
chain = prompt | model | parser

# è°ƒç”¨
result = chain.invoke({"text": "Hello, world!"})
print(result)  # "Bonjour, le monde!"
```

**æ‰§è¡Œæµç¨‹**ï¼š

```
è¾“å…¥: {"text": "Hello, world!"}
  â”‚
  â–¼
prompt.invoke({"text": "Hello, world!"})
  â”‚
  â–¼
ChatPromptValue([HumanMessage(content="Translate to French: Hello, world!")])
  â”‚
  â–¼
model.invoke([HumanMessage(...)])
  â”‚
  â–¼
AIMessage(content="Bonjour, le monde!")
  â”‚
  â–¼
parser.invoke(AIMessage(...))
  â”‚
  â–¼
è¾“å‡º: "Bonjour, le monde!"
```

**æ•°å­¦è¡¨ç¤º**ï¼š

$$
\text{chain}(x) = \text{parser}(\text{model}(\text{prompt}(x)))
$$

### 2.2.2 Prompt â†’ Model â†’ Parser åŸºç¡€æ¨¡å¼

è¿™æ˜¯ LCEL ä¸­æœ€å¸¸è§çš„æ¨¡å¼ï¼š

```python
# æ¨¡å¼æ¨¡æ¿
chain = (
    prompt_template    # ç”Ÿæˆæç¤º
    | language_model   # LLM å¤„ç†
    | output_parser    # è§£æè¾“å‡º
)
```

**å®é™…ç¤ºä¾‹**ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# 1. æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful translator."),
    ("human", "Translate '{text}' to {language}.")
])

# 2. è¯­è¨€æ¨¡å‹
model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 3. è¾“å‡ºè§£æå™¨
parser = StrOutputParser()

# 4. ç»„åˆ
translation_chain = prompt | model | parser

# 5. ä½¿ç”¨
result = translation_chain.invoke({
    "text": "Good morning",
    "language": "Spanish"
})
print(result)  # "Buenos dÃ­as"
```

### 2.2.3 é“¾çš„ç±»å‹æ ‡æ³¨ä¸ IDE æ”¯æŒ

**ç±»å‹æ ‡æ³¨**ï¼š

```python
from langchain_core.runnables import Runnable

# æ˜ç¡®è¾“å…¥è¾“å‡ºç±»å‹
translation_chain: Runnable[dict, str] = prompt | model | parser

# IDE ä¼šè‡ªåŠ¨æç¤º
result: str = translation_chain.invoke({"text": "Hi", "language": "French"})
```

**è‡ªå®šä¹‰ç±»å‹**ï¼š

```python
from typing import TypedDict

class TranslationInput(TypedDict):
    text: str
    language: str

def create_translation_chain() -> Runnable[TranslationInput, str]:
    prompt = ChatPromptTemplate.from_template("Translate to {language}: {text}")
    model = ChatOpenAI(model="gpt-4o-mini")
    parser = StrOutputParser()
    return prompt | model | parser

# ä½¿ç”¨
chain = create_translation_chain()
result = chain.invoke({"text": "Hello", "language": "German"})
```

---

## 2.3 å¸¸è§ç®€å•é“¾æ¨¡å¼

### 2.3.0 é”™è¯¯å¤„ç†ç­–ç•¥

<div data-component="ErrorHandlingFlow"></div>

### 2.3.1 ç¿»è¯‘é“¾ï¼ˆTranslation Chainï¼‰

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

def create_translator(target_language: str):
    """åˆ›å»ºç¿»è¯‘å™¨"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", f"You are a professional translator. Translate all inputs to {target_language}."),
        ("human", "{text}")
    ])
    
    model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    parser = StrOutputParser()
    
    return prompt | model | parser

# ä½¿ç”¨
french_translator = create_translator("French")
spanish_translator = create_translator("Spanish")

print(french_translator.invoke({"text": "Hello"}))    # "Bonjour"
print(spanish_translator.invoke({"text": "Hello"}))   # "Hola"
```

**å¤šè¯­è¨€ç¿»è¯‘**ï¼š

```python
from langchain_core.runnables import RunnableParallel

# å¹¶è¡Œç¿»è¯‘åˆ°å¤šç§è¯­è¨€
multi_translator = RunnableParallel(
    french=create_translator("French"),
    spanish=create_translator("Spanish"),
    german=create_translator("German")
)

result = multi_translator.invoke({"text": "Good morning"})
print(result)
# {
#   'french': 'Bonjour',
#   'spanish': 'Buenos dÃ­as',
#   'german': 'Guten Morgen'
# }
```

### 2.3.2 æ‘˜è¦é“¾ï¼ˆSummarization Chainï¼‰

```python
def create_summarizer(max_words: int = 50):
    """åˆ›å»ºæ‘˜è¦ç”Ÿæˆå™¨"""
    prompt = ChatPromptTemplate.from_template(
        "Summarize the following text in {max_words} words or less:\n\n{text}"
    )
    
    model = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
    parser = StrOutputParser()
    
    chain = prompt | model | parser
    
    # éƒ¨åˆ†å¡«å…… max_words
    return chain.partial(max_words=max_words)

# ä½¿ç”¨
summarizer = create_summarizer(max_words=30)

long_text = """
LangChain is a framework for developing applications powered by language models. 
It enables applications that are context-aware and reason about their actions. 
The framework consists of several parts: LangChain Libraries, LangChain Templates, 
LangServe, LangSmith, and LangChain Hub.
"""

summary = summarizer.invoke({"text": long_text})
print(summary)
# "LangChain is a framework for building context-aware, reasoning language model 
#  applications, comprising Libraries, Templates, LangServe, LangSmith, and Hub."
```

**åˆ†çº§æ‘˜è¦**ï¼š

```python
from langchain_core.runnables import RunnablePassthrough

# ä¸¤çº§æ‘˜è¦ï¼šå…ˆæ‘˜è¦åˆ°100è¯ï¼Œå†æ‘˜è¦åˆ°20è¯
two_level_summary = (
    create_summarizer(max_words=100)
    | (lambda x: {"text": x})
    | create_summarizer(max_words=20)
)

result = two_level_summary.invoke({"text": long_text})
```

### 2.3.3 é—®ç­”é“¾ï¼ˆQA Chainï¼‰

```python
def create_qa_chain():
    """åˆ›å»ºé—®ç­”é“¾"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Answer the question based on the context provided. "
                   "If you cannot answer, say 'I don't know'."),
        ("human", "Context: {context}\n\nQuestion: {question}")
    ])
    
    model = ChatOpenAI(model="gpt-4o", temperature=0)
    parser = StrOutputParser()
    
    return prompt | model | parser

# ä½¿ç”¨
qa_chain = create_qa_chain()

context = """
LangChain was created by Harrison Chase in October 2022. 
It is an open-source framework that helps developers build LLM applications.
"""

answer = qa_chain.invoke({
    "context": context,
    "question": "Who created LangChain?"
})
print(answer)  # "Harrison Chase created LangChain."

answer = qa_chain.invoke({
    "context": context,
    "question": "When was LangChain released?"
})
print(answer)  # "LangChain was created in October 2022."
```

### 2.3.4 å®ä½“æå–é“¾ï¼ˆEntity Extractionï¼‰

```python
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

class ExtractedEntities(BaseModel):
    """å®ä½“æ•°æ®æ¨¡å‹"""
    people: list[str] = Field(description="List of people mentioned")
    organizations: list[str] = Field(description="List of organizations")
    locations: list[str] = Field(description="List of locations")

def create_entity_extractor():
    """åˆ›å»ºå®ä½“æå–å™¨"""
    parser = PydanticOutputParser(pydantic_object=ExtractedEntities)
    
    prompt = ChatPromptTemplate.from_template(
        "Extract entities from the following text.\n"
        "{format_instructions}\n\n"
        "Text: {text}"
    )
    
    model = ChatOpenAI(model="gpt-4o", temperature=0)
    
    chain = (
        prompt.partial(format_instructions=parser.get_format_instructions())
        | model
        | parser
    )
    
    return chain

# ä½¿ç”¨
extractor = create_entity_extractor()

text = """
Elon Musk announced that Tesla will open a new factory in Berlin, Germany. 
The company plans to hire 10,000 employees in the next year.
"""

entities = extractor.invoke({"text": text})
print(entities)
# ExtractedEntities(
#     people=['Elon Musk'],
#     organizations=['Tesla'],
#     locations=['Berlin', 'Germany']
# )
```

---

## 2.4 é“¾çš„è°ƒè¯•ä¸æ£€æŸ¥

### 2.4.1 get_graph()ï¼šæŸ¥çœ‹é“¾ç»“æ„

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("Translate to French: {text}")
model = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

chain = prompt | model | parser

# è·å–é“¾çš„å›¾ç»“æ„
graph = chain.get_graph()
print(graph.draw_ascii())
```

**è¾“å‡º**ï¼š

```
           +--------------+              
           | PromptInput  |              
           +--------------+              
                   *                     
                   *                     
                   *                     
         +--------------------+          
         | ChatPromptTemplate |          
         +--------------------+          
                   *                     
                   *                     
                   *                     
           +--------------+              
           | ChatOpenAI   |              
           +--------------+              
                   *                     
                   *                     
                   *                     
        +---------------------+          
        | StrOutputParser     |          
        +---------------------+          
                   *                     
                   *                     
                   *                     
          +----------------+             
          | StrOutputParser |             
          +----------------+             
```

<div data-component="ChainGraphVisualizer"></div>

### 2.4.2 verbose=Trueï¼šè¯¦ç»†æ—¥å¿—

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
chain = prompt | model.with_config({"verbose": True}) | parser

result = chain.invoke({"text": "Hello"})

# è¾“å‡ºï¼š
# [chain/start] Entering Chain
# [chat_model/start] Entering ChatOpenAI
# [chat_model/end] ChatOpenAI output: AIMessage(content="Bonjour")
# [chain/end] Chain output: "Bonjour"
```

**è‡ªå®šä¹‰å›è°ƒ**ï¼š

```python
from langchain.callbacks import StdOutCallbackHandler

callback = StdOutCallbackHandler()

result = chain.invoke(
    {"text": "Hello"},
    config={"callbacks": [callback]}
)
```

### 2.4.3 LangSmith Tracing åˆæ¢

**å¯ç”¨ LangSmith**ï¼š

```python
import os

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "ls__your_api_key"
os.environ["LANGCHAIN_PROJECT"] = "my-first-project"

# æ­¤åæ‰€æœ‰é“¾è°ƒç”¨è‡ªåŠ¨è¿½è¸ª
result = chain.invoke({"text": "Hello"})

# åœ¨ https://smith.langchain.com æŸ¥çœ‹è¿½è¸ª
```

**æŸ¥çœ‹è¿½è¸ªä¿¡æ¯**ï¼š
- è¾“å…¥/è¾“å‡º
- å»¶è¿Ÿæ—¶é—´
- Token æ¶ˆè€—
- é”™è¯¯å †æ ˆ
- åµŒå¥—è°ƒç”¨å…³ç³»

---

## 2.5 é”™è¯¯å¤„ç†åŸºç¡€

### 2.5.1 try-except åŒ…è£…

```python
try:
    result = chain.invoke({"text": "Hello"})
    print(result)
except Exception as e:
    print(f"Error: {e}")
    # è®°å½•æ—¥å¿—ã€è¿”å›é»˜è®¤å€¼ç­‰
```

**å¸¸è§é”™è¯¯**ï¼š

```python
from openai import AuthenticationError, RateLimitError

try:
    result = chain.invoke({"text": "Hello"})
except AuthenticationError:
    print("Invalid API key")
except RateLimitError:
    print("Rate limit exceeded, retry later")
except Exception as e:
    print(f"Unexpected error: {e}")
```

### 2.5.2 Fallback æœºåˆ¶é¢„è§ˆ

**with_fallbacks()** åœ¨ä¸»é“¾å¤±è´¥æ—¶åˆ‡æ¢åˆ°å¤‡ç”¨é“¾ã€‚

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# ä¸»æ¨¡å‹
primary_model = ChatOpenAI(model="gpt-4o")

# å¤‡ç”¨æ¨¡å‹
fallback_model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

# å¸¦é™çº§çš„é“¾
chain = (
    prompt 
    | primary_model.with_fallbacks([fallback_model])
    | parser
)

# å¦‚æœ GPT-4 å¤±è´¥ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° Claude
result = chain.invoke({"text": "Hello"})
```

### 2.5.3 é‡è¯•ç­–ç•¥

**with_retry()** è‡ªåŠ¨é‡è¯•å¤±è´¥çš„è°ƒç”¨ã€‚

```python
from langchain_core.runnables import RunnableRetry

# è‡ªåŠ¨é‡è¯•ï¼ˆæœ€å¤š3æ¬¡ï¼‰
model_with_retry = model.with_retry(
    stop_after_attempt=3,
    wait_exponential_multiplier=1,  # æŒ‡æ•°é€€é¿
    wait_exponential_max=10
)

chain = prompt | model_with_retry | parser

# é‡åˆ°ä¸´æ—¶é”™è¯¯ä¼šè‡ªåŠ¨é‡è¯•
result = chain.invoke({"text": "Hello"})
```

**è‡ªå®šä¹‰é‡è¯•æ¡ä»¶**ï¼š

```python
def should_retry(error: Exception) -> bool:
    """åªå¯¹ç‰¹å®šé”™è¯¯é‡è¯•"""
    return isinstance(error, RateLimitError)

model_with_custom_retry = model.with_retry(
    retry_if_exception=should_retry,
    stop_after_attempt=5
)
```

---

## ğŸ¯ æœ¬ç« å°ç»“

**æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **LCEL ä¼˜äº Legacy Chain**ï¼šè¯­æ³•ç®€æ´ã€ç±»å‹å®‰å…¨ã€æ€§èƒ½æ›´å¥½
2. **ç®¡é“æ“ä½œç¬¦**ï¼š`prompt | model | parser` æ˜¯æœ€åŸºç¡€çš„æ¨¡å¼
3. **å¸¸è§é“¾**ï¼šç¿»è¯‘é“¾ã€æ‘˜è¦é“¾ã€é—®ç­”é“¾ã€å®ä½“æå–é“¾
4. **è°ƒè¯•å·¥å…·**ï¼šget_graph()ã€verboseã€LangSmith
5. **é”™è¯¯å¤„ç†**ï¼štry-exceptã€fallbacksã€retry

**æŒæ¡æ£€æŸ¥**ï¼š

- [ ] èƒ½è§£é‡Š LCEL ç›¸æ¯” Legacy Chain çš„ä¼˜åŠ¿
- [ ] èƒ½ç”¨ LCEL æ„å»ºç¿»è¯‘é“¾å’Œæ‘˜è¦é“¾
- [ ] èƒ½ä½¿ç”¨ PydanticOutputParser æå–ç»“æ„åŒ–æ•°æ®
- [ ] èƒ½ç”¨ get_graph() æŸ¥çœ‹é“¾ç»“æ„
- [ ] èƒ½é…ç½® fallback å’Œ retry æœºåˆ¶

**ç»ƒä¹ é¢˜**ï¼š

1. **å¤šæ­¥éª¤é“¾**ï¼šæ„å»ºä¸€ä¸ªé“¾ï¼Œå…ˆç¿»è¯‘æ–‡æœ¬åˆ°æ³•è¯­ï¼Œå†å¯¹æ³•è¯­æ–‡æœ¬è¿›è¡Œæ‘˜è¦
2. **æ¡ä»¶æ‰§è¡Œ**ï¼šæ ¹æ®è¾“å…¥è¯­è¨€è‡ªåŠ¨é€‰æ‹©ç›®æ ‡è¯­è¨€ï¼ˆè‹±è¯­â†’æ³•è¯­ï¼Œæ³•è¯­â†’è‹±è¯­ï¼‰
3. **æ‰¹é‡å¤„ç†**ï¼šç”¨ batch() æ–¹æ³•åŒæ—¶ç¿»è¯‘ 10 æ¡æ¶ˆæ¯ï¼Œæµ‹é‡æ€»è€—æ—¶
4. **é”™è¯¯æ¢å¤**ï¼šå®ç°ä¸€ä¸ªå¸¦é‡è¯•å’Œé™çº§çš„ç¿»è¯‘é“¾ï¼Œä¸»æ¨¡å‹å¤±è´¥æ—¶åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼š

Chapter 3 å°†æ·±å…¥ LCEL çš„é«˜çº§ç‰¹æ€§ï¼ŒåŒ…æ‹¬ RunnablePassthroughã€RunnableParallelã€é…ç½®åŒ–ã€Fallbackã€Retry ç­‰ã€‚

---

## ğŸ“š æ‰©å±•é˜…è¯»

- [LCEL å®Œæ•´æŒ‡å—](https://python.langchain.com/docs/concepts/lcel)
- [ä» Legacy Chain è¿ç§»](https://python.langchain.com/docs/versions/migrating_chains/)
- [å¸¸è§é“¾æ¨¡å¼](https://python.langchain.com/docs/how_to/)
- [LangSmith è¿½è¸ª](https://docs.smith.langchain.com/observability/how_to_guides/tracing)
- [é”™è¯¯å¤„ç†æœ€ä½³å®è·µ](https://python.langchain.com/docs/how_to/fallbacks)
