# Chapter 28: é«˜çº§ Agent æ¨¡å¼ä¸äººæœºåä½œ

åœ¨å‰é¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å·²ç»å­¦ä¹ äº† Agent çš„åŸºç¡€æ¶æ„ã€ReAct æ¨¡å¼ã€å¤š Agent ç³»ç»Ÿä»¥åŠè§„åˆ’ä¸åæ€æœºåˆ¶ã€‚ç„¶è€Œï¼Œåœ¨å®é™…çš„ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒAgent ç³»ç»Ÿå¾€å¾€éœ€è¦å¤„ç†æ›´åŠ å¤æ‚çš„åœºæ™¯ï¼šå¦‚ä½•è®© Agent åœ¨å…³é”®å†³ç­–ç‚¹æš‚åœå¹¶è¯·æ±‚äººå·¥å®¡æ‰¹ï¼Ÿå¦‚ä½•ä¸º Agent æ„å»ºé•¿æœŸè®°å¿†ä»¥æ”¯æŒè·¨ä¼šè¯çš„ä¸Šä¸‹æ–‡å»¶ç»­ï¼Ÿå¦‚ä½•ä¼˜é›…åœ°ç¼–æ’å¤šä¸ªå·¥å…·çš„è°ƒç”¨é¡ºåºä¸ä¾èµ–å…³ç³»ï¼Ÿæœ¬ç« å°†æ·±å…¥æ¢è®¨è¿™äº›é«˜çº§ Agent æ¨¡å¼ï¼Œå¸®åŠ©æ‚¨æ„å»ºæ›´åŠ æ™ºèƒ½ã€å¯æ§ã€å¯é çš„ä¼ä¸šçº§ Agent ç³»ç»Ÿã€‚

> **æœ¬ç« æ ¸å¿ƒå†…å®¹**ï¼š
> - äººæœºåä½œï¼ˆHuman-in-the-Loopï¼‰ï¼šä¸­æ–­æœºåˆ¶ã€å®¡æ‰¹æµç¨‹ã€åé¦ˆæ³¨å…¥
> - é•¿æœŸè®°å¿†ç³»ç»Ÿï¼šè·¨ä¼šè¯è®°å¿†ã€çŸ¥è¯†å›¾è°±ã€å‘é‡è®°å¿†
> - å·¥å…·ç¼–æ’ä¸ä¾èµ–ç®¡ç†ï¼šå·¥å…·é“¾ã€æ¡ä»¶å·¥å…·ã€åŠ¨æ€å·¥å…·åŠ è½½
> - Agent è°ƒè¯•ä¸å¯è§‚æµ‹æ€§ï¼šä¸­é—´çŠ¶æ€æ£€æŸ¥ã€å†³ç­–è·¯å¾„è¿½è¸ª
> - é«˜çº§é”™è¯¯æ¢å¤ï¼šè‡ªä¿®å¤ã€é™çº§ç­–ç•¥ã€äººå·¥æ¥ç®¡

## 28.1 äººæœºåä½œï¼ˆHuman-in-the-Loopï¼‰åŸºç¡€

### 28.1.1 ä¸ºä»€ä¹ˆéœ€è¦äººæœºåä½œï¼Ÿ

åœ¨è®¸å¤šä¼ä¸šåœºæ™¯ä¸­ï¼Œå®Œå…¨è‡ªåŠ¨åŒ–çš„ Agent å¯èƒ½å¹¶ä¸åˆé€‚ï¼š

1. **é«˜é£é™©å†³ç­–**ï¼šè´¢åŠ¡å®¡æ‰¹ã€æ•°æ®åˆ é™¤ã€åˆåŒç­¾ç½²ç­‰æ“ä½œéœ€è¦äººå·¥ç¡®è®¤
2. **ä¸ç¡®å®šæ€§å¤„ç†**ï¼šå½“ Agent å¯¹ç»“æœä¸ç¡®å®šæ—¶ï¼Œéœ€è¦äººå·¥æŒ‡å¯¼
3. **åˆè§„æ€§è¦æ±‚**ï¼šæŸäº›è¡Œä¸šï¼ˆé‡‘èã€åŒ»ç–—ï¼‰è¦æ±‚äººå·¥å®¡æ ¸å…³é”®æ­¥éª¤
4. **è´¨é‡æ§åˆ¶**ï¼šåœ¨ Agent è¾“å‡ºæœ€ç»ˆç»“æœå‰ï¼Œéœ€è¦äººå·¥è¯„å®¡
5. **æŒç»­å­¦ä¹ **ï¼šé€šè¿‡äººå·¥åé¦ˆæ”¹è¿› Agent è¡Œä¸º

### 28.1.2 LangGraph çš„ä¸­æ–­æœºåˆ¶

LangGraph æä¾›äº†å†…ç½®çš„ä¸­æ–­ï¼ˆinterruptï¼‰åŠŸèƒ½ï¼Œå…è®¸ Agent åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­æš‚åœå¹¶ç­‰å¾…äººå·¥è¾“å…¥ã€‚

**åŸºæœ¬ä¸­æ–­ç¤ºä¾‹**ï¼š

```python
from typing import TypedDict, Annotated
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage

# å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    messages: Annotated[list, "æ¶ˆæ¯å†å²"]
    approval_needed: bool
    approved: bool | None

# å®šä¹‰èŠ‚ç‚¹
def analyze_request(state: AgentState):
    """åˆ†æç”¨æˆ·è¯·æ±‚"""
    last_message = state["messages"][-1].content
    
    # æ£€æµ‹æ˜¯å¦éœ€è¦å®¡æ‰¹ï¼ˆä¾‹å¦‚ï¼šåˆ é™¤æ“ä½œï¼‰
    needs_approval = "delete" in last_message.lower() or "remove" in last_message.lower()
    
    return {
        "messages": state["messages"] + [AIMessage(content="å·²åˆ†æè¯·æ±‚ï¼Œæ£€æµ‹åˆ°é«˜é£é™©æ“ä½œ")],
        "approval_needed": needs_approval,
        "approved": None
    }

def request_approval(state: AgentState):
    """è¯·æ±‚äººå·¥å®¡æ‰¹ï¼ˆæ­¤èŠ‚ç‚¹ä¼šè§¦å‘ä¸­æ–­ï¼‰"""
    return {
        "messages": state["messages"] + [AIMessage(content="âš ï¸ è¯·å®¡æ‰¹æ­¤æ“ä½œï¼šæ˜¯å¦å…è®¸æ‰§è¡Œï¼Ÿ")],
    }

def execute_action(state: AgentState):
    """æ‰§è¡Œå®é™…æ“ä½œ"""
    if state.get("approved"):
        result = "âœ… æ“ä½œå·²æ‰§è¡Œ"
    else:
        result = "âŒ æ“ä½œå·²æ‹’ç»"
    
    return {
        "messages": state["messages"] + [AIMessage(content=result)]
    }

# æ„å»ºå›¾
workflow = StateGraph(AgentState)
workflow.add_node("analyze", analyze_request)
workflow.add_node("request_approval", request_approval)
workflow.add_node("execute", execute_action)

# æ·»åŠ è¾¹
workflow.add_edge(START, "analyze")
workflow.add_conditional_edges(
    "analyze",
    lambda x: "request_approval" if x["approval_needed"] else "execute"
)
workflow.add_edge("request_approval", "execute")
workflow.add_edge("execute", END)

# ç¼–è¯‘æ—¶æ·»åŠ ä¸­æ–­ç‚¹
memory = MemorySaver()
app = workflow.compile(
    checkpointer=memory,
    interrupt_before=["execute"]  # åœ¨æ‰§è¡Œå‰ä¸­æ–­
)
```

**ä½¿ç”¨ä¸­æ–­çš„ Agent**ï¼š

```python
# é…ç½®
config = {"configurable": {"thread_id": "approval-demo-1"}}

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šè§¦å‘ä¸­æ–­
initial_input = {
    "messages": [HumanMessage(content="è¯·åˆ é™¤ç”¨æˆ· ID 12345 çš„æ‰€æœ‰æ•°æ®")],
    "approval_needed": False,
    "approved": None
}

# è¿è¡Œåˆ°ä¸­æ–­ç‚¹
result = app.invoke(initial_input, config)
print("ä¸­æ–­å‰çš„çŠ¶æ€ï¼š")
print(result["messages"][-1].content)
# è¾“å‡ºï¼šâš ï¸ è¯·å®¡æ‰¹æ­¤æ“ä½œï¼šæ˜¯å¦å…è®¸æ‰§è¡Œï¼Ÿ

# æ£€æŸ¥å½“å‰çŠ¶æ€
state = app.get_state(config)
print(f"ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼š{state.next}")  # ('execute',)

# äººå·¥å®¡æ‰¹ï¼šæ›´æ–°çŠ¶æ€å¹¶ç»§ç»­
app.update_state(config, {"approved": True})

# ç»§ç»­æ‰§è¡Œ
final_result = app.invoke(None, config)
print("æœ€ç»ˆç»“æœï¼š")
print(final_result["messages"][-1].content)
# è¾“å‡ºï¼šâœ… æ“ä½œå·²æ‰§è¡Œ
```

### 28.1.3 å¤šçº§å®¡æ‰¹æµç¨‹

åœ¨å¤æ‚çš„ä¼ä¸šåœºæ™¯ä¸­ï¼Œå¯èƒ½éœ€è¦å¤šçº§å®¡æ‰¹ï¼š

```python
from enum import Enum

class ApprovalLevel(str, Enum):
    NONE = "none"
    MANAGER = "manager"
    DIRECTOR = "director"
    CEO = "ceo"

class MultiLevelState(TypedDict):
    messages: list
    request_type: str
    amount: float | None
    current_approval_level: ApprovalLevel
    approvals: dict[ApprovalLevel, bool]

def determine_approval_level(state: MultiLevelState):
    """æ ¹æ®é‡‘é¢ç¡®å®šå®¡æ‰¹çº§åˆ«"""
    amount = state.get("amount", 0)
    
    if amount < 1000:
        level = ApprovalLevel.NONE
    elif amount < 10000:
        level = ApprovalLevel.MANAGER
    elif amount < 100000:
        level = ApprovalLevel.DIRECTOR
    else:
        level = ApprovalLevel.CEO
    
    return {"current_approval_level": level}

def manager_approval(state: MultiLevelState):
    """ç»ç†å®¡æ‰¹"""
    return {
        "messages": state["messages"] + [
            AIMessage(content=f"ç­‰å¾…ç»ç†å®¡æ‰¹é‡‘é¢ ${state['amount']}")
        ]
    }

def director_approval(state: MultiLevelState):
    """æ€»ç›‘å®¡æ‰¹"""
    return {
        "messages": state["messages"] + [
            AIMessage(content=f"ç­‰å¾…æ€»ç›‘å®¡æ‰¹é‡‘é¢ ${state['amount']}")
        ]
    }

def ceo_approval(state: MultiLevelState):
    """CEO å®¡æ‰¹"""
    return {
        "messages": state["messages"] + [
            AIMessage(content=f"ç­‰å¾… CEO å®¡æ‰¹é‡‘é¢ ${state['amount']}")
        ]
    }

# æ„å»ºå¤šçº§å®¡æ‰¹æµç¨‹
workflow = StateGraph(MultiLevelState)
workflow.add_node("determine_level", determine_approval_level)
workflow.add_node("manager_approval", manager_approval)
workflow.add_node("director_approval", director_approval)
workflow.add_node("ceo_approval", ceo_approval)
workflow.add_node("execute", execute_action)

workflow.add_edge(START, "determine_level")
workflow.add_conditional_edges(
    "determine_level",
    lambda x: {
        ApprovalLevel.NONE: "execute",
        ApprovalLevel.MANAGER: "manager_approval",
        ApprovalLevel.DIRECTOR: "director_approval",
        ApprovalLevel.CEO: "ceo_approval"
    }[x["current_approval_level"]]
)

# æ‰€æœ‰å®¡æ‰¹èŠ‚ç‚¹éƒ½è¿æ¥åˆ°æ‰§è¡ŒèŠ‚ç‚¹
workflow.add_edge("manager_approval", "execute")
workflow.add_edge("director_approval", "execute")
workflow.add_edge("ceo_approval", "execute")
workflow.add_edge("execute", END)

# ç¼–è¯‘æ—¶åœ¨æ‰€æœ‰å®¡æ‰¹èŠ‚ç‚¹å‰ä¸­æ–­
app = workflow.compile(
    checkpointer=MemorySaver(),
    interrupt_before=["manager_approval", "director_approval", "ceo_approval"]
)
```

<div data-component="HumanInLoopFlow"></div>

### 28.1.4 å®¡æ‰¹è¶…æ—¶ä¸è‡ªåŠ¨é™çº§

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œäººå·¥å®¡æ‰¹å¯èƒ½å› ä¸ºå„ç§åŸå› å»¶è¿Ÿï¼Œéœ€è¦è®¾ç½®è¶…æ—¶æœºåˆ¶ï¼š

```python
import time
from datetime import datetime, timedelta

class TimeoutState(TypedDict):
    messages: list
    approval_requested_at: str | None
    approval_timeout_seconds: int
    approved: bool | None

def check_timeout(state: TimeoutState):
    """æ£€æŸ¥å®¡æ‰¹æ˜¯å¦è¶…æ—¶"""
    if not state.get("approval_requested_at"):
        return {"approved": False}
    
    requested_at = datetime.fromisoformat(state["approval_requested_at"])
    timeout = state.get("approval_timeout_seconds", 300)  # é»˜è®¤ 5 åˆ†é’Ÿ
    
    if datetime.now() - requested_at > timedelta(seconds=timeout):
        # è¶…æ—¶ï¼šè‡ªåŠ¨æ‹’ç»æˆ–é™çº§å¤„ç†
        return {
            "approved": False,
            "messages": state["messages"] + [
                AIMessage(content="â±ï¸ å®¡æ‰¹è¶…æ—¶ï¼Œæ“ä½œå·²è‡ªåŠ¨æ‹’ç»")
            ]
        }
    
    return {}

# åœ¨å®é™…åº”ç”¨ä¸­çš„ä½¿ç”¨
def request_approval_with_timeout(state: TimeoutState):
    return {
        "messages": state["messages"] + [AIMessage(content="ç­‰å¾…å®¡æ‰¹...")],
        "approval_requested_at": datetime.now().isoformat()
    }
```

### 28.1.5 äººå·¥åé¦ˆæ³¨å…¥

é™¤äº†ç®€å•çš„æ‰¹å‡†/æ‹’ç»ï¼Œäººå·¥è¿˜å¯ä»¥æä¾›è¯¦ç»†çš„åé¦ˆå’ŒæŒ‡å¯¼ï¼š

```python
class FeedbackState(TypedDict):
    messages: list
    agent_proposal: str | None
    human_feedback: str | None
    revision_count: int

def generate_proposal(state: FeedbackState):
    """Agent ç”Ÿæˆæè®®"""
    # å®é™…åº”ç”¨ä¸­è°ƒç”¨ LLM
    proposal = "æˆ‘å»ºè®®é‡‡ç”¨æ–¹æ¡ˆ Aï¼Œå› ä¸ºå®ƒæˆæœ¬æœ€ä½"
    
    return {
        "agent_proposal": proposal,
        "messages": state["messages"] + [AIMessage(content=f"ğŸ“ æè®®ï¼š{proposal}")]
    }

def incorporate_feedback(state: FeedbackState):
    """æ ¹æ®äººå·¥åé¦ˆä¿®è®¢æè®®"""
    feedback = state.get("human_feedback")
    
    if not feedback:
        # æ— åé¦ˆï¼Œä½¿ç”¨åŸæè®®
        return {"messages": state["messages"] + [AIMessage(content="é‡‡ç”¨åŸæ–¹æ¡ˆ")]}
    
    # å®é™…åº”ç”¨ä¸­è°ƒç”¨ LLM æ•´åˆåé¦ˆ
    revised = f"æ ¹æ®åé¦ˆ'{feedback}'ï¼Œæˆ‘ä¿®è®¢æè®®ä¸ºï¼šæ–¹æ¡ˆ B"
    
    return {
        "messages": state["messages"] + [AIMessage(content=f"ğŸ“ ä¿®è®¢åï¼š{revised}")],
        "revision_count": state.get("revision_count", 0) + 1
    }

# æ„å»ºåé¦ˆå¾ªç¯
workflow = StateGraph(FeedbackState)
workflow.add_node("generate", generate_proposal)
workflow.add_node("incorporate", incorporate_feedback)

workflow.add_edge(START, "generate")
workflow.add_edge("generate", "incorporate")

# æ¡ä»¶è¾¹ï¼šå¦‚æœæœ‰åé¦ˆä¸”æœªè¶…è¿‡æœ€å¤§ä¿®è®¢æ¬¡æ•°ï¼Œç»§ç»­ä¿®è®¢
workflow.add_conditional_edges(
    "incorporate",
    lambda x: "generate" if x.get("human_feedback") and x.get("revision_count", 0) < 3 else END
)

app = workflow.compile(
    checkpointer=MemorySaver(),
    interrupt_after=["generate"]  # ç”Ÿæˆåä¸­æ–­ï¼Œç­‰å¾…åé¦ˆ
)
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
config = {"configurable": {"thread_id": "feedback-loop-1"}}

# åˆå§‹è¿è¡Œ
initial_state = {
    "messages": [HumanMessage(content="è¯·ä¸ºæ–°é¡¹ç›®æ¨èæŠ€æœ¯æ–¹æ¡ˆ")],
    "agent_proposal": None,
    "human_feedback": None,
    "revision_count": 0
}

result = app.invoke(initial_state, config)
print(result["agent_proposal"])
# è¾“å‡ºï¼šæˆ‘å»ºè®®é‡‡ç”¨æ–¹æ¡ˆ Aï¼Œå› ä¸ºå®ƒæˆæœ¬æœ€ä½

# æä¾›äººå·¥åé¦ˆ
app.update_state(config, {
    "human_feedback": "æˆæœ¬ä¸æ˜¯ä¸»è¦è€ƒè™‘å› ç´ ï¼Œæ€§èƒ½æ›´é‡è¦"
})

# ç»§ç»­æ‰§è¡Œ
result = app.invoke(None, config)
print(result["messages"][-1].content)
# è¾“å‡ºï¼šğŸ“ ä¿®è®¢åï¼šæ ¹æ®åé¦ˆ'æˆæœ¬ä¸æ˜¯ä¸»è¦è€ƒè™‘å› ç´ ï¼Œæ€§èƒ½æ›´é‡è¦'ï¼Œæˆ‘ä¿®è®¢æè®®ä¸ºï¼šæ–¹æ¡ˆ B
```

## 28.2 é•¿æœŸè®°å¿†ç³»ç»Ÿ

### 28.2.1 ä¸ºä»€ä¹ˆ Agent éœ€è¦é•¿æœŸè®°å¿†ï¼Ÿ

ä¼ ç»Ÿçš„å¯¹è¯è®°å¿†ï¼ˆå¦‚ ConversationBufferMemoryï¼‰åªèƒ½ç»´æŠ¤å•æ¬¡ä¼šè¯çš„ä¸Šä¸‹æ–‡ï¼Œä½†åœ¨è®¸å¤šåœºæ™¯ä¸­ï¼ŒAgent éœ€è¦è·¨ä¼šè¯çš„é•¿æœŸè®°å¿†ï¼š

1. **ä¸ªæ€§åŒ–æœåŠ¡**ï¼šè®°ä½ç”¨æˆ·çš„åå¥½ã€å†å²è¡Œä¸º
2. **çŸ¥è¯†ç§¯ç´¯**ï¼šä»å¤šæ¬¡äº¤äº’ä¸­å­¦ä¹ æ–°çŸ¥è¯†
3. **å…³ç³»ç»´æŠ¤**ï¼šè®°ä½ä¸ç”¨æˆ·çš„å…³ç³»å†å²
4. **ä»»åŠ¡å»¶ç»­**ï¼šè·¨ä¼šè¯è¿½è¸ªé•¿æœŸä»»åŠ¡çš„è¿›åº¦

### 28.2.2 é•¿æœŸè®°å¿†çš„æ¶æ„è®¾è®¡

ä¸€ä¸ªå®Œæ•´çš„é•¿æœŸè®°å¿†ç³»ç»Ÿé€šå¸¸åŒ…å«ä¸‰ä¸ªå±‚æ¬¡ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         çŸ­æœŸè®°å¿†ï¼ˆWorking Memoryï¼‰        â”‚  â† å½“å‰ä¼šè¯çš„ä¸Šä¸‹æ–‡
â”‚  ConversationBuffer / ConversationSummaryâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ä¸­æœŸè®°å¿†ï¼ˆEpisodic Memoryï¼‰         â”‚  â† æœ€è¿‘å‡ æ¬¡ä¼šè¯çš„æ‘˜è¦
â”‚   å‘é‡å­˜å‚¨ + æ—¶é—´ç´¢å¼• + é‡è¦æ€§è¯„åˆ†        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      é•¿æœŸè®°å¿†ï¼ˆSemantic Memoryï¼‰         â”‚  â† ç»“æ„åŒ–çŸ¥è¯†åº“
â”‚    çŸ¥è¯†å›¾è°± + å®ä½“å…³ç³» + æŒä¹…åŒ–äº‹å®       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 28.2.3 åŸºäºå‘é‡å­˜å‚¨çš„ä¸­æœŸè®°å¿†

ä½¿ç”¨å‘é‡å­˜å‚¨ä¿å­˜ä¼šè¯æ‘˜è¦ï¼Œå¹¶é€šè¿‡ç›¸ä¼¼åº¦æ£€ç´¢ç›¸å…³å†å²ï¼š

```python
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from datetime import datetime
import uuid

class VectorMemorySystem:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma(
            collection_name="agent_memory",
            embedding_function=self.embeddings,
            persist_directory="./agent_memory_db"
        )
        self.llm = ChatOpenAI(temperature=0)
    
    def save_conversation(self, user_id: str, conversation: list, importance: float = 0.5):
        """ä¿å­˜å¯¹è¯æ‘˜è¦åˆ°å‘é‡å­˜å‚¨"""
        # ä½¿ç”¨ LLM ç”Ÿæˆå¯¹è¯æ‘˜è¦
        summary_prompt = f"è¯·ç”¨ä¸€å¥è¯æ€»ç»“ä»¥ä¸‹å¯¹è¯ï¼š\n{conversation}"
        summary = self.llm.invoke(summary_prompt).content
        
        # æ„å»ºæ–‡æ¡£
        doc = Document(
            page_content=summary,
            metadata={
                "user_id": user_id,
                "timestamp": datetime.now().isoformat(),
                "importance": importance,
                "conversation_id": str(uuid.uuid4()),
                "full_conversation": str(conversation)
            }
        )
        
        # ä¿å­˜åˆ°å‘é‡å­˜å‚¨
        self.vectorstore.add_documents([doc])
        print(f"âœ… å·²ä¿å­˜å¯¹è¯æ‘˜è¦ï¼š{summary}")
    
    def recall_relevant_memories(self, user_id: str, current_query: str, k: int = 3):
        """æ£€ç´¢ç›¸å…³çš„å†å²è®°å¿†"""
        # å‘é‡æ£€ç´¢
        results = self.vectorstore.similarity_search(
            current_query,
            k=k,
            filter={"user_id": user_id}
        )
        
        # æŒ‰é‡è¦æ€§å’Œæ—¶é—´æ’åº
        sorted_results = sorted(
            results,
            key=lambda x: (
                x.metadata.get("importance", 0.5),
                x.metadata.get("timestamp", "")
            ),
            reverse=True
        )
        
        return [
            {
                "summary": doc.page_content,
                "timestamp": doc.metadata["timestamp"],
                "importance": doc.metadata["importance"]
            }
            for doc in sorted_results
        ]
    
    def forget_old_memories(self, user_id: str, days_threshold: int = 30):
        """æ¸…ç†æ—§è®°å¿†ï¼ˆæ ¹æ®æ—¶é—´å’Œé‡è¦æ€§ï¼‰"""
        # å®é™…åº”ç”¨ä¸­éœ€è¦å®ç°åŸºäºæ—¶é—´çš„è¿‡æ»¤å’Œåˆ é™¤
        pass

# ä½¿ç”¨ç¤ºä¾‹
memory_system = VectorMemorySystem()

# ä¿å­˜å¯¹è¯
memory_system.save_conversation(
    user_id="user123",
    conversation=[
        "ç”¨æˆ·ï¼šæˆ‘å–œæ¬¢ Python ç¼–ç¨‹",
        "åŠ©æ‰‹ï¼šå¤ªå¥½äº†ï¼Python æ˜¯ä¸€é—¨å¾ˆæ£’çš„è¯­è¨€"
    ],
    importance=0.7
)

# æ£€ç´¢ç›¸å…³è®°å¿†
memories = memory_system.recall_relevant_memories(
    user_id="user123",
    current_query="ä½ çŸ¥é“æˆ‘å–œæ¬¢ä»€ä¹ˆç¼–ç¨‹è¯­è¨€å—ï¼Ÿ",
    k=3
)
print("ç›¸å…³è®°å¿†ï¼š", memories)
```

### 28.2.4 åŸºäºçŸ¥è¯†å›¾è°±çš„é•¿æœŸè®°å¿†

å¯¹äºç»“æ„åŒ–çš„çŸ¥è¯†ï¼Œä½¿ç”¨çŸ¥è¯†å›¾è°±æ›´åŠ é«˜æ•ˆï¼š

```python
from typing import List, Tuple
import networkx as nx
import json

class KnowledgeGraphMemory:
    def __init__(self):
        self.graph = nx.DiGraph()
    
    def add_entity(self, entity: str, entity_type: str, attributes: dict = None):
        """æ·»åŠ å®ä½“"""
        self.graph.add_node(
            entity,
            type=entity_type,
            attributes=attributes or {}
        )
    
    def add_relation(self, subject: str, predicate: str, object: str, metadata: dict = None):
        """æ·»åŠ å…³ç³»ä¸‰å…ƒç»„"""
        self.graph.add_edge(
            subject,
            object,
            relation=predicate,
            metadata=metadata or {}
        )
    
    def query_relations(self, entity: str, relation_type: str = None) -> List[Tuple]:
        """æŸ¥è¯¢å®ä½“çš„å…³ç³»"""
        results = []
        
        # æŸ¥è¯¢å‡ºè¾¹ï¼ˆå®ä½“ä½œä¸ºä¸»è¯­ï¼‰
        for target in self.graph.successors(entity):
            edge_data = self.graph[entity][target]
            if relation_type is None or edge_data.get("relation") == relation_type:
                results.append((entity, edge_data["relation"], target))
        
        # æŸ¥è¯¢å…¥è¾¹ï¼ˆå®ä½“ä½œä¸ºå®¾è¯­ï¼‰
        for source in self.graph.predecessors(entity):
            edge_data = self.graph[source][entity]
            if relation_type is None or edge_data.get("relation") == relation_type:
                results.append((source, edge_data["relation"], entity))
        
        return results
    
    def get_entity_context(self, entity: str, depth: int = 2) -> dict:
        """è·å–å®ä½“çš„ä¸Šä¸‹æ–‡ï¼ˆå‘¨è¾¹å…³ç³»ï¼‰"""
        # è·å– N è·³å†…çš„æ‰€æœ‰é‚»å±…
        subgraph_nodes = set([entity])
        current_level = {entity}
        
        for _ in range(depth):
            next_level = set()
            for node in current_level:
                next_level.update(self.graph.successors(node))
                next_level.update(self.graph.predecessors(node))
            subgraph_nodes.update(next_level)
            current_level = next_level
        
        subgraph = self.graph.subgraph(subgraph_nodes)
        
        return {
            "entity": entity,
            "relations": [
                (u, subgraph[u][v]["relation"], v)
                for u, v in subgraph.edges()
            ]
        }
    
    def save(self, filepath: str):
        """æŒä¹…åŒ–çŸ¥è¯†å›¾è°±"""
        data = nx.node_link_data(self.graph)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def load(self, filepath: str):
        """åŠ è½½çŸ¥è¯†å›¾è°±"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        self.graph = nx.node_link_graph(data)

# ä½¿ç”¨ç¤ºä¾‹
kg_memory = KnowledgeGraphMemory()

# æ„å»ºç”¨æˆ·çŸ¥è¯†å›¾è°±
kg_memory.add_entity("Alice", "Person", {"age": 28, "occupation": "Engineer"})
kg_memory.add_entity("Python", "ProgrammingLanguage", {"paradigm": "multi-paradigm"})
kg_memory.add_entity("ProjectX", "Project", {"status": "active"})

kg_memory.add_relation("Alice", "likes", "Python")
kg_memory.add_relation("Alice", "works_on", "ProjectX")
kg_memory.add_relation("ProjectX", "uses", "Python")

# æŸ¥è¯¢
relations = kg_memory.query_relations("Alice")
print("Alice çš„å…³ç³»ï¼š", relations)
# è¾“å‡ºï¼š[('Alice', 'likes', 'Python'), ('Alice', 'works_on', 'ProjectX')]

context = kg_memory.get_entity_context("Alice", depth=2)
print("Alice çš„ä¸Šä¸‹æ–‡ï¼š", context)
```

<div data-component="LongTermMemoryArchitecture"></div>

### 28.2.5 æ··åˆè®°å¿†ç³»ç»Ÿï¼šæ•´åˆçŸ­æœŸã€ä¸­æœŸå’Œé•¿æœŸè®°å¿†

```python
from typing import TypedDict
from langchain_core.messages import BaseMessage

class HybridMemoryState(TypedDict):
    messages: list[BaseMessage]  # çŸ­æœŸè®°å¿†ï¼šå½“å‰ä¼šè¯
    recent_memories: list[dict]  # ä¸­æœŸè®°å¿†ï¼šç›¸å…³å†å²ä¼šè¯
    knowledge_graph: dict        # é•¿æœŸè®°å¿†ï¼šç»“æ„åŒ–çŸ¥è¯†

class HybridMemoryAgent:
    def __init__(self, user_id: str):
        self.user_id = user_id
        self.vector_memory = VectorMemorySystem()
        self.kg_memory = KnowledgeGraphMemory()
        self.llm = ChatOpenAI(temperature=0.7)
    
    def process_query(self, state: HybridMemoryState, query: str):
        """å¤„ç†æŸ¥è¯¢ï¼Œæ•´åˆä¸‰å±‚è®°å¿†"""
        # 1. ä»å‘é‡å­˜å‚¨æ£€ç´¢ç›¸å…³å†å²
        recent_memories = self.vector_memory.recall_relevant_memories(
            self.user_id,
            query,
            k=3
        )
        
        # 2. ä»çŸ¥è¯†å›¾è°±æå–ç›¸å…³å®ä½“
        # ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦ NER æå–æŸ¥è¯¢ä¸­çš„å®ä½“ï¼‰
        entities = ["Alice"]  # ç¤ºä¾‹
        kg_context = self.kg_memory.get_entity_context(entities[0], depth=2)
        
        # 3. æ„å»ºå¢å¼ºçš„æç¤º
        context_prompt = f"""
ç›¸å…³å†å²è®°å¿†ï¼š
{chr(10).join([f"- {m['summary']} ({m['timestamp']})" for m in recent_memories])}

çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡ï¼š
{chr(10).join([f"- {s} {r} {o}" for s, r, o in kg_context['relations']])}

å½“å‰å¯¹è¯å†å²ï¼š
{chr(10).join([f"{m.type}: {m.content}" for m in state['messages'][-5:]])}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æ ¹æ®ä»¥ä¸Šæ‰€æœ‰ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
"""
        
        response = self.llm.invoke(context_prompt).content
        return response
    
    def save_conversation(self, conversation: list, importance: float = 0.5):
        """ä¿å­˜å¯¹è¯åˆ°ä¸­æœŸè®°å¿†"""
        self.vector_memory.save_conversation(
            self.user_id,
            conversation,
            importance
        )
    
    def update_knowledge(self, subject: str, relation: str, object: str):
        """æ›´æ–°é•¿æœŸçŸ¥è¯†"""
        self.kg_memory.add_relation(subject, relation, object)

# ä½¿ç”¨ç¤ºä¾‹
agent = HybridMemoryAgent(user_id="user123")

state = {
    "messages": [
        HumanMessage(content="æˆ‘æœ€è¿‘åœ¨å­¦ Rust"),
        AIMessage(content="Rust æ˜¯ä¸€é—¨å¾ˆæ£’çš„ç³»ç»Ÿç¼–ç¨‹è¯­è¨€")
    ],
    "recent_memories": [],
    "knowledge_graph": {}
}

response = agent.process_query(state, "æˆ‘ä¹‹å‰æåˆ°è¿‡å–œæ¬¢ä»€ä¹ˆè¯­è¨€ï¼Ÿ")
print(response)

# ä¿å­˜å½“å‰å¯¹è¯
agent.save_conversation(
    [m.content for m in state["messages"]],
    importance=0.8
)

# æ›´æ–°çŸ¥è¯†å›¾è°±
agent.update_knowledge("user123", "learning", "Rust")
```

## 28.3 å·¥å…·ç¼–æ’ä¸ä¾èµ–ç®¡ç†

### 28.3.1 å·¥å…·é“¾ï¼šé¡ºåºå·¥å…·è°ƒç”¨

åœ¨æŸäº›åœºæ™¯ä¸­ï¼Œå·¥å…·éœ€è¦æŒ‰ç‰¹å®šé¡ºåºè°ƒç”¨ï¼Œåç»­å·¥å…·ä¾èµ–å‰é¢å·¥å…·çš„è¾“å‡ºï¼š

```python
from langchain.tools import tool
from typing import List, Dict

@tool
def fetch_user_info(user_id: str) -> dict:
    """è·å–ç”¨æˆ·ä¿¡æ¯"""
    # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
    return {
        "user_id": user_id,
        "name": "Alice",
        "email": "alice@example.com",
        "preferences": {"language": "zh-CN"}
    }

@tool
def get_user_orders(user_id: str) -> list:
    """è·å–ç”¨æˆ·è®¢å•"""
    # æ¨¡æ‹Ÿè®¢å•æŸ¥è¯¢
    return [
        {"order_id": "ORD001", "amount": 299.99, "status": "shipped"},
        {"order_id": "ORD002", "amount": 149.99, "status": "processing"}
    ]

@tool
def calculate_total_spent(orders: list) -> float:
    """è®¡ç®—ç”¨æˆ·æ€»æ¶ˆè´¹"""
    return sum(order["amount"] for order in orders)

@tool
def send_personalized_email(user_info: dict, total_spent: float) -> str:
    """å‘é€ä¸ªæ€§åŒ–é‚®ä»¶"""
    email_body = f"""
    å°Šæ•¬çš„ {user_info['name']}ï¼Œ
    
    æ‚¨åœ¨æˆ‘ä»¬å¹³å°çš„ç´¯è®¡æ¶ˆè´¹ä¸º ${total_spent:.2f}ã€‚
    æ„Ÿè°¢æ‚¨çš„æ”¯æŒï¼
    """
    return f"é‚®ä»¶å·²å‘é€åˆ° {user_info['email']}"

# å·¥å…·é“¾ç¼–æ’
class ToolChain:
    def __init__(self, tools: List):
        self.tools = {tool.name: tool for tool in tools}
    
    def execute_chain(self, steps: List[Dict], initial_input: Dict):
        """æ‰§è¡Œå·¥å…·é“¾"""
        context = initial_input.copy()
        results = {}
        
        for step in steps:
            tool_name = step["tool"]
            input_mapping = step["input"]
            output_key = step["output"]
            
            # æ„å»ºå·¥å…·è¾“å…¥ï¼ˆä»ä¸Šä¸‹æ–‡ä¸­æå–ï¼‰
            tool_input = {}
            for arg_name, source_key in input_mapping.items():
                if source_key in context:
                    tool_input[arg_name] = context[source_key]
                elif source_key in results:
                    tool_input[arg_name] = results[source_key]
                else:
                    raise ValueError(f"Missing input: {source_key}")
            
            # è°ƒç”¨å·¥å…·
            tool = self.tools[tool_name]
            result = tool.invoke(tool_input)
            
            # ä¿å­˜ç»“æœ
            results[output_key] = result
            context[output_key] = result
            
            print(f"âœ… {tool_name}: {result}")
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
tools = [fetch_user_info, get_user_orders, calculate_total_spent, send_personalized_email]
chain = ToolChain(tools)

# å®šä¹‰å·¥å…·é“¾æ­¥éª¤
steps = [
    {
        "tool": "fetch_user_info",
        "input": {"user_id": "user_id"},
        "output": "user_info"
    },
    {
        "tool": "get_user_orders",
        "input": {"user_id": "user_id"},
        "output": "orders"
    },
    {
        "tool": "calculate_total_spent",
        "input": {"orders": "orders"},
        "output": "total_spent"
    },
    {
        "tool": "send_personalized_email",
        "input": {"user_info": "user_info", "total_spent": "total_spent"},
        "output": "email_result"
    }
]

# æ‰§è¡Œ
results = chain.execute_chain(steps, initial_input={"user_id": "12345"})
print("\næœ€ç»ˆç»“æœï¼š", results["email_result"])
```

### 28.3.2 æ¡ä»¶å·¥å…·è°ƒç”¨

æ ¹æ®ä¸­é—´ç»“æœå†³å®šæ˜¯å¦è°ƒç”¨æŸä¸ªå·¥å…·ï¼š

```python
def execute_conditional_chain(self, steps: List[Dict], initial_input: Dict):
    """æ‰§è¡Œå¸¦æ¡ä»¶çš„å·¥å…·é“¾"""
    context = initial_input.copy()
    results = {}
    
    for step in steps:
        # æ£€æŸ¥æ¡ä»¶
        if "condition" in step:
            condition_func = step["condition"]
            if not condition_func(context, results):
                print(f"â­ï¸  è·³è¿‡ {step['tool']}ï¼ˆæ¡ä»¶ä¸æ»¡è¶³ï¼‰")
                continue
        
        # æ‰§è¡Œå·¥å…·ï¼ˆåŒä¸Šï¼‰
        tool_name = step["tool"]
        tool = self.tools[tool_name]
        
        # ... æ‰§è¡Œé€»è¾‘ ...
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
conditional_steps = [
    {
        "tool": "fetch_user_info",
        "input": {"user_id": "user_id"},
        "output": "user_info"
    },
    {
        "tool": "get_user_orders",
        "input": {"user_id": "user_id"},
        "output": "orders"
    },
    {
        "tool": "send_personalized_email",
        "input": {"user_info": "user_info", "total_spent": "total_spent"},
        "output": "email_result",
        "condition": lambda ctx, res: len(res.get("orders", [])) > 0  # ä»…å½“æœ‰è®¢å•æ—¶å‘é€
    }
]
```

### 28.3.3 åŠ¨æ€å·¥å…·åŠ è½½

æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€å†³å®šåŠ è½½å“ªäº›å·¥å…·ï¼š

```python
from typing import Callable

class DynamicToolLoader:
    def __init__(self):
        self.tool_registry = {}
    
    def register_tool(self, tool_name: str, tool_func: Callable, requires: List[str] = None):
        """æ³¨å†Œå·¥å…·åŠå…¶ä¾èµ–"""
        self.tool_registry[tool_name] = {
            "func": tool_func,
            "requires": requires or []
        }
    
    def get_available_tools(self, context: dict) -> List[str]:
        """æ ¹æ®å½“å‰ä¸Šä¸‹æ–‡è¿”å›å¯ç”¨çš„å·¥å…·"""
        available = []
        
        for tool_name, tool_info in self.tool_registry.items():
            # æ£€æŸ¥æ‰€æœ‰ä¾èµ–æ˜¯å¦æ»¡è¶³
            if all(req in context for req in tool_info["requires"]):
                available.append(tool_name)
        
        return available
    
    def auto_plan_execution(self, goal: str, context: dict):
        """è‡ªåŠ¨è§„åˆ’å·¥å…·æ‰§è¡Œé¡ºåº"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨æ‹“æ‰‘æ’åº
        available = self.get_available_tools(context)
        
        # å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½è§„åˆ’
        print(f"ç›®æ ‡ï¼š{goal}")
        print(f"å¯ç”¨å·¥å…·ï¼š{available}")
        
        return available

# ä½¿ç”¨ç¤ºä¾‹
loader = DynamicToolLoader()

loader.register_tool("fetch_user", fetch_user_info, requires=[])
loader.register_tool("get_orders", get_user_orders, requires=["user_id"])
loader.register_tool("calculate_total", calculate_total_spent, requires=["orders"])

# åœºæ™¯ 1ï¼šåˆšå¼€å§‹ï¼Œåªæœ‰ user_id
context1 = {"user_id": "12345"}
print("é˜¶æ®µ 1 å¯ç”¨å·¥å…·ï¼š", loader.get_available_tools(context1))
# è¾“å‡ºï¼š['fetch_user', 'get_orders']

# åœºæ™¯ 2ï¼šå·²è·å–è®¢å•
context2 = {"user_id": "12345", "orders": [{"amount": 100}]}
print("é˜¶æ®µ 2 å¯ç”¨å·¥å…·ï¼š", loader.get_available_tools(context2))
# è¾“å‡ºï¼š['fetch_user', 'get_orders', 'calculate_total']
```

<div data-component="ToolOrchestrationVisualizer"></div>

### 28.3.4 å·¥å…·æ‰§è¡Œçš„å¹¶å‘æ§åˆ¶

å¯¹äºç‹¬ç«‹çš„å·¥å…·è°ƒç”¨ï¼Œå¯ä»¥å¹¶å‘æ‰§è¡Œä»¥æé«˜æ•ˆç‡ï¼š

```python
import asyncio
from typing import List, Dict

class AsyncToolChain:
    def __init__(self, tools: List):
        self.tools = {tool.name: tool for tool in tools}
    
    async def execute_parallel_tools(self, tool_calls: List[Dict]) -> Dict:
        """å¹¶å‘æ‰§è¡Œå¤šä¸ªç‹¬ç«‹çš„å·¥å…·"""
        tasks = []
        
        for call in tool_calls:
            tool_name = call["tool"]
            tool_input = call["input"]
            
            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            tool = self.tools[tool_name]
            task = asyncio.create_task(
                asyncio.to_thread(tool.invoke, tool_input)
            )
            tasks.append((call["output"], task))
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = {}
        for output_key, task in tasks:
            results[output_key] = await task
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    chain = AsyncToolChain([fetch_user_info, get_user_orders])
    
    # å¹¶å‘è°ƒç”¨å¤šä¸ªå·¥å…·
    parallel_calls = [
        {"tool": "fetch_user_info", "input": {"user_id": "12345"}, "output": "user"},
        {"tool": "get_user_orders", "input": {"user_id": "12345"}, "output": "orders"}
    ]
    
    results = await chain.execute_parallel_tools(parallel_calls)
    print("å¹¶å‘ç»“æœï¼š", results)

# è¿è¡Œ
# asyncio.run(main())
```

## 28.4 Agent è°ƒè¯•ä¸å¯è§‚æµ‹æ€§

### 28.4.1 ä¸­é—´çŠ¶æ€æ£€æŸ¥

LangGraph å…è®¸åœ¨ä»»æ„èŠ‚ç‚¹åæ£€æŸ¥çŠ¶æ€ï¼š

```python
from langgraph.graph import StateGraph
from langgraph.checkpoint.memory import MemorySaver

# æ„å»ºå›¾ï¼ˆä½¿ç”¨å‰é¢çš„ç¤ºä¾‹ï¼‰
workflow = StateGraph(AgentState)
# ... æ·»åŠ èŠ‚ç‚¹å’Œè¾¹ ...

# ç¼–è¯‘æ—¶å¯ç”¨æ£€æŸ¥ç‚¹
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# æ‰§è¡Œ
config = {"configurable": {"thread_id": "debug-1"}}
result = app.invoke(initial_input, config)

# æ£€æŸ¥æ‰§è¡Œå†å²
state_history = app.get_state_history(config)
for i, state in enumerate(state_history):
    print(f"æ­¥éª¤ {i}:")
    print(f"  èŠ‚ç‚¹: {state.next}")
    print(f"  çŠ¶æ€: {state.values}")
    print()
```

### 28.4.2 å†³ç­–è·¯å¾„è¿½è¸ª

è®°å½• Agent çš„æ¯ä¸ªå†³ç­–ç‚¹ï¼š

```python
class TrackedAgentState(TypedDict):
    messages: list
    decision_log: list[dict]

def tracked_decision_node(state: TrackedAgentState):
    """è®°å½•å†³ç­–çš„èŠ‚ç‚¹"""
    # åšå‡ºå†³ç­–
    decision = "æ‰§è¡Œæ“ä½œ A"
    
    # è®°å½•å†³ç­–
    decision_entry = {
        "timestamp": datetime.now().isoformat(),
        "decision": decision,
        "reasoning": "å› ä¸º X æ¡ä»¶æ»¡è¶³",
        "confidence": 0.85
    }
    
    return {
        "decision_log": state.get("decision_log", []) + [decision_entry]
    }

# æ‰§è¡Œååˆ†æå†³ç­–è·¯å¾„
def analyze_decision_path(state: TrackedAgentState):
    """åˆ†æå†³ç­–è·¯å¾„"""
    for i, entry in enumerate(state["decision_log"]):
        print(f"å†³ç­– {i+1}: {entry['decision']}")
        print(f"  åŸå› : {entry['reasoning']}")
        print(f"  ç½®ä¿¡åº¦: {entry['confidence']}")
        print(f"  æ—¶é—´: {entry['timestamp']}")
        print()
```

### 28.4.3 æ€§èƒ½åˆ†æ

```python
import time
from functools import wraps

def measure_performance(func):
    """è£…é¥°å™¨ï¼šæµ‹é‡èŠ‚ç‚¹æ‰§è¡Œæ—¶é—´"""
    @wraps(func)
    def wrapper(state):
        start_time = time.time()
        result = func(state)
        elapsed = time.time() - start_time
        
        # å°†æ€§èƒ½æ•°æ®æ·»åŠ åˆ°çŠ¶æ€
        perf_data = state.get("performance_log", [])
        perf_data.append({
            "node": func.__name__,
            "duration_seconds": elapsed,
            "timestamp": datetime.now().isoformat()
        })
        
        result["performance_log"] = perf_data
        return result
    
    return wrapper

# ä½¿ç”¨ç¤ºä¾‹
@measure_performance
def slow_analysis_node(state):
    """æ¨¡æ‹Ÿè€—æ—¶èŠ‚ç‚¹"""
    time.sleep(2)
    return {"messages": state["messages"] + [AIMessage(content="åˆ†æå®Œæˆ")]}

# æ‰§è¡ŒåæŸ¥çœ‹æ€§èƒ½æŠ¥å‘Š
def print_performance_report(state):
    """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
    total_time = sum(log["duration_seconds"] for log in state["performance_log"])
    
    print("=== æ€§èƒ½æŠ¥å‘Š ===")
    for log in state["performance_log"]:
        percentage = (log["duration_seconds"] / total_time) * 100
        print(f"{log['node']}: {log['duration_seconds']:.2f}s ({percentage:.1f}%)")
    print(f"æ€»è®¡: {total_time:.2f}s")
```

## 28.5 é«˜çº§é”™è¯¯æ¢å¤

### 28.5.1 è‡ªä¿®å¤æœºåˆ¶

å½“å·¥å…·è°ƒç”¨å¤±è´¥æ—¶ï¼ŒAgent å¯ä»¥å°è¯•è‡ªåŠ¨ä¿®å¤ï¼š

```python
class SelfHealingState(TypedDict):
    messages: list
    tool_call: dict | None
    error: str | None
    retry_count: int

def execute_tool_with_healing(state: SelfHealingState):
    """æ‰§è¡Œå·¥å…·ï¼Œå¤±è´¥æ—¶å°è¯•è‡ªä¿®å¤"""
    tool_call = state["tool_call"]
    retry_count = state.get("retry_count", 0)
    
    try:
        # å°è¯•æ‰§è¡Œå·¥å…·
        result = execute_tool(tool_call)
        return {
            "messages": state["messages"] + [AIMessage(content=f"âœ… {result}")],
            "error": None
        }
    except Exception as e:
        error_msg = str(e)
        
        # å°è¯•è‡ªä¿®å¤
        if retry_count < 3:
            # ä½¿ç”¨ LLM åˆ†æé”™è¯¯å¹¶ç”Ÿæˆä¿®å¤æ–¹æ¡ˆ
            heal_prompt = f"""
            å·¥å…·è°ƒç”¨å¤±è´¥ï¼š
            å·¥å…·: {tool_call['name']}
            å‚æ•°: {tool_call['args']}
            é”™è¯¯: {error_msg}
            
            è¯·åˆ†æé”™è¯¯åŸå› å¹¶æä¾›ä¿®å¤åçš„å‚æ•°ã€‚
            åªè¿”å› JSON æ ¼å¼çš„ä¿®å¤å‚æ•°ã€‚
            """
            
            llm = ChatOpenAI(temperature=0)
            fixed_args = llm.invoke(heal_prompt).content
            
            return {
                "tool_call": {"name": tool_call["name"], "args": fixed_args},
                "retry_count": retry_count + 1,
                "error": error_msg,
                "messages": state["messages"] + [
                    AIMessage(content=f"âš ï¸ é”™è¯¯ï¼š{error_msg}ï¼Œæ­£åœ¨å°è¯•ä¿®å¤...")
                ]
            }
        else:
            # æ”¾å¼ƒä¿®å¤ï¼Œè½¬äººå·¥å¤„ç†
            return {
                "error": error_msg,
                "messages": state["messages"] + [
                    AIMessage(content=f"âŒ è‡ªåŠ¨ä¿®å¤å¤±è´¥ï¼Œéœ€è¦äººå·¥ä»‹å…¥")
                ]
            }
```

### 28.5.2 é™çº§ç­–ç•¥

å½“é«˜çº§åŠŸèƒ½ä¸å¯ç”¨æ—¶ï¼Œå›é€€åˆ°ç®€å•æ–¹æ³•ï¼š

```python
class FallbackState(TypedDict):
    messages: list
    strategy: str  # "advanced" | "standard" | "basic"

def advanced_strategy_node(state: FallbackState):
    """é«˜çº§ç­–ç•¥ï¼ˆå¯èƒ½å¤±è´¥ï¼‰"""
    try:
        # å°è¯•é«˜çº§ API
        result = call_advanced_api()
        return {"messages": state["messages"] + [AIMessage(content=result)]}
    except Exception as e:
        # é™çº§åˆ°æ ‡å‡†ç­–ç•¥
        return {"strategy": "standard", "messages": state["messages"]}

def standard_strategy_node(state: FallbackState):
    """æ ‡å‡†ç­–ç•¥"""
    try:
        result = call_standard_api()
        return {"messages": state["messages"] + [AIMessage(content=result)]}
    except Exception as e:
        # å†é™çº§åˆ°åŸºç¡€ç­–ç•¥
        return {"strategy": "basic", "messages": state["messages"]}

def basic_strategy_node(state: FallbackState):
    """åŸºç¡€ç­–ç•¥ï¼ˆä¿è¯å¯ç”¨ï¼‰"""
    result = simple_fallback_logic()
    return {"messages": state["messages"] + [AIMessage(content=result)]}

# æ„å»ºé™çº§æµç¨‹
workflow = StateGraph(FallbackState)
workflow.add_node("advanced", advanced_strategy_node)
workflow.add_node("standard", standard_strategy_node)
workflow.add_node("basic", basic_strategy_node)

workflow.add_edge(START, "advanced")
workflow.add_conditional_edges(
    "advanced",
    lambda x: "standard" if x.get("strategy") == "standard" else END
)
workflow.add_conditional_edges(
    "standard",
    lambda x: "basic" if x.get("strategy") == "basic" else END
)
workflow.add_edge("basic", END)
```

### 28.5.3 äººå·¥æ¥ç®¡

å½“ Agent é‡åˆ°æ— æ³•å¤„ç†çš„æƒ…å†µï¼Œå¹³æ»‘è¿‡æ¸¡åˆ°äººå·¥å®¢æœï¼š

```python
class HandoffState(TypedDict):
    messages: list
    confidence: float
    handoff_triggered: bool
    handoff_reason: str | None

def check_handoff_condition(state: HandoffState):
    """æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥æ¥ç®¡"""
    confidence = state.get("confidence", 1.0)
    
    # ä½ç½®ä¿¡åº¦ â†’ è½¬äººå·¥
    if confidence < 0.3:
        return {
            "handoff_triggered": True,
            "handoff_reason": "ç½®ä¿¡åº¦è¿‡ä½"
        }
    
    # ç”¨æˆ·æ˜ç¡®è¦æ±‚ â†’ è½¬äººå·¥
    last_message = state["messages"][-1].content.lower()
    if "äººå·¥" in last_message or "å®¢æœ" in last_message:
        return {
            "handoff_triggered": True,
            "handoff_reason": "ç”¨æˆ·è¯·æ±‚äººå·¥æœåŠ¡"
        }
    
    return {"handoff_triggered": False}

def handoff_to_human(state: HandoffState):
    """è½¬äººå·¥å¤„ç†"""
    reason = state.get("handoff_reason", "æœªçŸ¥åŸå› ")
    
    # å®é™…åº”ç”¨ä¸­ï¼šé€šçŸ¥äººå·¥å®¢æœç³»ç»Ÿã€å‘é€å·¥å•ç­‰
    handoff_message = f"""
    å·²ä¸ºæ‚¨è½¬æ¥äººå·¥å®¢æœã€‚
    åŸå› ï¼š{reason}
    è¯·ç¨ç­‰ï¼Œå®¢æœäººå‘˜é©¬ä¸Šä¸ºæ‚¨æœåŠ¡ã€‚
    """
    
    return {
        "messages": state["messages"] + [AIMessage(content=handoff_message)]
    }

# åœ¨ Agent æµç¨‹ä¸­æ·»åŠ æ£€æŸ¥ç‚¹
workflow.add_conditional_edges(
    "agent_response",
    lambda x: "handoff" if x.get("handoff_triggered") else END
)
```

## 28.6 ç»¼åˆæ¡ˆä¾‹ï¼šä¼ä¸šçº§å®¢æœ Agent

è®©æˆ‘ä»¬æ•´åˆæœ¬ç« çš„æ‰€æœ‰æ¦‚å¿µï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§å®¢æœ Agentï¼š

```python
from typing import TypedDict, Literal
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

class CustomerServiceState(TypedDict):
    messages: list
    user_id: str
    session_id: str
    
    # è®°å¿†ç³»ç»Ÿ
    recent_memories: list[dict]
    user_profile: dict
    
    # å·¥å…·æ‰§è¡Œ
    pending_tools: list[dict]
    tool_results: dict
    
    # äººæœºåä½œ
    needs_approval: bool
    approved: bool | None
    
    # é”™è¯¯å¤„ç†
    error_count: int
    confidence: float
    handoff_triggered: bool

def load_user_context(state: CustomerServiceState):
    """åŠ è½½ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼ˆé•¿æœŸè®°å¿†ï¼‰"""
    # ä»å‘é‡å­˜å‚¨æ£€ç´¢å†å²å¯¹è¯
    memory_system = VectorMemorySystem()
    recent_memories = memory_system.recall_relevant_memories(
        state["user_id"],
        state["messages"][-1].content,
        k=3
    )
    
    # ä»çŸ¥è¯†å›¾è°±åŠ è½½ç”¨æˆ·ç”»åƒ
    kg = KnowledgeGraphMemory()
    kg.load(f"./users/{state['user_id']}_kg.json")
    user_profile = kg.get_entity_context(state["user_id"], depth=1)
    
    return {
        "recent_memories": recent_memories,
        "user_profile": user_profile
    }

def analyze_intent(state: CustomerServiceState):
    """åˆ†æç”¨æˆ·æ„å›¾"""
    llm = ChatOpenAI(temperature=0)
    
    # æ„å»ºä¸Šä¸‹æ–‡å¢å¼ºçš„æç¤º
    context = f"""
ç”¨æˆ·å†å²ï¼š
{chr(10).join([f"- {m['summary']}" for m in state['recent_memories']])}

ç”¨æˆ·ç”»åƒï¼š
{state['user_profile']}

å½“å‰å¯¹è¯ï¼š
{chr(10).join([f"{m.type}: {m.content}" for m in state['messages'][-3:]])}

è¯·åˆ†æç”¨æˆ·çš„æ„å›¾ï¼Œå¹¶è¿”å› JSONï¼š
{{"intent": "æŸ¥è¯¢è®¢å•|é€€æ¬¾ç”³è¯·|æŠ€æœ¯æ”¯æŒ|å…¶ä»–", "confidence": 0.0-1.0, "entities": []}}
"""
    
    result = llm.invoke(context).content
    # è§£æ JSONï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´ä¸¥æ ¼çš„è§£æï¼‰
    
    return {"confidence": 0.8}  # ç¤ºä¾‹

def plan_tool_execution(state: CustomerServiceState):
    """è§„åˆ’å·¥å…·æ‰§è¡Œ"""
    # æ ¹æ®æ„å›¾é€‰æ‹©å·¥å…·
    intent = "æŸ¥è¯¢è®¢å•"  # ä»ä¸Šä¸€æ­¥è·å–
    
    if intent == "æŸ¥è¯¢è®¢å•":
        tools = [
            {"tool": "fetch_user_info", "input": {"user_id": state["user_id"]}},
            {"tool": "get_user_orders", "input": {"user_id": state["user_id"]}}
        ]
    elif intent == "é€€æ¬¾ç”³è¯·":
        tools = [
            {"tool": "check_refund_eligibility", "input": {}},
            {"tool": "submit_refund_request", "input": {}}
        ]
        # é€€æ¬¾éœ€è¦å®¡æ‰¹
        return {"pending_tools": tools, "needs_approval": True}
    else:
        tools = []
    
    return {"pending_tools": tools, "needs_approval": False}

def execute_tools(state: CustomerServiceState):
    """æ‰§è¡Œå·¥å…·é“¾"""
    results = {}
    
    for tool_spec in state["pending_tools"]:
        try:
            # æ‰§è¡Œå·¥å…·ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
            result = {"status": "success"}
            results[tool_spec["tool"]] = result
        except Exception as e:
            return {"error_count": state.get("error_count", 0) + 1}
    
    return {"tool_results": results}

def generate_response(state: CustomerServiceState):
    """ç”Ÿæˆå“åº”"""
    llm = ChatOpenAI(temperature=0.7)
    
    prompt = f"""
ç”¨æˆ·é—®é¢˜ï¼š{state['messages'][-1].content}

å·¥å…·æ‰§è¡Œç»“æœï¼š
{state['tool_results']}

è¯·ç”Ÿæˆå‹å¥½ã€ä¸“ä¸šçš„å®¢æœå›å¤ã€‚
"""
    
    response = llm.invoke(prompt).content
    
    return {
        "messages": state["messages"] + [AIMessage(content=response)]
    }

def check_quality(state: CustomerServiceState):
    """è´¨é‡æ£€æŸ¥ï¼šå†³å®šæ˜¯å¦éœ€è¦äººå·¥ä»‹å…¥"""
    confidence = state.get("confidence", 1.0)
    error_count = state.get("error_count", 0)
    
    if confidence < 0.3 or error_count > 2:
        return {"handoff_triggered": True}
    
    return {}

# æ„å»ºå®Œæ•´çš„å®¢æœ Agent å›¾
workflow = StateGraph(CustomerServiceState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("load_context", load_user_context)
workflow.add_node("analyze_intent", analyze_intent)
workflow.add_node("plan_tools", plan_tool_execution)
workflow.add_node("execute_tools", execute_tools)
workflow.add_node("generate_response", generate_response)
workflow.add_node("check_quality", check_quality)
workflow.add_node("handoff", handoff_to_human)

# æ·»åŠ è¾¹
workflow.add_edge(START, "load_context")
workflow.add_edge("load_context", "analyze_intent")
workflow.add_edge("analyze_intent", "plan_tools")

# æ¡ä»¶è¾¹ï¼šæ˜¯å¦éœ€è¦å®¡æ‰¹
workflow.add_conditional_edges(
    "plan_tools",
    lambda x: "wait_approval" if x.get("needs_approval") else "execute_tools"
)

workflow.add_edge("execute_tools", "generate_response")
workflow.add_edge("generate_response", "check_quality")

# æ¡ä»¶è¾¹ï¼šæ˜¯å¦è½¬äººå·¥
workflow.add_conditional_edges(
    "check_quality",
    lambda x: "handoff" if x.get("handoff_triggered") else END
)

workflow.add_edge("handoff", END)

# ç¼–è¯‘
app = workflow.compile(
    checkpointer=MemorySaver(),
    interrupt_before=["execute_tools"]  # å®¡æ‰¹ç‚¹
)

# ä½¿ç”¨ç¤ºä¾‹
config = {"configurable": {"thread_id": "customer-123-session-456"}}

initial_state = {
    "messages": [HumanMessage(content="æˆ‘è¦ç”³è¯·é€€æ¬¾")],
    "user_id": "customer-123",
    "session_id": "session-456",
    "recent_memories": [],
    "user_profile": {},
    "pending_tools": [],
    "tool_results": {},
    "needs_approval": False,
    "approved": None,
    "error_count": 0,
    "confidence": 1.0,
    "handoff_triggered": False
}

# æ‰§è¡Œ
result = app.invoke(initial_state, config)
print("Agent å“åº”ï¼š", result["messages"][-1].content)
```

## 28.7 æœ€ä½³å®è·µä¸ç”Ÿäº§å»ºè®®

### 28.7.1 äººæœºåä½œçš„è®¾è®¡åŸåˆ™

1. **æ˜ç¡®çš„æƒé™è¾¹ç•Œ**ï¼šæ¸…æ™°å®šä¹‰å“ªäº›æ“ä½œå¿…é¡»äººå·¥å®¡æ‰¹
2. **ä½æ‘©æ“¦ä½“éªŒ**ï¼šå®¡æ‰¹æµç¨‹åº”ç®€å•å¿«æ·ï¼Œé¿å…è¿‡åº¦æ‰“æ–­
3. **è¶…æ—¶å¤„ç†**ï¼šè®¾ç½®åˆç†çš„å®¡æ‰¹è¶…æ—¶ï¼Œé¿å… Agent æ— é™æœŸç­‰å¾…
4. **å®¡è®¡æ—¥å¿—**ï¼šè®°å½•æ‰€æœ‰å®¡æ‰¹å†³ç­–ï¼Œç”¨äºåˆè§„å’Œåˆ†æ

### 28.7.2 é•¿æœŸè®°å¿†çš„ç®¡ç†ç­–ç•¥

1. **è®°å¿†è¡°å‡**ï¼šæ—§è®°å¿†åº”é€æ¸é™ä½æƒé‡æˆ–åˆ é™¤
2. **éšç§ä¿æŠ¤**ï¼šæ•æ„Ÿä¿¡æ¯åº”åŠ å¯†å­˜å‚¨æˆ–å®šæœŸæ¸…ç†
3. **è®°å¿†ä¸€è‡´æ€§**ï¼šå‘é‡è®°å¿†å’ŒçŸ¥è¯†å›¾è°±åº”ä¿æŒåŒæ­¥
4. **æˆæœ¬æ§åˆ¶**ï¼šé™åˆ¶è®°å¿†å­˜å‚¨çš„æ€»é‡ï¼Œé¿å…æ— é™è†¨èƒ€

### 28.7.3 å·¥å…·ç¼–æ’çš„ä¼˜åŒ–

1. **æœ€å°åŒ–å·¥å…·è°ƒç”¨**ï¼šé¿å…ä¸å¿…è¦çš„ API è¯·æ±‚
2. **å¹¶å‘æ‰§è¡Œ**ï¼šå¯¹äºç‹¬ç«‹çš„å·¥å…·è°ƒç”¨ï¼Œä½¿ç”¨å¼‚æ­¥å¹¶å‘
3. **ç¼“å­˜ç»“æœ**ï¼šå¯¹äºé¢‘ç¹æŸ¥è¯¢çš„æ•°æ®ï¼Œä½¿ç”¨ç¼“å­˜
4. **é”™è¯¯ä¼ æ’­**ï¼šåˆç†å¤„ç†å·¥å…·é“¾ä¸­çš„é”™è¯¯ä¼ æ’­

### 28.7.4 å¯è§‚æµ‹æ€§çš„å…³é”®æŒ‡æ ‡

1. **å†³ç­–è´¨é‡**ï¼šAgent å†³ç­–çš„å‡†ç¡®ç‡ã€ç½®ä¿¡åº¦åˆ†å¸ƒ
2. **æ‰§è¡Œæ•ˆç‡**ï¼šæ¯ä¸ªèŠ‚ç‚¹çš„è€—æ—¶ã€å·¥å…·è°ƒç”¨å»¶è¿Ÿ
3. **äººå·¥ä»‹å…¥ç‡**ï¼šéœ€è¦äººå·¥å®¡æ‰¹æˆ–æ¥ç®¡çš„æ¯”ä¾‹
4. **ç”¨æˆ·æ»¡æ„åº¦**ï¼šé€šè¿‡åé¦ˆæ”¶é›†ç”¨æˆ·è¯„ä»·

## 28.8 ç« èŠ‚æ€»ç»“

æœ¬ç« æ·±å…¥æ¢è®¨äº†é«˜çº§ Agent æ¨¡å¼ï¼Œé‡ç‚¹æ¶µç›–ï¼š

1. **äººæœºåä½œï¼ˆHITLï¼‰**ï¼š
   - LangGraph çš„ä¸­æ–­æœºåˆ¶ä¸å®¡æ‰¹æµç¨‹
   - å¤šçº§å®¡æ‰¹ã€è¶…æ—¶å¤„ç†ã€åé¦ˆæ³¨å…¥
   - äººå·¥æ¥ç®¡çš„å¹³æ»‘è¿‡æ¸¡

2. **é•¿æœŸè®°å¿†ç³»ç»Ÿ**ï¼š
   - ä¸‰å±‚è®°å¿†æ¶æ„ï¼šçŸ­æœŸã€ä¸­æœŸã€é•¿æœŸ
   - åŸºäºå‘é‡å­˜å‚¨çš„æƒ…èŠ‚è®°å¿†
   - åŸºäºçŸ¥è¯†å›¾è°±çš„è¯­ä¹‰è®°å¿†
   - æ··åˆè®°å¿†ç³»ç»Ÿçš„æ•´åˆ

3. **å·¥å…·ç¼–æ’**ï¼š
   - å·¥å…·é“¾çš„é¡ºåºæ‰§è¡Œä¸ä¾èµ–ç®¡ç†
   - æ¡ä»¶å·¥å…·è°ƒç”¨ä¸åŠ¨æ€åŠ è½½
   - å¹¶å‘æ§åˆ¶ä¸æ€§èƒ½ä¼˜åŒ–

4. **è°ƒè¯•ä¸å¯è§‚æµ‹æ€§**ï¼š
   - ä¸­é—´çŠ¶æ€æ£€æŸ¥ä¸å†³ç­–è·¯å¾„è¿½è¸ª
   - æ€§èƒ½åˆ†æä¸ç“¶é¢ˆè¯†åˆ«
   - LangSmith é›†æˆï¼ˆè¯¦è§ Chapter 22-24ï¼‰

5. **é”™è¯¯æ¢å¤**ï¼š
   - è‡ªä¿®å¤æœºåˆ¶ä¸é‡è¯•ç­–ç•¥
   - é™çº§æ–¹æ¡ˆä¸å®¹é”™è®¾è®¡
   - äººå·¥æ¥ç®¡çš„è§¦å‘æ¡ä»¶

é€šè¿‡è¿™äº›é«˜çº§æ¨¡å¼ï¼Œæ‚¨å¯ä»¥æ„å»ºæ›´åŠ æ™ºèƒ½ã€å¯æ§ã€å¯é çš„ä¼ä¸šçº§ Agent ç³»ç»Ÿï¼Œé€‚åº”å¤æ‚çš„ç”Ÿäº§ç¯å¢ƒéœ€æ±‚ã€‚

ä¸‹ä¸€ç« ï¼ˆChapter 29ï¼‰å°†æ¢è®¨ LangChain ä¸å…¶ä»–æ¡†æ¶çš„ç”Ÿæ€é›†æˆï¼Œä»¥åŠå¦‚ä½•å¹³æ»‘è¿ç§»ç°æœ‰é¡¹ç›®ã€‚

---

**æ‰©å±•é˜…è¯»**ï¼š
- [LangGraph Human-in-the-Loop å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/how-tos/human-in-the-loop/)
- [LangChain Memory ç³»ç»Ÿè¯¦è§£](https://python.langchain.com/docs/modules/memory/)
- [ä¼ä¸šçº§ Agent æ¶æ„è®¾è®¡æ¨¡å¼](https://www.langchain.com/blog/)
