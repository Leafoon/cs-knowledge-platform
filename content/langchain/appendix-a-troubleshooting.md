# Appendix A: å¸¸è§é—®é¢˜ä¸è°ƒè¯•

> **æœ¬é™„å½•æ±‡æ€» LangChain ç”Ÿæ€ä¸­æœ€å¸¸é‡åˆ°çš„é—®é¢˜ã€é”™è¯¯æç¤ºã€è°ƒè¯•æŠ€å·§ä¸è§£å†³æ–¹æ¡ˆï¼Œæ¶µç›– LangChain Coreã€LangGraphã€LangSmithã€LangServe å„ç¯èŠ‚çš„å®æˆ˜æ’éšœç»éªŒã€‚**

---

## A.1 LangSmith Tracing ä¸ç”Ÿæ•ˆ

### é—®é¢˜è¡¨ç°

æ‰§è¡Œ LangChain ä»£ç åï¼ŒLangSmith å¹³å°ä¸Šçœ‹ä¸åˆ°ä»»ä½•è¿½è¸ªè®°å½•ï¼ˆTraceï¼‰ï¼Œæˆ–ä»…æ˜¾ç¤ºéƒ¨åˆ†æ­¥éª¤ã€‚

### å¸¸è§åŸå› ä¸è§£å†³æ–¹æ¡ˆ

#### åŸå›  1ï¼šç¯å¢ƒå˜é‡æœªæ­£ç¡®è®¾ç½®

**æ£€æŸ¥æ¸…å•ï¼š**

```python
import os

# å¿…éœ€çš„ç¯å¢ƒå˜é‡
required_vars = {
    "LANGCHAIN_TRACING_V2": "true",           # å¯ç”¨è¿½è¸ª
    "LANGCHAIN_API_KEY": "lsv2_...",          # API Key
    "LANGCHAIN_PROJECT": "my-project",         # é¡¹ç›®åï¼ˆå¯é€‰ï¼Œä½†å»ºè®®è®¾ç½®ï¼‰
}

# è¯Šæ–­è„šæœ¬
for key, expected in required_vars.items():
    actual = os.getenv(key)
    if not actual:
        print(f"âŒ {key} æœªè®¾ç½®")
    elif key == "LANGCHAIN_TRACING_V2" and actual.lower() != "true":
        print(f"âš ï¸  {key}={actual}ï¼ˆåº”ä¸º 'true'ï¼‰")
    else:
        print(f"âœ… {key} å·²è®¾ç½®")
```

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# .env æ–‡ä»¶
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=lsv2_pt_your_key_here
LANGCHAIN_PROJECT=production-app
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com  # é€šå¸¸ä¸éœ€è¦ï¼ˆä½¿ç”¨é»˜è®¤å€¼ï¼‰
```

```python
from dotenv import load_dotenv
load_dotenv()  # åœ¨å¯¼å…¥ langchain ä¹‹å‰è°ƒç”¨
```

#### åŸå›  2ï¼šä»£ç†/ç½‘ç»œé—®é¢˜

åœ¨ä¸­å›½å¤§é™†æˆ–æŸäº›ä¼ä¸šç½‘ç»œç¯å¢ƒä¸­ï¼Œå¯èƒ½æ— æ³•è¿æ¥åˆ° `api.smith.langchain.com`ã€‚

**éªŒè¯è¿æ¥æ€§ï¼š**

```python
import requests

try:
    response = requests.get(
        "https://api.smith.langchain.com/info",
        timeout=5
    )
    print(f"âœ… è¿æ¥æˆåŠŸ: {response.status_code}")
except Exception as e:
    print(f"âŒ è¿æ¥å¤±è´¥: {e}")
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# è®¾ç½®ä»£ç†
os.environ["HTTP_PROXY"] = "http://proxy.company.com:8080"
os.environ["HTTPS_PROXY"] = "http://proxy.company.com:8080"

# æˆ–ä½¿ç”¨è‡ªæ‰˜ç®¡ LangSmithï¼ˆä¼ä¸šç‰ˆï¼‰
os.environ["LANGCHAIN_ENDPOINT"] = "https://langsmith.internal.company.com"
```

#### åŸå›  3ï¼šRunnable æ²¡æœ‰é€šè¿‡ LCEL æ„å»º

ä½¿ç”¨ä¼ ç»Ÿ Chain æˆ–è‡ªå®šä¹‰å‡½æ•°æ—¶ï¼Œå¯èƒ½æœªæ­£ç¡®ç»§æ‰¿è¿½è¸ªä¸Šä¸‹æ–‡ã€‚

**é”™è¯¯ç¤ºä¾‹ï¼š**

```python
# âŒ æ™®é€š Python å‡½æ•°ä¸ä¼šè‡ªåŠ¨è¿½è¸ª
def my_chain(input_text):
    response = llm.invoke(input_text)
    return response.upper()

result = my_chain("Hello")  # ä¸ä¼šå‡ºç°åœ¨ LangSmith
```

**æ­£ç¡®ç¤ºä¾‹ï¼š**

```python
from langchain_core.runnables import RunnableLambda

# âœ… åŒ…è£…ä¸º Runnable
my_chain = RunnableLambda(lambda x: llm.invoke(x).upper())
result = my_chain.invoke("Hello")  # ä¼šè¿½è¸ª
```

#### åŸå›  4ï¼šå¼‚æ­¥ä»£ç æœªæ­£ç¡®å¤„ç†

åœ¨ Jupyter Notebook æˆ–å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œéœ€ç¡®ä¿äº‹ä»¶å¾ªç¯æ­£ç¡®ç®¡ç†ã€‚

**é—®é¢˜ä»£ç ï¼š**

```python
# âŒ åœ¨ Jupyter ä¸­æ··ç”¨ sync/async
import asyncio

async def run():
    result = await chain.ainvoke("Hello")
    return result

# ç›´æ¥è°ƒç”¨å¯èƒ½å¯¼è‡´è¿½è¸ªä¸¢å¤±
asyncio.run(run())  # å¯èƒ½åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# âœ… ä½¿ç”¨ awaitï¼ˆåœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼‰
result = await chain.ainvoke("Hello")

# âœ… æˆ–ä½¿ç”¨ nest_asyncioï¼ˆJupyterï¼‰
import nest_asyncio
nest_asyncio.apply()
```

#### åŸå›  5ï¼šæ‰¹å¤„ç†ä¸­çš„éƒ¨åˆ†å¤±è´¥

æ‰¹å¤„ç†æ—¶ï¼Œå¦‚æœæŸäº›é¡¹å¤±è´¥ä½†è¢«å¿½ç•¥ï¼Œå¯èƒ½å¯¼è‡´è¿½è¸ªä¸å®Œæ•´ã€‚

**è¯Šæ–­ï¼š**

```python
from langchain.callbacks.tracers import ConsoleCallbackHandler

# æ·»åŠ æœ¬åœ°å›è°ƒæŸ¥çœ‹æ‰§è¡Œç»†èŠ‚
chain.batch(
    ["input1", "input2", "input3"],
    config={"callbacks": [ConsoleCallbackHandler()]}
)
```

### è°ƒè¯•æŠ€å·§

#### ä½¿ç”¨ `langsmith.utils.tracing_context`

```python
from langsmith.run_helpers import traceable

@traceable(run_type="chain", name="custom-chain")
def my_custom_chain(input_text: str) -> str:
    """å¼ºåˆ¶è¿½è¸ªè‡ªå®šä¹‰å‡½æ•°"""
    result = llm.invoke(input_text)
    return result.content

# ä¼šåœ¨ LangSmith ä¸­æ˜¾ç¤ºä¸ºç‹¬ç«‹çš„ Run
my_custom_chain("Hello")
```

#### å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger("langchain")
logger.setLevel(logging.DEBUG)

# æ‰§è¡Œé“¾åæŸ¥çœ‹æ—¥å¿—ä¸­çš„è¿½è¸ª URL
chain.invoke("Hello")
# è¾“å‡º: View trace at https://smith.langchain.com/...
```

---

## A.2 LCEL ç±»å‹æ¨æ–­é”™è¯¯

### é—®é¢˜è¡¨ç°

IDE æç¤ºç±»å‹ä¸åŒ¹é…ã€è¿è¡Œæ—¶å‡ºç° `AttributeError`ã€æˆ–é“¾æ— æ³•æ­£ç¡®ç»„åˆã€‚

### å¸¸è§é”™è¯¯ä¸è§£å†³æ–¹æ¡ˆ

#### é”™è¯¯ 1ï¼šè¾“å…¥è¾“å‡ºç±»å‹ä¸åŒ¹é…

**é”™è¯¯ä»£ç ï¼š**

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_template("Translate to {language}: {text}")
model = ChatOpenAI()
parser = StrOutputParser()

# âŒ ç±»å‹ä¸åŒ¹é…ï¼šprompt éœ€è¦ dictï¼Œä½†å¯èƒ½ä¼ å…¥ str
chain = prompt | model | parser
chain.invoke("Hello")  # TypeError: Expected dict, got str
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# âœ… ä¼ å…¥æ­£ç¡®çš„å­—å…¸æ ¼å¼
chain.invoke({"language": "French", "text": "Hello"})

# âœ… æˆ–æ·»åŠ è¾“å…¥é€‚é…å™¨
from langchain_core.runnables import RunnableLambda

input_adapter = RunnableLambda(lambda x: {"language": "French", "text": x})
chain = input_adapter | prompt | model | parser
chain.invoke("Hello")  # ç°åœ¨å¯ä»¥æ¥å— str
```

#### é”™è¯¯ 2ï¼šRunnablePassthrough ä½¿ç”¨ä¸å½“

**é”™è¯¯ä»£ç ï¼š**

```python
from langchain_core.runnables import RunnablePassthrough

# âŒ ä»¥ä¸ºä¼šé€ä¼ æ•´ä¸ªå­—å…¸
chain = RunnablePassthrough() | model
chain.invoke({"text": "Hello"})  # model æ”¶åˆ° dict è€Œé str
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# âœ… ä½¿ç”¨ RunnablePassthrough.assign() æ·»åŠ å­—æ®µ
chain = (
    RunnablePassthrough.assign(
        response=model
    )
)
chain.invoke({"text": "Hello"})
# è¾“å‡º: {"text": "Hello", "response": AIMessage(...)}

# âœ… æˆ–ä½¿ç”¨ itemgetter æå–å­—æ®µ
from operator import itemgetter

chain = (
    {"text": itemgetter("text")}
    | ChatPromptTemplate.from_template("Translate: {text}")
    | model
)
```

#### é”™è¯¯ 3ï¼šPydantic æ¨¡å‹éªŒè¯å¤±è´¥

**é”™è¯¯ä»£ç ï¼š**

```python
from pydantic import BaseModel
from langchain_core.output_parsers import PydanticOutputParser

class Person(BaseModel):
    name: str
    age: int

parser = PydanticOutputParser(pydantic_object=Person)

# âŒ LLM è¾“å‡ºæ ¼å¼ä¸ç¬¦åˆ JSON
chain = model | parser
chain.invoke("Extract person from: John is 30 years old")
# JSONDecodeError: Expecting value...
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# âœ… ä½¿ç”¨ with_structured_outputï¼ˆæ¨èï¼‰
model_with_structure = model.with_structured_output(Person)
result = model_with_structure.invoke("John is 30 years old")
print(result)  # Person(name='John', age=30)

# âœ… æˆ–æ˜¾å¼æŒ‡å¯¼ LLM è¾“å‡º JSON
prompt = ChatPromptTemplate.from_template(
    "Extract person info as JSON:\n{text}\n\n{format_instructions}"
)
chain = (
    {"text": RunnablePassthrough(), "format_instructions": lambda _: parser.get_format_instructions()}
    | prompt
    | model
    | parser
)
```

### ç±»å‹æ³¨è§£æœ€ä½³å®è·µ

```python
from typing import TypedDict
from langchain_core.runnables import Runnable

# âœ… å®šä¹‰è¾“å…¥è¾“å‡ºç±»å‹
class ChainInput(TypedDict):
    language: str
    text: str

class ChainOutput(TypedDict):
    translation: str

# ç±»å‹æ ‡æ³¨é“¾
chain: Runnable[ChainInput, ChainOutput] = (
    prompt | model | StrOutputParser()
)

# IDE ç°åœ¨å¯ä»¥æä¾›è‡ªåŠ¨è¡¥å…¨
result = chain.invoke({"language": "French", "text": "Hello"})
print(result["translation"])  # âœ… IDE çŸ¥é“è¿™æ˜¯ str
```

---

## A.3 LangGraph çŠ¶æ€æ›´æ–°å¤±è´¥

### é—®é¢˜è¡¨ç°

- èŠ‚ç‚¹æ‰§è¡ŒåçŠ¶æ€æœªæ›´æ–°
- `checkpointer.get()` è¿”å› `None`
- æ¡ä»¶è¾¹æ— æ³•æ­£ç¡®è·¯ç”±

### å¸¸è§åŸå› ä¸è§£å†³æ–¹æ¡ˆ

#### åŸå›  1ï¼šçŠ¶æ€é”®åä¸åŒ¹é…

**é”™è¯¯ä»£ç ï¼š**

```python
from typing import TypedDict
from langgraph.graph import StateGraph

class State(TypedDict):
    messages: list
    context: str

def node_a(state: State) -> dict:
    # âŒ è¿”å›äº†ä¸åœ¨ State ä¸­çš„é”®
    return {"message": "Hello"}  # åº”ä¸º "messages"

graph = StateGraph(State)
graph.add_node("node_a", node_a)
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
def node_a(state: State) -> dict:
    # âœ… è¿”å›ç¬¦åˆ State å®šä¹‰çš„é”®
    return {"messages": state["messages"] + ["Hello"]}
```

#### åŸå›  2ï¼šReducer é…ç½®é”™è¯¯

**é”™è¯¯ä»£ç ï¼š**

```python
from typing import Annotated
from operator import add

class State(TypedDict):
    # âŒ ä½¿ç”¨ add ä½†ä¼ å…¥çš„ä¸æ˜¯å¯åŠ ç±»å‹
    messages: Annotated[list, add]

def node_a(state: State) -> dict:
    # è¿”å›å•ä¸ª str è€Œé list
    return {"messages": "Hello"}  # TypeError: can only concatenate list to list
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
class State(TypedDict):
    messages: Annotated[list, add]  # âœ… ç¡®ä¿è¿”å› list

def node_a(state: State) -> dict:
    return {"messages": ["Hello"]}  # âœ… åŒ…è£…ä¸ºåˆ—è¡¨
```

#### åŸå›  3ï¼šCheckpointer æœªæ­£ç¡®é…ç½®

**é—®é¢˜ä»£ç ï¼š**

```python
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()
graph = StateGraph(State)
# ... æ·»åŠ èŠ‚ç‚¹/è¾¹
app = graph.compile(checkpointer=checkpointer)

# âŒ è°ƒç”¨æ—¶æœªæä¾› thread_id
result = app.invoke({"messages": []})
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# âœ… æä¾› config åŒ…å« thread_id
config = {"configurable": {"thread_id": "conversation-1"}}
result = app.invoke({"messages": []}, config=config)

# éªŒè¯çŠ¶æ€å·²ä¿å­˜
snapshot = checkpointer.get(config)
print(snapshot)  # CheckpointTuple(...)
```

#### åŸå›  4ï¼šæ¡ä»¶è¾¹è¿”å›å€¼é”™è¯¯

**é”™è¯¯ä»£ç ï¼š**

```python
def should_continue(state: State) -> str:
    if len(state["messages"]) > 10:
        return "end"  # âŒ ä½†å›¾ä¸­æ²¡æœ‰åä¸º "end" çš„èŠ‚ç‚¹
    return "continue"

graph.add_conditional_edges("node_a", should_continue)
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
from langgraph.graph import END

def should_continue(state: State) -> str:
    if len(state["messages"]) > 10:
        return END  # âœ… ä½¿ç”¨ LangGraph çš„ END å¸¸é‡
    return "node_b"  # âœ… ç¡®ä¿èŠ‚ç‚¹å­˜åœ¨

graph.add_conditional_edges(
    "node_a",
    should_continue,
    {
        END: END,
        "node_b": "node_b"
    }
)
```

### è°ƒè¯•æŠ€å·§

#### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger("langgraph")
logger.setLevel(logging.DEBUG)

# æ‰§è¡Œå›¾
app.invoke({"messages": []}, config=config)
# æŸ¥çœ‹æ—¥å¿—ä¸­çš„çŠ¶æ€æ›´æ–°
```

#### 2. æ‰“å°ä¸­é—´çŠ¶æ€

```python
def debug_node(state: State) -> dict:
    print(f"ğŸ” Current state: {state}")
    return {}

# åœ¨å…³é”®ä½ç½®æ’å…¥è°ƒè¯•èŠ‚ç‚¹
graph.add_node("debug", debug_node)
graph.add_edge("node_a", "debug")
graph.add_edge("debug", "node_b")
```

#### 3. ä½¿ç”¨ `stream` æŸ¥çœ‹æ‰§è¡Œè¿‡ç¨‹

```python
for event in app.stream({"messages": []}, config=config):
    print(f"ğŸ“¦ Event: {event}")
    # è¾“å‡ºæ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥/è¾“å‡º
```

---

## A.4 Agent é™·å…¥æ— é™å¾ªç¯

### é—®é¢˜è¡¨ç°

Agent åå¤è°ƒç”¨ç›¸åŒå·¥å…·ã€ç”Ÿæˆç›¸åŒè¾“å‡ºã€æˆ–é•¿æ—¶é—´ä¸è¿”å›ç»“æœã€‚

### å¸¸è§åŸå› ä¸è§£å†³æ–¹æ¡ˆ

#### åŸå›  1ï¼šå·¥å…·è¾“å‡ºæ ¼å¼ä¸ç¬¦åˆé¢„æœŸ

**é”™è¯¯åœºæ™¯ï¼š**

```python
from langchain.tools import tool

@tool
def search_tool(query: str) -> str:
    """Search the web"""
    # âŒ è¿”å›ç©ºå­—ç¬¦ä¸²æˆ–æ— ç”¨ä¿¡æ¯
    return ""

# Agent æ— æ³•åˆ¤æ–­ä»»åŠ¡å®Œæˆï¼Œç»§ç»­è°ƒç”¨å·¥å…·
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
@tool
def search_tool(query: str) -> str:
    """Search the web"""
    results = perform_search(query)
    if not results:
        # âœ… è¿”å›æ˜ç¡®çš„å¤±è´¥ä¿¡æ¯
        return "No results found. Try a different query."
    # âœ… è¿”å›ç»“æ„åŒ–ä¸”æœ‰ä¿¡æ¯é‡çš„ç»“æœ
    return f"Found {len(results)} results:\n" + "\n".join(results[:3])
```

#### åŸå›  2ï¼šç¼ºå°‘æœ€å¤§è¿­ä»£é™åˆ¶

**é”™è¯¯ä»£ç ï¼š**

```python
from langgraph.prebuilt import create_react_agent

# âŒ æ²¡æœ‰è®¾ç½® max_iterations
agent = create_react_agent(model, tools)
agent.invoke({"messages": [("user", "Find info about X")]})
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# âœ… è®¾ç½®æœ€å¤§è¿­ä»£æ¬¡æ•°
agent = create_react_agent(
    model,
    tools,
    state_modifier="You must complete the task in 5 steps or less."
)

# æˆ–åœ¨è‡ªå®šä¹‰å›¾ä¸­æ·»åŠ å¾ªç¯æ£€æµ‹
from langgraph.graph import END

def should_continue(state):
    if len(state["messages"]) > 20:  # âœ… å¼ºåˆ¶é€€å‡º
        return END
    # ... å…¶ä»–é€»è¾‘
```

#### åŸå›  3ï¼šAgent æç¤ºè¯ä¸æ˜ç¡®

**é—®é¢˜æç¤ºï¼š**

```python
system_prompt = "You are a helpful assistant."
# âŒ æ²¡æœ‰æ˜ç¡®ä½•æ—¶åœæ­¢
```

**æ”¹è¿›æç¤ºï¼š**

```python
system_prompt = """You are a helpful assistant. Follow these rules:
1. Use tools to gather information
2. Once you have enough information, provide a FINAL ANSWER
3. Do NOT call the same tool twice with the same arguments
4. If a tool returns no results, try a different approach OR admit you cannot find the answer
5. ALWAYS end your response with "FINAL ANSWER:" when task is complete
"""
```

#### åŸå›  4ï¼šå·¥å…·ä¾èµ–å¾ªç¯

**é”™è¯¯åœºæ™¯ï¼š**

```python
# Tool A çš„è¾“å‡ºéœ€è¦ Tool B å¤„ç†
# Tool B çš„è¾“å‡ºåˆéœ€è¦ Tool A å¤„ç†
# å½¢æˆæ­»å¾ªç¯
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# âœ… ä½¿ç”¨ LangGraph æ˜¾å¼å®šä¹‰å·¥å…·è°ƒç”¨é¡ºåº
graph = StateGraph(State)
graph.add_node("tool_a", tool_a_node)
graph.add_node("tool_b", tool_b_node)
graph.add_edge("tool_a", "tool_b")  # å¼ºåˆ¶é¡ºåº
graph.add_edge("tool_b", END)  # é˜²æ­¢å¾ªç¯
```

### è°ƒè¯•æŠ€å·§

#### 1. ç›‘æ§å·¥å…·è°ƒç”¨

```python
from langchain.callbacks import get_openai_callback

with get_openai_callback() as cb:
    result = agent.invoke({"messages": [("user", "Query")]})
    print(f"Total calls: {cb.successful_requests}")
    print(f"Total tokens: {cb.total_tokens}")
    # å¦‚æœè°ƒç”¨æ¬¡æ•°å¼‚å¸¸é«˜ï¼Œè¯´æ˜æœ‰å¾ªç¯
```

#### 2. è®°å½•æ¯æ¬¡è¿­ä»£

```python
class IterationTracker:
    def __init__(self):
        self.iterations = []
    
    def track(self, action: str):
        self.iterations.append(action)
        if len(self.iterations) > 10:
            raise RuntimeError("Too many iterations!")

tracker = IterationTracker()

# åœ¨èŠ‚ç‚¹ä¸­è°ƒç”¨
def agent_node(state):
    tracker.track(state["next_action"])
    # ...
```

---

## A.5 RAG æ£€ç´¢è´¨é‡å·®

### é—®é¢˜è¡¨ç°

- æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸æŸ¥è¯¢æ— å…³
- ç›¸ä¼¼åº¦åˆ†æ•°å¾ˆä½
- æ˜æ˜æœ‰ç›¸å…³æ–‡æ¡£å´æ£€ç´¢ä¸åˆ°

### å¸¸è§åŸå› ä¸è§£å†³æ–¹æ¡ˆ

#### åŸå›  1ï¼šEmbedding æ¨¡å‹ä¸åŒ¹é…

**é—®é¢˜ä»£ç ï¼š**

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

# ç´¢å¼•æ—¶ä½¿ç”¨ OpenAI embeddings
embeddings_v1 = OpenAIEmbeddings(model="text-embedding-ada-002")
vectorstore = Chroma.from_documents(docs, embeddings_v1)

# âŒ æ£€ç´¢æ—¶æ¢äº†æ¨¡å‹
embeddings_v2 = OpenAIEmbeddings(model="text-embedding-3-small")
retriever = vectorstore.as_retriever(embedding=embeddings_v2)
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# âœ… å§‹ç»ˆä½¿ç”¨åŒä¸€ä¸ª embedding å®ä¾‹
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# ç´¢å¼•
vectorstore = Chroma.from_documents(docs, embeddings)

# æ£€ç´¢ï¼ˆä½¿ç”¨ç›¸åŒå®ä¾‹ï¼‰
retriever = vectorstore.as_retriever()
```

#### åŸå›  2ï¼šæ–‡æ¡£åˆ†å—ä¸åˆç†

**é”™è¯¯åˆ†å—ï¼š**

```python
from langchain.text_splitter import CharacterTextSplitter

# âŒ Chunk å¤ªå¤§ï¼ˆ2000 å­—ç¬¦ï¼‰å¯¼è‡´è¯­ä¹‰æ··æ‚
splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)
docs = splitter.split_documents(raw_docs)
```

**ä¼˜åŒ–åˆ†å—ï¼š**

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# âœ… é€‚ä¸­çš„ chunk sizeï¼Œå¸¦é‡å 
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,          # æ ¹æ® embedding æ¨¡å‹è°ƒæ•´
    chunk_overlap=50,        # ä¿ç•™ä¸Šä¸‹æ–‡
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
)
docs = splitter.split_documents(raw_docs)
```

#### åŸå›  3ï¼šæŸ¥è¯¢æ”¹å†™ä¸è¶³

**åŸºç¡€æŸ¥è¯¢ï¼š**

```python
# âŒ ç›´æ¥ä½¿ç”¨ç”¨æˆ·åŸå§‹æŸ¥è¯¢
query = "Python æ€ä¹ˆè¯»æ–‡ä»¶ï¼Ÿ"
docs = retriever.get_relevant_documents(query)
```

**æŸ¥è¯¢æ”¹å†™ï¼š**

```python
from langchain.retrievers import MultiQueryRetriever

# âœ… ç”Ÿæˆå¤šä¸ªå˜ä½“æŸ¥è¯¢
retriever_with_rewrite = MultiQueryRetriever.from_llm(
    retriever=base_retriever,
    llm=ChatOpenAI(temperature=0)
)
docs = retriever_with_rewrite.get_relevant_documents(query)
```

#### åŸå›  4ï¼šæœªä½¿ç”¨æ··åˆæ£€ç´¢

**çº¯å‘é‡æ£€ç´¢ï¼š**

```python
# âŒ ä»…ä¾èµ–è¯­ä¹‰ç›¸ä¼¼åº¦
retriever = vectorstore.as_retriever(search_type="similarity")
```

**æ··åˆæ£€ç´¢ï¼š**

```python
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers import BM25Retriever

# âœ… ç»“åˆå‘é‡æ£€ç´¢ + BM25 å…³é”®è¯æ£€ç´¢
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
bm25_retriever = BM25Retriever.from_documents(docs)
bm25_retriever.k = 5

ensemble_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.5, 0.5]  # å„å  50%
)
```

#### åŸå›  5ï¼šç¼ºå°‘å…ƒæ•°æ®è¿‡æ»¤

**æ— è¿‡æ»¤ï¼š**

```python
# âŒ æ£€ç´¢æ‰€æœ‰æ–‡æ¡£ï¼ŒåŒ…æ‹¬æ— å…³æ¥æº
docs = retriever.get_relevant_documents("2024 å¹´æ”¿ç­–")
```

**å…ƒæ•°æ®è¿‡æ»¤ï¼š**

```python
# âœ… æ·»åŠ æ—¶é—´/æ¥æºè¿‡æ»¤
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 5,
        "filter": {"year": {"$gte": 2024}}  # ä»…æ£€ç´¢ 2024 å¹´åçš„æ–‡æ¡£
    }
)
```

### è°ƒè¯•æŠ€å·§

#### 1. æ£€æŸ¥æ–‡æ¡£åµŒå…¥è´¨é‡

```python
# æŸ¥çœ‹ embedding å‘é‡
query_embedding = embeddings.embed_query("æµ‹è¯•æŸ¥è¯¢")
print(f"Embedding dimension: {len(query_embedding)}")
print(f"First 5 values: {query_embedding[:5]}")

# è®¡ç®—ç›¸ä¼¼åº¦
from numpy import dot
from numpy.linalg import norm

def cosine_similarity(a, b):
    return dot(a, b) / (norm(a) * norm(b))

doc_embedding = embeddings.embed_query("æ–‡æ¡£å†…å®¹")
similarity = cosine_similarity(query_embedding, doc_embedding)
print(f"Similarity: {similarity:.4f}")  # åº” > 0.7 æ‰ç®—ç›¸å…³
```

#### 2. å¯è§†åŒ–æ£€ç´¢ç»“æœ

```python
docs_with_scores = vectorstore.similarity_search_with_score(query, k=10)

for i, (doc, score) in enumerate(docs_with_scores):
    print(f"\n{'='*60}")
    print(f"Rank {i+1} | Score: {score:.4f}")
    print(f"Content: {doc.page_content[:200]}...")
    print(f"Metadata: {doc.metadata}")
```

#### 3. A/B æµ‹è¯•ä¸åŒé…ç½®

```python
# å¯¹æ¯”ä¸åŒé…ç½®çš„æ£€ç´¢è´¨é‡
configs = [
    {"chunk_size": 500, "k": 5},
    {"chunk_size": 1000, "k": 3},
    {"chunk_size": 200, "k": 10},
]

for config in configs:
    # é‡æ–°ç´¢å¼•
    splitter = RecursiveCharacterTextSplitter(chunk_size=config["chunk_size"])
    docs = splitter.split_documents(raw_docs)
    vectorstore = Chroma.from_documents(docs, embeddings)
    
    # æ£€ç´¢
    retriever = vectorstore.as_retriever(search_kwargs={"k": config["k"]})
    results = retriever.get_relevant_documents(test_query)
    
    # äººå·¥è¯„ä¼°
    print(f"\nConfig: {config}")
    for doc in results[:3]:
        print(f"  - {doc.page_content[:100]}...")
```

---

## A.6 æµå¼è¾“å‡ºä¸­æ–­æˆ–ä¹±ç 

### é—®é¢˜è¡¨ç°

- `astream()` ä¸­é€”åœæ­¢
- Token é¡ºåºé”™ä¹±
- ä¸­æ–‡å­—ç¬¦æ˜¾ç¤ºä¸º `ï¿½`

### è§£å†³æ–¹æ¡ˆ

#### é—®é¢˜ 1ï¼šç¼“å†²åŒºæœªåˆ·æ–°

```python
import sys

async for chunk in chain.astream("Hello"):
    print(chunk, end="", flush=True)  # âœ… ç«‹å³åˆ·æ–°ç¼“å†²åŒº
```

#### é—®é¢˜ 2ï¼šç¼–ç é—®é¢˜

```python
# âœ… ç¡®ä¿ UTF-8 ç¼–ç 
import sys
sys.stdout.reconfigure(encoding='utf-8')
```

#### é—®é¢˜ 3ï¼šå¼‚å¸¸æœªæ•è·

```python
try:
    async for chunk in chain.astream("Hello"):
        print(chunk, end="")
except Exception as e:
    print(f"\nâŒ Stream error: {e}")
```

---

## A.7 LangServe éƒ¨ç½² 422 é”™è¯¯

### é—®é¢˜è¡¨ç°

è°ƒç”¨ `/invoke` ç«¯ç‚¹æ—¶è¿”å› `422 Unprocessable Entity`ã€‚

### å¸¸è§åŸå› 

#### åŸå›  1ï¼šè¯·æ±‚ä½“æ ¼å¼é”™è¯¯

```python
# âŒ é”™è¯¯æ ¼å¼
requests.post(
    "http://localhost:8000/chain/invoke",
    json={"input": "Hello"}  # ç¼ºå°‘å¿…éœ€çš„ "input" åŒ…è£…
)

# âœ… æ­£ç¡®æ ¼å¼
requests.post(
    "http://localhost:8000/chain/invoke",
    json={"input": {"text": "Hello"}}  # æ ¹æ®é“¾çš„è¾“å…¥ç»“æ„
)
```

#### åŸå›  2ï¼šSchema ä¸åŒ¹é…

```python
# æœåŠ¡ç«¯é“¾å®šä¹‰
class ChainInput(BaseModel):
    text: str
    language: str

# âŒ å®¢æˆ·ç«¯é—æ¼å­—æ®µ
requests.post(url, json={"input": {"text": "Hello"}})

# âœ… æä¾›å®Œæ•´å­—æ®µ
requests.post(url, json={"input": {"text": "Hello", "language": "en"}})
```

### è°ƒè¯•æŠ€å·§

```python
# æŸ¥çœ‹é“¾çš„è¾“å…¥ schema
response = requests.get("http://localhost:8000/chain/input_schema")
print(response.json())  # æŸ¥çœ‹æœŸæœ›çš„è¾“å…¥æ ¼å¼
```

---

## A.8 å†…å­˜å ç”¨è¿‡é«˜

### é—®é¢˜è¡¨ç°

è¿è¡Œ RAG åº”ç”¨æˆ– Agent æ—¶å†…å­˜æŒç»­å¢é•¿ï¼Œæœ€ç»ˆ OOMã€‚

### å¸¸è§åŸå› ä¸è§£å†³æ–¹æ¡ˆ

#### åŸå›  1ï¼šå¯¹è¯å†å²æœªé™åˆ¶

```python
# âŒ æ— é™å¢é•¿çš„å†å²
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory()
# æ¯æ¬¡å¯¹è¯éƒ½è¿½åŠ ï¼Œæ°¸ä¸æ¸…ç†
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# âœ… ä½¿ç”¨çª—å£è®°å¿†
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(k=10)  # ä»…ä¿ç•™æœ€è¿‘ 10 è½®

# âœ… æˆ–ä½¿ç”¨æ‘˜è¦è®°å¿†
from langchain.memory import ConversationSummaryMemory

memory = ConversationSummaryMemory(llm=llm)  # è‡ªåŠ¨å‹ç¼©
```

#### åŸå›  2ï¼šå‘é‡åº“åŠ è½½åˆ°å†…å­˜

```python
# âŒ Chroma é»˜è®¤åœ¨å†…å­˜ä¸­
vectorstore = Chroma.from_documents(docs, embeddings)
```

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# âœ… æŒä¹…åŒ–åˆ°ç£ç›˜
vectorstore = Chroma.from_documents(
    docs,
    embeddings,
    persist_directory="./chroma_db"
)
```

#### åŸå›  3ï¼šæ–‡æ¡£æœªåŠæ—¶é‡Šæ”¾

```python
# âœ… ä½¿ç”¨ç”Ÿæˆå™¨è€Œéåˆ—è¡¨
def load_docs():
    for file in files:
        yield load_file(file)

# è€Œé
docs = [load_file(f) for f in files]  # âŒ å…¨éƒ¨åŠ è½½åˆ°å†…å­˜
```

---

## A.9 LangGraph Checkpoint æ¢å¤å¤±è´¥

### é—®é¢˜è¡¨ç°

è°ƒç”¨ `get_state()` è¿”å› `None`ï¼Œæˆ–çŠ¶æ€ä¸é¢„æœŸä¸ç¬¦ã€‚

### è§£å†³æ–¹æ¡ˆ

#### 1. ç¡®è®¤ thread_id ä¸€è‡´

```python
# ä¿å­˜æ—¶
config1 = {"configurable": {"thread_id": "abc"}}
app.invoke(input, config=config1)

# æ¢å¤æ—¶å¿…é¡»ä½¿ç”¨ç›¸åŒ ID
config2 = {"configurable": {"thread_id": "abc"}}  # âœ…
state = app.get_state(config2)
```

#### 2. ä½¿ç”¨æŒä¹…åŒ– Checkpointer

```python
# âŒ MemorySaver é‡å¯åä¸¢å¤±
from langgraph.checkpoint.memory import MemorySaver
checkpointer = MemorySaver()

# âœ… SqliteSaver æŒä¹…åŒ–
from langgraph.checkpoint.sqlite import SqliteSaver
checkpointer = SqliteSaver.from_conn_string("checkpoints.db")
```

---

## A.10 å¿«é€Ÿè¯Šæ–­æ¸…å•

å½“é‡åˆ°é—®é¢˜æ—¶ï¼ŒæŒ‰ä»¥ä¸‹é¡ºåºæ’æŸ¥ï¼š

```python
# 1. æ£€æŸ¥ç‰ˆæœ¬
import langchain
print(f"LangChain: {langchain.__version__}")

# 2. æ£€æŸ¥ç¯å¢ƒå˜é‡
import os
print(f"Tracing: {os.getenv('LANGCHAIN_TRACING_V2')}")
print(f"API Key: {os.getenv('OPENAI_API_KEY')[:10]}...")

# 3. æµ‹è¯•åŸºç¡€åŠŸèƒ½
from langchain_openai import ChatOpenAI
llm = ChatOpenAI()
response = llm.invoke("Hello")
print(f"LLM working: {bool(response)}")

# 4. æ£€æŸ¥ç½‘ç»œè¿æ¥
import requests
try:
    requests.get("https://api.openai.com", timeout=5)
    print("âœ… Network OK")
except:
    print("âŒ Network issue")

# 5. æŸ¥çœ‹æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

**ä¸‹ä¸€æ­¥**ï¼šå®Œæˆå…¶ä»–é™„å½•ï¼ˆB-Eï¼‰ä»¥åŠæœ€ç»ˆé›†æˆæµ‹è¯•ã€‚
