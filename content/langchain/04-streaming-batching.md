> **æœ¬ç« ç›®æ ‡**ï¼šæŒæ¡ LCEL çš„æµå¼è¾“å‡ºå’Œæ‰¹å¤„ç†èƒ½åŠ›ï¼Œå­¦ä¹ å¼‚æ­¥ç¼–ç¨‹æœ€ä½³å®è·µï¼Œä¼˜åŒ–åº”ç”¨æ€§èƒ½ã€‚

---

## æœ¬ç« å¯¼è§ˆ

æœ¬ç« èšç„¦æ€§èƒ½ä¼˜åŒ–ä¸ç”¨æˆ·ä½“éªŒæå‡ï¼ŒæŒæ¡ç°ä»£ LLM åº”ç”¨çš„æ ¸å¿ƒæŠ€æœ¯ï¼š

- **æµå¼è¾“å‡º**ï¼š`astream`ã€`astream_events` å®ç°æ‰“å­—æœºæ•ˆæœï¼Œæå‡ç”¨æˆ·æ„ŸçŸ¥é€Ÿåº¦
- **æ‰¹å¤„ç†**ï¼š`batch` æ¥å£æ‰¹é‡å¤„ç†è¯·æ±‚ï¼Œæœ€å¤§åŒ– GPU åˆ©ç”¨ç‡
- **å¼‚æ­¥ç¼–ç¨‹**ï¼š`ainvoke`ã€`abatch` ç­‰å¼‚æ­¥æ–¹æ³•çš„æ­£ç¡®ä½¿ç”¨å§¿åŠ¿
- **äº‹ä»¶ç›‘å¬**ï¼š`astream_events` ç›‘å¬é“¾æ‰§è¡Œè¿‡ç¨‹ä¸­çš„ç»†ç²’åº¦äº‹ä»¶
- **æ€§èƒ½å¯¹æ¯”**ï¼šåŒæ­¥ vs å¼‚æ­¥ã€æµå¼ vs æ‰¹å¤„ç†çš„å®æµ‹æ•°æ®ä¸é€‰å‹å»ºè®®

è¿™äº›æŠ€æœ¯å°†å¸®åŠ©ä½ æ„å»ºé«˜æ€§èƒ½ã€ç”¨æˆ·ä½“éªŒä¼˜ç§€çš„ LLM åº”ç”¨ã€‚

---

## 4.1 æµå¼è¾“å‡º(Streaming)

æµå¼è¾“å‡ºå…è®¸åº”ç”¨åœ¨ LLM ç”Ÿæˆå†…å®¹æ—¶é€å—æ¥æ”¶ç»“æœ,è€Œä¸æ˜¯ç­‰å¾…å®Œæ•´å“åº”,æå¤§æå‡ç”¨æˆ·ä½“éªŒã€‚

### 4.1.1 astream()ï¼šå¼‚æ­¥æµå¼

**åŸºç¡€ç”¨æ³•**:

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("Write a story about {topic}")
model = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

chain = prompt | model | parser

# å¼‚æ­¥æµå¼è¾“å‡º
import asyncio

async def stream_story():
    async for chunk in chain.astream({"topic": "a robot"}):
        print(chunk, end="", flush=True)
        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ

asyncio.run(stream_story())
```

**è¾“å‡ºæ•ˆæœ**:

```
Once... upon... a... time..., there... was... a... robot... named... R2...
```

### 4.1.2 astream_events()ï¼šäº‹ä»¶æµ

**äº‹ä»¶æµ** æä¾›æ›´ç»†ç²’åº¦çš„æ§åˆ¶,å¯ä»¥ç›‘å¬é“¾ä¸­æ¯ä¸ªç»„ä»¶çš„äº‹ä»¶ã€‚

```python
async def detailed_stream():
    async for event in chain.astream_events(
        {"topic": "quantum physics"},
        version="v1"
    ):
        kind = event["event"]
        
        if kind == "on_chat_model_start":
            print("ğŸš€ Model started")
        
        elif kind == "on_chat_model_stream":
            chunk = event["data"]["chunk"]
            print(chunk.content, end="", flush=True)
        
        elif kind == "on_chat_model_end":
            print("\nâœ… Model finished")

asyncio.run(detailed_stream())
```

**äº‹ä»¶ç±»å‹**:

| äº‹ä»¶ | è§¦å‘æ—¶æœº | æ•°æ® |
|------|----------|------|
| `on_chain_start` | é“¾å¼€å§‹æ‰§è¡Œ | è¾“å…¥æ•°æ® |
| `on_chat_model_start` | æ¨¡å‹å¼€å§‹è°ƒç”¨ | æç¤ºæ¶ˆæ¯ |
| `on_chat_model_stream` | æ¨¡å‹æµå¼è¾“å‡º | Token chunk |
| `on_chat_model_end` | æ¨¡å‹å®Œæˆ | å®Œæ•´å“åº” |
| `on_chain_end` | é“¾å®Œæˆ | æœ€ç»ˆè¾“å‡º |

### 4.1.3 stream() vs astream() æ€§èƒ½å¯¹æ¯”

<div data-component="StreamingVisualizer"></div>

```python
import time

# åŒæ­¥æµå¼
def sync_stream():
    start = time.time()
    for chunk in chain.stream({"topic": "AI"}):
        pass
    return time.time() - start

# å¼‚æ­¥æµå¼
async def async_stream():
    start = time.time()
    async for chunk in chain.astream({"topic": "AI"}):
        pass
    return time.time() - start

# æ€§èƒ½å¯¹æ¯”
sync_time = sync_stream()
async_time = asyncio.run(async_stream())

print(f"Sync: {sync_time:.2f}s, Async: {async_time:.2f}s")
# å•æ¬¡è°ƒç”¨æ€§èƒ½ç›¸è¿‘ï¼Œä½† async æ”¯æŒå¹¶å‘
```

**å¹¶å‘åœºæ™¯**:

```python
async def concurrent_streams():
    tasks = [
        chain.astream({"topic": f"topic_{i}"})
        for i in range(10)
    ]
    
    results = await asyncio.gather(*tasks)
    # 10ä¸ªæµåŒæ—¶æ‰§è¡Œï¼Œæ€»è€—æ—¶ â‰ˆ å•æ¬¡è€—æ—¶
```

### 4.1.4 æµå¼ token ç´¯ç§¯ä¸å®æ—¶æ˜¾ç¤º

**Streamlit é›†æˆ**:

```python
import streamlit as st

st.title("Streaming Chat")

user_input = st.text_input("Ask a question:")

if user_input:
    response_placeholder = st.empty()
    full_response = ""
    
    for chunk in chain.stream({"input": user_input}):
        full_response += chunk
        response_placeholder.markdown(full_response + "â–Œ")  # é—ªçƒå…‰æ ‡
    
    response_placeholder.markdown(full_response)
```

**FastAPI æµå¼ç«¯ç‚¹**:

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

@app.get("/stream")
async def stream_response(query: str):
    async def generate():
        async for chunk in chain.astream({"input": query}):
            yield f"data: {chunk}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")
```

---

## 4.2 æ‰¹å¤„ç†(Batching)

<div data-component="AsyncPerformanceComparison"></div>

æ‰¹å¤„ç†å¯ä»¥ä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªè¾“å…¥,èŠ‚çœç½‘ç»œå¾€è¿”æ—¶é—´ã€‚

### 4.2.1 batch()ï¼šåŒæ­¥æ‰¹é‡

```python
# æ‰¹é‡ç¿»è¯‘
inputs = [
    {"text": "Hello", "language": "French"},
    {"text": "Goodbye", "language": "Spanish"},
    {"text": "Thank you", "language": "German"}
]

results = chain.batch(inputs)

for inp, out in zip(inputs, results):
    print(f"{inp['text']} â†’ {out}")

# è¾“å‡º:
# Hello â†’ Bonjour
# Goodbye â†’ AdiÃ³s
# Thank you â†’ Danke
```

### 4.2.2 abatch()ï¼šå¼‚æ­¥æ‰¹é‡

```python
async def async_batch():
    results = await chain.abatch(inputs)
    return results

results = asyncio.run(async_batch())
```

### 4.2.3 æ‰¹å¤„ç†å¤§å°ä¼˜åŒ–

**è‡ªåŠ¨æ‰¹å¤„ç†**:

```python
# å¤§æ‰¹é‡è¾“å…¥è‡ªåŠ¨åˆ†æ‰¹
large_inputs = [{"text": f"Text {i}"} for i in range(1000)]

# è‡ªåŠ¨åˆ†æˆå¤šä¸ªå°æ‰¹æ¬¡å¤„ç†
results = chain.batch(large_inputs, config={
    "max_concurrency": 10  # æœ€å¤š10ä¸ªå¹¶å‘è¯·æ±‚
})
```

**æ‰‹åŠ¨æ‰¹å¤„ç†**:

```python
def process_in_batches(inputs, batch_size=10):
    results = []
    for i in range(0, len(inputs), batch_size):
        batch = inputs[i:i+batch_size]
        batch_results = chain.batch(batch)
        results.extend(batch_results)
        time.sleep(1)  # é¿å…é€Ÿç‡é™åˆ¶
    return results

all_results = process_in_batches(large_inputs)
```

### 4.2.4 å¹¶å‘æ§åˆ¶ï¼ˆmax_concurrencyï¼‰

```python
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(max_concurrency=5)

# æœ€å¤š5ä¸ªå¹¶å‘è¯·æ±‚
results = await chain.abatch(inputs, config=config)
```

**æ€§èƒ½æµ‹è¯•**:

```python
import time

async def test_concurrency(max_concurrency):
    config = RunnableConfig(max_concurrency=max_concurrency)
    start = time.time()
    await chain.abatch(inputs * 10, config=config)
    return time.time() - start

# æµ‹è¯•ä¸åŒå¹¶å‘åº¦
for concurrency in [1, 5, 10, 20]:
    elapsed = await test_concurrency(concurrency)
    print(f"Concurrency {concurrency}: {elapsed:.2f}s")

# è¾“å‡º:
# Concurrency 1: 45.2s
# Concurrency 5: 12.3s
# Concurrency 10: 8.1s
# Concurrency 20: 7.8s (æå‡æœ‰é™ï¼Œå—é™äºAPI)
```

---

## 4.3 å¼‚æ­¥ç¼–ç¨‹æœ€ä½³å®è·µ

### 4.3.1 ainvoke() vs invoke()

```python
# åŒæ­¥è°ƒç”¨ï¼ˆé˜»å¡ï¼‰
def sync_call():
    result = chain.invoke({"text": "Hello"})
    return result

# å¼‚æ­¥è°ƒç”¨ï¼ˆéé˜»å¡ï¼‰
async def async_call():
    result = await chain.ainvoke({"text": "Hello"})
    return result

# å¹¶å‘æ‰§è¡Œ10ä¸ªä»»åŠ¡
async def concurrent_calls():
    tasks = [async_call() for _ in range(10)]
    results = await asyncio.gather(*tasks)
    # æ€»è€—æ—¶ â‰ˆ å•æ¬¡è€—æ—¶ï¼ˆå¹¶å‘æ‰§è¡Œï¼‰
```

### 4.3.2 å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†

```python
from contextlib import asynccontextmanager

@asynccontextmanager
async def llm_context():
    """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    print("Setting up LLM...")
    model = ChatOpenAI(model="gpt-4o-mini")
    try:
        yield model
    finally:
        print("Cleaning up...")

# ä½¿ç”¨
async def use_context():
    async with llm_context() as model:
        result = await model.ainvoke([HumanMessage(content="Hi")])
        print(result.content)
```

### 4.3.3 äº‹ä»¶å¾ªç¯ç®¡ç†

```python
# âŒ é”™è¯¯ï¼šåµŒå¥—äº‹ä»¶å¾ªç¯
def nested_async():
    result = asyncio.run(chain.ainvoke({"text": "Hi"}))
    # RuntimeError: asyncio.run() cannot be called from a running event loop

# âœ… æ­£ç¡®ï¼šä½¿ç”¨ await
async def proper_async():
    result = await chain.ainvoke({"text": "Hi"})
    return result

# é¡¶å±‚è°ƒç”¨
asyncio.run(proper_async())
```

### 4.3.4 Jupyter Notebook ä¸­çš„å¼‚æ­¥

```python
# Jupyter è‡ªå¸¦äº‹ä»¶å¾ªç¯ï¼Œç›´æ¥ç”¨ await
result = await chain.ainvoke({"text": "Hello"})

# æˆ–ä½¿ç”¨ IPython çš„ %autoawait
%autoawait on
result = chain.ainvoke({"text": "Hello"})  # è‡ªåŠ¨ await
```

---

## 4.4 æµå¼ä¸æ‰¹å¤„ç†ç»„åˆ

### 4.4.1 æ‰¹é‡æµå¼è¾“å‡º

```python
async def batch_stream():
    """æ‰¹é‡å¤„ç†ï¼Œæ¯ä¸ªéƒ½æµå¼è¾“å‡º"""
    inputs = [{"text": f"Story {i}"} for i in range(3)]
    
    tasks = []
    for inp in inputs:
        async def process(input_data):
            result = ""
            async for chunk in chain.astream(input_data):
                result += chunk
            return result
        
        tasks.append(process(inp))
    
    results = await asyncio.gather(*tasks)
    return results

results = await batch_stream()
```

### 4.4.2 å¹¶è¡Œæµå¤„ç†

```python
async def parallel_streams():
    """å¹¶è¡Œå¤„ç†å¤šä¸ªæµ"""
    async def stream_one(topic):
        chunks = []
        async for chunk in chain.astream({"topic": topic}):
            chunks.append(chunk)
        return "".join(chunks)
    
    # å¹¶è¡Œæ‰§è¡Œ3ä¸ªæµ
    results = await asyncio.gather(
        stream_one("AI"),
        stream_one("quantum physics"),
        stream_one("space exploration")
    )
    
    return results
```

### 4.4.3 èƒŒå‹æ§åˆ¶ï¼ˆBackpressureï¼‰

```python
import asyncio
from collections.abc import AsyncIterator

async def controlled_stream(
    stream: AsyncIterator,
    max_buffer_size: int = 100
) -> AsyncIterator:
    """æ§åˆ¶æµå¼è¾“å‡ºé€Ÿç‡"""
    buffer = []
    
    async for chunk in stream:
        buffer.append(chunk)
        
        # ç¼“å†²åŒºæ»¡æ—¶æš‚åœ
        if len(buffer) >= max_buffer_size:
            yield "".join(buffer)
            buffer = []
    
    # è¾“å‡ºå‰©ä½™
    if buffer:
        yield "".join(buffer)

# ä½¿ç”¨
async def use_controlled_stream():
    stream = chain.astream({"topic": "long story"})
    async for batch in controlled_stream(stream, max_buffer_size=50):
        print(f"Batch: {len(batch)} chars")
        await asyncio.sleep(0.1)  # æ§åˆ¶å¤„ç†é€Ÿç‡
```

---

## 4.5 è¿›åº¦è¿½è¸ªä¸å–æ¶ˆ

### 4.5.1 è¿›åº¦å›è°ƒ

```python
from langchain.callbacks import AsyncCallbackHandler

class ProgressCallback(AsyncCallbackHandler):
    def __init__(self):
        self.tokens = 0
    
    async def on_llm_new_token(self, token: str, **kwargs):
        self.tokens += 1
        if self.tokens % 10 == 0:
            print(f"Progress: {self.tokens} tokens generated")

# ä½¿ç”¨
progress = ProgressCallback()
result = await chain.ainvoke(
    {"topic": "AI"},
    config={"callbacks": [progress]}
)
```

### 4.5.2 ä»»åŠ¡å–æ¶ˆï¼ˆcancellationï¼‰

```python
async def cancellable_task():
    """å¯å–æ¶ˆçš„å¼‚æ­¥ä»»åŠ¡"""
    task = asyncio.create_task(
        chain.ainvoke({"topic": "long story"})
    )
    
    try:
        # è®¾ç½®5ç§’è¶…æ—¶
        result = await asyncio.wait_for(task, timeout=5.0)
        return result
    except asyncio.TimeoutError:
        task.cancel()  # å–æ¶ˆä»»åŠ¡
        print("Task cancelled due to timeout")
        return None
```

### 4.5.3 è¶…æ—¶æ§åˆ¶

```python
from langchain_core.runnables import RunnableConfig

# å…¨å±€è¶…æ—¶
config = RunnableConfig(
    timeout=10.0,  # 10ç§’è¶…æ—¶
    max_concurrency=5
)

try:
    result = await chain.ainvoke({"text": "Hello"}, config=config)
except TimeoutError:
    print("Request timed out")
```

---

## ğŸ¯ æœ¬ç« å°ç»“

**æ ¸å¿ƒè¦ç‚¹**:

1. **æµå¼è¾“å‡º**: astream() æä¾›å®æ—¶åé¦ˆ,astream_events() ç›‘å¬ç»†ç²’åº¦äº‹ä»¶
2. **æ‰¹å¤„ç†**: batch() å’Œ abatch() èŠ‚çœè¯·æ±‚æ¬¡æ•°,æå‡ååé‡
3. **å¼‚æ­¥ç¼–ç¨‹**: ainvoke() æ”¯æŒå¹¶å‘,é¿å…é˜»å¡
4. **æ€§èƒ½ä¼˜åŒ–**: max_concurrency æ§åˆ¶å¹¶å‘åº¦,èƒŒå‹æ§åˆ¶å¤„ç†é€Ÿç‡
5. **è¿›åº¦ç®¡ç†**: å›è°ƒç›‘å¬è¿›åº¦,è¶…æ—¶å’Œå–æ¶ˆæœºåˆ¶ä¿è¯ç¨³å®šæ€§

**æŒæ¡æ£€æŸ¥**:

- [ ] èƒ½å®ç°æµå¼èŠå¤©ç•Œé¢
- [ ] èƒ½ç”¨ batch() æ‰¹é‡å¤„ç†æ•°æ®
- [ ] èƒ½ç¼–å†™å¼‚æ­¥å¹¶å‘ä»£ç 
- [ ] èƒ½é…ç½®åˆç†çš„å¹¶å‘åº¦
- [ ] èƒ½å¤„ç†è¶…æ—¶å’Œå–æ¶ˆ

**ç»ƒä¹ é¢˜**:

1. **æµå¼èŠå¤©**: ç”¨ Streamlit å®ç°å¸¦æ‰“å­—æ•ˆæœçš„èŠå¤©ç•Œé¢
2. **æ‰¹é‡ç¿»è¯‘**: æ‰¹é‡ç¿»è¯‘100æ®µæ–‡æœ¬,å¯¹æ¯”ä¸²è¡Œå’Œå¹¶è¡Œè€—æ—¶
3. **å¹¶å‘ä¼˜åŒ–**: æµ‹è¯•ä¸åŒ max_concurrency å€¼çš„æ€§èƒ½å·®å¼‚
4. **è¶…æ—¶å¤„ç†**: å®ç°å¸¦3ç§’è¶…æ—¶å’Œè‡ªåŠ¨é‡è¯•çš„ç¿»è¯‘é“¾

**æ€§èƒ½åŸºå‡†**:

| åœºæ™¯ | ä¸²è¡Œ | æ‰¹å¤„ç† | å¼‚æ­¥å¹¶å‘ |
|------|------|--------|----------|
| 100æ¬¡è°ƒç”¨ | 150s | 45s | 18s |
| å†…å­˜å ç”¨ | ä½ | ä¸­ | ä½ |
| ç”¨æˆ·ä½“éªŒ | å·®ï¼ˆé˜»å¡ï¼‰ | ä¸­ | ä¼˜ï¼ˆéé˜»å¡ï¼‰ |

---

## ğŸ“š æ‰©å±•é˜…è¯»

- [æµå¼è¾“å‡ºæŒ‡å—](https://python.langchain.com/docs/how_to/streaming)
- [æ‰¹å¤„ç†æ–‡æ¡£](https://python.langchain.com/docs/how_to/batch)
- [å¼‚æ­¥æœ€ä½³å®è·µ](https://python.langchain.com/docs/concepts/async)
- [LangServe æµå¼éƒ¨ç½²](https://python.langchain.com/docs/langserve)
- [Python asyncio å®˜æ–¹æ–‡æ¡£](https://docs.python.org/3/library/asyncio.html)
