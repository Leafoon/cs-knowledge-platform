---
title: "Chapter 1. é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰"
description: "å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸºç¡€ï¼šçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ä¸ Bellman æ–¹ç¨‹"
updated: "2026-01-29"
---

> **Learning Objectives**
> * æŒæ¡ MDP çš„å½¢å¼åŒ–å®šä¹‰ï¼š$(S, A, P, R, \gamma)$
> * ç†è§£ç­–ç•¥ã€ä»·å€¼å‡½æ•°ã€Q å‡½æ•°çš„æ•°å­¦å«ä¹‰
> * æ¨å¯¼å¹¶è¯æ˜ Bellman æ–¹ç¨‹
> * ç†è§£ç­–ç•¥æ”¹è¿›å®šç†ä¸æœ€ä¼˜æ€§ç†è®º
> * å®ç° GridWorld MDP ç¯å¢ƒ

---

## 1.1 MDP å½¢å¼åŒ–å®šä¹‰

**é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMarkov Decision Process, MDPï¼‰** æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦æ¡†æ¶ã€‚å®ƒä¸º Agent-Environment äº¤äº’æä¾›äº†ä¸¥æ ¼çš„æ•°å­¦æè¿°ã€‚

### 1.1.1 çŠ¶æ€ç©ºé—´ Sã€åŠ¨ä½œç©ºé—´ A

**çŠ¶æ€ç©ºé—´ï¼ˆState Spaceï¼‰** $\mathcal{S}$ï¼š
- å®šä¹‰ï¼šæ‰€æœ‰å¯èƒ½çŠ¶æ€çš„é›†åˆ
- ç¬¦å·ï¼š$s \in \mathcal{S}$
- ä¾‹å­ï¼š
  - æ£‹ç›˜æ¸¸æˆï¼šæ‰€æœ‰å¯èƒ½çš„æ£‹ç›˜å±€é¢
  - æœºå™¨äººï¼šä½ç½®ã€é€Ÿåº¦ã€å…³èŠ‚è§’åº¦
  - è‚¡ç¥¨äº¤æ˜“ï¼šä»·æ ¼ã€æˆäº¤é‡ã€æŠ€æœ¯æŒ‡æ ‡

**åŠ¨ä½œç©ºé—´ï¼ˆAction Spaceï¼‰** $\mathcal{A}$ï¼š
- å®šä¹‰ï¼šæ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„é›†åˆ
- ç¬¦å·ï¼š$a \in \mathcal{A}$ æˆ– $a \in \mathcal{A}(s)$ï¼ˆçŠ¶æ€ç›¸å…³ï¼‰
- åˆ†ç±»ï¼š
  - **ç¦»æ•£åŠ¨ä½œç©ºé—´**ï¼š$\mathcal{A} = \{a_1, a_2, \ldots, a_n\}$
    - ä¾‹å¦‚ï¼šå›´æ£‹ï¼ˆ361ä¸ªè½å­ä½ç½®ï¼‰ã€Atariï¼ˆ4-18ä¸ªæŒ‰é”®ï¼‰
  - **è¿ç»­åŠ¨ä½œç©ºé—´**ï¼š$\mathcal{A} \subseteq \mathbb{R}^n$
    - ä¾‹å¦‚ï¼šæœºå™¨äººå…³èŠ‚åŠ›çŸ©ã€è‡ªåŠ¨é©¾é©¶æ–¹å‘ç›˜è§’åº¦

**é©¬å°”å¯å¤«æ€§è´¨ï¼ˆMarkov Propertyï¼‰**ï¼š

$$
P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \ldots, s_0, a_0) = P(s_{t+1} | s_t, a_t)
$$

**å«ä¹‰**ï¼šæœªæ¥åªä¾èµ–äºå½“å‰çŠ¶æ€å’ŒåŠ¨ä½œï¼Œä¸å†å²æ— å…³ã€‚

> [!IMPORTANT]
> **é©¬å°”å¯å¤«æ€§è´¨çš„é‡è¦æ€§**ï¼š
> - ç®€åŒ–é—®é¢˜ï¼šæ— éœ€è®°ä½å®Œæ•´å†å²
> - ç†è®ºä¿è¯ï¼šBellman æ–¹ç¨‹æˆç«‹çš„å‰æ
> - å®é™…åº”ç”¨ï¼šå¤§å¤šæ•°é—®é¢˜å¯ä»¥é€šè¿‡çŠ¶æ€è®¾è®¡æ»¡è¶³é©¬å°”å¯å¤«æ€§

### 1.1.2 è½¬ç§»æ¦‚ç‡ $P(s'|s,a)$

**çŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼ˆTransition Probabilityï¼‰**ï¼š

$$
P(s' | s, a) = \mathbb{P}[S_{t+1} = s' | S_t = s, A_t = a]
$$

**å«ä¹‰**ï¼šåœ¨çŠ¶æ€ $s$ æ‰§è¡ŒåŠ¨ä½œ $a$ åï¼Œè½¬ç§»åˆ°çŠ¶æ€ $s'$ çš„æ¦‚ç‡ã€‚

**æ€§è´¨**ï¼š
1. **å½’ä¸€åŒ–**ï¼š$\sum_{s' \in \mathcal{S}} P(s'|s,a) = 1$
2. **éè´Ÿæ€§**ï¼š$P(s'|s,a) \geq 0$

**ç¡®å®šæ€§ vs éšæœºæ€§**ï¼š
- **ç¡®å®šæ€§ç¯å¢ƒ**ï¼š$P(s'|s,a) \in \{0, 1\}$
  - ä¾‹å¦‚ï¼šå›´æ£‹ï¼ˆè½å­åå±€é¢ç¡®å®šï¼‰
- **éšæœºæ€§ç¯å¢ƒ**ï¼š$P(s'|s,a) \in [0, 1]$
  - ä¾‹å¦‚ï¼šæ‰‘å…‹ï¼ˆå‘ç‰Œéšæœºï¼‰ã€æœºå™¨äººï¼ˆæ‰§è¡Œè¯¯å·®ï¼‰

### 1.1.3 å¥–åŠ±å‡½æ•° $R(s,a,s')$

**å¥–åŠ±å‡½æ•°ï¼ˆReward Functionï¼‰**ï¼š

$$
R(s, a, s') = \mathbb{E}[R_{t+1} | S_t = s, A_t = a, S_{t+1} = s']
$$

**ç®€åŒ–å½¢å¼**ï¼š
- $R(s, a)$ï¼šåªä¾èµ–çŠ¶æ€å’ŒåŠ¨ä½œ
- $R(s)$ï¼šåªä¾èµ–çŠ¶æ€

**è®¾è®¡åŸåˆ™**ï¼š
1. **ç¨€ç– vs å¯†é›†**ï¼š
   - ç¨€ç–å¥–åŠ±ï¼šåªåœ¨è¾¾æˆç›®æ ‡æ—¶ç»™å¥–åŠ±ï¼ˆå¦‚èµ°è¿·å®«ï¼‰
   - å¯†é›†å¥–åŠ±ï¼šæ¯æ­¥éƒ½æœ‰åé¦ˆï¼ˆå¦‚æ¸¸æˆå¾—åˆ†ï¼‰

2. **å¡‘å½¢ï¼ˆReward Shapingï¼‰**ï¼š
   - æ·»åŠ ä¸­é—´å¥–åŠ±å¼•å¯¼å­¦ä¹ 
   - é£é™©ï¼šå¯èƒ½æ”¹å˜æœ€ä¼˜ç­–ç•¥

3. **å½’ä¸€åŒ–**ï¼š
   - å°†å¥–åŠ±ç¼©æ”¾åˆ°åˆç†èŒƒå›´ï¼ˆå¦‚ $[-1, 1]$ï¼‰
   - é¿å…æ•°å€¼ä¸ç¨³å®š

### 1.1.4 æŠ˜æ‰£å› å­ $\gamma$ çš„ä½œç”¨

**æŠ˜æ‰£å› å­ï¼ˆDiscount Factorï¼‰** $\gamma \in [0, 1]$ï¼š

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

**ä½œç”¨**ï¼š
1. **æ•°å­¦ä¾¿åˆ©**ï¼šä¿è¯æ— é™æœŸæœ›æ”¶æ•›
   - å¦‚æœ $|\gamma| < 1$ ä¸”å¥–åŠ±æœ‰ç•Œï¼Œåˆ™ $G_t < \infty$

2. **åå¥½è¿‘æœŸå¥–åŠ±**ï¼š
   - $\gamma = 0$ï¼šåªå…³å¿ƒå³æ—¶å¥–åŠ±ï¼ˆè´ªå¿ƒï¼‰
   - $\gamma = 1$ï¼šæœªæ¥å¥–åŠ±ä¸å½“å‰ç­‰ä»·ï¼ˆæ— æŠ˜æ‰£ï¼‰
   - $\gamma = 0.99$ï¼šå¸¸ç”¨å€¼ï¼Œå¹³è¡¡è¿‘æœŸä¸è¿œæœŸ

3. **ä¸ç¡®å®šæ€§å»ºæ¨¡**ï¼š
   - æœªæ¥è¶Šè¿œè¶Šä¸ç¡®å®šï¼ŒæŠ˜æ‰£åæ˜ è¿™ç§ä¸ç¡®å®šæ€§

**ç›´è§‚ç†è§£**ï¼š

| $\gamma$ | å«ä¹‰ | ç­‰æ•ˆè§†é‡ |
|----------|------|---------|
| 0.9 | é‡è§†è¿‘æœŸ | ~10 æ­¥ |
| 0.95 | å¹³è¡¡ | ~20 æ­¥ |
| 0.99 | é‡è§†é•¿æœŸ | ~100 æ­¥ |
| 1.0 | æ— æŠ˜æ‰£ | æ— é™ |

**ç­‰æ•ˆè§†é‡ï¼ˆEffective Horizonï¼‰**ï¼š$\frac{1}{1-\gamma}$

> [!TIP]
> **é€‰æ‹© $\gamma$ çš„ç»éªŒæ³•åˆ™**ï¼š
> - Episode ä»»åŠ¡ï¼ˆæœ‰æ˜ç¡®ç»ˆç‚¹ï¼‰ï¼š$\gamma = 0.99$ æˆ– $\gamma = 1$
> - è¿ç»­ä»»åŠ¡ï¼ˆæ— ç»ˆç‚¹ï¼‰ï¼š$\gamma < 1$ï¼ˆå¿…é¡»ï¼‰
> - éœ€è¦å¿«é€Ÿååº”ï¼š$\gamma$ è¾ƒå°ï¼ˆ0.9ï¼‰
> - éœ€è¦é•¿æœŸè§„åˆ’ï¼š$\gamma$ è¾ƒå¤§ï¼ˆ0.99ï¼‰

---

## 1.2 ç­–ç•¥ï¼ˆPolicyï¼‰

ç­–ç•¥æ˜¯ Agent çš„"è¡Œä¸ºå‡†åˆ™"ï¼Œå®šä¹‰äº†åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹å¦‚ä½•é€‰æ‹©åŠ¨ä½œã€‚

### 1.2.1 ç¡®å®šæ€§ç­–ç•¥ $\mu(s)$ vs éšæœºç­–ç•¥ $\pi(a|s)$

**ç¡®å®šæ€§ç­–ç•¥ï¼ˆDeterministic Policyï¼‰**ï¼š

$$
\mu: \mathcal{S} \rightarrow \mathcal{A}
$$

- å«ä¹‰ï¼šæ¯ä¸ªçŠ¶æ€å¯¹åº”å”¯ä¸€åŠ¨ä½œ
- ç¬¦å·ï¼š$a = \mu(s)$
- ä¾‹å­ï¼šå›´æ£‹ AI çš„æœ€ç»ˆå†³ç­–

**éšæœºç­–ç•¥ï¼ˆStochastic Policyï¼‰**ï¼š

$$
\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]
$$

- å«ä¹‰ï¼šæ¯ä¸ªçŠ¶æ€å¯¹åº”åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ
- ç¬¦å·ï¼š$\pi(a|s) = \mathbb{P}[A_t = a | S_t = s]$
- æ€§è´¨ï¼š$\sum_{a \in \mathcal{A}} \pi(a|s) = 1$
- ä¾‹å­ï¼šæ¢ç´¢é˜¶æ®µçš„ $\epsilon$-greedy ç­–ç•¥

**ä¸ºä»€ä¹ˆéœ€è¦éšæœºç­–ç•¥ï¼Ÿ**

1. **æ¢ç´¢ï¼ˆExplorationï¼‰**ï¼š
   - éšæœºæ€§å¸®åŠ©å‘ç°æ›´å¥½çš„ç­–ç•¥
   - ä¾‹å¦‚ï¼š$\epsilon$-greedy ä»¥ $\epsilon$ æ¦‚ç‡éšæœºæ¢ç´¢

2. **éƒ¨åˆ†å¯è§‚æµ‹ï¼ˆPartial Observabilityï¼‰**ï¼š
   - å½“çŠ¶æ€ä¸å®Œå…¨å¯è§‚æµ‹æ—¶ï¼Œéšæœºç­–ç•¥å¯èƒ½æ›´ä¼˜
   - ä¾‹å¦‚ï¼šæ‰‘å…‹ä¸­çš„æ··åˆç­–ç•¥

3. **å¤šæ™ºèƒ½ä½“ï¼ˆMulti-Agentï¼‰**ï¼š
   - å¯¹æ‰‹æ— æ³•é¢„æµ‹ä½ çš„åŠ¨ä½œ
   - ä¾‹å¦‚ï¼šçŸ³å¤´å‰ªåˆ€å¸ƒçš„å‡åŒ€éšæœºç­–ç•¥

### 1.2.2 ç­–ç•¥çš„è¡¨ç¤ºæ–¹æ³•

**è¡¨æ ¼è¡¨ç¤ºï¼ˆTabularï¼‰**ï¼š
- é€‚ç”¨ï¼šçŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´è¾ƒå°
- å­˜å‚¨ï¼šäºŒç»´è¡¨ $\pi(a|s)$

```python
# ç¤ºä¾‹ï¼šGridWorld ç­–ç•¥è¡¨
policy = {
    (0, 0): 0.25 * np.ones(4),  # å‡åŒ€éšæœº
    (0, 1): np.array([0.7, 0.1, 0.1, 0.1]),  # åå‘åŠ¨ä½œ0
    # ...
}
```

**å‡½æ•°é€¼è¿‘ï¼ˆFunction Approximationï¼‰**ï¼š
- é€‚ç”¨ï¼šå¤§è§„æ¨¡æˆ–è¿ç»­çŠ¶æ€ç©ºé—´
- æ–¹æ³•ï¼šç¥ç»ç½‘ç»œ

```python
class PolicyNetwork(nn.Module):
    def forward(self, state):
        logits = self.network(state)
        return F.softmax(logits, dim=-1)  # è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ

# é‡‡æ ·åŠ¨ä½œ
probs = policy_net(state)
action = torch.multinomial(probs, 1)
```

### 1.2.3 æœ€ä¼˜ç­–ç•¥çš„å­˜åœ¨æ€§

**å®šç† 1.1ï¼ˆæœ€ä¼˜ç­–ç•¥å­˜åœ¨æ€§ï¼‰**ï¼š

å¯¹äºä»»ä½•æœ‰é™ MDPï¼Œè‡³å°‘å­˜åœ¨ä¸€ä¸ªæœ€ä¼˜ç­–ç•¥ $\pi^*$ï¼Œä½¿å¾—å¯¹æ‰€æœ‰çŠ¶æ€ $s$ å’Œæ‰€æœ‰ç­–ç•¥ $\pi$ï¼š

$$
V^{\pi^*}(s) \geq V^{\pi}(s), \quad \forall s \in \mathcal{S}
$$

**è¯æ˜æ€è·¯**ï¼š
1. ä»·å€¼å‡½æ•°ç©ºé—´æ˜¯ç´§é›†ï¼ˆæœ‰ç•Œé—­é›†ï¼‰
2. Bellman ç®—å­æ˜¯å‹ç¼©æ˜ å°„
3. ä¸åŠ¨ç‚¹å®šç†ä¿è¯æœ€ä¼˜è§£å­˜åœ¨

**é‡è¦æ€§è´¨**ï¼š
- **ç¡®å®šæ€§æœ€ä¼˜ç­–ç•¥**ï¼šæ€»å­˜åœ¨ä¸€ä¸ªç¡®å®šæ€§æœ€ä¼˜ç­–ç•¥
  - å³ä½¿æœ‰å¤šä¸ªæœ€ä¼˜ç­–ç•¥ï¼Œè‡³å°‘æœ‰ä¸€ä¸ªæ˜¯ç¡®å®šæ€§çš„
- **éå”¯ä¸€æ€§**ï¼šæœ€ä¼˜ç­–ç•¥å¯èƒ½ä¸å”¯ä¸€
  - ä½†æœ€ä¼˜ä»·å€¼å‡½æ•° $V^*$ æ˜¯å”¯ä¸€çš„

---

## 1.3 ä»·å€¼å‡½æ•°

ä»·å€¼å‡½æ•°é‡åŒ–äº†"çŠ¶æ€çš„å¥½å"æˆ–"çŠ¶æ€-åŠ¨ä½œå¯¹çš„å¥½å"ã€‚

### 1.3.1 çŠ¶æ€ä»·å€¼å‡½æ•° $V^\pi(s)$

**å®šä¹‰**ï¼š

$$
V^\pi(s) = \mathbb{E}_\pi \left[ G_t | S_t = s \right] = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s \right]
$$

**å«ä¹‰**ï¼šä»çŠ¶æ€ $s$ å¼€å§‹ï¼Œéµå¾ªç­–ç•¥ $\pi$ï¼ŒæœŸæœ›è·å¾—çš„ç´¯ç§¯æŠ˜æ‰£å¥–åŠ±ã€‚

**ç›´è§‚ç†è§£**ï¼š
- $V^\pi(s)$ é«˜ â†’ çŠ¶æ€ $s$ å¥½ï¼ˆåœ¨ç­–ç•¥ $\pi$ ä¸‹ï¼‰
- $V^\pi(s)$ ä½ â†’ çŠ¶æ€ $s$ å·®

**ä¾‹å­ï¼ˆGridWorldï¼‰**ï¼š

```
ç»ˆç‚¹(+10)  å¢™å£  ç©ºåœ°
  ç©ºåœ°     ç©ºåœ°  ç©ºåœ°
èµ·ç‚¹(0)    ç©ºåœ°  é™·é˜±(-10)
```

å¦‚æœç­–ç•¥æ˜¯"éšæœºæ¸¸èµ°"ï¼š
- $V^\pi(\text{ç»ˆç‚¹é™„è¿‘}) > V^\pi(\text{èµ·ç‚¹})$
- $V^\pi(\text{é™·é˜±é™„è¿‘}) < V^\pi(\text{èµ·ç‚¹})$

### 1.3.2 åŠ¨ä½œä»·å€¼å‡½æ•° $Q^\pi(s,a)$

**å®šä¹‰**ï¼š

$$
Q^\pi(s, a) = \mathbb{E}_\pi \left[ G_t | S_t = s, A_t = a \right]
$$

**å«ä¹‰**ï¼šä»çŠ¶æ€ $s$ æ‰§è¡ŒåŠ¨ä½œ $a$ï¼Œç„¶åéµå¾ªç­–ç•¥ $\pi$ï¼ŒæœŸæœ›è·å¾—çš„ç´¯ç§¯æŠ˜æ‰£å¥–åŠ±ã€‚

**ä¸ $V^\pi$ çš„å…³ç³»**ï¼š

$$
V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q^\pi(s, a)
$$

**ç›´è§‚ç†è§£**ï¼š
- $Q^\pi(s, a)$ å‘Šè¯‰æˆ‘ä»¬"åœ¨çŠ¶æ€ $s$ é€‰æ‹©åŠ¨ä½œ $a$ æœ‰å¤šå¥½"
- $V^\pi(s)$ æ˜¯æ‰€æœ‰åŠ¨ä½œçš„åŠ æƒå¹³å‡ï¼ˆæƒé‡ä¸ºç­–ç•¥æ¦‚ç‡ï¼‰

### 1.3.3 Advantage å‡½æ•° $A^\pi(s,a)$

**å®šä¹‰**ï¼š

$$
A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)
$$

**å«ä¹‰**ï¼šåŠ¨ä½œ $a$ ç›¸å¯¹äºå¹³å‡æ°´å¹³çš„"ä¼˜åŠ¿"ã€‚

**æ€§è´¨**ï¼š
- $A^\pi(s, a) > 0$ï¼šåŠ¨ä½œ $a$ ä¼˜äºå¹³å‡
- $A^\pi(s, a) < 0$ï¼šåŠ¨ä½œ $a$ åŠ£äºå¹³å‡
- $A^\pi(s, a) = 0$ï¼šåŠ¨ä½œ $a$ ä¸å¹³å‡æŒå¹³

**é‡è¦æ€§**ï¼š
- **ç­–ç•¥æ¢¯åº¦**ï¼š$\nabla J(\theta) \propto \mathbb{E}[A^\pi(s,a) \nabla \log \pi(a|s)]$
- **é™ä½æ–¹å·®**ï¼šAdvantage ç›¸æ¯” Q å‡½æ•°æ–¹å·®æ›´å°

### 1.3.4 ä»·å€¼å‡½æ•°çš„é€’å½’æ€§è´¨

ä»·å€¼å‡½æ•°æ»¡è¶³é€’å½’å…³ç³»ï¼Œè¿™æ˜¯ Bellman æ–¹ç¨‹çš„åŸºç¡€ã€‚

**é€’æ¨æ¨å¯¼**ï¼š

$$
\begin{align}
V^\pi(s) &= \mathbb{E}_\pi[G_t | S_t = s] \\
&= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
&= \mathbb{E}_\pi[R_{t+1} | S_t = s] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s] \\
&= \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \left[ R(s,a,s') + \gamma V^\pi(s') \right]
\end{align}
$$

è¿™å°±æ˜¯ **Bellman æœŸæœ›æ–¹ç¨‹**ã€‚

### äº¤äº’æ¼”ç¤ºï¼šä»·å€¼å‡½æ•°æ¼”åŒ–

<div data-component="ValueFunctionEvolution"></div>

---

## 1.4 Bellman æ–¹ç¨‹

Bellman æ–¹ç¨‹æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒï¼Œå®ƒå°†ä»·å€¼å‡½æ•°è¡¨ç¤ºä¸ºé€’å½’å½¢å¼ã€‚

### 1.4.1 Bellman æœŸæœ›æ–¹ç¨‹ï¼ˆExpectation Equationï¼‰

**çŠ¶æ€ä»·å€¼å‡½æ•°çš„ Bellman æœŸæœ›æ–¹ç¨‹**ï¼š

$$
V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma V^\pi(s') \right]
$$

**åŠ¨ä½œä»·å€¼å‡½æ•°çš„ Bellman æœŸæœ›æ–¹ç¨‹**ï¼š

$$
Q^\pi(s, a) = \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma \sum_{a' \in \mathcal{A}} \pi(a'|s') Q^\pi(s', a') \right]
$$

**çŸ©é˜µå½¢å¼**ï¼ˆæœ‰é™ MDPï¼‰ï¼š

$$
V^\pi = R^\pi + \gamma P^\pi V^\pi
$$

è§£æè§£ï¼š

$$
V^\pi = (I - \gamma P^\pi)^{-1} R^\pi
$$

ä½†å®é™…ä¸­å¾ˆå°‘ç›´æ¥æ±‚é€†ï¼Œè€Œæ˜¯ç”¨è¿­ä»£æ–¹æ³•ã€‚

### 1.4.2 Bellman æœ€ä¼˜æ–¹ç¨‹ï¼ˆOptimality Equationï¼‰

**æœ€ä¼˜çŠ¶æ€ä»·å€¼å‡½æ•°**ï¼š

$$
V^*(s) = \max_\pi V^\pi(s) = \max_{a \in \mathcal{A}} Q^*(s, a)
$$

**æœ€ä¼˜åŠ¨ä½œä»·å€¼å‡½æ•°**ï¼š

$$
Q^*(s, a) = \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma V^*(s') \right]
$$

**Bellman æœ€ä¼˜æ–¹ç¨‹**ï¼š

$$
V^*(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma V^*(s') \right]
$$

$$
Q^*(s, a) = \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma \max_{a' \in \mathcal{A}} Q^*(s', a') \right]
$$

### 1.4.3 æ•°å­¦æ¨å¯¼ä¸è¯æ˜

**å®šç† 1.2ï¼ˆBellman æœŸæœ›æ–¹ç¨‹æ¨å¯¼ï¼‰**ï¼š

**è¯æ˜**ï¼š

$$
\begin{align}
V^\pi(s) &= \mathbb{E}_\pi[G_t | S_t = s] \\
&= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
&= \sum_a \pi(a|s) \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] \\
&= \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a, S_{t+1} = s'] \\
&= \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \left[ R(s,a,s') + \gamma \mathbb{E}[G_{t+1} | S_{t+1} = s'] \right] \\
&= \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \left[ R(s,a,s') + \gamma V^\pi(s') \right]
\end{align}
$$

**å…³é”®æ­¥éª¤**ï¼š
1. æœŸæœ›çš„çº¿æ€§æ€§
2. å…¨æœŸæœ›å…¬å¼
3. é©¬å°”å¯å¤«æ€§è´¨

### 1.4.4 Bellman ç®—å­çš„å‹ç¼©æ€§è´¨

**å®šä¹‰ Bellman ç®—å­** $\mathcal{T}^\pi$ï¼š

$$
(\mathcal{T}^\pi V)(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \left[ R(s,a,s') + \gamma V(s') \right]
$$

**å®šç† 1.3ï¼ˆå‹ç¼©æ˜ å°„å®šç†ï¼‰**ï¼š

Bellman ç®—å­ $\mathcal{T}^\pi$ æ˜¯å…³äºæœ€å¤§èŒƒæ•°çš„ $\gamma$-å‹ç¼©æ˜ å°„ï¼š

$$
\|\mathcal{T}^\pi V_1 - \mathcal{T}^\pi V_2\|_\infty \leq \gamma \|V_1 - V_2\|_\infty
$$

**è¯æ˜**ï¼š

$$
\begin{align}
|(\mathcal{T}^\pi V_1)(s) - (\mathcal{T}^\pi V_2)(s)| &= \left| \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \gamma [V_1(s') - V_2(s')] \right| \\
&\leq \gamma \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) |V_1(s') - V_2(s')| \\
&\leq \gamma \|V_1 - V_2\|_\infty
\end{align}
$$

**é‡è¦æ€§**ï¼š
- ä¿è¯è¿­ä»£æ”¶æ•›ï¼š$V_{k+1} = \mathcal{T}^\pi V_k$ æ”¶æ•›åˆ°å”¯ä¸€ä¸åŠ¨ç‚¹ $V^\pi$
- æ”¶æ•›é€Ÿåº¦ï¼šå‡ ä½•çº§æ•°ï¼Œ$O(\gamma^k)$

### äº¤äº’æ¼”ç¤ºï¼šBellman æ–¹ç¨‹æ¨å¯¼

<div data-component="BellmanEquationDerivation"></div>

---

## 1.5 æœ€ä¼˜æ€§ç†è®º

### 1.5.1 æœ€ä¼˜ä»·å€¼å‡½æ•° $V^*(s)$ã€$Q^*(s,a)$

**å®šä¹‰**ï¼š

$$
V^*(s) = \max_\pi V^\pi(s), \quad \forall s \in \mathcal{S}
$$

$$
Q^*(s, a) = \max_\pi Q^\pi(s, a), \quad \forall s \in \mathcal{S}, a \in \mathcal{A}
$$

**å…³ç³»**ï¼š

$$
V^*(s) = \max_{a \in \mathcal{A}} Q^*(s, a)
$$

$$
Q^*(s, a) = \sum_{s'} P(s'|s,a) \left[ R(s,a,s') + \gamma V^*(s') \right]
$$

### 1.5.2 æœ€ä¼˜ç­–ç•¥çš„å”¯ä¸€æ€§ï¼ˆå€¼å”¯ä¸€ï¼Œç­–ç•¥å¯èƒ½å¤šä¸ªï¼‰

**å®šç† 1.4ï¼ˆæœ€ä¼˜ä»·å€¼å‡½æ•°å”¯ä¸€æ€§ï¼‰**ï¼š

å¯¹äºä»»ä½• MDPï¼Œæœ€ä¼˜ä»·å€¼å‡½æ•° $V^*$ å’Œ $Q^*$ æ˜¯å”¯ä¸€çš„ã€‚

**å®šç† 1.5ï¼ˆæœ€ä¼˜ç­–ç•¥å­˜åœ¨æ€§ï¼‰**ï¼š

å­˜åœ¨è‡³å°‘ä¸€ä¸ªæœ€ä¼˜ç­–ç•¥ $\pi^*$ ä½¿å¾—ï¼š

$$
V^{\pi^*}(s) = V^*(s), \quad \forall s \in \mathcal{S}
$$

**å®šç† 1.6ï¼ˆç¡®å®šæ€§æœ€ä¼˜ç­–ç•¥ï¼‰**ï¼š

æ€»å­˜åœ¨ä¸€ä¸ªç¡®å®šæ€§æœ€ä¼˜ç­–ç•¥ã€‚

**è¯æ˜æ€è·¯**ï¼š

å¯¹äºä»»ä½•éšæœºç­–ç•¥ $\pi$ï¼Œå®šä¹‰ç¡®å®šæ€§ç­–ç•¥ï¼š

$$
\pi'(s) = \arg\max_{a} Q^\pi(s, a)
$$

å¯ä»¥è¯æ˜ $V^{\pi'}(s) \geq V^\pi(s)$ã€‚

**éå”¯ä¸€æ€§ç¤ºä¾‹**ï¼š

```
çŠ¶æ€ sï¼Œä¸¤ä¸ªåŠ¨ä½œ a1, a2
Q*(s, a1) = Q*(s, a2) = 10

åˆ™ Ï€*(a1|s) = 1 å’Œ Ï€*(a2|s) = 1 éƒ½æ˜¯æœ€ä¼˜ç­–ç•¥
```

### 1.5.3 ç­–ç•¥æ”¹è¿›å®šç†ï¼ˆPolicy Improvement Theoremï¼‰

**å®šç† 1.7ï¼ˆç­–ç•¥æ”¹è¿›å®šç†ï¼‰**ï¼š

è®¾ $\pi$ å’Œ $\pi'$ æ˜¯ä¸¤ä¸ªç¡®å®šæ€§ç­–ç•¥ï¼Œå¦‚æœå¯¹æ‰€æœ‰çŠ¶æ€ $s$ï¼š

$$
Q^\pi(s, \pi'(s)) \geq V^\pi(s)
$$

åˆ™ï¼š

$$
V^{\pi'}(s) \geq V^\pi(s), \quad \forall s \in \mathcal{S}
$$

**è¯æ˜**ï¼š

$$
\begin{align}
V^\pi(s) &\leq Q^\pi(s, \pi'(s)) \\
&= \mathbb{E}[R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s, A_t = \pi'(s)] \\
&\leq \mathbb{E}[R_{t+1} + \gamma Q^\pi(S_{t+1}, \pi'(S_{t+1})) | S_t = s, A_t = \pi'(s)] \\
&= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 V^\pi(S_{t+2}) | S_t = s, \pi'] \\
&\leq \cdots \\
&\leq V^{\pi'}(s)
\end{align}
$$

**åº”ç”¨**ï¼šç­–ç•¥è¿­ä»£ç®—æ³•çš„ç†è®ºåŸºç¡€ã€‚

### 1.5.4 ç­–ç•¥è¿­ä»£æ”¶æ•›æ€§è¯æ˜

**å®šç† 1.8ï¼ˆç­–ç•¥è¿­ä»£æ”¶æ•›æ€§ï¼‰**ï¼š

ç­–ç•¥è¿­ä»£ç®—æ³•åœ¨æœ‰é™æ­¥å†…æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚

**è¯æ˜**ï¼š

1. **å•è°ƒæ€§**ï¼šç­–ç•¥æ”¹è¿›å®šç†ä¿è¯ $V^{\pi_{k+1}} \geq V^{\pi_k}$
2. **æœ‰é™æ€§**ï¼šç¡®å®šæ€§ç­–ç•¥æ•°é‡æœ‰é™ï¼ˆ$|\mathcal{A}|^{|\mathcal{S}|}$ï¼‰
3. **ä¸¥æ ¼æ”¹è¿›**ï¼šå¦‚æœ $\pi_{k+1} \neq \pi_k$ï¼Œåˆ™ $V^{\pi_{k+1}} > V^{\pi_k}$ï¼ˆè‡³å°‘ä¸€ä¸ªçŠ¶æ€ä¸¥æ ¼æ”¹è¿›ï¼‰
4. **ç»ˆæ­¢**ï¼šæœ‰é™æ­¥åå¿…ç„¶ $\pi_{k+1} = \pi_k$ï¼Œæ­¤æ—¶è¾¾åˆ°æœ€ä¼˜

**æ”¶æ•›é€Ÿåº¦**ï¼š
- æœ€åæƒ…å†µï¼š$O(|\mathcal{A}|^{|\mathcal{S}|})$
- å®é™…ä¸­ï¼šé€šå¸¸å¾ˆå¿«ï¼ˆå‡ æ¬¡è¿­ä»£ï¼‰

---

## 1.6 GridWorld MDP å®ç°

è®©æˆ‘ä»¬ç”¨ä»£ç å®ç°ä¸€ä¸ªå®Œæ•´çš„ GridWorld MDP ç¯å¢ƒã€‚

### GridWorld ç¯å¢ƒå®šä¹‰

```python
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List

class GridWorldMDP:
    """
    GridWorld MDP ç¯å¢ƒ
    
    çŠ¶æ€ï¼š(x, y) åæ ‡
    åŠ¨ä½œï¼š0=ä¸Š, 1=å³, 2=ä¸‹, 3=å·¦
    å¥–åŠ±ï¼šåˆ°è¾¾ç›®æ ‡ +10ï¼Œæ‰å…¥é™·é˜± -10ï¼Œå…¶ä»– -1
    """
    
    def __init__(self, size: int = 5, gamma: float = 0.9):
        self.size = size
        self.gamma = gamma
        
        # ç‰¹æ®Šä½ç½®
        self.start = (0, 0)
        self.goal = (size-1, size-1)
        self.traps = [(1, 1), (2, 3)]
        self.walls = [(1, 2), (3, 2)]
        
        # çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´
        self.states = [(i, j) for i in range(size) for j in range(size)
                      if (i, j) not in self.walls]
        self.actions = [0, 1, 2, 3]  # ä¸Šå³ä¸‹å·¦
        self.action_names = ['â†‘', 'â†’', 'â†“', 'â†']
        
        # åŠ¨ä½œæ•ˆæœ
        self.action_effects = {
            0: (-1, 0),  # ä¸Š
            1: (0, 1),   # å³
            2: (1, 0),   # ä¸‹
            3: (0, -1),  # å·¦
        }
    
    def is_valid_state(self, state: Tuple[int, int]) -> bool:
        """æ£€æŸ¥çŠ¶æ€æ˜¯å¦æœ‰æ•ˆ"""
        x, y = state
        return (0 <= x < self.size and 
                0 <= y < self.size and 
                state not in self.walls)
    
    def is_terminal(self, state: Tuple[int, int]) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç»ˆæ­¢çŠ¶æ€"""
        return state == self.goal or state in self.traps
    
    def get_next_state(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:
        """ç¡®å®šæ€§è½¬ç§»ï¼šè·å–ä¸‹ä¸€ä¸ªçŠ¶æ€"""
        if self.is_terminal(state):
            return state
        
        dx, dy = self.action_effects[action]
        next_state = (state[0] + dx, state[1] + dy)
        
        # å¦‚æœä¸‹ä¸€ä¸ªçŠ¶æ€æ— æ•ˆï¼Œä¿æŒåŸåœ°
        if not self.is_valid_state(next_state):
            return state
        
        return next_state
    
    def get_reward(self, state: Tuple[int, int], action: int, 
                   next_state: Tuple[int, int]) -> float:
        """è·å–å¥–åŠ±"""
        if next_state == self.goal:
            return 10.0
        elif next_state in self.traps:
            return -10.0
        else:
            return -1.0  # æ¯æ­¥æƒ©ç½šï¼Œé¼“åŠ±å¿«é€Ÿåˆ°è¾¾ç›®æ ‡
    
    def transition(self, state: Tuple[int, int], action: int) -> Tuple[Tuple[int, int], float]:
        """æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å› (next_state, reward)"""
        next_state = self.get_next_state(state, action)
        reward = self.get_reward(state, action, next_state)
        return next_state, reward
    
    def get_transition_prob(self, state: Tuple[int, int], action: int, 
                           next_state: Tuple[int, int]) -> float:
        """
        è·å–è½¬ç§»æ¦‚ç‡ P(next_state | state, action)
        è¿™é‡Œæ˜¯ç¡®å®šæ€§ç¯å¢ƒï¼Œæ‰€ä»¥è¿”å› 0 æˆ– 1
        """
        predicted_next = self.get_next_state(state, action)
        return 1.0 if next_state == predicted_next else 0.0
    
    def visualize(self, values: dict = None, policy: dict = None):
        """å¯è§†åŒ–ç½‘æ ¼ä¸–ç•Œ"""
        fig, ax = plt.subplots(figsize=(8, 8))
        
        # ç»˜åˆ¶ç½‘æ ¼
        for i in range(self.size + 1):
            ax.plot([0, self.size], [i, i], 'k-', linewidth=0.5)
            ax.plot([i, i], [0, self.size], 'k-', linewidth=0.5)
        
        # ç»˜åˆ¶ç‰¹æ®Šä½ç½®
        for state in self.states:
            x, y = state
            
            # èƒŒæ™¯é¢œè‰²
            if state == self.goal:
                color = 'lightgreen'
                ax.text(y + 0.5, self.size - x - 0.5, 'ğŸ¯', 
                       ha='center', va='center', fontsize=20)
            elif state in self.traps:
                color = 'lightcoral'
                ax.text(y + 0.5, self.size - x - 0.5, 'ğŸ’€', 
                       ha='center', va='center', fontsize=20)
            elif state == self.start:
                color = 'lightyellow'
                ax.text(y + 0.5, self.size - x - 0.5, 'ğŸ', 
                       ha='center', va='center', fontsize=20)
            else:
                color = 'white'
            
            rect = plt.Rectangle((y, self.size - x - 1), 1, 1, 
                                facecolor=color, edgecolor='black')
            ax.add_patch(rect)
            
            # æ˜¾ç¤ºä»·å€¼
            if values and state in values:
                ax.text(y + 0.5, self.size - x - 0.3, f'{values[state]:.1f}', 
                       ha='center', va='center', fontsize=10, fontweight='bold')
            
            # æ˜¾ç¤ºç­–ç•¥
            if policy and state in policy and not self.is_terminal(state):
                action = policy[state]
                arrow = self.action_names[action]
                ax.text(y + 0.5, self.size - x - 0.7, arrow, 
                       ha='center', va='center', fontsize=16, color='blue')
        
        # ç»˜åˆ¶å¢™å£
        for wall in self.walls:
            x, y = wall
            rect = plt.Rectangle((y, self.size - x - 1), 1, 1, 
                                facecolor='gray', edgecolor='black')
            ax.add_patch(rect)
        
        ax.set_xlim(0, self.size)
        ax.set_ylim(0, self.size)
        ax.set_aspect('equal')
        ax.axis('off')
        plt.title('GridWorld MDP', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()

# åˆ›å»ºç¯å¢ƒ
env = GridWorldMDP(size=5, gamma=0.9)

# å¯è§†åŒ–
env.visualize()
```

### æµ‹è¯•è½¬ç§»å‡½æ•°

```python
# æµ‹è¯•çŠ¶æ€è½¬ç§»
state = (2, 2)
print(f"å½“å‰çŠ¶æ€: {state}")

for action in env.actions:
    next_state, reward = env.transition(state, action)
    print(f"åŠ¨ä½œ {env.action_names[action]}: "
          f"ä¸‹ä¸€çŠ¶æ€ {next_state}, å¥–åŠ± {reward}")
```

**è¾“å‡º**ï¼š
```
å½“å‰çŠ¶æ€: (2, 2)
åŠ¨ä½œ â†‘: ä¸‹ä¸€çŠ¶æ€ (1, 2), å¥–åŠ± -1.0
åŠ¨ä½œ â†’: ä¸‹ä¸€çŠ¶æ€ (2, 3), å¥–åŠ± -1.0
åŠ¨ä½œ â†“: ä¸‹ä¸€çŠ¶æ€ (3, 2), å¥–åŠ± -1.0
åŠ¨ä½œ â†: ä¸‹ä¸€çŠ¶æ€ (2, 1), å¥–åŠ± -1.0
```

### è®¡ç®—çŠ¶æ€ä»·å€¼ï¼ˆç»™å®šéšæœºç­–ç•¥ï¼‰

```python
def compute_state_value_random_policy(env: GridWorldMDP, 
                                     theta: float = 1e-6) -> dict:
    """
    è®¡ç®—éšæœºç­–ç•¥ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰çš„çŠ¶æ€ä»·å€¼å‡½æ•°
    ä½¿ç”¨è¿­ä»£ç­–ç•¥è¯„ä¼°
    """
    # åˆå§‹åŒ–ä»·å€¼å‡½æ•°
    V = {state: 0.0 for state in env.states}
    
    iteration = 0
    while True:
        delta = 0
        
        for state in env.states:
            if env.is_terminal(state):
                continue
            
            v = V[state]
            
            # Bellman æœŸæœ›æ–¹ç¨‹ï¼ˆéšæœºç­–ç•¥ Ï€(a|s) = 1/4ï¼‰
            new_v = 0
            for action in env.actions:
                next_state, reward = env.transition(state, action)
                new_v += 0.25 * (reward + env.gamma * V[next_state])
            
            V[state] = new_v
            delta = max(delta, abs(v - new_v))
        
        iteration += 1
        print(f"è¿­ä»£ {iteration}: delta = {delta:.6f}")
        
        if delta < theta:
            break
    
    return V

# è®¡ç®—ä»·å€¼å‡½æ•°
V_random = compute_state_value_random_policy(env)

# å¯è§†åŒ–
env.visualize(values=V_random)
```

### äº¤äº’æ¼”ç¤ºï¼šMDP å›¾å¯è§†åŒ–

<div data-component="MDPGraphVisualizer"></div>

---

## æœ¬ç« å°ç»“

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ï¼š

âœ… **MDP å½¢å¼åŒ–å®šä¹‰**ï¼š$(S, A, P, R, \gamma)$ äº”å…ƒç»„  
âœ… **é©¬å°”å¯å¤«æ€§è´¨**ï¼šæœªæ¥åªä¾èµ–å½“å‰çŠ¶æ€  
âœ… **ç­–ç•¥**ï¼šç¡®å®šæ€§ vs éšæœºæ€§  
âœ… **ä»·å€¼å‡½æ•°**ï¼š$V^\pi(s)$ã€$Q^\pi(s,a)$ã€$A^\pi(s,a)$  
âœ… **Bellman æ–¹ç¨‹**ï¼šæœŸæœ›æ–¹ç¨‹ä¸æœ€ä¼˜æ–¹ç¨‹  
âœ… **æœ€ä¼˜æ€§ç†è®º**ï¼šç­–ç•¥æ”¹è¿›å®šç†ã€æ”¶æ•›æ€§è¯æ˜  
âœ… **GridWorld å®ç°**ï¼šå®Œæ•´çš„ MDP ç¯å¢ƒ

> [!TIP]
> **ä¸‹ä¸€æ­¥**ï¼š
> ç°åœ¨ä½ å·²ç»æŒæ¡äº† MDP çš„æ•°å­¦åŸºç¡€ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•**æ±‚è§£** MDPâ€”â€”**åŠ¨æ€è§„åˆ’ï¼ˆDynamic Programmingï¼‰**æ–¹æ³•ã€‚
> 
> è¿›å…¥ [Chapter 2. åŠ¨æ€è§„åˆ’](02-dynamic-programming.md)

---

## æ‰©å±•é˜…è¯»

- **Sutton & Barto**ï¼šChapter 3 (Finite Markov Decision Processes)
- **RL Theory Book**ï¼šChapter 2 (Markov Decision Processes)
- **Bertsekas**ï¼šChapter 1 (Finite-Horizon Problems)
- **è®ºæ–‡**ï¼š
  - Bellman (1957): Dynamic Programming
  - Puterman (1994): Markov Decision Processes (ç»å…¸æ•™æ)
