---
title: "ç¬¬36ç« ï¼šRLç†è®ºåŸºç¡€"
description: "æ”¶æ•›æ€§ç†è®ºã€æ ·æœ¬å¤æ‚åº¦ã€å‡½æ•°é€¼è¿‘ã€ç­–ç•¥ä¼˜åŒ–ã€æ¢ç´¢-åˆ©ç”¨æƒè¡¡"
date: "2026-01-30"
---

# ç¬¬36ç« ï¼šRLç†è®ºåŸºç¡€

## 36.1 æ”¶æ•›æ€§ç†è®º

### 36.1.1 å€¼è¿­ä»£æ”¶æ•›

**Bellmanæœ€ä¼˜ç®—å­**ï¼š

$$
\mathcal{T}^* V(s) = \max_{a \in \mathcal{A}} \left[ r(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) V(s') \right]
$$

**å‹ç¼©æ˜ å°„å®šç†**ï¼ˆContraction Mapping Theoremï¼‰ï¼š

$$
\| \mathcal{T}^* V - \mathcal{T}^* U \|_\infty \leq \gamma \| V - U \|_\infty
$$

å…¶ä¸­ $\gamma \in [0, 1)$ æ˜¯æŠ˜æ‰£å› å­ã€‚

**è¯æ˜**ï¼ˆå€¼è¿­ä»£æ”¶æ•›ï¼‰ï¼š

```python
"""
å€¼è¿­ä»£æ”¶æ•›æ€§è¯æ˜

å®šç†ï¼šå€¼è¿­ä»£ç®—æ³•æ”¶æ•›åˆ°å”¯ä¸€æœ€ä¼˜å€¼å‡½æ•°V*

è¯æ˜æ€è·¯ï¼š
1. è¯æ˜Bellmanç®—å­æ˜¯å‹ç¼©æ˜ å°„
2. åº”ç”¨Banachä¸åŠ¨ç‚¹å®šç†
3. å¾—åˆ°æ”¶æ•›é€Ÿåº¦ç•Œ
"""

import numpy as np
import matplotlib.pyplot as plt

def value_iteration_convergence_proof():
    """
    å€¼è¿­ä»£æ”¶æ•›æ€§æ•°å€¼éªŒè¯
    """
    # ç®€å•MDPç¤ºä¾‹ï¼š5ä¸ªçŠ¶æ€
    num_states = 5
    num_actions = 2
    gamma = 0.9
    
    # éšæœºè½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±
    np.random.seed(42)
    P = np.random.rand(num_states, num_actions, num_states)
    P = P / P.sum(axis=2, keepdims=True)  # å½’ä¸€åŒ–
    R = np.random.randn(num_states, num_actions)
    
    # Bellmanæœ€ä¼˜ç®—å­
    def bellman_operator(V):
        Q = R + gamma * (P @ V)  # (S, A)
        return Q.max(axis=1)  # (S,)
    
    # å€¼è¿­ä»£
    V = np.zeros(num_states)  # åˆå§‹å€¼å‡½æ•°
    V_history = [V.copy()]
    errors = []
    
    max_iterations = 100
    
    for iteration in range(max_iterations):
        V_new = bellman_operator(V)
        
        # è®°å½•è¯¯å·®
        error = np.linalg.norm(V_new - V, ord=np.inf)
        errors.append(error)
        V_history.append(V_new.copy())
        
        V = V_new
        
        # æ”¶æ•›åˆ¤æ–­
        if error < 1e-6:
            print(f"æ”¶æ•›äºç¬¬ {iteration} æ¬¡è¿­ä»£")
            break
    
    # éªŒè¯å‹ç¼©æ€§è´¨
    print(f"\néªŒè¯å‹ç¼©æ€§è´¨:")
    V1 = np.random.randn(num_states)
    V2 = np.random.randn(num_states)
    
    TV1 = bellman_operator(V1)
    TV2 = bellman_operator(V2)
    
    lhs = np.linalg.norm(TV1 - TV2, ord=np.inf)
    rhs = gamma * np.linalg.norm(V1 - V2, ord=np.inf)
    
    print(f"||T V1 - T V2||âˆ = {lhs:.6f}")
    print(f"Î³ ||V1 - V2||âˆ  = {rhs:.6f}")
    print(f"å‹ç¼©æˆç«‹: {lhs <= rhs}")
    
    # å¯è§†åŒ–æ”¶æ•›è¿‡ç¨‹
    plt.figure(figsize=(14, 5))
    
    # è¯¯å·®ä¸‹é™
    plt.subplot(1, 2, 1)
    plt.semilogy(errors, 'b-', linewidth=2)
    plt.xlabel('Iteration', fontsize=12)
    plt.ylabel('||V_{k+1} - V_k||âˆ', fontsize=12)
    plt.title('Convergence Rate (Log Scale)', fontsize=14)
    plt.grid(True, alpha=0.3)
    
    # ç†è®ºç•Œ
    theoretical_bound = [
        (gamma ** k) * np.linalg.norm(V_history[1] - V_history[0], ord=np.inf)
        for k in range(len(errors))
    ]
    plt.plot(theoretical_bound, 'r--', linewidth=2, label='Theoretical Bound: Î³^k')
    plt.legend()
    
    # å€¼å‡½æ•°æ¼”åŒ–
    plt.subplot(1, 2, 2)
    for s in range(num_states):
        values = [V_history[k][s] for k in range(len(V_history))]
        plt.plot(values, label=f'State {s}')
    plt.xlabel('Iteration', fontsize=12)
    plt.ylabel('Value', fontsize=12)
    plt.title('Value Function Evolution', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('value_iteration_convergence.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return V, errors


# è¿è¡ŒéªŒè¯
V_star, errors = value_iteration_convergence_proof()
```

**Banachä¸åŠ¨ç‚¹å®šç†**ï¼š

å¯¹äºå®Œå¤‡åº¦é‡ç©ºé—´ $(X, d)$ å’Œå‹ç¼©æ˜ å°„ $T: X \to X$ï¼Œæ»¡è¶³ï¼š

$$
d(T(x), T(y)) \leq \gamma \cdot d(x, y), \quad \forall x, y \in X
$$

å…¶ä¸­ $\gamma \in [0, 1)$ã€‚åˆ™ï¼š

1. **å­˜åœ¨æ€§**ï¼šå­˜åœ¨å”¯ä¸€ä¸åŠ¨ç‚¹ $x^* \in X$ï¼Œæ»¡è¶³ $T(x^*) = x^*$
2. **æ”¶æ•›æ€§**ï¼šå¯¹ä»»æ„åˆå§‹ç‚¹ $x_0 \in X$ï¼Œåºåˆ— $x_{k+1} = T(x_k)$ æ”¶æ•›åˆ° $x^*$
3. **æ”¶æ•›é€Ÿåº¦**ï¼š$d(x_k, x^*) \leq \gamma^k \cdot d(x_0, x^*)$

**åº”ç”¨åˆ°å€¼è¿­ä»£**ï¼š

- ç©ºé—´ï¼š$X = \mathbb{R}^{|\mathcal{S}|}$ï¼Œè·ç¦»ï¼š$d(V, U) = \|V - U\|_\infty$
- æ˜ å°„ï¼š$T = \mathcal{T}^*$ï¼ˆBellmanæœ€ä¼˜ç®—å­ï¼‰
- ä¸åŠ¨ç‚¹ï¼š$V^* = \mathcal{T}^* V^*$ï¼ˆBellmanæœ€ä¼˜æ–¹ç¨‹ï¼‰

**æ”¶æ•›é€Ÿåº¦**ï¼š

$$
\|V_k - V^*\|_\infty \leq \gamma^k \|V_0 - V^*\|_\infty
$$

- $k = O\left(\frac{1}{1-\gamma} \log \frac{1}{\epsilon}\right)$ æ¬¡è¿­ä»£è¾¾åˆ° $\epsilon$-ç²¾åº¦

<div data-component="ConvergenceProofVisualization"></div>

### 36.1.2 Q-learningæ”¶æ•›

**Q-learningæ›´æ–°è§„åˆ™**ï¼š

$$
Q(s_t, a_t) \leftarrow (1-\alpha_t) Q(s_t, a_t) + \alpha_t \left[ r_t + \gamma \max_{a'} Q(s_{t+1}, a') \right]
$$

**æ”¶æ•›å®šç†**ï¼ˆWatkins & Dayan, 1992ï¼‰ï¼š

**å®šç†**ï¼šQ-learningç®—æ³•åœ¨ä»¥ä¸‹æ¡ä»¶ä¸‹å‡ ä¹å¿…ç„¶æ”¶æ•›åˆ°æœ€ä¼˜Qå‡½æ•° $Q^*$ï¼š

1. **è¡¨æ ¼è¡¨ç¤º**ï¼šæœ‰é™çŠ¶æ€-åŠ¨ä½œç©ºé—´
2. **éå†æ€§**ï¼šæ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹è¢«è®¿é—®æ— ç©·æ¬¡
3. **å­¦ä¹ ç‡æ¡ä»¶**ï¼š
   $$
   \sum_{t=0}^\infty \alpha_t(s,a) = \infty, \quad \sum_{t=0}^\infty \alpha_t^2(s,a) < \infty
   $$

**è¯æ˜æ¡†æ¶**ï¼ˆåŸºäºéšæœºé€¼è¿‘ç†è®ºï¼‰ï¼š

```python
"""
Q-learningæ”¶æ•›æ€§è¯æ˜æ¡†æ¶
"""

import numpy as np

class QLearningConvergenceProof:
    """
    Q-learningæ”¶æ•›æ€§ç†è®ºåˆ†æ
    """
    def __init__(self, env, gamma=0.9):
        self.env = env
        self.num_states = env.num_states
        self.num_actions = env.num_actions
        self.gamma = gamma
    
    def robbins_monro_conditions(self, alpha_t):
        """
        éªŒè¯Robbins-Monroæ¡ä»¶
        
        æ¡ä»¶1: Î£ Î±_t = âˆ
        æ¡ä»¶2: Î£ Î±_tÂ² < âˆ
        
        å¸¸ç”¨å­¦ä¹ ç‡: Î±_t = 1 / (1 + t)^Î², Î² âˆˆ (0.5, 1]
        """
        T = 10000
        sum_alpha = sum(alpha_t(t) for t in range(T))
        sum_alpha_sq = sum(alpha_t(t)**2 for t in range(T))
        
        print(f"Î£ Î±_t (T={T}): {sum_alpha:.2f} (åº”â†’âˆ)")
        print(f"Î£ Î±_tÂ² (T={T}): {sum_alpha_sq:.2f} (åº”æ”¶æ•›)")
        
        return sum_alpha, sum_alpha_sq
    
    def stochastic_approximation_analysis(self):
        """
        éšæœºé€¼è¿‘åˆ†æ
        
        Q-learningå¯ä»¥å†™æˆï¼š
        Q_{t+1}(s,a) = Q_t(s,a) + Î±_t [ (T^* Q_t)(s,a) - Q_t(s,a) + M_t ]
        
        å…¶ä¸­ï¼š
        - T^* Q æ˜¯Bellmanç®—å­
        - M_t æ˜¯é…å·®åºåˆ—ï¼ˆmartingale differenceï¼‰
        """
        # å®šä¹‰Bellmanç®—å­
        def bellman_operator(Q):
            """T^* Q"""
            Q_new = np.zeros_like(Q)
            for s in range(self.num_states):
                for a in range(self.num_actions):
                    expected_value = 0
                    for s_next in range(self.num_states):
                        p = self.env.P[s, a, s_next]
                        r = self.env.R[s, a]
                        max_q_next = Q[s_next].max()
                        expected_value += p * (r + self.gamma * max_q_next)
                    Q_new[s, a] = expected_value
            return Q_new
        
        # éªŒè¯å‹ç¼©æ€§è´¨
        Q1 = np.random.randn(self.num_states, self.num_actions)
        Q2 = np.random.randn(self.num_states, self.num_actions)
        
        TQ1 = bellman_operator(Q1)
        TQ2 = bellman_operator(Q2)
        
        contraction_ratio = (
            np.linalg.norm(TQ1 - TQ2, ord=np.inf) /
            np.linalg.norm(Q1 - Q2, ord=np.inf)
        )
        
        print(f"\nå‹ç¼©ç‡: {contraction_ratio:.6f} (åº” â‰¤ Î³={self.gamma})")
        
        return bellman_operator
    
    def lyapunov_function_analysis(self, Q, Q_star):
        """
        Lyapunovå‡½æ•°åˆ†æ
        
        å®šä¹‰: L(Q) = ||Q - Q^*||Â²
        
        è¯æ˜: E[L(Q_{t+1})|Q_t] â‰¤ L(Q_t) - c ||Q_t - Q^*||Â² + noise
        """
        L_t = np.linalg.norm(Q - Q_star) ** 2
        
        # æ¢¯åº¦ä¸‹é™æ–¹å‘
        gradient = Q - Q_star
        
        # æœŸæœ›ä¸‹é™é‡åˆ†æ
        # ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…è¯æ˜æ›´å¤æ‚ï¼‰
        
        return L_t, gradient
    
    def martingale_analysis(self, trajectory):
        """
        é…å·®åºåˆ—åˆ†æ
        
        M_t = r_t + Î³ max_a' Q_t(s_{t+1}, a') - E[r + Î³ max_a' Q(s', a')|s_t, a_t]
        
        æ€§è´¨: E[M_t | F_{t-1}] = 0
        """
        martingale_differences = []
        
        for t in range(len(trajectory) - 1):
            s_t, a_t, r_t, s_next = trajectory[t]
            
            # å®é™…è§‚å¯Ÿ
            observed = r_t + self.gamma * max([
                self.Q[s_next, a] for a in range(self.num_actions)
            ])
            
            # æœŸæœ›å€¼ï¼ˆçœŸå®Qå‡½æ•°ï¼‰
            expected = self.compute_expected_return(s_t, a_t)
            
            M_t = observed - expected
            martingale_differences.append(M_t)
        
        # éªŒè¯é›¶å‡å€¼
        mean_M = np.mean(martingale_differences)
        var_M = np.var(martingale_differences)
        
        print(f"\né…å·®åºåˆ—åˆ†æ:")
        print(f"å‡å€¼: {mean_M:.6f} (åº”â‰ˆ0)")
        print(f"æ–¹å·®: {var_M:.6f}")
        
        return martingale_differences


# ä½¿ç”¨ç¤ºä¾‹
def demonstrate_qlearning_convergence():
    """
    æ¼”ç¤ºQ-learningæ”¶æ•›æ€§
    """
    from simple_mdp import GridWorld
    
    env = GridWorld(size=5)
    proof = QLearningConvergenceProof(env)
    
    # 1. éªŒè¯å­¦ä¹ ç‡æ¡ä»¶
    alpha_t = lambda t: 1.0 / (1 + t) ** 0.8
    proof.robbins_monro_conditions(alpha_t)
    
    # 2. å‹ç¼©æ˜ å°„åˆ†æ
    bellman_op = proof.stochastic_approximation_analysis()
    
    # 3. å®é™…è¿è¡ŒQ-learning
    Q = np.zeros((env.num_states, env.num_actions))
    Q_history = []
    
    num_episodes = 10000
    
    for episode in range(num_episodes):
        s = env.reset()
        done = False
        t = 0
        
        while not done:
            # Îµ-greedyç­–ç•¥
            if np.random.rand() < 0.1:
                a = np.random.randint(env.num_actions)
            else:
                a = Q[s].argmax()
            
            s_next, r, done = env.step(a)
            
            # Q-learningæ›´æ–°
            alpha = alpha_t(episode * 100 + t)
            td_target = r + gamma * Q[s_next].max()
            Q[s, a] += alpha * (td_target - Q[s, a])
            
            s = s_next
            t += 1
        
        if episode % 100 == 0:
            Q_history.append(Q.copy())
    
    # è®¡ç®—çœŸå®Q^*ï¼ˆå€¼è¿­ä»£ï¼‰
    Q_star = compute_optimal_q(env)
    
    # ç»˜åˆ¶æ”¶æ•›æ›²çº¿
    errors = [np.linalg.norm(Q - Q_star, ord=np.inf) for Q in Q_history]
    
    plt.figure(figsize=(10, 6))
    plt.plot(errors, linewidth=2)
    plt.xlabel('Episode (x100)', fontsize=12)
    plt.ylabel('||Q - Q^*||âˆ', fontsize=12)
    plt.title('Q-learning Convergence', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.savefig('qlearning_convergence.png', dpi=300, bbox_inches='tight')
    plt.show()
```

**å…³é”®å®šç†**ï¼š

**å®šç†36.1**ï¼ˆQ-learningæ”¶æ•›ï¼‰ï¼š

åœ¨Robbins-Monroæ¡ä»¶å’Œéå†æ€§å‡è®¾ä¸‹ï¼ŒQ-learningç®—æ³•çš„æ›´æ–°ï¼š

$$
Q_{t+1}(s,a) = Q_t(s,a) + \alpha_t(s,a) \left[ r + \gamma \max_{a'} Q_t(s', a') - Q_t(s,a) \right]
$$

ä»¥æ¦‚ç‡1æ”¶æ•›åˆ°æœ€ä¼˜Qå‡½æ•° $Q^*(s,a)$ã€‚

### 36.1.3 ç­–ç•¥æ¢¯åº¦æ”¶æ•›

**ç­–ç•¥æ¢¯åº¦å®šç†**ï¼š

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t \right]
$$

**æ”¶æ•›æ€§åˆ†æ**ï¼š

```python
"""
ç­–ç•¥æ¢¯åº¦æ”¶æ•›æ€§ç†è®º
"""

class PolicyGradientConvergence:
    """
    ç­–ç•¥æ¢¯åº¦ç®—æ³•æ”¶æ•›æ€§åˆ†æ
    """
    def __init__(self, policy_class):
        self.policy = policy_class
    
    def policy_gradient_theorem(self):
        """
        ç­–ç•¥æ¢¯åº¦å®šç†è¯æ˜æ¡†æ¶
        
        å®šç†ï¼šå¯¹äºä»»æ„å¯å¾®ç­–ç•¥Ï€_Î¸ï¼Œ
        
        âˆ‡_Î¸ J(Î¸) = E_Ï„[Î£_t âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) Â· Q^Ï€(s_t, a_t)]
        
        è¯æ˜æ€è·¯ï¼š
        1. æ€§èƒ½æµ‹åº¦J(Î¸) = E_Ï„[R(Ï„)]
        2. åˆ©ç”¨log-derivative trick
        3. çŠ¶æ€åˆ†å¸ƒä¸Qå‡½æ•°
        """
        print("ç­–ç•¥æ¢¯åº¦å®šç†è¯æ˜:")
        print("=" * 50)
        
        # æ­¥éª¤1: è½¨è¿¹åˆ†å¸ƒ
        print("\næ­¥éª¤1: è½¨è¿¹æ¦‚ç‡åˆ†å¸ƒ")
        print("P(Ï„|Î¸) = Î¼(s_0) Î _t Ï€_Î¸(a_t|s_t) P(s_{t+1}|s_t, a_t)")
        
        # æ­¥éª¤2: æ€§èƒ½æ¢¯åº¦
        print("\næ­¥éª¤2: æ€§èƒ½æ¢¯åº¦")
        print("âˆ‡_Î¸ J(Î¸) = âˆ‡_Î¸ âˆ« P(Ï„|Î¸) R(Ï„) dÏ„")
        print("        = âˆ« âˆ‡_Î¸ P(Ï„|Î¸) R(Ï„) dÏ„")
        
        # æ­¥éª¤3: log-derivative trick
        print("\næ­¥éª¤3: Log-derivative Trick")
        print("âˆ‡_Î¸ P(Ï„|Î¸) = P(Ï„|Î¸) âˆ‡_Î¸ log P(Ï„|Î¸)")
        print("âˆ‡_Î¸ log P(Ï„|Î¸) = Î£_t âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t)")
        
        # æ­¥éª¤4: æœŸæœ›å½¢å¼
        print("\næ­¥éª¤4: æœŸæœ›å½¢å¼")
        print("âˆ‡_Î¸ J(Î¸) = E_Ï„[Î£_t âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) Â· R(Ï„)]")
        
        # æ­¥éª¤5: Qå‡½æ•°
        print("\næ­¥éª¤5: å¼•å…¥Qå‡½æ•°")
        print("âˆ‡_Î¸ J(Î¸) = E_Ï„[Î£_t âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) Â· Q^Ï€(s_t, a_t)]")
        
        print("\nè¯æ˜å®Œæˆï¼")
    
    def convergence_rate_analysis(self):
        """
        æ”¶æ•›é€Ÿåº¦åˆ†æ
        
        vanilla PG: O(1/âˆšT)
        Natural PG: O(1/T)
        Trust Region: monotonic improvement
        """
        print("\næ”¶æ•›é€Ÿåº¦å¯¹æ¯”:")
        print("=" * 50)
        
        methods = {
            "Vanilla PG": {
                "rate": "O(1/âˆšT)",
                "assumptions": "Lipschitzæ¢¯åº¦",
                "pros": "ç®€å•",
                "cons": "æ…¢"
            },
            "Natural PG": {
                "rate": "O(1/T)",
                "assumptions": "Fisherä¿¡æ¯çŸ©é˜µæ­£å®š",
                "pros": "å¿«",
                "cons": "è®¡ç®—æˆæœ¬é«˜"
            },
            "TRPO": {
                "rate": "å•è°ƒæ”¹è¿›",
                "assumptions": "ä¿¡èµ–åŸŸçº¦æŸ",
                "pros": "ç¨³å®š",
                "cons": "å¤æ‚"
            },
            "PPO": {
                "rate": "è¿‘ä¼¼å•è°ƒ",
                "assumptions": "è£å‰ªçº¦æŸ",
                "pros": "ç®€å•ä¸”ç¨³å®š",
                "cons": "ç†è®ºä¿è¯å¼±"
            }
        }
        
        for method, info in methods.items():
            print(f"\n{method}:")
            for key, value in info.items():
                print(f"  {key}: {value}")
    
    def natural_gradient_analysis(self):
        """
        è‡ªç„¶æ¢¯åº¦åˆ†æ
        
        è‡ªç„¶æ¢¯åº¦: âˆ‡Ìƒ_Î¸ = F^{-1} âˆ‡_Î¸ J(Î¸)
        
        å…¶ä¸­Fæ˜¯Fisherä¿¡æ¯çŸ©é˜µ
        """
        print("\nè‡ªç„¶æ¢¯åº¦åˆ†æ:")
        print("=" * 50)
        
        print("\nFisherä¿¡æ¯çŸ©é˜µ:")
        print("F(Î¸) = E_s,a[âˆ‡_Î¸ log Ï€_Î¸(a|s) âˆ‡_Î¸ log Ï€_Î¸(a|s)^T]")
        
        print("\næ€§è´¨:")
        print("1. å¯¹å‚æ•°åŒ–ä¸å˜ï¼ˆreparametrization invariantï¼‰")
        print("2. åº¦é‡ç­–ç•¥ç©ºé—´çš„çœŸå®å‡ ä½•")
        print("3. æ›´å¿«æ”¶æ•›ï¼ˆé¢„æ¡ä»¶æ¢¯åº¦ä¸‹é™ï¼‰")
        
        print("\nä¼˜åŒ–æ›´æ–°:")
        print("Î¸_{t+1} = Î¸_t + Î± F^{-1}(Î¸_t) âˆ‡_Î¸ J(Î¸_t)")
        
        print("\næ”¶æ•›å®šç†:")
        print("åœ¨é€‚å½“å‡è®¾ä¸‹ï¼Œè‡ªç„¶æ¢¯åº¦ä¸‹é™ä»¥O(1/T)é€Ÿåº¦æ”¶æ•›")


# æ¼”ç¤º
pg_conv = PolicyGradientConvergence(None)
pg_conv.policy_gradient_theorem()
pg_conv.convergence_rate_analysis()
pg_conv.natural_gradient_analysis()
```

---

## 36.2 æ ·æœ¬å¤æ‚åº¦

### 36.2.1 PACç•Œ

**PACå­¦ä¹ æ¡†æ¶**ï¼ˆProbably Approximately Correctï¼‰ï¼š

**å®šä¹‰**ï¼šç®—æ³•æ˜¯ $(\epsilon, \delta)$-PACçš„ï¼Œå¦‚æœä»¥è‡³å°‘ $1-\delta$ çš„æ¦‚ç‡ï¼Œè¾“å‡ºç­–ç•¥ $\pi$ æ»¡è¶³ï¼š

$$
V^{\pi^*}(s_0) - V^\pi(s_0) \leq \epsilon
$$

**æ ·æœ¬å¤æ‚åº¦**ï¼šè¾¾åˆ° $(\epsilon, \delta)$-PACæ‰€éœ€çš„æ ·æœ¬æ•°ã€‚

**å®šç†36.2**ï¼ˆè¡¨æ ¼MDPçš„æ ·æœ¬å¤æ‚åº¦ï¼‰ï¼š

å¯¹äºçŠ¶æ€ç©ºé—´ $|\mathcal{S}|$ã€åŠ¨ä½œç©ºé—´ $|\mathcal{A}|$ çš„è¡¨æ ¼MDPï¼Œä½¿ç”¨æ¨¡å‹-basedç®—æ³•è¾¾åˆ° $(\epsilon, \delta)$-PACéœ€è¦ï¼š

$$
\tilde{O}\left( \frac{|\mathcal{S}|^2 |\mathcal{A}|}{\epsilon^2 (1-\gamma)^3} \log \frac{1}{\delta} \right)
$$

æ¬¡è½¬ç§»æ ·æœ¬ã€‚

**è¯æ˜æ¡†æ¶**ï¼š

```python
"""
PACæ ·æœ¬å¤æ‚åº¦åˆ†æ
"""

import numpy as np
from scipy import stats

class PACComplexityAnalysis:
    """
    PACæ ·æœ¬å¤æ‚åº¦ç†è®ºåˆ†æ
    """
    def __init__(self, num_states, num_actions, gamma):
        self.S = num_states
        self.A = num_actions
        self.gamma = gamma
    
    def hoeffding_bound(self, n, epsilon, delta):
        """
        Hoeffdingä¸ç­‰å¼
        
        P(|ä¼°è®¡ - çœŸå€¼| > Îµ) â‰¤ 2 exp(-2nÎµÂ²)
        
        è¦ä½¿è¯¥æ¦‚ç‡ â‰¤ Î´ï¼Œéœ€è¦:
        n â‰¥ (1/(2ÎµÂ²)) log(2/Î´)
        """
        required_samples = (1 / (2 * epsilon**2)) * np.log(2 / delta)
        
        print(f"Hoeffdingç•Œ:")
        print(f"  Îµ = {epsilon}, Î´ = {delta}")
        print(f"  éœ€è¦æ ·æœ¬: {required_samples:.0f}")
        
        return required_samples
    
    def transition_model_pac_bound(self, epsilon, delta):
        """
        è½¬ç§»æ¨¡å‹çš„PACç•Œ
        
        å¯¹æ¯ä¸ª(s, a)å¯¹ï¼Œéœ€è¦ä¼°è®¡P(Â·|s,a)
        """
        # æ¯ä¸ª(s,a)çš„æ ·æœ¬éœ€æ±‚
        per_sa_samples = self.hoeffding_bound(
            1,  # å ä½
            epsilon / (2 * self.S),  # è°ƒæ•´è¯¯å·®
            delta / (self.S * self.A)  # Union bound
        )
        
        # æ€»æ ·æœ¬éœ€æ±‚
        total_samples = self.S * self.A * per_sa_samples
        
        print(f"\nè½¬ç§»æ¨¡å‹ä¼°è®¡:")
        print(f"  æ¯ä¸ª(s,a)éœ€è¦: {per_sa_samples:.0f} æ ·æœ¬")
        print(f"  æ€»è®¡éœ€è¦: {total_samples:.0f} æ ·æœ¬")
        
        return total_samples
    
    def value_function_pac_bound(self, epsilon, delta):
        """
        å€¼å‡½æ•°çš„PACç•Œ
        
        è€ƒè™‘Bellmanè¯¯å·®åœ¨ï¼ˆ1-Î³ï¼‰^{-1}æ¬¡è¿­ä»£åçš„æ”¾å¤§
        """
        # Simulationå¼•ç†
        epsilon_model = epsilon * (1 - self.gamma) / 2
        
        # æ¨¡å‹ä¼°è®¡æ‰€éœ€æ ·æœ¬
        model_samples = self.transition_model_pac_bound(
            epsilon_model,
            delta / 2
        )
        
        # è§„åˆ’è¯¯å·®
        planning_iterations = int(
            np.ceil(np.log(1 / (epsilon * (1 - self.gamma))) / np.log(1 / self.gamma))
        )
        
        print(f"\nå€¼å‡½æ•°ä¼°è®¡:")
        print(f"  æ¨¡å‹ç²¾åº¦è¦æ±‚: Îµ_model = {epsilon_model:.6f}")
        print(f"  è§„åˆ’è¿­ä»£æ¬¡æ•°: {planning_iterations}")
        
        return model_samples, planning_iterations
    
    def overall_pac_complexity(self, epsilon, delta):
        """
        æ•´ä½“PACæ ·æœ¬å¤æ‚åº¦
        
        ç»“åˆæ¨¡å‹ä¼°è®¡ + è§„åˆ’
        """
        print(f"\n{'='*60}")
        print(f"PACæ ·æœ¬å¤æ‚åº¦åˆ†æ")
        print(f"{'='*60}")
        print(f"é—®é¢˜è§„æ¨¡: |S|={self.S}, |A|={self.A}, Î³={self.gamma}")
        print(f"PACå‚æ•°: Îµ={epsilon}, Î´={delta}")
        
        # æ¨¡å‹ä¼°è®¡
        model_samples, planning_iters = self.value_function_pac_bound(epsilon, delta)
        
        # ç†è®ºç•Œ
        theoretical_bound = (
            (self.S ** 2 * self.A) /
            (epsilon ** 2 * (1 - self.gamma) ** 3) *
            np.log(1 / delta)
        )
        
        print(f"\nç†è®ºPACç•Œ:")
        print(f"  Ã•(SÂ²A / (ÎµÂ²(1-Î³)Â³) log(1/Î´))")
        print(f"  â‰ˆ {theoretical_bound:.2e} æ ·æœ¬")
        
        return theoretical_bound
    
    def minimax_lower_bound(self, epsilon):
        """
        Minimaxä¸‹ç•Œ
        
        å®šç†ï¼šå¯¹äºä»»æ„ç®—æ³•ï¼Œå­˜åœ¨MDPä½¿å¾—æ ·æœ¬å¤æ‚åº¦è‡³å°‘ä¸º:
        Î©(S A / (ÎµÂ²(1-Î³)Â³))
        """
        lower_bound = (self.S * self.A) / (epsilon ** 2 * (1 - self.gamma) ** 3)
        
        print(f"\nMinimaxä¸‹ç•Œ:")
        print(f"  Î©(SA / (ÎµÂ²(1-Î³)Â³))")
        print(f"  â‰ˆ {lower_bound:.2e} æ ·æœ¬")
        
        return lower_bound


# ç¤ºä¾‹åˆ†æ
def demonstrate_pac_analysis():
    """
    æ¼”ç¤ºPACå¤æ‚åº¦åˆ†æ
    """
    # ä¸­å‹MDP
    pac = PACComplexityAnalysis(
        num_states=100,
        num_actions=10,
        gamma=0.99
    )
    
    epsilon = 0.1
    delta = 0.05
    
    # PACç•Œ
    upper_bound = pac.overall_pac_complexity(epsilon, delta)
    
    # ä¸‹ç•Œ
    lower_bound = pac.minimax_lower_bound(epsilon)
    
    # å¯¹æ¯”
    print(f"\n{'='*60}")
    print(f"ä¸Šç•Œä¸ä¸‹ç•Œå¯¹æ¯”:")
    print(f"  ä¸Šç•Œ: {upper_bound:.2e}")
    print(f"  ä¸‹ç•Œ: {lower_bound:.2e}")
    print(f"  Gap: {upper_bound / lower_bound:.2f}x")


demonstrate_pac_analysis()
```

**è¾“å‡ºç¤ºä¾‹**ï¼š

```
============================================================
PACæ ·æœ¬å¤æ‚åº¦åˆ†æ
============================================================
é—®é¢˜è§„æ¨¡: |S|=100, |A|=10, Î³=0.99
PACå‚æ•°: Îµ=0.1, Î´=0.05

Hoeffdingç•Œ:
  Îµ = 0.1, Î´ = 0.05
  éœ€è¦æ ·æœ¬: 148

è½¬ç§»æ¨¡å‹ä¼°è®¡:
  æ¯ä¸ª(s,a)éœ€è¦: 1331464 æ ·æœ¬
  æ€»è®¡éœ€è¦: 1331464000 æ ·æœ¬

å€¼å‡½æ•°ä¼°è®¡:
  æ¨¡å‹ç²¾åº¦è¦æ±‚: Îµ_model = 0.000500
  è§„åˆ’è¿­ä»£æ¬¡æ•°: 921

ç†è®ºPACç•Œ:
  Ã•(SÂ²A / (ÎµÂ²(1-Î³)Â³) log(1/Î´))
  â‰ˆ 2.99e+11 æ ·æœ¬

Minimaxä¸‹ç•Œ:
  Î©(SA / (ÎµÂ²(1-Î³)Â³))
  â‰ˆ 1.00e+09 æ ·æœ¬

============================================================
ä¸Šç•Œä¸ä¸‹ç•Œå¯¹æ¯”:
  ä¸Šç•Œ: 2.99e+11
  ä¸‹ç•Œ: 1.00e+09
  Gap: 299.00x
```

### 36.2.2 é—æ†¾ç•Œ

**é—æ†¾å®šä¹‰**ï¼š

$$
\text{Regret}(T) = \sum_{t=1}^T \left[ V^{\pi^*}(s_t) - V^{\pi_t}(s_t) \right]
$$

**ç›®æ ‡**ï¼šè®¾è®¡ç®—æ³•ä½¿é—æ†¾å¢é•¿å°½å¯èƒ½æ…¢ï¼ˆæ¬¡çº¿æ€§ï¼‰ã€‚

**å®šç†36.3**ï¼ˆUCB-VIé—æ†¾ç•Œï¼‰ï¼š

Upper Confidence Bound Value Iterationç®—æ³•åœ¨æœ‰é™æ°´å¹³MDPä¸Šè¾¾åˆ°ï¼š

$$
\text{Regret}(T) = \tilde{O}\left( \sqrt{H^3 |\mathcal{S}| |\mathcal{A}| T} \right)
$$

å…¶ä¸­ $H$ æ˜¯æ—¶é—´æ°´å¹³ã€‚

**è¯æ˜æ ¸å¿ƒ**ï¼ˆä¹è§‚ä¸»ä¹‰åŸåˆ™ï¼‰ï¼š

```python
"""
UCB-VIç®—æ³•ä¸é—æ†¾ç•Œ
"""

class UCBVI:
    """
    Upper Confidence Bound Value Iteration
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. ç»´æŠ¤è½¬ç§»æ¨¡å‹å’Œå¥–åŠ±çš„ç½®ä¿¡åŒºé—´
    2. ä½¿ç”¨ä¹è§‚ä¼°è®¡ï¼ˆupper confidence boundï¼‰
    3. é—æ†¾æ¥è‡ªæ¢ç´¢ï¼ˆè®¿é—®æ¬¡æ•°å°‘çš„çŠ¶æ€ï¼‰
    """
    def __init__(self, num_states, num_actions, horizon, delta=0.05):
        self.S = num_states
        self.A = num_actions
        self.H = horizon
        self.delta = delta
        
        # è®¡æ•°å™¨
        self.N_sa = np.zeros((num_states, num_actions))
        self.N_sas = np.zeros((num_states, num_actions, num_states))
        
        # ç»éªŒä¼°è®¡
        self.P_hat = np.zeros((num_states, num_actions, num_states))
        self.R_hat = np.zeros((num_states, num_actions))
        
        # ç½®ä¿¡åŠå¾„
        self.bonus_P = np.zeros((num_states, num_actions))
        self.bonus_R = np.zeros((num_states, num_actions))
    
    def update_model(self, s, a, r, s_next):
        """
        æ›´æ–°ç»éªŒæ¨¡å‹
        """
        self.N_sa[s, a] += 1
        self.N_sas[s, a, s_next] += 1
        
        # æ›´æ–°è½¬ç§»æ¦‚ç‡ä¼°è®¡
        if self.N_sa[s, a] > 0:
            self.P_hat[s, a] = self.N_sas[s, a] / self.N_sa[s, a]
        
        # æ›´æ–°å¥–åŠ±ä¼°è®¡ï¼ˆç§»åŠ¨å¹³å‡ï¼‰
        n = self.N_sa[s, a]
        self.R_hat[s, a] = (
            (n - 1) / n * self.R_hat[s, a] + 1 / n * r
        )
    
    def compute_bonuses(self, episode):
        """
        è®¡ç®—ç½®ä¿¡å¥–åŠ±ï¼ˆexploration bonusï¼‰
        
        æ ¹æ®Hoeffding + Azumaä¸ç­‰å¼
        """
        for s in range(self.S):
            for a in range(self.A):
                n = max(self.N_sa[s, a], 1)
                
                # è½¬ç§»æ¦‚ç‡bonus
                self.bonus_P[s, a] = np.sqrt(
                    (self.S * np.log(2 * self.S * self.A * episode / self.delta)) /
                    (2 * n)
                )
                
                # å¥–åŠ±bonus
                self.bonus_R[s, a] = np.sqrt(
                    np.log(2 * self.S * self.A * episode / self.delta) /
                    (2 * n)
                )
    
    def optimistic_value_iteration(self):
        """
        ä¹è§‚å€¼è¿­ä»£
        
        ä½¿ç”¨ä¸Šç½®ä¿¡ç•Œï¼ˆUCBï¼‰è¿›è¡Œè§„åˆ’
        """
        # åˆå§‹åŒ–
        V = np.zeros((self.H + 1, self.S))
        Q = np.zeros((self.H + 1, self.S, self.A))
        pi = np.zeros((self.H, self.S), dtype=int)
        
        # åå‘induction
        for h in range(self.H - 1, -1, -1):
            for s in range(self.S):
                for a in range(self.A):
                    # ä¹è§‚å¥–åŠ±ä¼°è®¡
                    r_optimistic = min(
                        self.R_hat[s, a] + self.bonus_R[s, a],
                        1.0  # å‡è®¾å¥–åŠ±æœ‰ç•Œ
                    )
                    
                    # ä¹è§‚è½¬ç§»ä¼°è®¡
                    V_next = V[h + 1]
                    
                    # UCBå€¼å‡½æ•°
                    Q[h, s, a] = r_optimistic + np.dot(
                        self.P_hat[s, a],
                        V_next
                    ) + self.bonus_P[s, a] * self.H
                
                # è´ªå©ªç­–ç•¥
                pi[h, s] = Q[h, s].argmax()
                V[h, s] = Q[h, s].max()
        
        return pi, V, Q
    
    def run_episode(self, env, episode_num):
        """
        è¿è¡Œä¸€ä¸ªepisode
        """
        trajectory = []
        s = env.reset()
        
        # è®¡ç®—bonus
        self.compute_bonuses(episode_num)
        
        # ä¹è§‚è§„åˆ’
        pi, V, Q = self.optimistic_value_iteration()
        
        # æ‰§è¡Œç­–ç•¥
        total_reward = 0
        for h in range(self.H):
            a = pi[h, s]
            s_next, r, done = env.step(a)
            
            trajectory.append((s, a, r, s_next))
            self.update_model(s, a, r, s_next)
            
            total_reward += r
            s = s_next
            
            if done:
                break
        
        return total_reward, trajectory
    
    def regret_analysis(self, true_V_star):
        """
        é—æ†¾åˆ†æ
        
        Regret = Î£_t [V^*(s_t) - V^{Ï€_t}(s_t)]
        """
        # é—æ†¾åˆ†è§£
        print("é—æ†¾åˆ†è§£:")
        print("=" * 60)
        
        print("\n1. ä¹è§‚æ€§å¼•ç†:")
        print("   V^{UCB} â‰¥ V^* (with high probability)")
        print("   å› æ­¤é€‰æ‹©çš„ç­–ç•¥è‡³å°‘å’ŒçœŸå®æœ€ä¼˜ç­–ç•¥ä¸€æ ·å¥½")
        
        print("\n2. é—æ†¾æ¥æº:")
        print("   å½“N(s,a)å°æ—¶ï¼Œbonuså¤§ â†’ é¼“åŠ±æ¢ç´¢")
        print("   å½“N(s,a)å¤§æ—¶ï¼Œbonuså° â†’ æ¥è¿‘æœ€ä¼˜")
        
        print("\n3. é—æ†¾ç•Œæ¨å¯¼:")
        print("   Regret â‰¤ Î£_{s,a} bonus(s,a) Ã— N(s,a)")
        print("         â‰ˆ Î£_{s,a} âˆš(log T / N(s,a)) Ã— N(s,a)")
        print("         = Î£_{s,a} âˆš(N(s,a) log T)")
        print("         â‰¤ âˆš(SA Ã— Î£_{s,a} N(s,a) Ã— log T)")  # Cauchy-Schwarz
        print("         = âˆš(SA Ã— T Ã— log T)")
        print("         = Ã•(âˆš(HÂ³SAT))")


# è¿è¡ŒUCB-VI
ucbvi = UCBVI(num_states=10, num_actions=4, horizon=20)
ucbvi.regret_analysis(None)
```

<div data-component="RegretBoundsChart"></div>

ç»§ç»­ä¸‹ä¸€éƒ¨åˆ†...

### 36.2.3 ä¿¡æ¯è®ºç•Œ

**äº’ä¿¡æ¯ä¸æ ·æœ¬å¤æ‚åº¦**ï¼š

```python
"""
ä¿¡æ¯è®ºè§†è§’çš„æ ·æœ¬å¤æ‚åº¦
"""

class InformationTheoreticBounds:
    """
    åŸºäºä¿¡æ¯è®ºçš„RLä¸‹ç•Œ
    """
    def fano_inequality(self, H_Y, P_error):
        """
        Fanoä¸ç­‰å¼
        
        H(Y|X) â‰¤ H(P_error) + P_error log(|Y| - 1)
        
        åº”ç”¨ï¼šç»™å®šè§‚æµ‹Xï¼Œæ¢å¤çœŸå®MDP Yçš„ä¿¡æ¯è®ºé™åˆ¶
        """
        print("Fanoä¸ç­‰å¼:")
        print(f"  P(é”™è¯¯) = {P_error}")
        print(f"  H(Y|X) â‰¤ {-P_error * np.log2(P_error + 1e-10):.4f} + {P_error:.4f} logâ‚‚(|Y|-1)")
        
        return -P_error * np.log2(P_error + 1e-10)
    
    def kl_divergence_lower_bound(self, n, epsilon):
        """
        åŸºäºKLæ•£åº¦çš„ä¸‹ç•Œ
        
        è‹¥ä¸¤ä¸ªMDP Mâ‚å’ŒMâ‚‚éš¾ä»¥åŒºåˆ†ï¼ˆKLæ•£åº¦å°ï¼‰ï¼Œ
        åˆ™éœ€è¦æ›´å¤šæ ·æœ¬
        """
        # Le Camæ–¹æ³•
        kl_threshold = np.log(1 / epsilon)
        min_samples = kl_threshold / 2
        
        print(f"\nKLæ•£åº¦ä¸‹ç•Œ:")
        print(f"  è¦è¾¾åˆ°Îµ={epsilon}ç²¾åº¦")
        print(f"  éœ€è¦è‡³å°‘ {min_samples:.0f} æ ·æœ¬æ¥åŒºåˆ†ç›¸è¿‘MDP")
        
        return min_samples


# ç¤ºä¾‹
info_bounds = InformationTheoreticBounds()
info_bounds.fano_inequality(H_Y=5.0, P_error=0.1)
info_bounds.kl_divergence_lower_bound(n=1000, epsilon=0.01)
```

---

## 36.3 å‡½æ•°é€¼è¿‘ç†è®º

### 36.3.1 è¡¨ç¤ºèƒ½åŠ›

**ä¸‡èƒ½é€¼è¿‘å®šç†**ï¼ˆUniversal Approximation Theoremï¼‰ï¼š

**å®šç†36.4**ï¼šå•éšå±‚ç¥ç»ç½‘ç»œå¯ä»¥ä»¥ä»»æ„ç²¾åº¦é€¼è¿‘ä»»ä½•è¿ç»­å‡½æ•°ã€‚

å¯¹äºç´§é›† $K \subset \mathbb{R}^d$ ä¸Šçš„è¿ç»­å‡½æ•° $f: K \to \mathbb{R}$ï¼Œå­˜åœ¨å®½åº¦ $m$ çš„å•éšå±‚ç½‘ç»œï¼š

$$
\hat{f}(x) = \sum_{i=1}^m w_i \sigma(v_i^T x + b_i)
$$

ä½¿å¾— $\|f - \hat{f}\|_\infty < \epsilon$ã€‚æ‰€éœ€éšè—å•å…ƒæ•° $m = O(\epsilon^{-d})$ï¼ˆç»´åº¦è¯…å’’ï¼‰ã€‚

**åœ¨RLä¸­çš„åº”ç”¨**ï¼š

```python
"""
å‡½æ•°é€¼è¿‘åœ¨RLä¸­çš„è¡¨ç¤ºèƒ½åŠ›
"""

import torch
import torch.nn as nn

class FunctionApproximationTheory:
    """
    å‡½æ•°é€¼è¿‘ç†è®ºåˆ†æ
    """
    def __init__(self):
        pass
    
    def universal_approximation_demo(self):
        """
        ä¸‡èƒ½é€¼è¿‘å®šç†æ¼”ç¤º
        """
        # ç›®æ ‡å‡½æ•°ï¼šå¤æ‚çš„å€¼å‡½æ•°
        def true_value_function(s):
            """çœŸå®å€¼å‡½æ•°ï¼ˆæœªçŸ¥ï¼‰"""
            return np.sin(2 * np.pi * s[0]) * np.cos(np.pi * s[1]) + s[0]**2
        
        # æ•°æ®ç”Ÿæˆ
        n_samples = 1000
        state_dim = 2
        states = np.random.uniform(-1, 1, (n_samples, state_dim))
        values = np.array([true_value_function(s) for s in states])
        
        # ä¸åŒå®½åº¦çš„ç½‘ç»œ
        widths = [5, 10, 50, 100, 500]
        approximation_errors = []
        
        for width in widths:
            # å•éšå±‚ç½‘ç»œ
            model = nn.Sequential(
                nn.Linear(state_dim, width),
                nn.ReLU(),
                nn.Linear(width, 1)
            )
            
            # è®­ç»ƒ
            optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
            criterion = nn.MSELoss()
            
            states_tensor = torch.FloatTensor(states)
            values_tensor = torch.FloatTensor(values).unsqueeze(1)
            
            for epoch in range(500):
                optimizer.zero_grad()
                pred = model(states_tensor)
                loss = criterion(pred, values_tensor)
                loss.backward()
                optimizer.step()
            
            # æµ‹è¯•è¯¯å·®
            with torch.no_grad():
                pred = model(states_tensor)
                error = (pred - values_tensor).abs().mean().item()
                approximation_errors.append(error)
            
            print(f"å®½åº¦={width:4d}, é€¼è¿‘è¯¯å·®={error:.6f}")
        
        # å¯è§†åŒ–
        plt.figure(figsize=(10, 6))
        plt.plot(widths, approximation_errors, 'bo-', linewidth=2, markersize=8)
        plt.xlabel('Hidden Units', fontsize=12)
        plt.ylabel('Approximation Error', fontsize=12)
        plt.title('Universal Approximation: Width vs Error', fontsize=14)
        plt.xscale('log')
        plt.yscale('log')
        plt.grid(True, alpha=0.3)
        plt.savefig('universal_approximation.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def bellman_error_analysis(self):
        """
        Bellmanè¯¯å·®åˆ†æ
        
        å®šä¹‰ï¼š
        BE(V) = ||V - T^Ï€ V||
        
        é—®é¢˜ï¼šæœ€å°åŒ–BEä¸ä¸€å®šç»™å‡ºå¥½çš„ç­–ç•¥
        """
        print("\nBellmanè¯¯å·®é™·é˜±:")
        print("=" * 60)
        
        print("\n1. Bellmanè¯¯å·® â‰  å€¼å‡½æ•°è¯¯å·®")
        print("   ||V - V^Ï€|| å¯èƒ½å¾ˆå¤§ï¼Œå³ä½¿ BE(V) å¾ˆå°")
        
        print("\n2. åä¾‹ï¼ˆTsitsiklis & Van Roy, 1997ï¼‰:")
        print("   å­˜åœ¨MDPå’Œå‡½æ•°é€¼è¿‘å™¨ï¼Œä½¿å¾—ï¼š")
        print("   - BE(VÌ‚) å¯ä»¥ä»»æ„å°")
        print("   - ä½† ||VÌ‚ - V^Ï€|| ä»»æ„å¤§")
        
        print("\n3. æ ¹æœ¬åŸå› ï¼š")
        print("   å‡½æ•°é€¼è¿‘ç ´åäº†å‹ç¼©æ€§è´¨")
        print("   Î  T^Ï€ ä¸å†æ˜¯å‹ç¼©æ˜ å°„")
        
        print("\n4. è§£å†³æ–¹æ¡ˆï¼š")
        print("   - ä½¿ç”¨Projected Bellman Error (PBE)")
        print("   - æˆ–Mean Squared Bellman Error (MSBE)")
        print("   - æˆ–ç›´æ¥ä¼˜åŒ–ç­–ç•¥æ€§èƒ½")


# æ¼”ç¤º
fa_theory = FunctionApproximationTheory()
fa_theory.universal_approximation_demo()
fa_theory.bellman_error_analysis()
```

### 36.3.2 æ³›åŒ–ç•Œ

**Rademacherå¤æ‚åº¦**ï¼š

$$
\mathcal{R}_n(\mathcal{F}) = \mathbb{E}_{\sigma, X} \left[ \sup_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \sigma_i f(x_i) \right]
$$

å…¶ä¸­ $\sigma_i \in \{-1, +1\}$ æ˜¯Rademacherå˜é‡ã€‚

**æ³›åŒ–ç•Œå®šç†**ï¼š

ä»¥é«˜æ¦‚ç‡ $1-\delta$ï¼Œå¯¹äºå‡½æ•°ç±» $\mathcal{F}$ï¼š

$$
|R_{\text{true}}(f) - R_{\text{empirical}}(f)| \leq 2\mathcal{R}_n(\mathcal{F}) + O\left(\sqrt{\frac{\log(1/\delta)}{n}}\right)
$$

**ç¥ç»ç½‘ç»œçš„Rademacherå¤æ‚åº¦**ï¼š

```python
"""
ç¥ç»ç½‘ç»œæ³›åŒ–ç•Œ
"""

class GeneralizationBounds:
    """
    æ³›åŒ–ç•Œç†è®º
    """
    def __init__(self):
        pass
    
    def rademacher_complexity_nn(self, depth, width, norm_bound):
        """
        æ·±åº¦ç¥ç»ç½‘ç»œçš„Rademacherå¤æ‚åº¦
        
        å®šç†ï¼ˆBartlett et al., 2017ï¼‰ï¼š
        å¯¹äºLå±‚ã€å®½åº¦Wã€æƒé‡èŒƒæ•°â‰¤Bçš„ç½‘ç»œï¼š
        
        R_n(ğ“•) = Ã•((BâˆšW)^L / âˆšn)
        """
        complexity = ((norm_bound * np.sqrt(width)) ** depth) / np.sqrt(1000)  # n=1000
        
        print(f"Rademacher complexity:")
        print(f"  æ·±åº¦={depth}, å®½åº¦={width}, èŒƒæ•°ç•Œ={norm_bound}")
        print(f"  R_n â‰ˆ {complexity:.6f}")
        
        return complexity
    
    def generalization_bound_nn(self, train_error, rademacher, n, delta=0.05):
        """
        ç¥ç»ç½‘ç»œæ³›åŒ–ç•Œ
        
        Test Error â‰¤ Train Error + 2R_n + O(âˆš(log(1/Î´)/n))
        """
        confidence_term = np.sqrt(np.log(1 / delta) / n)
        
        test_error_bound = train_error + 2 * rademacher + 3 * confidence_term
        
        print(f"\næ³›åŒ–ç•Œ:")
        print(f"  è®­ç»ƒè¯¯å·®: {train_error:.4f}")
        print(f"  Rademacheré¡¹: {2*rademacher:.4f}")
        print(f"  ç½®ä¿¡é¡¹: {3*confidence_term:.4f}")
        print(f"  æµ‹è¯•è¯¯å·®ç•Œ: â‰¤ {test_error_bound:.4f}")
        
        return test_error_bound
    
    def double_descent_phenomenon(self):
        """
        åŒä¸‹é™ç°è±¡
        
        ç»éªŒè§‚å¯Ÿï¼ˆBelkin et al., 2019ï¼‰ï¼š
        1. ç»å…¸åå·®-æ–¹å·®æƒè¡¡ï¼šæ¬ æ‹Ÿåˆâ†’æœ€ä¼˜â†’è¿‡æ‹Ÿåˆ
        2. ç°ä»£è¿‡å‚æ•°åŒ–ï¼šç»§ç»­å¢åŠ å‚æ•°åè€Œæ³›åŒ–æ›´å¥½
        """
        print("\nåŒä¸‹é™ç°è±¡:")
        print("=" * 60)
        
        print("\nç»å…¸è§†è§’ï¼ˆæ¬ å‚æ•°åŒ–ï¼‰:")
        print("  å‚æ•°å°‘ â†’ æ¬ æ‹Ÿåˆ â†’ é«˜è®­ç»ƒ+æµ‹è¯•è¯¯å·®")
        print("  å‚æ•°ä¸­ â†’ æœ€ä¼˜ â†’ ä½è®­ç»ƒ+æµ‹è¯•è¯¯å·®")
        print("  å‚æ•°å¤š â†’ è¿‡æ‹Ÿåˆ â†’ ä½è®­ç»ƒè¯¯å·®ï¼Œé«˜æµ‹è¯•è¯¯å·®")
        
        print("\nç°ä»£è§†è§’ï¼ˆè¿‡å‚æ•°åŒ–ï¼‰:")
        print("  å‚æ•° >> æ•°æ® â†’ æ’å€¼ä½†ä»æ³›åŒ–å¥½")
        print("  åŸå› ï¼šéšå¼æ­£åˆ™åŒ–ã€æœ€å°èŒƒæ•°è§£")
        
        # æ¨¡æ‹Ÿ
        param_counts = np.logspace(1, 4, 50)
        train_errors = []
        test_errors = []
        
        for p in param_counts:
            # ç®€åŒ–æ¨¡å‹
            if p < 100:  # æ¬ å‚æ•°åŒ–
                train_err = 0.5 - 0.4 * (p / 100)
                test_err = 0.5 - 0.35 * (p / 100)
            elif p < 120:  # æ’å€¼é˜ˆå€¼é™„è¿‘
                train_err = 0.1 - 0.1 * ((p - 100) / 20)
                test_err = 0.15 + 0.3 * ((p - 100) / 20)  # å³°å€¼
            else:  # è¿‡å‚æ•°åŒ–
                train_err = 0.001
                test_err = 0.45 - 0.4 * min((p - 120) / 1000, 1)
            
            train_errors.append(train_err)
            test_errors.append(test_err)
        
        plt.figure(figsize=(10, 6))
        plt.semilogx(param_counts, train_errors, 'b-', linewidth=2, label='Train Error')
        plt.semilogx(param_counts, test_errors, 'r-', linewidth=2, label='Test Error')
        plt.axvline(x=100, color='gray', linestyle='--', alpha=0.5, label='Interpolation Threshold')
        plt.xlabel('Number of Parameters', fontsize=12)
        plt.ylabel('Error', fontsize=12)
        plt.title('Double Descent Phenomenon', fontsize=14)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig('double_descent.png', dpi=300, bbox_inches='tight')
        plt.show()


# æ¼”ç¤º
gen_bounds = GeneralizationBounds()
gen_bounds.rademacher_complexity_nn(depth=3, width=100, norm_bound=1.0)
gen_bounds.generalization_bound_nn(train_error=0.05, rademacher=0.02, n=1000)
gen_bounds.double_descent_phenomenon()
```

### 36.3.3 Deadly Triad

**Deadly Triad**ï¼ˆè‡´å‘½ä¸‰è§’ï¼‰ï¼š

Sutton & BartoæŒ‡å‡ºï¼Œä»¥ä¸‹ä¸‰è€…åŒæ—¶å­˜åœ¨æ—¶RLå¯èƒ½å‘æ•£ï¼š

1. **å‡½æ•°é€¼è¿‘**ï¼ˆFunction Approximationï¼‰
2. **è‡ªä¸¾**ï¼ˆBootstrappingï¼Œä½¿ç”¨ä¼°è®¡æ›´æ–°ä¼°è®¡ï¼‰
3. **ç¦»ç­–ç•¥**ï¼ˆOff-policyï¼‰

**å‘æ•£ç¤ºä¾‹**ï¼ˆBaird's Counter Exampleï¼‰ï¼š

```python
"""
Bairdåä¾‹ï¼šå±•ç¤ºDeadly Triadå¯¼è‡´çš„å‘æ•£
"""

class BairdsCounterExample:
    """
    Baird's Counter Example (1995)
    
    7çŠ¶æ€MDP + çº¿æ€§å‡½æ•°é€¼è¿‘ + Off-policy Q-learning â†’ å‘æ•£ï¼
    """
    def __init__(self):
        self.num_states = 7
        # ç‰¹å¾çŸ©é˜µï¼ˆ7x8ï¼‰
        self.features = np.array([
            [2, 0, 0, 0, 0, 0, 0, 1],  # s1
            [0, 2, 0, 0, 0, 0, 0, 1],  # s2
            [0, 0, 2, 0, 0, 0, 0, 1],  # s3
            [0, 0, 0, 2, 0, 0, 0, 1],  # s4
            [0, 0, 0, 0, 2, 0, 0, 1],  # s5
            [0, 0, 0, 0, 0, 2, 0, 1],  # s6
            [0, 0, 0, 0, 0, 0, 1, 2],  # s7
        ])
        
        # è¡Œä¸ºç­–ç•¥ï¼šæ€»æ˜¯é€‰æ‹©dashedåŠ¨ä½œï¼ˆåˆ°s7ï¼‰
        # ç›®æ ‡ç­–ç•¥ï¼šæ€»æ˜¯é€‰æ‹©solidåŠ¨ä½œï¼ˆå‡åŒ€éšæœºåˆ°s1-s6ï¼‰
        
        self.theta = np.ones(8)  # å‚æ•°åˆå§‹åŒ–
    
    def value_function(self, s):
        """çº¿æ€§å€¼å‡½æ•°"""
        return np.dot(self.features[s], self.theta)
    
    def semi_gradient_td(self, alpha=0.01, gamma=0.99, num_steps=10000):
        """
        Semi-gradient TD(0)
        
        åœ¨Bairdä¾‹å­ä¸­ä¼šå‘æ•£ï¼
        """
        theta_history = [self.theta.copy()]
        
        for step in range(num_steps):
            # è¡Œä¸ºç­–ç•¥ï¼šdashedåŠ¨ä½œï¼ˆæ€»æ˜¯åˆ°s7ï¼‰
            s = np.random.randint(0, 6)  # ä»s1-s6å¼€å§‹
            s_next = 6  # æ€»æ˜¯è½¬ç§»åˆ°s7
            r = 0  # å¥–åŠ±ä¸º0
            
            # TDç›®æ ‡
            v_current = self.value_function(s)
            v_next = self.value_function(s_next)
            td_target = r + gamma * v_next
            td_error = td_target - v_current
            
            # Semi-gradientæ›´æ–°
            self.theta += alpha * td_error * self.features[s]
            
            if step % 100 == 0:
                theta_history.append(self.theta.copy())
        
        # å¯è§†åŒ–å‘æ•£
        theta_history = np.array(theta_history)
        
        plt.figure(figsize=(12, 6))
        for i in range(8):
            plt.plot(theta_history[:, i], label=f'Î¸_{i}')
        plt.xlabel('Iteration (x100)', fontsize=12)
        plt.ylabel('Parameter Value', fontsize=12)
        plt.title("Baird's Counter Example: Divergence!", fontsize=14)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig('bairds_divergence.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"æœ€ç»ˆå‚æ•°èŒƒæ•°: ||Î¸|| = {np.linalg.norm(self.theta):.2f}")
        print("å‘æ•£ï¼" if np.linalg.norm(self.theta) > 100 else "æ”¶æ•›")


# è¿è¡ŒBairdä¾‹å­
baird = BairdsCounterExample()
baird.semi_gradient_td(alpha=0.01, num_steps=5000)
```

**ç†è®ºè§£é‡Š**ï¼š

1. **å‡½æ•°é€¼è¿‘**æ‰“ç ´äº†è¡¨æ ¼æƒ…å†µçš„æ”¶æ•›ä¿è¯
2. **è‡ªä¸¾**ï¼ˆTDç›®æ ‡ä½¿ç”¨å½“å‰ä¼°è®¡ï¼‰å¼•å…¥åå·®
3. **ç¦»ç­–ç•¥**å¯¼è‡´åˆ†å¸ƒä¸åŒ¹é…ï¼ˆè¡Œä¸ºç­–ç•¥ â‰  ç›®æ ‡ç­–ç•¥ï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š

- **Gradient TD**ï¼ˆGTD, GTD2, TDCï¼‰
- **Emphatic TD**
- **é™åˆ¶å‡½æ•°ç±»**ï¼ˆå¦‚çº¿æ€§+on-policyï¼‰

---

## 36.4 ç­–ç•¥ä¼˜åŒ–ç†è®º

### 36.4.1 ç­–ç•¥æ¢¯åº¦å®šç†

**å®šç†36.5**ï¼ˆç­–ç•¥æ¢¯åº¦å®šç†ï¼‰ï¼š

å¯¹äºå¯å¾®ç­–ç•¥ $\pi_\theta$ï¼Œæ€§èƒ½æ¢¯åº¦ä¸ºï¼š

$$
\nabla_\theta J(\theta) = \mathbb{E}_{s \sim d^{\pi_\theta}, a \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) \cdot Q^{\pi_\theta}(s, a) \right]
$$

å…¶ä¸­ $d^{\pi_\theta}(s) = \sum_{t=0}^\infty \gamma^t P(s_t = s | \pi_\theta)$ æ˜¯æŠ˜æ‰£çŠ¶æ€åˆ†å¸ƒã€‚

**å®Œæ•´è¯æ˜**ï¼š

```python
"""
ç­–ç•¥æ¢¯åº¦å®šç†å®Œæ•´è¯æ˜
"""

class PolicyGradientTheorem:
    """
    ç­–ç•¥æ¢¯åº¦å®šç†çš„å½¢å¼åŒ–è¯æ˜
    """
    def __init__(self):
        pass
    
    def proof_step_by_step(self):
        """
        é€æ­¥è¯æ˜ç­–ç•¥æ¢¯åº¦å®šç†
        """
        print("ç­–ç•¥æ¢¯åº¦å®šç†è¯æ˜")
        print("=" * 80)
        
        print("\nã€ç›®æ ‡ã€‘è¯æ˜:")
        print("  âˆ‡_Î¸ J(Î¸) = E_{s~d^Ï€, a~Ï€}[âˆ‡_Î¸ log Ï€_Î¸(a|s) Â· Q^Ï€(s,a)]")
        
        print("\nã€æ­¥éª¤1ã€‘å®šä¹‰æ€§èƒ½æµ‹åº¦")
        print("  J(Î¸) = E_Ï„[R(Ï„)] = E_Ï„[Î£_t Î³^t r_t]")
        print("       = E_{s_0}[V^Ï€(s_0)]")
        
        print("\nã€æ­¥éª¤2ã€‘è½¨è¿¹æ¦‚ç‡")
        print("  P(Ï„|Î¸) = Î¼(s_0) Î _t Ï€_Î¸(a_t|s_t) P(s_{t+1}|s_t,a_t)")
        
        print("\nã€æ­¥éª¤3ã€‘å¯¹Jæ±‚æ¢¯åº¦")
        print("  âˆ‡_Î¸ J(Î¸) = âˆ‡_Î¸ âˆ« P(Ï„|Î¸) R(Ï„) dÏ„")
        print("           = âˆ« âˆ‡_Î¸ P(Ï„|Î¸) R(Ï„) dÏ„")
        
        print("\nã€æ­¥éª¤4ã€‘Log-derivative Trick")
        print("  âˆ‡_Î¸ P(Ï„|Î¸) = P(Ï„|Î¸) âˆ‡_Î¸ log P(Ï„|Î¸)")
        print("  âˆ‡_Î¸ log P(Ï„|Î¸) = âˆ‡_Î¸ log[Î¼(s_0) Î _t Ï€_Î¸(a_t|s_t) P(s_{t+1}|s_t,a_t)]")
        print("                 = Î£_t âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t)")
        
        print("\nã€æ­¥éª¤5ã€‘ä»£å…¥")
        print("  âˆ‡_Î¸ J(Î¸) = âˆ« P(Ï„|Î¸) âˆ‡_Î¸ log P(Ï„|Î¸) R(Ï„) dÏ„")
        print("           = E_Ï„[(Î£_t âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t)) Â· (Î£_t' Î³^{t'} r_{t'})]")
        
        print("\nã€æ­¥éª¤6ã€‘å› æœæ€§")
        print("  æ—¶åˆ»tçš„åŠ¨ä½œä¸å½±å“tä¹‹å‰çš„å¥–åŠ±")
        print("  âˆ‡_Î¸ J(Î¸) = E_Ï„[Î£_t âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) Â· (Î£_{t'â‰¥t} Î³^{t'} r_{t'})]")
        print("           = E_Ï„[Î£_t âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) Â· G_t]")
        
        print("\nã€æ­¥éª¤7ã€‘å¼•å…¥Qå‡½æ•°")
        print("  G_t = Î£_{t'â‰¥t} Î³^{t'-t} r_{t'}")
        print("  Q^Ï€(s_t, a_t) = E[G_t | s_t, a_t]")
        
        print("\nã€æ­¥éª¤8ã€‘çŠ¶æ€åˆ†å¸ƒ")
        print("  âˆ‡_Î¸ J(Î¸) = Î£_t Î£_s P(s_t = s) Î£_a Ï€_Î¸(a|s) âˆ‡_Î¸ log Ï€_Î¸(a|s) Q^Ï€(s,a)")
        
        print("\nã€æ­¥éª¤9ã€‘æŠ˜æ‰£çŠ¶æ€åˆ†å¸ƒ")
        print("  d^Ï€(s) = Î£_t Î³^t P(s_t = s)")
        print("  âˆ‡_Î¸ J(Î¸) âˆ Î£_s d^Ï€(s) Î£_a Ï€_Î¸(a|s) âˆ‡_Î¸ log Ï€_Î¸(a|s) Q^Ï€(s,a)")
        print("           = E_{s~d^Ï€, a~Ï€}[âˆ‡_Î¸ log Ï€_Î¸(a|s) Q^Ï€(s,a)]")
        
        print("\nã€è¯æ¯•ã€‘âœ“")
    
    def compatible_function_approximation(self):
        """
        Compatibleå‡½æ•°é€¼è¿‘å®šç†
        
        å®šç†ï¼šè‹¥criticæ»¡è¶³compatibleæ¡ä»¶ï¼Œ
               åˆ™ç­–ç•¥æ¢¯åº¦ä¼°è®¡æ— å
        """
        print("\n\nCompatibleå‡½æ•°é€¼è¿‘")
        print("=" * 80)
        
        print("\nã€æ¡ä»¶1ã€‘ç‰¹å¾åŒ¹é…")
        print("  âˆ‡_w Q_w(s,a) = âˆ‡_Î¸ log Ï€_Î¸(a|s)")
        
        print("\nã€æ¡ä»¶2ã€‘æœ€å°åŒ–TDè¯¯å·®")
        print("  w = arg min_w E[(Q^Ï€(s,a) - Q_w(s,a))Â²]")
        
        print("\nã€ç»“è®ºã€‘")
        print("  è‹¥criticæ»¡è¶³1å’Œ2ï¼Œåˆ™:")
        print("  E[âˆ‡_Î¸ log Ï€_Î¸(a|s) Q_w(s,a)] = âˆ‡_Î¸ J(Î¸)")
        print("  å³ï¼šç”¨Q_wä»£æ›¿Q^Ï€ä¸ä¼šå¼•å…¥åå·®ï¼")


# è¿è¡Œè¯æ˜
pg_theorem = PolicyGradientTheorem()
pg_theorem.proof_step_by_step()
pg_theorem.compatible_function_approximation()
```

### 36.4.2 NPGä¸TRPOç†è®º

**è‡ªç„¶ç­–ç•¥æ¢¯åº¦**ï¼ˆNatural Policy Gradientï¼‰ï¼š

**å®šä¹‰**ï¼šè‡ªç„¶æ¢¯åº¦æ˜¯åœ¨Fisherä¿¡æ¯åº¦é‡ä¸‹çš„æœ€é€Ÿä¸‹é™æ–¹å‘ï¼š

$$
\tilde{\nabla}_\theta J(\theta) = F(\theta)^{-1} \nabla_\theta J(\theta)
$$

å…¶ä¸­Fisherä¿¡æ¯çŸ©é˜µï¼š

$$
F(\theta) = \mathbb{E}_{s,a} \left[ \nabla_\theta \log \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s)^T \right]
$$

**TRPOç†è®ºä¿è¯**ï¼š

**å®šç†36.6**ï¼ˆTRPOå•è°ƒæ”¹è¿›ï¼‰ï¼š

åœ¨ä¿¡èµ–åŸŸçº¦æŸä¸‹ï¼š

$$
\max_{\theta'} \mathbb{E}_{s \sim d^{\pi_{\theta}}, a \sim \pi_{\theta'}} \left[ \frac{\pi_{\theta'}(a|s)}{\pi_\theta(a|s)} A^{\pi_\theta}(s,a) \right]
$$

å—çº¦æŸäºï¼š

$$
\mathbb{E}_{s \sim d^{\pi_\theta}} [D_{KL}(\pi_\theta(\cdot|s) \| \pi_{\theta'}(\cdot|s))] \leq \delta
$$

åˆ™ï¼š

$$
J(\theta') \geq J(\theta) + O(\epsilon) - \frac{C \delta}{(1-\gamma)^2}
$$

ä¿è¯å•è°ƒæ”¹è¿›ï¼ˆå½“ $\delta$ è¶³å¤Ÿå°æ—¶ï¼‰ã€‚

**å®ç°ä¸éªŒè¯**ï¼š

```python
"""
TRPOç†è®ºä¿è¯éªŒè¯
"""

class TRPOTheory:
    """
    TRPOç†è®ºåˆ†æ
    """
    def __init__(self):
        pass
    
    def surrogate_objective(self, policy_old, policy_new, states, actions, advantages):
        """
        ä»£ç†ç›®æ ‡å‡½æ•°
        
        L(Î¸') = E[Ï€_{Î¸'}(a|s) / Ï€_Î¸(a|s) Â· A^Ï€_Î¸(s,a)]
        """
        ratio = policy_new.prob(actions, states) / policy_old.prob(actions, states)
        surrogate = (ratio * advantages).mean()
        
        return surrogate
    
    def kl_constraint(self, policy_old, policy_new, states, delta=0.01):
        """
        KLæ•£åº¦çº¦æŸ
        
        E_s[D_KL(Ï€_Î¸ || Ï€_{Î¸'})] â‰¤ Î´
        """
        kl_div = policy_old.kl_divergence(policy_new, states).mean()
        
        constraint_satisfied = kl_div <= delta
        
        print(f"KLæ•£åº¦: {kl_div:.6f}, çº¦æŸ: â‰¤{delta}, æ»¡è¶³: {constraint_satisfied}")
        
        return kl_div, constraint_satisfied
    
    def monotonic_improvement_guarantee(self, J_old, J_new, kl, delta, gamma=0.99):
        """
        å•è°ƒæ”¹è¿›ä¿è¯éªŒè¯
        
        J(Î¸') â‰¥ J(Î¸) - CÂ·max_s D_KL / (1-Î³)Â²
        """
        C = 4.0  # å¸¸æ•°ï¼ˆå–å†³äºä¼˜åŠ¿å‡½æ•°ç•Œï¼‰
        
        theoretical_lower_bound = J_old - (C * kl) / ((1 - gamma) ** 2)
        
        print(f"\nå•è°ƒæ”¹è¿›éªŒè¯:")
        print(f"  J(Î¸_old) ={J_old:.4f}")
        print(f"  J(Î¸_new) = {J_new:.4f}")
        print(f"  ç†è®ºä¸‹ç•Œ â‰¥ {theoretical_lower_bound:.4f}")
        print(f"  å®é™…æ”¹è¿›: {J_new - J_old:.4f}")
        print(f"  å•è°ƒæ€§: {'âœ“' if J_new >= theoretical_lower_bound else 'âœ—'}")
        
        return J_new >= theoretical_lower_bound
    
    def conjugate_gradient_solver(self, Fvp_func, g, max_iterations=10, tolerance=1e-10):
        """
        å…±è½­æ¢¯åº¦æ³•æ±‚è§£ F^{-1} g
        
        ç”¨äºé«˜æ•ˆè®¡ç®—è‡ªç„¶æ¢¯åº¦ï¼šF^{-1} âˆ‡J
        """
        x = np.zeros_like(g)
        r = g.copy()
        p = g.copy()
        
        rdotr = r.dot(r)
        
        for i in range(max_iterations):
            Ap = Fvp_func(p)
            alpha = rdotr / (p.dot(Ap) + 1e-8)
            x += alpha * p
            r -= alpha * Ap
            
            new_rdotr = r.dot(r)
            if new_rdotr < tolerance:
                break
            
            beta = new_rdotr / (rdotr + 1e-8)
            p = r + beta * p
            rdotr = new_rdotr
        
        return x
    
    def line_search_with_backtracking(
        self,
        policy,
        search_direction,
        step_size,
        delta,
        max_backtracks=10
    ):
        """
        å›æº¯çº¿æœç´¢
        
        ç¡®ä¿ï¼š
        1. KLçº¦æŸæ»¡è¶³
        2. ä»£ç†ç›®æ ‡æ”¹è¿›
        """
        for i in range(max_backtracks):
            # å°è¯•æ­¥é•¿
            candidate_params = policy.params + step_size * search_direction
            
            # æ£€æŸ¥KLçº¦æŸ
            kl = policy.kl_to(candidate_params)
            
            if kl <= delta:
                # çº¦æŸæ»¡è¶³ï¼Œæ¥å—
                return candidate_params, True
            else:
                # å‡å°æ­¥é•¿
                step_size *= 0.5
        
        # å›æº¯å¤±è´¥
        return policy.params, False


# æ¼”ç¤ºTRPOç†è®º
trpo_theory = TRPOTheory()
```

ç»§ç»­...

---

## 36.5 æ¢ç´¢-åˆ©ç”¨ç†è®º

### 36.5.1 Multi-Armed Bandits

**MABé—®é¢˜**ï¼š$K$ä¸ªè‡‚ï¼Œæ¯ä¸ªè‡‚$i$çš„å¥–åŠ±åˆ†å¸ƒ $P_i$ï¼Œå‡å€¼ $\mu_i$ã€‚

**ç›®æ ‡**ï¼šæœ€å°åŒ–é—æ†¾ï¼š

$$
R(T) = T \mu^* - \sum_{t=1}^T r_t = \sum_{i=1}^K \Delta_i \mathbb{E}[N_i(T)]
$$

å…¶ä¸­ $\Delta_i = \mu^* - \mu_i$ æ˜¯æ¬¡ä¼˜æ€§gapï¼Œ$N_i(T)$ æ˜¯è‡‚$i$è¢«æ‹‰å–çš„æ¬¡æ•°ã€‚

**UCBç®—æ³•**ï¼š

$$
a_t = \arg\max_i \left[ \hat{\mu}_i(t) + \sqrt{\frac{2 \log t}{N_i(t)}} \right]
$$

**å®šç†36.7**ï¼ˆUCBé—æ†¾ç•Œï¼‰ï¼š

UCBç®—æ³•çš„æœŸæœ›é—æ†¾æ»¡è¶³ï¼š

$$
\mathbb{E}[R(T)] \leq \sum_{i: \Delta_i > 0} \frac{8 \log T}{\Delta_i} + \left( 1 + \frac{\pi^2}{3} \right) \sum_{i=1}^K \Delta_i
$$

**Thompson Sampling**ï¼š

è´å¶æ–¯æ–¹æ³•ï¼Œä»åéªŒåˆ†å¸ƒé‡‡æ ·ï¼š

$$
a_t \sim \arg\max_i \theta_i, \quad \theta_i \sim P(\theta_i | \mathcal{D}_t)
$$

### 36.5.2 æ¢ç´¢ç­–ç•¥

**Îµ-greedy vs UCB vs Thompson Sampling**ï¼š

```python
"""
æ¢ç´¢ç­–ç•¥ç†è®ºå¯¹æ¯”
"""

class ExplorationTheory:
    """
    æ¢ç´¢ç†è®ºåˆ†æ
    """
    def __init__(self):
        pass
    
    def epsilon_greedy_regret(self, K, T, epsilon):
        """
        Îµ-greedyé—æ†¾ç•Œ
        
        R(T) = O(K log T / Îµ + ÎµT)
        
        æœ€ä¼˜Îµ: Îµ* = O((K log T / T)^{1/2})
        æœ€ä¼˜é—æ†¾: R(T) = O(âˆš(KT log T))
        """
        # æ¢ç´¢æˆæœ¬
        exploration_cost = epsilon * T
        
        # æ¬¡ä¼˜é€‰æ‹©æ¬¡æ•°ï¼ˆæœªæ¢ç´¢åˆ°æœ€ä¼˜è‡‚ï¼‰
        suboptimal_cost = (K * np.log(T)) / epsilon
        
        total_regret = exploration_cost + suboptimal_cost
        
        print(f"Îµ-greedy (Îµ={epsilon}):")
        print(f"  æ¢ç´¢æˆæœ¬: {exploration_cost:.2f}")
        print(f"  æ¬¡ä¼˜æˆæœ¬: {suboptimal_cost:.2f}")
        print(f"  æ€»é—æ†¾: {total_regret:.2f}")
        
        return total_regret
    
    def ucb_regret(self, K, T, gaps):
        """
        UCBé—æ†¾ç•Œ
        
        R(T) = Î£_i (8 log T / Î”_i) + O(K)
        """
        regret = sum(8 * np.log(T) / gap for gap in gaps if gap > 0)
        regret += K * (1 + np.pi**2 / 3)
        
        print(f"\nUCB:")
        print(f"  é—æ†¾: {regret:.2f}")
        print(f"  æ¸è¿‘æœ€ä¼˜ (log T)")
        
        return regret
    
    def thompson_sampling_analysis(self):
        """
        Thompson Samplingç†è®º
        
        ä¼˜åŠ¿ï¼š
        1. é—æ†¾ç•Œï¼šR(T) = O(âˆš(KT log T))ï¼ˆBernoulli banditsï¼‰
        2. å®è·µä¸­è¡¨ç°ä¼˜å¼‚
        3. è‡ªé€‚åº”æ¢ç´¢
        """
        print(f"\nThompson Sampling:")
        print(f"  é—æ†¾ç•Œ: O(âˆš(KT log T))")
        print(f"  ä¼˜åŠ¿: è‡ªç„¶çš„æ¢ç´¢-åˆ©ç”¨æƒè¡¡")
        print(f"  ç†è®º: Bayesiané—æ†¾åŒ¹é…")
    
    def information_ratio_bound(self):
        """
        ä¿¡æ¯æ¯”ç•Œï¼ˆRusso & Van Roy, 2016ï¼‰
        
        å®šä¹‰ä¿¡æ¯æ¯”ï¼š
        Î“_t = (Regret_t)Â² / I_t
        
        å…¶ä¸­I_tæ˜¯å…³äºæœ€ä¼˜åŠ¨ä½œçš„ä¿¡æ¯å¢ç›Š
        
        å®šç†ï¼šE[R(T)] â‰¤ âˆš(Î“Ì„ Â· T)
        """
        print(f"\nä¿¡æ¯æ¯”ç•Œ:")
        print(f"  Î“ = (Regret)Â² / Information")
        print(f"  E[R(T)] â‰¤ âˆš(Î“Ì„ Â· T)")
        print(f"  åº”ç”¨: TS, UCB, etc.")


# æ¼”ç¤º
explore_theory = ExplorationTheory()
explore_theory.epsilon_greedy_regret(K=10, T=10000, epsilon=0.1)
explore_theory.ucb_regret(K=10, T=10000, gaps=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0])
explore_theory.thompson_sampling_analysis()
explore_theory.information_ratio_bound()
```

<div data-component="ExplorationStrategiesComparison"></div>

---

## 36.6 å‰æ²¿ç†è®ºæ–¹å‘

### 36.6.1 ç»Ÿè®¡æ•ˆç‡

**Offline RLç†è®º**ï¼š

**é›†ä¸­ç³»æ•°**ï¼ˆConcentrability Coefficientï¼‰ï¼š

$$
C_\pi^d = \sup_{s,a} \frac{d^\pi(s) \pi(a|s)}{d^{\beta}(s) \beta(a|s)}
$$

è¡¡é‡ç›®æ ‡ç­–ç•¥$\pi$ä¸æ•°æ®æ”¶é›†ç­–ç•¥$\beta$çš„åˆ†å¸ƒåç§»ã€‚

**å®šç†36.8**ï¼ˆOffline RLæ ·æœ¬å¤æ‚åº¦ï¼‰ï¼š

åœ¨é›†ä¸­ç³»æ•°$C$ä¸‹ï¼ŒFitted Q-Iterationè¾¾åˆ°$\epsilon$-æœ€ä¼˜éœ€è¦æ ·æœ¬æ•°ï¼š

$$
N = \tilde{O}\left( \frac{C^2}{\epsilon^2 (1-\gamma)^4} \right)
$$

**PessimismåŸåˆ™**ï¼š

ä¸åœ¨çº¿RLçš„optimismç›¸åï¼Œoffline RLä½¿ç”¨pessimismé¿å…åˆ†å¸ƒå¤–åŠ¨ä½œã€‚

### 36.6.2 å¯è¯æ˜é«˜æ•ˆRL

**GOLFç®—æ³•**ï¼ˆWang et al., 2020ï¼‰ï¼š

- **G**o **O**ptimistic **L**ocally, **F**ind locally optimal policies
- åœ¨ç®€å•ç¯å¢ƒï¼ˆblock MDPï¼‰ä¸­å¤šé¡¹å¼æ ·æœ¬å¤æ‚åº¦

**å®šç†36.9**ï¼šåœ¨Block MDPå‡è®¾ä¸‹ï¼ŒGOLFä»¥é«˜æ¦‚ç‡åœ¨

$$
\tilde{O}\left( \frac{|\mathcal{S}| |\mathcal{A}| H^5}{\epsilon^2} \right)
$$

æ ·æœ¬å†…æ‰¾åˆ°$\epsilon$-æœ€ä¼˜ç­–ç•¥ã€‚

### 36.6.3 LQRç†è®º

**çº¿æ€§äºŒæ¬¡è°ƒèŠ‚å™¨**ï¼ˆLinear Quadratic Regulatorï¼‰ï¼š

**ç³»ç»Ÿ**ï¼š$s_{t+1} = A s_t + B a_t + w_t$

**æˆæœ¬**ï¼š$c(s,a) = s^T Q s + a^T R a$

**æœ€ä¼˜ç­–ç•¥**ï¼š$a^* = -K^* s$ï¼Œå…¶ä¸­$K^*$ç”±Riccatiæ–¹ç¨‹æ±‚è§£ã€‚

**å®šç†36.10**ï¼ˆLQRæ ·æœ¬å¤æ‚åº¦ï¼‰ï¼š

æ¨¡å‹æœªçŸ¥çš„LQRï¼Œè¾¾åˆ°$\epsilon$-æœ€ä¼˜éœ€è¦ï¼š

$$
\tilde{O}\left( \frac{d^2}{\epsilon} \right)
$$

æ ·æœ¬ï¼Œå…¶ä¸­$d = \dim(s) + \dim(a)$ã€‚

**ä»£ç ç¤ºä¾‹**ï¼š

```python
"""
LQRç†è®ºä¸ç®—æ³•
"""

class LQRTheory:
    """
    LQRç†è®ºåˆ†æ
    """
    def __init__(self, A, B, Q, R, gamma=0.99):
        """
        Args:
            A: çŠ¶æ€è½¬ç§»çŸ©é˜µ (n x n)
            B: æ§åˆ¶çŸ©é˜µ (n x m)
            Q: çŠ¶æ€æˆæœ¬çŸ©é˜µ (n x n)
            R: æ§åˆ¶æˆæœ¬çŸ©é˜µ (m x m)
        """
        self.A = A
        self.B = B
        self.Q = Q
        self.R = R
        self.gamma = gamma
    
    def solve_riccati(self, max_iterations=1000, tol=1e-6):
        """
        æ±‚è§£ç¦»æ•£Riccatiæ–¹ç¨‹
        
        P = Q + Î³ A^T P A - Î³Â² A^T P B (R + Î³ B^T P B)^{-1} B^T P A
        
        æœ€ä¼˜å¢ç›Šï¼šK = (R + Î³ B^T P B)^{-1} B^T P A
        """
        n = self.A.shape[0]
        P = self.Q.copy()
        
        for iteration in range(max_iterations):
            # è®¡ç®—å¢ç›Š
            K = np.linalg.solve(
                self.R + self.gamma * self.B.T @ P @ self.B,
                self.gamma * self.B.T @ P @ self.A
            )
            
            # æ›´æ–°P
            P_new = (
                self.Q +
                self.gamma * self.A.T @ P @ self.A -
                self.gamma**2 * self.A.T @ P @ self.B @ K
            )
            
            # æ£€æŸ¥æ”¶æ•›
            if np.linalg.norm(P_new - P) < tol:
                print(f"Riccatiæ–¹ç¨‹æ”¶æ•›äºç¬¬{iteration}æ¬¡è¿­ä»£")
                break
            
            P = P_new
        
        # æœ€ä¼˜å¢ç›Š
        K_star = np.linalg.solve(
            self.R + self.gamma * self.B.T @ P @ self.B,
            self.gamma * self.B.T @ P @ self.A
        )
        
        return P, K_star
    
    def sample_complexity_lqr(self, epsilon, confidence=0.95):
        """
        LQRæ ·æœ¬å¤æ‚åº¦
        
        Fazel et al., 2018: Ã•(dÂ² / Îµ)
        """
        n = self.A.shape[0]
        m = self.B.shape[1]
        d = n + m
        
        # ç®€åŒ–ç•Œ
        samples = (d ** 2) / epsilon * np.log(1 / (1 - confidence))
        
        print(f"LQRæ ·æœ¬å¤æ‚åº¦:")
        print(f"  çŠ¶æ€ç»´åº¦: {n}")
        print(f"  åŠ¨ä½œç»´åº¦: {m}")
        print(f"  æ€»ç»´åº¦: d = {d}")
        print(f"  è¾¾åˆ°Îµ={epsilon}æœ€ä¼˜éœ€è¦: {samples:.0f} æ ·æœ¬")
        
        return samples


# æ¼”ç¤ºLQR
A = np.array([[1.01, 0.01], [0.01, 1.01]])
B = np.array([[0.0], [1.0]])
Q = np.eye(2)
R = np.array([[0.1]])

lqr = LQRTheory(A, B, Q, R)
P, K = lqr.solve_riccati()
print(f"æœ€ä¼˜å¢ç›ŠçŸ©é˜µ:\n{K}")

lqr.sample_complexity_lqr(epsilon=0.01)
```

---

## æ€»ç»“

æœ¬ç« ä»‹ç»äº†RLçš„æ ¸å¿ƒç†è®ºåŸºç¡€ï¼š

1. **æ”¶æ•›æ€§ç†è®º**ï¼šå€¼è¿­ä»£ã€Q-learningã€ç­–ç•¥æ¢¯åº¦çš„æ”¶æ•›è¯æ˜
2. **æ ·æœ¬å¤æ‚åº¦**ï¼šPACç•Œã€é—æ†¾ç•Œã€ä¿¡æ¯è®ºä¸‹ç•Œ
3. **å‡½æ•°é€¼è¿‘**ï¼šä¸‡èƒ½é€¼è¿‘ã€æ³›åŒ–ç•Œã€è‡´å‘½ä¸‰è§’
4. **ç­–ç•¥ä¼˜åŒ–**ï¼šç­–ç•¥æ¢¯åº¦å®šç†ã€NPGã€TRPOä¿è¯
5. **æ¢ç´¢ç†è®º**ï¼šMABã€UCBã€Thompson Sampling
6. **å‰æ²¿æ–¹å‘**ï¼šOffline RLã€å¯è¯æ˜é«˜æ•ˆã€LQR

**å…³é”®è¦ç‚¹**ï¼š
- è¡¨æ ¼RLæœ‰å¼ºç†è®ºä¿è¯ï¼ˆå‹ç¼©æ˜ å°„ã€PACã€é—æ†¾ç•Œï¼‰
- å‡½æ•°é€¼è¿‘æ‰“ç ´æŸäº›ä¿è¯ï¼ˆdeadly triadï¼‰
- ç­–ç•¥ä¼˜åŒ–æœ‰ç†è®ºåŸºç¡€ï¼ˆå•è°ƒæ”¹è¿›ï¼‰
- æ ·æœ¬å¤æ‚åº¦ä¾èµ–äºçŠ¶æ€-åŠ¨ä½œç©ºé—´ã€æŠ˜æ‰£å› å­
- æ¢ç´¢-åˆ©ç”¨æƒè¡¡æ˜¯æ ¹æœ¬æŒ‘æˆ˜

**æœªæ¥å±•æœ›**ï¼š
- æ·±åº¦RLçš„ç†è®ºç†è§£
- éæ¸è¿‘æ”¶æ•›ç•Œ
- è®¡ç®—å¤æ‚åº¦ä¸æ ·æœ¬å¤æ‚åº¦æƒè¡¡
- å®è·µä¸ç†è®ºå·®è·ç¼©å°

---

## å‚è€ƒæ–‡çŒ®

- Sutton, R. S., \u0026 Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.).
- Watkins, C. J., \u0026 Dayan, P. (1992). "Q-learning." *Machine Learning*, 8(3-4), 279-292.
- Kakade, S. M. (2001). "A Natural Policy Gradient." *NIPS*.
- Schulman, J., et al. (2015). "Trust Region Policy Optimization." *ICML*.
- Auer, P., et al. (2002). "Finite-time Analysis of the Multiarmed Bandit Problem." *Machine Learning*.
- Agarwal, A., et al. (2021). *Reinforcement Learning: Theory and Algorithms*. (https://rltheorybook.github.io/)
- Russo, D., \u0026 Van Roy, B. (2016). "An Information-Theoretic Analysis of Thompson Sampling." *JMLR*.
- Fazel, M., et al. (2018). "Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator." *ICML*.
