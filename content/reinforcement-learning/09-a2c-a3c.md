---
title: "Chapter 9. Advantage Actor-Critic (A2C/A3C)"
description: "从 Actor-Critic 到现代策略优化的桥梁"
updated: "2026-01-29"
---

> **Learning Objectives**
> * 理解 Advantage 函数的作用和计算
> * 掌握 A2C 同步训练架构
> * 学习 A3C 异步并行训练
> * 理解广义优势估计（GAE）
> * 掌握实现技巧：共享网络、熵正则化

---

## 9.1 Advantage 函数

Advantage 函数是策略梯度方法中降低方差的核心概念。

### 9.1.1 A(s,a) = Q(s,a) - V(s) 定义

**定义**：

$$
A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s)
$$

**直觉理解**：
- $Q^{\pi}(s, a)$：在状态 $s$ 选择动作 $a$ 的价值
- $V^{\pi}(s)$：状态 $s$ 的平均价值（所有动作的期望）
- $A^{\pi}(s, a)$：动作 $a$ **相对于平均**的优势

**示例**：

假设在某个状态下：
- $V(s) = 5$（平均价值）
- $Q(s, a_1) = 8$ → $A(s, a_1) = 3$（好动作）
- $Q(s, a_2) = 5$ → $A(s, a_2) = 0$（平均）
- $Q(s, a_3) = 2$ → $A(s, a_3) = -3$（差动作）

**关键性质**：

$$
\mathbb{E}_{a \sim \pi}[A^{\pi}(s, a)] = 0
$$

证明：

$$
\begin{align}
\mathbb{E}_{a \sim \pi}[A^{\pi}(s, a)] &= \mathbb{E}_{a \sim \pi}[Q^{\pi}(s, a) - V^{\pi}(s)] \\
&= \mathbb{E}_{a \sim \pi}[Q^{\pi}(s, a)] - V^{\pi}(s) \\
&= V^{\pi}(s) - V^{\pi}(s) = 0
\end{align}
$$

<div data-component="AdvantageEstimation"></div>

### 9.1.2 降低方差的原理

**为什么 Advantage 降低方差？**

原始策略梯度：

$$
\nabla_{\boldsymbol{\theta}} J = \mathbb{E}\left[\nabla \log \pi(a|s) \cdot Q^{\pi}(s, a)\right]
$$

**问题**：$Q^{\pi}(s, a)$ 的值可能很大，方差大。

使用 Advantage：

$$
\nabla_{\boldsymbol{\theta}} J = \mathbb{E}\left[\nabla \log \pi(a|s) \cdot A^{\pi}(s, a)\right]
$$

**优势**：
- $A^{\pi}(s, a)$ 的期望为 0，中心化
- 只关注**相对**差异，而非绝对值
- 方差显著降低（30-50%）

**数学证明（方差降低）**：

$$
\text{Var}[A(s,a)] < \text{Var}[Q(s,a)]
$$

因为 $A(s,a) = Q(s,a) - V(s)$，减去常数（相对于动作）减小了方差。

### 9.1.3 不改变梯度期望的证明

**定理**：使用 Advantage 而非 Q 不改变梯度的期望。

**证明**：

$$
\begin{align}
&\mathbb{E}\left[\nabla \log \pi(a|s) \cdot (Q(s,a) - V(s))\right] \\
&= \mathbb{E}\left[\nabla \log \pi(a|s) \cdot Q(s,a)\right] - \mathbb{E}\left[\nabla \log \pi(a|s) \cdot V(s)\right] \\
&= \mathbb{E}\left[\nabla \log \pi(a|s) \cdot Q(s,a)\right] - V(s) \cdot \mathbb{E}\left[\nabla \log \pi(a|s)\right]
\end{align}
$$

由于：

$$
\mathbb{E}_{a \sim \pi}\left[\nabla \log \pi(a|s)\right] = \nabla \sum_a \pi(a|s) = \nabla 1 = 0
$$

因此：

$$
\mathbb{E}\left[\nabla \log \pi \cdot A\right] = \mathbb{E}\left[\nabla \log \pi \cdot Q\right]
$$

**结论**：Advantage 保持梯度期望不变，但降低方差！

---

## 9.2 A2C 算法

A2C（Advantage Actor-Critic）是同步版本的 Actor-Critic。

### 9.2.1 同步 Actor-Critic

**核心思想**：
- 使用多个并行环境同步采样
- 一次性收集一个 batch 的经验
- 同步更新网络参数

**与标准 Actor-Critic 的区别**：

| 特性 | 标准 Actor-Critic | A2C |
|------|------------------|-----|
| 环境数量 | 单个 | 多个并行 |
| 更新频率 | 每步 | 每 n 步 |
| 样本效率 | 低 | 高 |
| 稳定性 | 较低 | 较高 |

### 9.2.2 多步 TD 估计

**n-step Return**：

$$
G_t^{(n)} = r_t + \gamma r_{t+1} + \cdots + \gamma^{n-1} r_{t+n-1} + \gamma^n V(s_{t+n})
$$

**Advantage 估计**：

$$
A_t^{(n)} = G_t^{(n)} - V(s_t)
$$

**优势**：
- 相比 1-step TD：更准确（less biased）
- 相比 Monte Carlo：更低方差

### 9.2.3 熵正则化

**目的**：鼓励探索，防止策略过早收敛到确定性。

**熵定义**：

$$
H(\pi(·|s)) = -\sum_a \pi(a|s) \log \pi(a|s)
$$

**添加熵 bonus**：

$$
J(\boldsymbol{\theta}) = J_{\text{原始}}(\boldsymbol{\theta}) + \beta H(\pi(·|s))
$$

其中 $\beta$ 是熵系数（例如 0.01）。

**效果**：
- $H$ 大：策略接近均匀分布（探索）
- $H$ 小：策略接近确定性（利用）

### 9.2.4 并行环境采样

**架构**：

```
环境 1 → 经验 1 ↘
环境 2 → 经验 2  → [聚合] → 批量更新 → 共享网络
...             ↗
环境 N → 经验 N ↗
```

**实现**（使用 Gymnasium 的 VectorEnv）：

```python
import gymnasium as gym
from gymnasium.vector import AsyncVectorEnv

# 创建并行环境
def make_env(env_name):
    def _init():
        return gym.make(env_name)
    return _init

num_envs = 8
envs = AsyncVectorEnv([make_env('CartPole-v1') for _ in range(num_envs)])

# 同步采样
states = envs.reset()[0]  # shape: (num_envs, state_dim)
actions = ...  # shape: (num_envs,)
next_states, rewards, dones, truncated, infos = envs.step(actions)
```

**优势**：
- 大幅提高采样效率
- 增加样本多样性
- 稳定训练

---

## 9.3 A3C 算法

A3C（Asynchronous Advantage Actor-Critic）是异步并行训练的开创性工作。

### 9.3.1 异步训练架构

**架构**：

```
Worker 1 (自己的环境) → 计算梯度 → 异步更新全局网络
Worker 2 (自己的环境) → 计算梯度 → 异步更新全局网络
...
Worker N (自己的环境) → 计算梯度 → 异步更新全局网络
                                 ↓
                            全局共享网络
```

**特点**：
- 每个 worker 独立运行
- 不需要同步等待
- 更高的吞吐量

<div data-component="A3CArchitecture"></div>

### 9.3.2 多线程并行

**Python 实现（伪代码）**：

```python
import threading
import torch.multiprocessing as mp

global_network = ActorCriticNetwork(state_dim, action_dim)
global_network.share_memory()  # 共享内存

optimizer = SharedAdam(global_network.parameters())

def worker(worker_id, global_network, optimizer):
    """Worker 线程"""
    env = gym.make('CartPole-v1')
    local_network = ActorCriticNetwork(state_dim, action_dim)
    
    while not done:
        # 1. 从全局网络复制参数
        local_network.load_state_dict(global_network.state_dict())
        
        # 2. 收集经验（例如 20 步）
        states, actions, rewards = collect_trajectories(env, local_network, n_steps=20)
        
        # 3. 计算梯度
        advantages, returns = compute_advantages(rewards, values)
        loss = compute_loss(states, actions, advantages, returns)
        
        # 4. 异步更新全局网络
        optimizer.zero_grad()
        loss.backward()
        
        # 复制梯度到全局网络
        for local_param, global_param in zip(local_network.parameters(), 
                                            global_network.parameters()):
            global_param._grad = local_param.grad
        
        optimizer.step()

# 启动多个 worker
processes = []
for worker_id in range(num_workers):
    p = mp.Process(target=worker, args=(worker_id, global_network, optimizer))
    p.start()
    processes.append(p)

for p in processes:
    p.join()
```

### 9.3.3 异步梯度更新

**A3C 更新流程**：

1. Worker 从全局网络**复制参数**
2. 本地收集经验并计算梯度
3. **锁定全局网络**
4. 将梯度应用到全局网络
5. **释放锁**

**优点**：
- 不需要等待所有 worker
- 更高效利用 CPU

**缺点**：
- 梯度可能基于过时的参数
- 需要careful tuning

### 9.3.4 与 A2C 的对比

| 特性 | A2C | A3C |
|------|-----|-----|
| **同步性** | 同步（等待所有环境） | 异步（各自更新） |
| **实现复杂度** | 简单 | 复杂（多线程） |
| **稳定性** | 更稳定 | 稍差（参数可能过时） |
| **吞吐量** | 中等 | 高 |
| **主流使用** | ✅ 更常用 | 逐渐被 A2C 替代 |

**实践建议**：现代实现更倾向于使用 **A2C**（同步版本），因为：
- GPU 并行化让同步成本降低
- 更稳定、更易调试
- 性能相当

---

## 9.4 广义优势估计（GAE）

GAE（Generalized Advantage Estimation）是一种优雅的 advantage 估计方法。

### 9.4.1 n-step Advantage 的指数加权

**动机**：不同 n 的 n-step advantage 有不同的偏差-方差权衡。

**1-step TD advantage**：

$$
A_t^{(1)} = r_t + \gamma V(s_{t+1}) - V(s_t)
$$

- 低方差，高偏差

**∞-step (Monte Carlo) advantage**：

$$
A_t^{(\infty)} = \sum_{k=0}^{\infty} \gamma^k r_{t+k} - V(s_t)
$$

- 高方差，低偏差

**GAE 的思想**：对所有 n-step advantages 进行指数加权平均。

### 9.4.2 GAE(λ) 公式

**定义**：

$$
A_t^{\text{GAE}(\lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
$$

其中 TD error：

$$
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$

**递归形式**（更实用）：

$$
A_t^{\text{GAE}(\lambda)} = \delta_t + \gamma \lambda A_{t+1}^{\text{GAE}(\lambda)}
$$

**特殊情况**：
- $\lambda = 0$：$A^{\text{GAE}(0)} = \delta_t$（1-step TD）
- $\lambda = 1$：$A^{\text{GAE}(1)} = \sum_k \gamma^k r_k - V(s_t)$（Monte Carlo）

<div data-component="GAEWeighting"></div>

### 9.4.3 偏差-方差权衡

**$\lambda$ 的作用**：

| $\lambda$ | 偏差 | 方差 | 适用场景 |
|-----------|------|------|---------|
| 0 (TD) | 高 | 低 | 不准确的价值函数 |
| 0.5 | 中 | 中 | 平衡 |
| **0.95** | 低 | 中 | **常用默认值** |
| 1.0 (MC) | 0 | 高 | 准确的价值函数 |

**实践**：$\lambda = 0.95$ 在大多数任务上表现良好。

### 9.4.4 λ 参数调优

**调优建议**：

1. **开始**：$\lambda = 0.95$
2. **高方差**（训练不稳定）→ 降低 $\lambda$（例如 0.9）
3. **低偏差需求**（想要更准确的 advantage）→ 提高 $\lambda$（例如 0.98）

**代码实现**：

```python
def compute_gae(rewards, values, next_value, gamma=0.99, lambda_=0.95, dones=None):
    """
    计算 GAE (Generalized Advantage Estimation)
    
    Args:
        rewards: List of rewards [r_0, r_1, ..., r_{T-1}]
        values: List of state values [V(s_0), V(s_1), ..., V(s_{T-1})]
        next_value: Next state value V(s_T)
        gamma: Discount factor
        lambda_: GAE parameter λ
        dones: List of done flags (optional)
    
    Returns:
        advantages: GAE advantages
        returns: Value targets (advantages + values)
    """
    advantages = []
    gae = 0
    
    if dones is None:
        dones = [False] * len(rewards)
    
    # 从后向前计算
    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_v = next_value
        else:
            next_v = values[t + 1]
        
        # TD error: δ_t = r_t + γV(s_{t+1}) - V(s_t)
        delta = rewards[t] + gamma * next_v * (1 - dones[t]) - values[t]
        
        # GAE: A_t = δ_t + γλ A_{t+1}
        gae = delta + gamma * lambda_ * (1 - dones[t]) * gae
        advantages.insert(0, gae)
    
    advantages = torch.FloatTensor(advantages)
    returns = advantages + torch.FloatTensor(values)
    
    return advantages, returns
```

---

## 9.5 实现技巧

### 9.5.1 共享网络层

**动机**：Actor 和 Critic 共享底层特征提取。

**架构**：

```
        输入状态 s
            ↓
    [共享特征层]  ← 共享参数
            ↓
        特征 φ(s)
       ↙        ↘
   [Actor头]  [Critic头]
      ↓           ↓
   π(a|s)       V(s)
```

**PyTorch 实现**：

```python
class SharedActorCritic(nn.Module):
    """
    共享网络的 Actor-Critic
    
    Args:
        state_dim: 状态维度
        action_dim: 动作数量
        hidden_dim: 隐藏层维度
    """
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(SharedActorCritic, self).__init__()
        
        # 共享特征层
        self.shared = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Actor 头（策略）
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)
        )
        
        # Critic 头（价值）
        self.critic = nn.Linear(hidden_dim, 1)
    
    def forward(self, state):
        """
        前向传播
        
        Returns:
            action_probs: 动作概率分布
            value: 状态价值
        """
        features = self.shared(state)
        action_probs = self.actor(features)
        value = self.critic(features)
        return action_probs, value
```

<div data-component="SharedNetworkVisualization"></div>

**优势**：
- ✅ 参数共享，减少模型大小
- ✅ 特征复用，提高学习效率
- ✅ 更稳定（Actor 和 Critic 一起学习）

### 9.5.2 梯度裁剪

**目的**：防止梯度爆炸，稳定训练。

**实现**：

```python
# 方法 1：裁剪梯度范数
torch.nn.utils.clip_grad_norm_(
    model.parameters(), 
    max_norm=0.5  # 常用：0.5 或 1.0
)

# 方法 2：裁剪梯度值
torch.nn.utils.clip_grad_value_(
    model.parameters(), 
    clip_value=1.0
)
```

**在训练循环中**：

```python
optimizer.zero_grad()
loss.backward()

# 梯度裁剪
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)

optimizer.step()
```

### 9.5.3 学习率调度

**常用策略**：

```python
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR

# 1. 阶梯衰减
scheduler = StepLR(optimizer, step_size=1000, gamma=0.9)

# 2. 余弦退火
scheduler = CosineAnnealingLR(optimizer, T_max=10000, eta_min=1e-5)

# 3. 自定义线性衰减
def linear_decay(initial_lr, final_lr, total_steps):
    def lr_lambda(step):
        return final_lr + (initial_lr - final_lr) * (1 - step / total_steps)
    return lr_lambda

scheduler = LambdaLR(optimizer, lr_lambda=linear_decay(3e-4, 1e-5, 100000))
```

### 9.5.4 奖励标准化

**目的**：缓解不同环境奖励尺度差异。

**运行时标准化**：

```python
class RunningMeanStd:
    """运行时均值和标准差估计"""
    def __init__(self, epsilon=1e-4, shape=()):
        self.mean = np.zeros(shape)
        self.var = np.ones(shape)
        self.count = epsilon
    
    def update(self, x):
        batch_mean = np.mean(x, axis=0)
        batch_var = np.var(x, axis=0)
        batch_count = x.shape[0]
        
        self.update_from_moments(batch_mean, batch_var, batch_count)
    
    def update_from_moments(self, batch_mean, batch_var, batch_count):
        delta = batch_mean - self.mean
        tot_count = self.count + batch_count
        
        new_mean = self.mean + delta * batch_count / tot_count
        m_a = self.var * self.count
        m_b = batch_var * batch_count
        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / tot_count
        new_var = M2 / tot_count
        
        self.mean = new_mean
        self.var = new_var
        self.count = tot_count

# 使用
reward_normalizer = RunningMeanStd()

# 在训练中
reward_normalizer.update(rewards)
normalized_rewards = (rewards - reward_normalizer.mean) / np.sqrt(reward_normalizer.var + 1e-8)
```

---

## 9.6 完整 A2C 实现

```python
import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
from gymnasium.vector import AsyncVectorEnv
import numpy as np

class A2CAgent:
    """
    A2C (Advantage Actor-Critic) Agent
    
    Args:
        state_dim: 状态维度
        action_dim: 动作数量
        num_envs: 并行环境数量
        hidden_dim: 隐藏层维度
        gamma: 折扣因子
        lambda_gae: GAE 参数
        entropy_coef: 熵系数
        value_coef: 价值损失系数
        max_grad_norm: 梯度裁剪范数
        lr: 学习率
    """
    def __init__(self, state_dim, action_dim, num_envs=8, hidden_dim=256,
                 gamma=0.99, lambda_gae=0.95, entropy_coef=0.01, 
                 value_coef=0.5, max_grad_norm=0.5, lr=7e-4):
        
        self.num_envs = num_envs
        self.gamma = gamma
        self.lambda_gae = lambda_gae
        self.entropy_coef = entropy_coef
        self.value_coef = value_coef
        self.max_grad_norm = max_grad_norm
        
        # 网络
        self.model = SharedActorCritic(state_dim, action_dim, hidden_dim)
        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)
    
    def select_actions(self, states):
        """
        选择动作
        
        Args:
            states: (num_envs, state_dim)
        
        Returns:
            actions: (num_envs,)
            log_probs: (num_envs,)
            values: (num_envs,)
        """
        with torch.no_grad():
            states_tensor = torch.FloatTensor(states)
            action_probs, values = self.model(states_tensor)
            
            dist = torch.distributions.Categorical(action_probs)
            actions = dist.sample()
            log_probs = dist.log_prob(actions)
            
            return actions.numpy(), log_probs.numpy(), values.numpy()
    
    def compute_gae(self, rewards, values, next_value, dones):
        """计算 GAE"""
        advantages = []
        gae = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_v = next_value
            else:
                next_v = values[t + 1]
            
            delta = rewards[t] + self.gamma * next_v * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.lambda_gae * (1 - dones[t]) * gae
            advantages.insert(0, gae)
        
        advantages = np.array(advantages)
        returns = advantages + values
        
        return advantages, returns
    
    def train_step(self, rollout_data):
        """
        训练步骤
        
        Args:
            rollout_data: dict with keys 'states', 'actions', 'advantages', 'returns'
        """
        states = torch.FloatTensor(rollout_data['states'])
        actions = torch.LongTensor(rollout_data['actions'])
        advantages = torch.FloatTensor(rollout_data['advantages'])
        returns = torch.FloatTensor(rollout_data['returns'])
        
        # 前向传播
        action_probs, values = self.model(states)
        dist = torch.distributions.Categorical(action_probs)
        
        # Actor loss (策略损失)
        log_probs = dist.log_prob(actions)
        actor_loss = -(log_probs * advantages).mean()
        
        # Entropy bonus (鼓励探索)
        entropy = dist.entropy().mean()
        
        # Critic loss (价值损失)
        critic_loss = nn.MSELoss()(values.squeeze(), returns)
        
        # 总损失
        loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy
        
        # 优化
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
        self.optimizer.step()
        
        return {
            'loss': loss.item(),
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'entropy': entropy.item()
        }

def train_a2c(env_name='CartPole-v1', num_envs=8, num_steps=5, total_timesteps=1000000):
    """
    A2C 训练主循环
    
    Args:
        env_name: 环境名称
        num_envs: 并行环境数量
        num_steps: 每次 rollout 的步数
        total_timesteps: 总训练步数
    """
    # 创建并行环境
    def make_env():
        return lambda: gym.make(env_name)
    
    envs = AsyncVectorEnv([make_env() for _ in range(num_envs)])
    
    # 初始化 agent
    state_dim = envs.single_observation_space.shape[0]
    action_dim = envs.single_action_space.n
    agent = A2CAgent(state_dim, action_dim, num_envs=num_envs)
    
    # 训练循环
    states = envs.reset()[0]
    episode_rewards = []
    
    num_updates = total_timesteps // (num_envs * num_steps)
    
    for update in range(num_updates):
        # 收集 rollout
        rollout_states = []
        rollout_actions = []
        rollout_rewards = []
        rollout_dones = []
        rollout_values = []
        
        for step in range(num_steps):
            actions, log_probs, values = agent.select_actions(states)
            
            next_states, rewards, dones, truncated, infos = envs.step(actions)
            dones = np.logical_or(dones, truncated)
            
            rollout_states.append(states)
            rollout_actions.append(actions)
            rollout_rewards.append(rewards)
            rollout_dones.append(dones)
            rollout_values.append(values)
            
            states = next_states
        
        # 计算 next value
        with torch.no_grad():
            _, next_value = agent.model(torch.FloatTensor(states))
            next_value = next_value.numpy()
        
        # 展平数据
        rollout_states = np.array(rollout_states).reshape(-1, state_dim)
        rollout_actions = np.array(rollout_actions).flatten()
        rollout_rewards = np.array(rollout_rewards)
        rollout_dones = np.array(rollout_dones)
        rollout_values = np.array(rollout_values)
        
        # 计算 advantages
        all_advantages = []
        all_returns = []
        
        for env_idx in range(num_envs):
            env_rewards = rollout_rewards[:, env_idx]
            env_values = rollout_values[:, env_idx]
            env_dones = rollout_dones[:, env_idx]
            
            advantages, returns = agent.compute_gae(
                env_rewards, env_values, next_value[env_idx], env_dones
            )
            all_advantages.append(advantages)
            all_returns.append(returns)
        
        all_advantages = np.concatenate(all_advantages)
        all_returns = np.concatenate(all_returns)
        
        # 标准化 advantages
        all_advantages = (all_advantages - all_advantages.mean()) / (all_advantages.std() + 1e-8)
        
        # 训练
        rollout_data = {
            'states': rollout_states,
            'actions': rollout_actions,
            'advantages': all_advantages,
            'returns': all_returns
        }
        
        metrics = agent.train_step(rollout_data)
        
        # 日志
        if update % 100 == 0:
            print(f"Update {update}/{num_updates}")
            print(f"  Loss: {metrics['loss']:.3f}")
            print(f"  Entropy: {metrics['entropy']:.3f}")
    
    envs.close()
    return agent

# 运行训练
if __name__ == "__main__":
    agent = train_a2c(env_name='CartPole-v1', num_envs=8, total_timesteps=500000)
```

---

## 本章小结

在本章中，我们学习了：

✅ **Advantage 函数**：$A(s,a) = Q(s,a) - V(s)$ 降低方差  
✅ **A2C**：同步并行 Actor-Critic  
✅ **A3C**：异步多线程训练  
✅ **GAE**：广义优势估计，优雅的偏差-方差权衡  
✅ **实现技巧**：共享网络、熵正则化、梯度裁剪  

> [!TIP]
> **核心要点**：
> - Advantage 函数中心化价值，显著降低方差
> - A2C 使用并行环境同步采样，现代实现的主流选择
> - GAE(λ) 优雅地权衡偏差和方差，$\lambda=0.95$ 是常用值
> - 共享网络层提高参数效率和稳定性
> - 熵正则化鼓励探索，防止过早收敛

> [!NOTE]
> **下一步**：
> Chapter 10 将学习**确定性策略梯度（DDPG/TD3）**：
> - 确定性策略 μ(s) 适合连续控制
> - DDPG 深度确定性策略梯度
> - TD3 的三大改进
> - 连续控制任务实战
> 
> 进入 [Chapter 10. DDPG & TD3](10-ddpg-td3.md)

---

## 扩展阅读

- **经典论文**：
  - Mnih et al. (2016): Asynchronous Methods for Deep RL (A3C)
  - Schulman et al. (2016): High-Dimensional Continuous Control Using GAE
- **实现资源**：
  - Stable-Baselines3: A2C
  - OpenAI Spinning Up: A2C/A3C
