---
title: "ç¬¬39ç« ï¼šç”Ÿäº§éƒ¨ç½²ä¸å·¥ç¨‹å®è·µ"
description: "æ¨¡å‹éƒ¨ç½²ã€åœ¨çº¿å­¦ä¹ ã€ç›‘æ§æ—¥å¿—ã€æ•°æ®ç®¡ç†ã€å·¥ç¨‹å·¥å…·é“¾ã€å®é™…æ¡ˆä¾‹"
date: "2026-01-30"
---

# ç¬¬39ç« ï¼šç”Ÿäº§éƒ¨ç½²ä¸å·¥ç¨‹å®è·µ

## 39.1 æ¨¡å‹éƒ¨ç½²

### 39.1.1 æ¨¡å‹å¯¼å‡º

**ONNXï¼ˆOpen Neural Network Exchangeï¼‰**ï¼š

```python
"""
å¯¼å‡ºPyTorchæ¨¡å‹åˆ°ONNX
"""

import torch
import torch.nn as nn
import onnx
import onnxruntime as ort

class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œ"""
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, state):
        return self.network(state)


# è®­ç»ƒå¥½çš„æ¨¡å‹
state_dim = 4
action_dim = 2
model = PolicyNetwork(state_dim, action_dim)
model.load_state_dict(torch.load('policy_weights.pth'))
model.eval()

# å¯¼å‡ºåˆ°ONNX
dummy_input = torch.randn(1, state_dim)

torch.onnx.export(
    model,
    dummy_input,
    "policy.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['state'],
    output_names=['action_probs'],
    dynamic_axes={
        'state': {0: 'batch_size'},
        'action_probs': {0: 'batch_size'}
    }
)

print("âœ… æ¨¡å‹å·²å¯¼å‡ºåˆ° policy.onnx")

# éªŒè¯ONNXæ¨¡å‹
onnx_model = onnx.load("policy.onnx")
onnx.checker.check_model(onnx_model)
print("âœ… ONNXæ¨¡å‹éªŒè¯é€šè¿‡")

# ä½¿ç”¨ONNX Runtimeæ¨ç†
ort_session = ort.InferenceSession("policy.onnx")

def predict_onnx(state):
    """ä½¿ç”¨ONNXæ¨¡å‹æ¨ç†"""
    ort_inputs = {ort_session.get_inputs()[0].name: state.numpy()}
    ort_outputs = ort_session.run(None, ort_inputs)
    return ort_outputs[0]


# å¯¹æ¯”PyTorchå’ŒONNXè¾“å‡º
test_state = torch.randn(1, state_dim)

with torch.no_grad():
    pytorch_output = model(test_state).numpy()

onnx_output = predict_onnx(test_state)

print(f"\nPyTorchè¾“å‡º: {pytorch_output}")
print(f"ONNXè¾“å‡º: {onnx_output}")
print(f"å·®å¼‚: {abs(pytorch_output - onnx_output).max():.6f}")
```

**TorchScript**ï¼š

```python
"""
å¯¼å‡ºPyTorchæ¨¡å‹åˆ°TorchScript
"""

import torch

# æ–¹æ³•1: Tracingï¼ˆè¿½è¸ªï¼‰
model.eval()
example_input = torch.randn(1, state_dim)

traced_model = torch.jit.trace(model, example_input)
traced_model.save("policy_traced.pt")

print("âœ… Tracedæ¨¡å‹å·²ä¿å­˜")

# æ–¹æ³•2: Scriptingï¼ˆè„šæœ¬åŒ–ï¼‰- æ”¯æŒæ§åˆ¶æµ
class PolicyNetworkWithControl(nn.Module):
    """å¸¦æ§åˆ¶æµçš„ç­–ç•¥ç½‘ç»œ"""
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, action_dim)
    
    def forward(self, state):
        x = torch.relu(self.fc1(state))
        
        # æ§åˆ¶æµï¼ˆTracingæ— æ³•å¤„ç†ï¼‰
        if x.sum() > 0:
            x = torch.relu(self.fc2(x))
        else:
            x = torch.tanh(self.fc2(x))
        
        return torch.softmax(self.fc3(x), dim=-1)


scripted_model = torch.jit.script(PolicyNetworkWithControl(state_dim, action_dim))
scripted_model.save("policy_scripted.pt")

print("âœ… Scriptedæ¨¡å‹å·²ä¿å­˜")

# åŠ è½½å¹¶ä½¿ç”¨
loaded_model = torch.jit.load("policy_traced.pt")
loaded_model.eval()

with torch.no_grad():
    output = loaded_model(test_state)
    print(f"TorchScriptè¾“å‡º: {output}")
```

### 39.1.2 é‡åŒ–ä¸å‹ç¼©

**åŠ¨æ€é‡åŒ–**ï¼ˆæ¨ç†æ—¶é‡åŒ–ï¼‰ï¼š

```python
"""
æ¨¡å‹é‡åŒ– - å‡å°æ¨¡å‹å¤§å°ï¼ŒåŠ é€Ÿæ¨ç†
"""

import torch.quantization

# åŸå§‹æ¨¡å‹å¤§å°
original_size = sum(p.numel() * p.element_size() for p in model.parameters())
print(f"åŸå§‹æ¨¡å‹å¤§å°: {original_size / 1024:.2f} KB")

# åŠ¨æ€é‡åŒ–ï¼ˆINT8ï¼‰
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {nn.Linear},  # é‡åŒ–Linearå±‚
    dtype=torch.qint8
)

# ä¿å­˜é‡åŒ–æ¨¡å‹
torch.save(quantized_model.state_dict(), 'policy_quantized.pth')

# é‡åŒ–åå¤§å°
quantized_size = sum(p.numel() * p.element_size() for p in quantized_model.parameters())
print(f"é‡åŒ–åå¤§å°: {quantized_size / 1024:.2f} KB")
print(f"å‹ç¼©æ¯”: {original_size / quantized_size:.2f}x")

# æ¨ç†é€Ÿåº¦å¯¹æ¯”
import time

test_batch = torch.randn(100, state_dim)

# åŸå§‹æ¨¡å‹
start = time.time()
with torch.no_grad():
    for _ in range(1000):
        _ = model(test_batch)
original_time = time.time() - start

# é‡åŒ–æ¨¡å‹
start = time.time()
with torch.no_grad():
    for _ in range(1000):
        _ = quantized_model(test_batch)
quantized_time = time.time() - start

print(f"\nåŸå§‹æ¨¡å‹æ¨ç†æ—¶é—´: {original_time:.3f}s")
print(f"é‡åŒ–æ¨¡å‹æ¨ç†æ—¶é—´: {quantized_time:.3f}s")
print(f"åŠ é€Ÿæ¯”: {original_time / quantized_time:.2f}x")
```

**æ¨¡å‹å‰ªæ**ï¼š

```python
"""
æ¨¡å‹å‰ªæ - ç§»é™¤ä¸é‡è¦çš„æƒé‡
"""

import torch.nn.utils.prune as prune

def prune_model(model, amount=0.3):
    """
    å‰ªææ¨¡å‹
    
    Args:
        amount: å‰ªææ¯”ä¾‹ï¼ˆ0.3 = ç§»é™¤30%çš„æƒé‡ï¼‰
    """
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            # L1 unstructured pruning
            prune.l1_unstructured(module, name='weight', amount=amount)
            
            # æ°¸ä¹…åŒ–å‰ªæ
            prune.remove(module, 'weight')
    
    return model


# å‰ªæ
pruned_model = prune_model(model, amount=0.3)

# ç»Ÿè®¡ç¨€ç–åº¦
def count_sparsity(model):
    """è®¡ç®—æ¨¡å‹ç¨€ç–åº¦"""
    total_params = 0
    zero_params = 0
    
    for param in model.parameters():
        total_params += param.numel()
        zero_params += (param == 0).sum().item()
    
    sparsity = zero_params / total_params * 100
    return sparsity


sparsity = count_sparsity(pruned_model)
print(f"æ¨¡å‹ç¨€ç–åº¦: {sparsity:.2f}%")
```

### 39.1.3 æ¨ç†ä¼˜åŒ–

**æ‰¹å¤„ç†æ¨ç†**ï¼š

```python
"""
æ‰¹å¤„ç†æ¨ç†ä¼˜åŒ–
"""

class BatchedInference:
    """æ‰¹å¤„ç†æ¨ç†å¼•æ“"""
    def __init__(self, model, batch_size=32, timeout=0.01):
        """
        Args:
            batch_size: æ‰¹å¤§å°
            timeout: ç­‰å¾…è¶…æ—¶ï¼ˆç§’ï¼‰
        """
        self.model = model
        self.batch_size = batch_size
        self.timeout = timeout
        
        self.queue = []
        self.results = {}
        
        import threading
        self.lock = threading.Lock()
        self.worker_thread = threading.Thread(target=self._worker, daemon=True)
        self.worker_thread.start()
    
    def predict(self, state, request_id):
        """
        å¼‚æ­¥é¢„æµ‹
        
        Args:
            state: è¾“å…¥çŠ¶æ€
            request_id: è¯·æ±‚ID
        """
        with self.lock:
            self.queue.append((request_id, state))
        
        # ç­‰å¾…ç»“æœ
        while request_id not in self.results:
            time.sleep(0.001)
        
        result = self.results.pop(request_id)
        return result
    
    def _worker(self):
        """åå°worker - æ‰¹å¤„ç†æ¨ç†"""
        import time
        
        while True:
            time.sleep(self.timeout)
            
            with self.lock:
                if len(self.queue) == 0:
                    continue
                
                # å–å‡ºä¸€æ‰¹
                batch = self.queue[:self.batch_size]
                self.queue = self.queue[self.batch_size:]
            
            # æ‰¹å¤„ç†æ¨ç†
            request_ids = [req_id for req_id, _ in batch]
            states = torch.stack([state for _, state in batch])
            
            with torch.no_grad():
                outputs = self.model(states)
            
            # å­˜å‚¨ç»“æœ
            with self.lock:
                for req_id, output in zip(request_ids, outputs):
                    self.results[req_id] = output


# ä½¿ç”¨
batched_engine = BatchedInference(model, batch_size=32)

# æ¨¡æ‹Ÿå¤šä¸ªå¹¶å‘è¯·æ±‚
import uuid

for i in range(100):
    state = torch.randn(state_dim)
    req_id = str(uuid.uuid4())
    result = batched_engine.predict(state, req_id)
    print(f"Request {i}: {result}")
```

### 39.1.4 è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²

**TensorFlow Liteè½¬æ¢**ï¼ˆç”¨äºç§»åŠ¨ç«¯ï¼‰ï¼š

```python
"""
è½¬æ¢ä¸ºTensorFlow Liteï¼ˆç§»åŠ¨ç«¯éƒ¨ç½²ï¼‰
"""

# å‡è®¾å·²æœ‰TensorFlowæ¨¡å‹
import tensorflow as tf

# è½¬æ¢ä¸ºTFLite
converter = tf.lite.TFLiteConverter.from_saved_model('saved_model_dir')

# ä¼˜åŒ–
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# é‡åŒ–
converter.target_spec.supported_types = [tf.float16]

tflite_model = converter.convert()

# ä¿å­˜
with open('policy.tflite', 'wb') as f:
    f.write(tflite_model)

print("âœ… TFLiteæ¨¡å‹å·²ä¿å­˜")

# ä½¿ç”¨TFLiteæ¨ç†
interpreter = tf.lite.Interpreter(model_path="policy.tflite")
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# æ¨ç†
input_data = np.random.randn(1, state_dim).astype(np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
print(f"TFLiteè¾“å‡º: {output_data}")
```

<div data-component="DeploymentPipeline"></div>

---

## 39.2 åœ¨çº¿å­¦ä¹ ç³»ç»Ÿ

### 39.2.1 æŒç»­è®­ç»ƒ

**åœ¨çº¿å­¦ä¹ æ¶æ„**ï¼š

```python
"""
åœ¨çº¿å­¦ä¹ ç³»ç»Ÿ
"""

import redis
import json
from collections import deque

class OnlineLearningSystem:
    """
    åœ¨çº¿å­¦ä¹ ç³»ç»Ÿ
    
    æŒç»­ä»ç”Ÿäº§ç¯å¢ƒæ”¶é›†æ•°æ®å¹¶æ›´æ–°æ¨¡å‹
    """
    def __init__(self, model, redis_host='localhost', redis_port=6379):
        self.model = model
        self.optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)
        
        # Redisè¿æ¥ï¼ˆç”¨äºæ•°æ®é˜Ÿåˆ—ï¼‰
        self.redis_client = redis.Redis(host=redis_host, port=redis_port)
        
        # ç»éªŒç¼“å†²
        self.buffer = deque(maxlen=10000)
        
        # ç»Ÿè®¡
        self.total_updates = 0
        self.performance_history = []
    
    def collect_experience(self, state, action, reward, next_state, done):
        """
        æ”¶é›†ç”Ÿäº§ç¯å¢ƒç»éªŒ
        
        Args:
            state, action, reward, next_state, done: SARS'å…ƒç»„
        """
        experience = {
            'state': state.tolist(),
            'action': action,
            'reward': reward,
            'next_state': next_state.tolist(),
            'done': done
        }
        
        # æ¨é€åˆ°Redisé˜Ÿåˆ—
        self.redis_client.rpush('experience_queue', json.dumps(experience))
        
        # æœ¬åœ°ç¼“å†²
        self.buffer.append(experience)
    
    def update_model(self, batch_size=64):
        """
        ä»ç¼“å†²åŒºé‡‡æ ·å¹¶æ›´æ–°æ¨¡å‹
        """
        if len(self.buffer) < batch_size:
            return
        
        # é‡‡æ ·batch
        import random
        batch = random.sample(self.buffer, batch_size)
        
        states = torch.FloatTensor([exp['state'] for exp in batch])
        actions = torch.LongTensor([exp['action'] for exp in batch])
        rewards = torch.FloatTensor([exp['reward'] for exp in batch])
        next_states = torch.FloatTensor([exp['next_state'] for exp in batch])
        dones = torch.FloatTensor([exp['done'] for exp in batch])
        
        # è®¡ç®—æŸå¤±ï¼ˆç¤ºä¾‹ï¼šç®€å•ç­–ç•¥æ¢¯åº¦ï¼‰
        action_probs = self.model(states)
        log_probs = torch.log(action_probs.gather(1, actions.unsqueeze(1)) + 1e-8)
        
        # ç®€åŒ–çš„æŸå¤±
        loss = -(log_probs * rewards.unsqueeze(1)).mean()
        
        # æ›´æ–°
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        self.total_updates += 1
        
        # è®°å½•
        if self.total_updates % 100 == 0:
            avg_reward = rewards.mean().item()
            self.performance_history.append(avg_reward)
            print(f"Update {self.total_updates}: Avg Reward = {avg_reward:.2f}")
    
    def save_checkpoint(self, path):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'total_updates': self.total_updates,
            'performance_history': self.performance_history
        }
        torch.save(checkpoint, path)
        print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ° {path}")
    
    def load_checkpoint(self, path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.total_updates = checkpoint['total_updates']
        self.performance_history = checkpoint['performance_history']
        print(f"âœ… æ£€æŸ¥ç‚¹å·²ä» {path} åŠ è½½")


# ä½¿ç”¨ç¤ºä¾‹
online_system = OnlineLearningSystem(model)

# æ¨¡æ‹Ÿç”Ÿäº§ç¯å¢ƒæ•°æ®æµ
for episode in range(1000):
    state = env.reset()
    done = False
    
    while not done:
        # ä½¿ç”¨å½“å‰æ¨¡å‹é€‰æ‹©åŠ¨ä½œ
        with torch.no_grad():
            action_probs = model(torch.FloatTensor(state).unsqueeze(0))
            action = torch.multinomial(action_probs, 1).item()
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, done, _ = env.step(action)
        
        # æ”¶é›†ç»éªŒ
        online_system.collect_experience(
            torch.FloatTensor(state),
            action,
            reward,
            torch.FloatTensor(next_state),
            done
        )
        
        # å®šæœŸæ›´æ–°
        if len(online_system.buffer) >= 64:
            online_system.update_model(batch_size=64)
        
        state = next_state
    
    # å®šæœŸä¿å­˜
    if episode % 100 == 0:
        online_system.save_checkpoint(f'checkpoint_ep{episode}.pth')
```

<div data-component="OnlineLearningArchitecture"></div>

### 39.2.2 A/Bæµ‹è¯•

**A/Bæµ‹è¯•æ¡†æ¶**ï¼š

```python
"""
A/Bæµ‹è¯•æ¡†æ¶
"""

import numpy as np
from scipy import stats

class ABTest:
    """
    A/Bæµ‹è¯•ç®¡ç†å™¨
    """
    def __init__(self, model_a, model_b, traffic_split=0.5):
        """
        Args:
            model_a: æ¨¡å‹Aï¼ˆå¯¹ç…§ç»„ï¼‰
            model_b: æ¨¡å‹Bï¼ˆå®éªŒç»„ï¼‰
            traffic_split: æµé‡åˆ†é…æ¯”ä¾‹ï¼ˆ0.5 = 50/50ï¼‰
        """
        self.model_a = model_a
        self.model_b = model_b
        self.traffic_split = traffic_split
        
        # ç»Ÿè®¡
        self.results_a = []
        self.results_b = []
    
    def select_model(self, user_id):
        """
        æ ¹æ®user_idåˆ†é…æ¨¡å‹ï¼ˆç¡®ä¿åŒä¸€ç”¨æˆ·å§‹ç»ˆçœ‹åˆ°åŒä¸€æ¨¡å‹ï¼‰
        """
        # å“ˆå¸Œuser_id
        import hashlib
        hash_value = int(hashlib.md5(str(user_id).encode()).hexdigest(), 16)
        
        # åˆ†é…
        if (hash_value % 100) < (self.traffic_split * 100):
            return 'A', self.model_a
        else:
            return 'B', self.model_b
    
    def record_result(self, variant, reward):
        """è®°å½•ç»“æœ"""
        if variant == 'A':
            self.results_a.append(reward)
        else:
            self.results_b.append(reward)
    
    def analyze(self, confidence=0.95):
        """
        åˆ†æA/Bæµ‹è¯•ç»“æœ
        
        Returns:
            dict: åˆ†æç»“æœ
        """
        if len(self.results_a) < 30 or len(self.results_b) < 30:
            return {"error": "æ ·æœ¬é‡ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘30ä¸ªï¼‰"}
        
        # å‡å€¼å’Œæ ‡å‡†å·®
        mean_a = np.mean(self.results_a)
        mean_b = np.mean(self.results_b)
        std_a = np.std(self.results_a)
        std_b = np.std(self.results_b)
        
        # t-æ£€éªŒ
        t_stat, p_value = stats.ttest_ind(self.results_a, self.results_b)
        
        # æ•ˆåº”é‡ï¼ˆCohen's dï¼‰
        pooled_std = np.sqrt((std_a**2 + std_b**2) / 2)
        cohens_d = (mean_b - mean_a) / pooled_std
        
        # ç½®ä¿¡åŒºé—´
        se_diff = np.sqrt(std_a**2/len(self.results_a) + std_b**2/len(self.results_b))
        t_crit = stats.t.ppf((1 + confidence) / 2, len(self.results_a) + len(self.results_b) - 2)
        ci_lower = (mean_b - mean_a) - t_crit * se_diff
        ci_upper = (mean_b - mean_a) + t_crit * se_diff
        
        # åˆ¤æ–­
        is_significant = p_value < (1 - confidence)
        winner = 'B' if mean_b > mean_a and is_significant else 'A' if mean_a > mean_b and is_significant else 'No clear winner'
        
        result = {
            'model_a': {
                'mean': mean_a,
                'std': std_a,
                'n': len(self.results_a)
            },
            'model_b': {
                'mean': mean_b,
                'std': std_b,
                'n': len(self.results_b)
            },
            'difference': mean_b - mean_a,
            'relative_improvement': ((mean_b - mean_a) / mean_a * 100) if mean_a != 0 else 0,
            'p_value': p_value,
            'cohens_d': cohens_d,
            'confidence_interval': (ci_lower, ci_upper),
            'is_significant': is_significant,
            'winner': winner
        }
        
        return result
    
    def print_report(self):
        """æ‰“å°æµ‹è¯•æŠ¥å‘Š"""
        result = self.analyze()
        
        if 'error' in result:
            print(f"âŒ {result['error']}")
            return
        
        print("\n" + "="*60)
        print("A/Bæµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        
        print(f"\næ¨¡å‹A (å¯¹ç…§ç»„):")
        print(f"  æ ·æœ¬æ•°: {result['model_a']['n']}")
        print(f"  å‡å€¼: {result['model_a']['mean']:.4f}")
        print(f"  æ ‡å‡†å·®: {result['model_a']['std']:.4f}")
        
        print(f"\næ¨¡å‹B (å®éªŒç»„):")
        print(f"  æ ·æœ¬æ•°: {result['model_b']['n']}")
        print(f"  å‡å€¼: {result['model_b']['mean']:.4f}")
        print(f"  æ ‡å‡†å·®: {result['model_b']['std']:.4f}")
        
        print(f"\nå·®å¼‚åˆ†æ:")
        print(f"  ç»å¯¹å·®å¼‚: {result['difference']:.4f}")
        print(f"  ç›¸å¯¹æå‡: {result['relative_improvement']:.2f}%")
        print(f"  på€¼: {result['p_value']:.4f}")
        print(f"  Cohen's d: {result['cohens_d']:.4f}")
        print(f"  95% CI: [{result['confidence_interval'][0]:.4f}, {result['confidence_interval'][1]:.4f}]")
        
        print(f"\nç»“è®º:")
        if result['is_significant']:
            print(f"  âœ… å·®å¼‚æ˜¾è‘— (p < 0.05)")
            print(f"  ğŸ† è·èƒœè€…: æ¨¡å‹{result['winner']}")
        else:
            print(f"  âŒ å·®å¼‚ä¸æ˜¾è‘— (p >= 0.05)")
            print(f"  å»ºè®®ç»§ç»­æ”¶é›†æ•°æ®æˆ–ä¿æŒç°çŠ¶")
        
        print("="*60)


# ä½¿ç”¨ç¤ºä¾‹
ab_test = ABTest(model_a=old_model, model_b=new_model, traffic_split=0.5)

# æ¨¡æ‹Ÿç”¨æˆ·è¯·æ±‚
for user_id in range(1000):
    variant, model = ab_test.select_model(user_id)
    
    # æ¨¡æ‹Ÿäº¤äº’
    state = env.reset()
    total_reward = 0
    
    for _ in range(100):
        with torch.no_grad():
            action_probs = model(torch.FloatTensor(state).unsqueeze(0))
            action = torch.multinomial(action_probs, 1).item()
        
        state, reward, done, _ = env.step(action)
        total_reward += reward
        
        if done:
            break
    
    # è®°å½•ç»“æœ
    ab_test.record_result(variant, total_reward)

# åˆ†æ
ab_test.print_report()
```

ç»§ç»­...

---

## 39.3 ç›‘æ§ä¸æ—¥å¿—

### 39.3.1 æ€§èƒ½ç›‘æ§

**å…³é”®æŒ‡æ ‡**ï¼š
1. **ç¯å¢ƒæŒ‡æ ‡**ï¼šEpisode Return, Episode Length, Envv Steps/sec
2. **ç­–ç•¥æŒ‡æ ‡**ï¼šEntropy, KL Divergence, Value Loss, Policy Loss
3. **ç³»ç»ŸæŒ‡æ ‡**ï¼šCPU/GPU Usage, RAM, Inference Latency

<div data-component="RLMonitoringDashboard"></div>

**Prometheusç›‘æ§é›†æˆ**ï¼š

```python
"""
Prometheusç›‘æ§é›†æˆ
"""

from prometheus_client import start_http_server, Gauge, Summary, Counter
import time
import random

# å®šä¹‰æŒ‡æ ‡
EPISODE_REWARD = Gauge('rl_episode_reward', 'Average reward per episode')
EPISODE_LENGTH = Gauge('rl_episode_length', 'Average length per episode')
TRAINING_LOSS = Gauge('rl_training_loss', 'Current training loss')
INFERENCE_LATENCY = Summary('rl_inference_latency_seconds', 'Time spent processing inference request')
TOTAL_STEPS = Counter('rl_total_steps', 'Total environment steps')

class PrometheusLogger:
    """
    Prometheusæ—¥å¿—è®°å½•å™¨
    """
    def __init__(self, port=8000):
        # å¯åŠ¨Prometheus metrics server
        start_http_server(port)
        print(f"Prometheus metrics server started on port {port}")
    
    def log_episode(self, reward, length):
        EPISODE_REWARD.set(reward)
        EPISODE_LENGTH.set(length)
    
    def log_step(self):
        TOTAL_STEPS.inc()
    
    def log_loss(self, loss):
        TRAINING_LOSS.set(loss)
    
    @INFERENCE_LATENCY.time()
    def process_inference(self):
        """æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹"""
        time.sleep(random.uniform(0.01, 0.05))


# ä½¿ç”¨
logger = PrometheusLogger(port=8000)

# æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
for step in range(100):
    # æ¨¡æ‹Ÿæ¨ç†
    logger.process_inference()
    logger.log_step()
    
    # æ¨¡æ‹Ÿepisodeç»“æŸ
    if step % 10 == 0:
        reward = random.uniform(0, 100)
        length = random.uniform(50, 200)
        logger.log_episode(reward, length)
        print(f"Logged episode: reward={reward:.2f}")
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ›´æ–°
    if step % 20 == 0:
        loss = random.uniform(0.1, 1.0)
        logger.log_loss(loss)

print("Metrics available at http://localhost:8000/metrics")
```

### 39.3.2 å¼‚å¸¸æ£€æµ‹

**æ£€æµ‹é€»è¾‘**ï¼š
- **æ€§èƒ½éª¤é™**ï¼šæœ€è¿‘Nä¸ªepisodeå¹³å‡å¥–åŠ± < å†å²å‡å€¼ - 3Ïƒ
- **åˆ†å¸ƒæ¼‚ç§»**ï¼šè§‚æµ‹å€¼åˆ†å¸ƒä¸è®­ç»ƒé›†KLæ•£åº¦ > é˜ˆå€¼
- **æ¢¯åº¦çˆ†ç‚¸**ï¼šæ¢¯åº¦èŒƒæ•° > é˜ˆå€¼

```python
"""
å¼‚å¸¸æ£€æµ‹å™¨
"""

class AnomalyDetector:
    """RLå¼‚å¸¸æ£€æµ‹"""
    def __init__(self, window_size=100, threshold_sigma=3.0):
        self.history = deque(maxlen=window_size)
        self.threshold_sigma = threshold_sigma
        
        # ç»Ÿè®¡é‡
        self.mean = 0
        self.std = 0
        self.n = 0
    
    def check(self, value):
        """
        æ£€æŸ¥æ–°å€¼æ˜¯å¦å¼‚å¸¸
        
        Args:
            value: æ–°çš„ç›‘æµ‹å€¼ï¼ˆå¦‚episode rewardï¼‰
            
        Returns:
            bool: æ˜¯å¦å¼‚å¸¸
        """
        if self.n < 10:  # é¢„çƒ­
            self._update(value)
            return False
        
        # Z-scoreæ£€æµ‹
        z_score = (value - self.mean) / (self.std + 1e-8)
        
        if abs(z_score) > self.threshold_sigma:
            print(f"âš ï¸ Anomaly detected! Value={value:.2f}, Mean={self.mean:.2f}, Z={z_score:.2f}")
            return True
        
        self._update(value)
        return False
    
    def _update(self, value):
        """åœ¨çº¿æ›´æ–°å‡å€¼å’Œæ–¹å·® (Welford's algorithm)"""
        self.n += 1
        delta = value - self.mean
        self.mean += delta / self.n
        delta2 = value - self.mean
        self.std = np.sqrt((self.std**2 * (self.n - 2) + delta * delta2) / (self.n - 1)) if self.n > 1 else 0
```

---

## 39.4 æ•°æ®ç®¡ç†

### 39.4.1 ç»éªŒå›æ”¾å­˜å‚¨

**å­˜å‚¨æ–¹æ¡ˆ**ï¼š
- **å†…å­˜ (Redis)**: é«˜ååï¼Œä½å»¶è¿Ÿï¼Œå®¹é‡æœ‰é™
- **NoSQL (Cassandra/DynamoDB)**: å¤§å®¹é‡ï¼Œåˆ†å¸ƒå¼
- **Data Lake (S3/HDFS)**: ç¦»çº¿åˆ†æï¼Œæ‰¹é‡è®­ç»ƒ

**æ•°æ®ç»“æ„è®¾è®¡**ï¼š

```json
{
  "episode_id": "uuid-v4",
  "timestamp": 1678900000,
  "steps": [
    {
      "step_id": 0,
      "state": [0.1, 0.5, -0.2, ...],
      "action": 1,
      "reward": 1.0,
      "info": {"latency": 0.02}
    },
    ...
  ],
  "metadata": {
    "model_version": "v1.2.3",
    "user_group": "A"
  }
}
```

### 39.4.2 éšç§ä¿æŠ¤

**PII (Personal Identifiable Information) ç§»é™¤**ï¼š
- åœ¨çŠ¶æ€è®¾è®¡æ—¶é¿å…åŒ…å«IDã€IPç­‰
- å­˜å‚¨å‰è¿›è¡Œæ©ç å¤„ç†
- å·®åˆ†éšç§ï¼ˆDifferential Privacyï¼‰è®­ç»ƒ

---

## 39.5 å·¥ç¨‹å·¥å…·é“¾

<div data-component="ToolchainComparison"></div>

### 39.5.1 Stable-Baselines3

**ç‰¹ç‚¹**ï¼š
- PyTorchå®ç°
- æ¥å£ç»Ÿä¸€ (`.learn()`, `.predict()`)
- æ–‡æ¡£ä¸»è¦ï¼Œç¤¾åŒºæ´»è·ƒ
- é€‚åˆï¼šç§‘ç ”ã€ä¸­å°å‹é¡¹ç›®ã€æ•™å­¦

```python
"""
Stable-Baselines3 ç¤ºä¾‹
"""

from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env

# å¹¶è¡Œç¯å¢ƒ
vec_env = make_vec_env("CartPole-v1", n_envs=4)

# åˆ›å»ºæ¨¡å‹
model = PPO(
    "MlpPolicy",
    vec_env,
    verbose=1,
    learning_rate=3e-4,
    device="auto"  # è‡ªåŠ¨é€‰æ‹©CPU/GPU
)

# è®­ç»ƒ
model.learn(total_timesteps=25000)

# ä¿å­˜ä¸åŠ è½½
model.save("ppo_cartpole")
loaded_model = PPO.load("ppo_cartpole")

# è¯„ä¼°
obs = vec_env.reset()
for _ in range(100):
    action, _ = loaded_model.predict(obs)
    obs, rewards, dones, info = vec_env.step(action)
```

### 39.5.2 RLlib (Ray)

**ç‰¹ç‚¹**ï¼š
- åˆ†å¸ƒå¼è®­ç»ƒï¼ˆScale outï¼‰
- æ”¯æŒå¤šæ™ºèƒ½ä½“ (MARL)
- å·¥ä¸šçº§å¼ºåº¦
- å­¦ä¹ æ›²çº¿è¾ƒé™¡

```python
"""
Ray RLlib ç¤ºä¾‹
"""

import ray
from ray.rllib.algorithms.ppo import PPOConfig

ray.init()

# é…ç½®
config = (
    PPOConfig()
    .environment("CartPole-v1")
    .framework("torch")
    .rollouts(num_rollout_workers=2)  # 2ä¸ªå¹¶è¡Œworker
    .training(lr=0.0003)
)

# æ„å»ºç®—æ³•
algo = config.build()

# è®­ç»ƒå¾ªç¯
for i in range(10):
    result = algo.train()
    print(f"Iter {i}: reward={result['episode_reward_mean']:.2f}")

# ä¿å­˜
checkpoint = algo.save()
print(f"Checkpoint saved at {checkpoint}")
```

### 39.5.3 CleanRL

**ç‰¹ç‚¹**ï¼š
- å•æ–‡ä»¶å®ç°ï¼ˆSingle-file implementationï¼‰
- æåº¦ç®€æ´ï¼Œä¾¿äºä¿®æ”¹
- é€‚åˆï¼šç®—æ³•ç ”ç©¶ã€é­”æ”¹

---

## 39.6 å®é™…æ¡ˆä¾‹

### 39.6.1 æ¨èç³»ç»Ÿ

**åœºæ™¯**ï¼šYouTube/TikTokè§†é¢‘æ¨è
- **Action**: ä»ç™¾ä¸‡å€™é€‰é›†ä¸­é€‰å‡ºTop-kè§†é¢‘
- **State**: ç”¨æˆ·å†å²è¡Œä¸ºåºåˆ—ã€å½“å‰ä¸Šä¸‹æ–‡
- **Reward**: è§‚çœ‹æ—¶é•¿ã€å®Œæ’­ç‡ã€äº’åŠ¨ï¼ˆç‚¹èµ/åˆ†äº«ï¼‰

**æ¶æ„**ï¼š
1. **å¬å› (Retrieval)**: åŒå¡”æ¨¡å‹ï¼Œå¿«é€Ÿç­›é€‰Top-1000
2. **ç²—æ’ (Pre-ranking)**: è¿‡æ»¤
3. **ç²¾æ’ (Ranking)**: RL/ç²¾ç»†æ¨¡å‹æ‰“åˆ†
4. **é‡æ’ (Re-ranking)**: RLè€ƒè™‘å¤šæ ·æ€§ã€é•¿æœŸæ”¶ç›Šï¼ˆSlate Optimizationï¼‰

### 39.6.2 æ¸¸æˆ AI

**åœºæ™¯**ï¼šMOBA (Dota2/ç‹è€…è£è€€)
- **æŒ‘æˆ˜**ï¼šé•¿æ—¶åºã€ä¸å®Œå…¨ä¿¡æ¯ã€å·¨å¤§çŠ¶æ€ç©ºé—´
- **æ–¹æ¡ˆ**ï¼š
  - **OpenAI Five**: PPO + Self-play + LSTM (Scale up)
  - **æ¶æ„**: Teacher-Student Distillation, Surgery (æ¨¡å‹æ‰‹æœ¯)
  - **Reward Shaping**: ç¨ å¯†å¥–åŠ± -> ç¨€ç–å¥–åŠ±

### 39.6.3 èµ„æºè°ƒåº¦

**åœºæ™¯**ï¼šæ•°æ®ä¸­å¿ƒå†·å´/ä½œä¸šè°ƒåº¦
- **DeepMind Google Data Center**: æ§åˆ¶å†·å´ç³»ç»Ÿ
- **State**: æ¸©åº¦ä¼ æ„Ÿå™¨ã€è´Ÿè½½ã€å¤©æ°”
- **Action**: åˆ¶å†·è®¾å®šç‚¹
- **Reward**: -èƒ½è€— (çº¦æŸï¼šæ¸©åº¦<å®‰å…¨é˜ˆå€¼)
- **æ•ˆæœ**: èŠ‚èƒ½40%

---

## æ€»ç»“

å·¥ç¨‹å®è·µæ˜¯å°†RLä»è®ºæ–‡å¸¦å…¥ç°å®çš„å…³é”®ï¼š
1. **éƒ¨ç½²**ï¼šONNXå¯¼å‡ºã€é‡åŒ–åŠ é€Ÿã€è¾¹ç¼˜è®¡ç®—
2. **ç³»ç»Ÿ**ï¼šRedisé˜Ÿåˆ—ã€åœ¨çº¿å­¦ä¹ é—­ç¯ã€A/Bæµ‹è¯•
3. **ç›‘æ§**ï¼šå…¨é“¾è·¯æŒ‡æ ‡ã€å¼‚å¸¸æ£€æµ‹ã€Prometheus/Grafana
4. **å·¥å…·**ï¼šSB3(æ˜“ç”¨) vs RLlib(æ‰©å±•) vs CleanRL(ç ”ç©¶)

**ç”Ÿäº§ç¯å¢ƒRLçš„é“å¾‹**ï¼š
- **Start Simple**: å…ˆç”¨ç®€å•çš„Baseline (å¦‚Heuristic/Supervised Learning)
- **Data First**: æ•°æ®è´¨é‡å†³å®šä¸Šé™
- **Safety**: å§‹ç»ˆè®¾ç½®å®‰å…¨å›é€€ç­–ç•¥ (Safety Fallback)

---

## å‚è€ƒèµ„æº

- **OpenNX**: https://onnx.ai/
- **Stable-Baselines3**: https://stable-baselines3.readthedocs.io/
- **Ray RLlib**: https://docs.ray.io/en/latest/rllib/index.html
- **Prometheus**: https://prometheus.io/
- **Chip Huyen**: "Designing Machine Learning Systems"
