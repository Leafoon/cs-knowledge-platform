---
title: "Chapter 3. è’™ç‰¹å¡æ´›æ–¹æ³•ï¼ˆMonte Carlo Methodsï¼‰"
description: "ä»ç»éªŒä¸­å­¦ä¹ ï¼šæ— éœ€æ¨¡å‹çš„é‡‡æ ·ä¼°è®¡æ–¹æ³•"
updated: "2026-01-29"
---

> **Learning Objectives**
> * ç†è§£è’™ç‰¹å¡æ´›æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³ï¼šä»å®Œæ•´ episode ä¸­å­¦ä¹ 
> * æŒæ¡ MC ç­–ç•¥è¯„ä¼°ï¼ˆFirst-Visit å’Œ Every-Visitï¼‰
> * å­¦ä¹  MC æ§åˆ¶ç®—æ³•ï¼ˆExploring Starts å’Œ Îµ-greedyï¼‰
> * ç†è§£ Off-policy MC å’Œé‡è¦æ€§é‡‡æ ·
> * åˆ†æ MC ä¸ DP çš„åŒºåˆ«å’Œé€‚ç”¨åœºæ™¯

---

## 3.1 MC åŸºæœ¬æ€æƒ³

è’™ç‰¹å¡æ´›æ–¹æ³•æ˜¯ç¬¬ä¸€ä¸ª**æ— éœ€ç¯å¢ƒæ¨¡å‹**çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡é‡‡æ ·å®é™…ç»éªŒæ¥ä¼°è®¡ä»·å€¼å‡½æ•°ã€‚

### 3.1.1 ä»ç»éªŒä¸­å­¦ä¹ ï¼ˆæ— éœ€æ¨¡å‹ï¼‰

**æ ¸å¿ƒå·®å¼‚**ï¼š

| ç»´åº¦ | åŠ¨æ€è§„åˆ’ï¼ˆDPï¼‰ | è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰ |
|------|--------------|--------------|
| æ¨¡å‹éœ€æ±‚ | éœ€è¦å®Œæ•´çš„ P(s'\|s,a) å’Œ R | **ä¸éœ€è¦æ¨¡å‹** |
| æ›´æ–°æ–¹å¼ | éå†æ‰€æœ‰çŠ¶æ€ | åªæ›´æ–°è®¿é—®è¿‡çš„çŠ¶æ€ |
| ç†è®ºåŸºç¡€ | Bellman æ–¹ç¨‹ï¼ˆæœŸæœ›ï¼‰ | å¤§æ•°å®šå¾‹ï¼ˆé‡‡æ ·å¹³å‡ï¼‰ |
| é€‚ç”¨åœºæ™¯ | å°è§„æ¨¡ã€å·²çŸ¥æ¨¡å‹ | å¤§è§„æ¨¡ã€æœªçŸ¥æ¨¡å‹ |

**ä¸ºä»€ä¹ˆå«"è’™ç‰¹å¡æ´›"ï¼Ÿ**

- æ¥æºäºæ‘©çº³å“¥çš„è’™ç‰¹å¡æ´›èµŒåœº
- é€šè¿‡**éšæœºé‡‡æ ·**ä¼°è®¡æœŸæœ›å€¼
- é‡‡æ ·è¶Šå¤šï¼Œä¼°è®¡è¶Šå‡†ç¡®

### 3.1.2 å®Œæ•´ episode é‡‡æ ·

**Episodeï¼ˆå›åˆï¼‰**ï¼šä»åˆå§‹çŠ¶æ€åˆ°ç»ˆæ­¢çŠ¶æ€çš„å®Œæ•´è½¨è¿¹ã€‚

$$
\tau = (S_0, A_0, R_1, S_1, A_1, R_2, \ldots, S_T)
$$

**MC çš„æ ¹æœ¬è¦æ±‚**ï¼šå¿…é¡»æœ‰**ç»ˆæ­¢çŠ¶æ€**ï¼ˆepisodic tasksï¼‰ã€‚

**ç¤ºä¾‹ä»»åŠ¡**ï¼š
- âœ… Blackjackï¼ˆæ¸¸æˆç»“æŸä¸ºç»ˆæ­¢ï¼‰
- âœ… è¿·å®«ï¼ˆåˆ°è¾¾å‡ºå£ä¸ºç»ˆæ­¢ï¼‰
- âŒ è‚¡ç¥¨äº¤æ˜“ï¼ˆæŒç»­ä»»åŠ¡ï¼Œæ— ç»ˆæ­¢ï¼‰
- âŒ æœºå™¨äººæ§åˆ¶ï¼ˆæŒç»­ä»»åŠ¡ï¼‰

> [!WARNING]
> **MC ä¸é€‚ç”¨äºæŒç»­ä»»åŠ¡**ï¼ˆcontinuing tasksï¼‰ã€‚å¯¹äºæ²¡æœ‰è‡ªç„¶ç»ˆæ­¢çš„ä»»åŠ¡ï¼Œéœ€è¦ä½¿ç”¨ TD å­¦ä¹ ï¼ˆChapter 4ï¼‰ã€‚

### 3.1.3 Return çš„æ— åä¼°è®¡

**Returnï¼ˆå›æŠ¥ï¼‰**ï¼š

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T
$$

**å…³é”®æ€§è´¨**ï¼š

$$
\mathbb{E}[G_t | S_t = s] = V^\pi(s)
$$

MC é€šè¿‡**å¹³å‡å¤šä¸ª episode çš„ Return** æ¥ä¼°è®¡ $V^\pi(s)$ï¼š

$$
V(s) \approx \frac{1}{N(s)} \sum_{i=1}^{N(s)} G_i(s)
$$

å…¶ä¸­ $N(s)$ æ˜¯è®¿é—®çŠ¶æ€ $s$ çš„æ¬¡æ•°ï¼Œ$G_i(s)$ æ˜¯ç¬¬ $i$ æ¬¡è®¿é—®æ—¶çš„ Returnã€‚

**æ— åæ€§**ï¼š

$$
\lim_{N(s) \to \infty} V(s) = V^\pi(s) \quad \text{(å¤§æ•°å®šå¾‹)}
$$

<div data-component="MCReturnEstimation"></div>

### 3.1.4 ä¸ DP çš„å¯¹æ¯”

**DPï¼ˆåŠ¨æ€è§„åˆ’ï¼‰æ›´æ–°**ï¼š

$$
V(s) \leftarrow \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V(s')]
$$

- éœ€è¦ $P(s'|s,a)$ å’Œ $R(s,a,s')$
- ä½¿ç”¨**æœŸæœ›**ï¼ˆæ‰€æœ‰å¯èƒ½çš„ä¸‹ä¸€çŠ¶æ€ï¼‰
- **Bootstrapping**ï¼šç”¨ä¼°è®¡æ›´æ–°ä¼°è®¡

**MC æ›´æ–°**ï¼š

$$
V(s) \leftarrow V(s) + \alpha [G_t - V(s)]
$$

- ä¸éœ€è¦æ¨¡å‹
- ä½¿ç”¨**é‡‡æ ·**ï¼ˆå®é™…ç»å†çš„è½¨è¿¹ï¼‰
- **æ—  Bootstrapping**ï¼šç”¨å®é™… Return æ›´æ–°

---

## 3.2 MC ç­–ç•¥è¯„ä¼°

ç»™å®šç­–ç•¥ $\pi$ï¼Œå¦‚ä½•ç”¨ MC ä¼°è®¡ $V^\pi(s)$ï¼Ÿ

### 3.2.1 First-Visit MC

**æ€æƒ³**ï¼šåªåœ¨æ¯ä¸ª episode ä¸­**ç¬¬ä¸€æ¬¡è®¿é—®**çŠ¶æ€ $s$ æ—¶è®°å½• Returnã€‚

**ç®—æ³•ï¼ˆFirst-Visit MC ç­–ç•¥è¯„ä¼°ï¼‰**ï¼š

```
åˆå§‹åŒ–ï¼š
    V(s) = 0, âˆ€s
    Returns(s) = ç©ºåˆ—è¡¨, âˆ€s

For each episode:
    ç”Ÿæˆ episode: Sâ‚€, Aâ‚€, Râ‚, Sâ‚, Aâ‚, Râ‚‚, ..., S_T-1, A_T-1, R_T
    G â† 0
    For t = T-1, T-2, ..., 0:
        G â† Î³G + R_{t+1}
        If S_t ä¸åœ¨ Sâ‚€, Sâ‚, ..., S_{t-1} ä¸­:  # First-visit
            Append G to Returns(S_t)
            V(S_t) â† average(Returns(S_t))
```

**Python å®ç°**ï¼š

```python
from collections import defaultdict
import numpy as np

def first_visit_mc_prediction(env, policy, num_episodes=10000, gamma=0.99):
    """
    First-Visit MC ç­–ç•¥è¯„ä¼°
    
    Args:
        env: Gym ç¯å¢ƒ
        policy: ç­–ç•¥å‡½æ•° policy(state) -> action
        num_episodes: episode æ•°é‡
        gamma: æŠ˜æ‰£å› å­
    
    Returns:
        V: çŠ¶æ€ä»·å€¼å‡½æ•°ä¼°è®¡
    """
    V = defaultdict(float)
    returns = defaultdict(list)
    
    for episode_num in range(num_episodes):
        # ç”Ÿæˆ episode
        episode = []
        state = env.reset()
        done = False
        
        while not done:
            action = policy(state)
            next_state, reward, done, _ = env.step(action)
            episode.append((state, action, reward))
            state = next_state
        
        # åå‘è®¡ç®— Return
        G = 0
        visited_states = set()
        
        for t in reversed(range(len(episode))):
            state, action, reward = episode[t]
            G = gamma * G + reward
            
            # First-visit: åªåœ¨ç¬¬ä¸€æ¬¡è®¿é—®æ—¶æ›´æ–°
            if state not in visited_states:
                visited_states.add(state)
                returns[state].append(G)
                V[state] = np.mean(returns[state])
    
    return dict(V)
```

### 3.2.2 Every-Visit MC

**æ€æƒ³**ï¼šæ¯æ¬¡è®¿é—®çŠ¶æ€ $s$ æ—¶éƒ½è®°å½• Returnã€‚

**ç®—æ³•å·®å¼‚**ï¼š

```python
# First-Visit
if state not in visited_states:
    visited_states.add(state)
    returns[state].append(G)
    V[state] = np.mean(returns[state])

# Every-Visit
returns[state].append(G)
V[state] = np.mean(returns[state])
```

**å¯¹æ¯”**ï¼š

| ç‰¹æ€§ | First-Visit | Every-Visit |
|------|------------|-------------|
| æ— åæ€§ | âœ… æ— å | âš ï¸ æœ‰åï¼ˆä½†æ¸è¿‘æ— åï¼‰ |
| æ”¶æ•›æ€§ | âœ… æ”¶æ•›åˆ° $V^\pi$ | âœ… æ”¶æ•›åˆ° $V^\pi$ |
| æ–¹å·® | è¾ƒé«˜ | è¾ƒä½ï¼ˆæ›´å¤šæ ·æœ¬ï¼‰ |
| å®è·µä½¿ç”¨ | æ›´å¸¸ç”¨ | è¾ƒå°‘ç”¨ |

### 3.2.3 å¢é‡å¼æ›´æ–°å…¬å¼

**é—®é¢˜**ï¼šå­˜å‚¨æ‰€æœ‰ Returns å ç”¨å†…å­˜è¿‡å¤§ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šå¢é‡å¼æ›´æ–°ã€‚

**æ¨å¯¼**ï¼š

$$
\begin{align}
V_{n+1}(s) &= \frac{1}{n} \sum_{i=1}^n G_i \\
&= \frac{1}{n} \left( G_n + \sum_{i=1}^{n-1} G_i \right) \\
&= \frac{1}{n} \left( G_n + (n-1) V_n(s) \right) \\
&= V_n(s) + \frac{1}{n} \left( G_n - V_n(s) \right)
\end{align}
$$

**å¢é‡å¼æ›´æ–°**ï¼š

$$
V(s) \leftarrow V(s) + \alpha [G - V(s)]
$$

å…¶ä¸­ $\alpha$ å¯ä»¥æ˜¯ï¼š
- **æ ·æœ¬å¹³å‡**ï¼š$\alpha = \frac{1}{N(s)}$
- **å›ºå®šæ­¥é•¿**ï¼š$\alpha = 0.01$ï¼ˆå¸¸ç”¨ï¼Œé€‚åº”éå¹³ç¨³ç¯å¢ƒï¼‰

**ä»£ç å®ç°**ï¼š

```python
def incremental_mc_prediction(env, policy, num_episodes=10000, 
                               alpha=0.01, gamma=0.99):
    """å¢é‡å¼ MC é¢„æµ‹"""
    V = defaultdict(float)
    
    for episode_num in range(num_episodes):
        episode = generate_episode(env, policy)
        
        G = 0
        visited_states = set()
        
        for t in reversed(range(len(episode))):
            state, action, reward = episode[t]
            G = gamma * G + reward
            
            if state not in visited_states:
                visited_states.add(state)
                # å¢é‡å¼æ›´æ–°
                V[state] += alpha * (G - V[state])
    
    return dict(V)
```

### 3.2.4 æ”¶æ•›æ€§åˆ†æï¼ˆå¤§æ•°å®šå¾‹ï¼‰

**å®šç† 3.1ï¼ˆMC æ”¶æ•›æ€§ï¼‰**ï¼š

åœ¨ä»¥ä¸‹æ¡ä»¶ä¸‹ï¼ŒFirst-Visit MC ä¼°è®¡æ”¶æ•›åˆ°çœŸå®å€¼ï¼š

1. æ‰€æœ‰çŠ¶æ€è¢«è®¿é—®æ— é™æ¬¡ï¼š$\lim_{n \to \infty} N(s) = \infty, \forall s$
2. Returns æœ‰ç•Œ

åˆ™ï¼š

$$
V(s) \xrightarrow{a.s.} V^\pi(s) \quad \text{(ä»¥æ¦‚ç‡1æ”¶æ•›)}
$$

**è¯æ˜**ï¼šå¤§æ•°å®šå¾‹ï¼ˆLaw of Large Numbersï¼‰ã€‚

**æ”¶æ•›é€Ÿåº¦**ï¼š

$$
\text{Var}[\hat{V}(s)] = \frac{\sigma^2(s)}{N(s)}
$$

å…¶ä¸­ $\sigma^2(s)$ æ˜¯ Return çš„æ–¹å·®ã€‚

**æ ‡å‡†è¯¯å·®**ï¼š

$$
\text{SE} = \frac{\sigma(s)}{\sqrt{N(s)}}
$$

è¦å°†è¯¯å·®å‡åŠï¼Œéœ€è¦ **4 å€çš„ episodes**ï¼

---

## 3.3 MC æ§åˆ¶

å¦‚ä½•ç”¨ MC æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ï¼Ÿ

### 3.3.1 MC Exploring Starts

**æ€æƒ³**ï¼šç»“åˆ GPI æ¡†æ¶ï¼Œç”¨ MC åšç­–ç•¥è¯„ä¼°ã€‚

**æŒ‘æˆ˜**ï¼šå¦‚ä½•ä¿è¯æ¢ç´¢æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹ï¼Ÿ

**Exploring Starts å‡è®¾**ï¼šæ¯ä¸ª episode ä»éšæœºçš„ $(s, a)$ å¯¹å¼€å§‹ã€‚

**ç®—æ³•ï¼ˆMC Exploring Startsï¼‰**ï¼š

```
åˆå§‹åŒ–ï¼š
    Q(s,a) = 0, âˆ€s,a
    Ï€(s) = ä»»æ„åŠ¨ä½œ, âˆ€s
    Returns(s,a) = ç©ºåˆ—è¡¨, âˆ€s,a

Repeat forever:
    # æ¢ç´¢å¼€å§‹
    éšæœºé€‰æ‹© sâ‚€ âˆˆ S, aâ‚€ âˆˆ A(sâ‚€)
    
    # ç”Ÿæˆ episodeï¼ˆä» (sâ‚€, aâ‚€) å¼€å§‹ï¼Œä¹‹åéµå¾ª Ï€ï¼‰
    Episode â† generate_episode(sâ‚€, aâ‚€, Ï€)
    
    # MC è¯„ä¼°
    For each (s,a) å‡ºç°åœ¨ Episode ä¸­:
        G â† (s,a) ä¹‹åçš„ return
        Append G to Returns(s,a)
        Q(s,a) â† average(Returns(s,a))
    
    # ç­–ç•¥æ”¹è¿›
    For each s in Episode:
        Ï€(s) â† argmax_a Q(s,a)
```

**é—®é¢˜**ï¼šExploring Starts åœ¨å®é™…ä¸­å¾ˆéš¾æ»¡è¶³ï¼ˆå¦‚çœŸå®æœºå™¨äººï¼‰ã€‚

### 3.3.2 Îµ-greedy ç­–ç•¥

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨ Îµ-greedy ç­–ç•¥ä¿è¯æŒç»­æ¢ç´¢ã€‚

**Îµ-greedy ç­–ç•¥å®šä¹‰**ï¼š

$$
\pi(a|s) = \begin{cases}
1 - \epsilon + \frac{\epsilon}{|A|} & \text{if } a = \arg\max_{a'} Q(s, a') \\
\frac{\epsilon}{|A|} & \text{otherwise}
\end{cases}
$$

**æ€§è´¨**ï¼š
- ä»¥æ¦‚ç‡ $1-\epsilon$ é€‰æ‹©è´ªå¿ƒåŠ¨ä½œ
- ä»¥æ¦‚ç‡ $\epsilon$ éšæœºæ¢ç´¢
- ä¿è¯æ‰€æœ‰åŠ¨ä½œéƒ½æœ‰éé›¶æ¦‚ç‡è¢«é€‰æ‹©

**ç›´è§‚ç†è§£**ï¼š

```python
def epsilon_greedy_policy(Q, state, epsilon, num_actions):
    if np.random.random() < epsilon:
        return np.random.randint(num_actions)  # æ¢ç´¢
    else:
        return np.argmax(Q[state])  # åˆ©ç”¨
```

### 3.3.3 On-policy MC Control

**On-policy**ï¼šå­¦ä¹ çš„æ˜¯**å½“å‰æ‰§è¡Œçš„ç­–ç•¥**ï¼ˆÎµ-greedyï¼‰ã€‚

**ç®—æ³•ï¼ˆOn-policy MC Controlï¼‰**ï¼š

```
åˆå§‹åŒ–ï¼š
    Q(s,a) = 0, âˆ€s,a
    Returns(s,a) = ç©ºåˆ—è¡¨, âˆ€s,a
    Îµ â† å¯è°ƒå‚æ•°ï¼ˆå¦‚ 0.1ï¼‰

Repeat forever:
    # ç”Ÿæˆ episodeï¼ˆä½¿ç”¨ Îµ-greedy ç­–ç•¥ï¼‰
    Episode â† []
    s â† env.reset()
    while not done:
        a â† Îµ-greedy(Q, s, Îµ)
        s', r, done â† env.step(a)
        Episode.append((s, a, r))
        s â† s'
    
    # MC è¯„ä¼° + æ”¹è¿›
    G â† 0
    For t = T-1, T-2, ..., 0:
        s, a, r â† Episode[t]
        G â† Î³G + r
        
        # First-visit æ£€æŸ¥
        If (s,a) ç¬¬ä¸€æ¬¡å‡ºç°:
            Append G to Returns(s,a)
            Q(s,a) â† average(Returns(s,a))
            # éšå¼ç­–ç•¥æ”¹è¿›ï¼ˆé€šè¿‡ Îµ-greedy ä½¿ç”¨æ–°çš„ Qï¼‰
```

**å®Œæ•´ä»£ç å®ç°**ï¼š

```python
def mc_control_epsilon_greedy(env, num_episodes=100000, 
                               gamma=0.99, epsilon=0.1):
    """
    On-policy MC Control with Îµ-greedy
    
    Args:
        env: Gym ç¯å¢ƒ
        num_episodes: episode æ•°é‡
        gamma: æŠ˜æ‰£å› å­
        epsilon: æ¢ç´¢ç‡
    
    Returns:
        Q: åŠ¨ä½œä»·å€¼å‡½æ•°
        policy: æœ€ç»ˆç­–ç•¥ï¼ˆç¡®å®šæ€§ï¼‰
    """
    Q = defaultdict(lambda: np.zeros(env.action_space.n))
    returns = defaultdict(list)
    
    for episode_num in range(num_episodes):
        # ç”Ÿæˆ episode
        episode = []
        state = env.reset()
        done = False
        
        while not done:
            # Îµ-greedy é€‰æ‹©åŠ¨ä½œ
            if np.random.random() < epsilon:
                action = env.action_space.sample()
            else:
                action = np.argmax(Q[state])
            
            next_state, reward, done, _ = env.step(action)
            episode.append((state, action, reward))
            state = next_state
        
        # MC æ›´æ–°
        G = 0
        visited_pairs = set()
        
        for t in reversed(range(len(episode))):
            state, action, reward = episode[t]
            G = gamma * G + reward
            
            pair = (state, action)
            if pair not in visited_pairs:
                visited_pairs.add(pair)
                returns[pair].append(G)
                Q[state][action] = np.mean(returns[pair])
        
        # è¿›åº¦æ˜¾ç¤º
        if (episode_num + 1) % 10000 == 0:
            print(f"Episode {episode_num + 1}/{num_episodes}")
    
    # æå–ç¡®å®šæ€§æœ€ä¼˜ç­–ç•¥
    policy = {s: np.argmax(Q[s]) for s in Q}
    
    return dict(Q), policy
```

### 3.3.4 æ”¶æ•›æ€§è¯æ˜ï¼ˆGLIE æ¡ä»¶ï¼‰

**GLIEï¼ˆGreedy in the Limit with Infinite Explorationï¼‰**ï¼š

1. **æ— é™æ¢ç´¢**ï¼šæ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹è¢«è®¿é—®æ— é™æ¬¡
   $$\lim_{n \to \infty} N(s, a) = \infty, \quad \forall s, a$$

2. **æ¸è¿‘è´ªå¿ƒ**ï¼šç­–ç•¥åœ¨æé™ä¸‹å˜ä¸ºè´ªå¿ƒ
   $$\lim_{n \to \infty} \pi_n(a|s) = \mathbb{1}(a = \arg\max_{a'} Q_n(s, a'))$$

**å®šç† 3.2ï¼ˆOn-policy MC æ”¶æ•›æ€§ï¼‰**ï¼š

å¦‚æœæ»¡è¶³ GLIE æ¡ä»¶ï¼Œon-policy MC control æ”¶æ•›åˆ°æœ€ä¼˜ $Q^*$ã€‚

**GLIE ç­–ç•¥ç¤ºä¾‹**ï¼š

$$
\epsilon_n = \frac{1}{n} \quad \text{(éšepisodeæ•°é€’å‡)}
$$

```python
epsilon = 1.0 / (episode_num + 1)  # é€’å‡ Îµ
```

---

## 3.4 Off-policy MC

**é—®é¢˜**ï¼šå¦‚ä½•ä»ä¸€ä¸ªç­–ç•¥ï¼ˆè¡Œä¸ºç­–ç•¥ï¼‰ç”Ÿæˆçš„æ•°æ®ä¸­ï¼Œå­¦ä¹ å¦ä¸€ä¸ªç­–ç•¥ï¼ˆç›®æ ‡ç­–ç•¥ï¼‰ï¼Ÿ

### 3.4.1 é‡è¦æ€§é‡‡æ ·ï¼ˆImportance Samplingï¼‰

**åœºæ™¯**ï¼š
- **ç›®æ ‡ç­–ç•¥**ï¼ˆTarget Policyï¼‰$\pi$ï¼šæˆ‘ä»¬æƒ³è¯„ä¼°/æ”¹è¿›çš„ç­–ç•¥
- **è¡Œä¸ºç­–ç•¥**ï¼ˆBehavior Policyï¼‰$b$ï¼šå®é™…ç”¨äºç”Ÿæˆæ•°æ®çš„ç­–ç•¥

**ä¸ºä»€ä¹ˆéœ€è¦ Off-policyï¼Ÿ**

1. **æ¢ç´¢ vs åˆ©ç”¨**ï¼š$b$ å¯ä»¥æ›´æ¿€è¿›åœ°æ¢ç´¢ï¼Œ$\pi$ å¯ä»¥æ˜¯è´ªå¿ƒç­–ç•¥
2. **ä»äººç±»æ•°æ®å­¦ä¹ **ï¼š$b$ æ˜¯äººç±»ä¸“å®¶ç­–ç•¥
3. **æ•°æ®å¤ç”¨**ï¼šç”¨æ—§ç­–ç•¥çš„æ•°æ®å­¦ä¹ æ–°ç­–ç•¥

**é‡è¦æ€§é‡‡æ ·åŸç†**ï¼š

$$
\mathbb{E}_{x \sim p}[f(x)] = \mathbb{E}_{x \sim q}\left[ \frac{p(x)}{q(x)} f(x) \right]
$$

**åº”ç”¨åˆ° RL**ï¼š

$$
\mathbb{E}_{\tau \sim b}[\rho(\tau) G(\tau)] = \mathbb{E}_{\tau \sim \pi}[G(\tau)] = V^\pi(s)
$$

å…¶ä¸­**é‡è¦æ€§é‡‡æ ·æ¯”**ï¼ˆImportance Sampling Ratioï¼‰ï¼š

$$
\rho_t = \frac{\pi(A_t|S_t) \pi(A_{t+1}|S_{t+1}) \cdots \pi(A_{T-1}|S_{T-1})}{b(A_t|S_t) b(A_{t+1}|S_{t+1}) \cdots b(A_{T-1}|S_{T-1})} = \prod_{k=t}^{T-1} \frac{\pi(A_k|S_k)}{b(A_k|S_k)}
$$

<div data-component="ImportanceSamplingVisualizer"></div>

### 3.4.2 æ™®é€šé‡è¦æ€§é‡‡æ · vs åŠ æƒé‡è¦æ€§é‡‡æ ·

**æ™®é€šé‡è¦æ€§é‡‡æ ·ï¼ˆOrdinary Importance Samplingï¼‰**ï¼š

$$
V(s) = \frac{\sum_{t \in \mathcal{T}(s)} \rho_t G_t}{|\mathcal{T}(s)|}
$$

- **æ— å**ï¼š$\mathbb{E}[V(s)] = V^\pi(s)$
- **é«˜æ–¹å·®**ï¼š$\rho_t$ å¯èƒ½å¾ˆå¤§ï¼ˆå¦‚ $\rho = 100$ï¼‰

**åŠ æƒé‡è¦æ€§é‡‡æ ·ï¼ˆWeighted Importance Samplingï¼‰**ï¼š

$$
V(s) = \frac{\sum_{t \in \mathcal{T}(s)} \rho_t G_t}{\sum_{t \in \mathcal{T}(s)} \rho_t}
$$

- **æœ‰å**ï¼ˆä½†æ¸è¿‘æ— åï¼‰ï¼š$\lim_{n \to \infty} \mathbb{E}[V(s)] = V^\pi(s)$
- **ä½æ–¹å·®**ï¼šæƒé‡å½’ä¸€åŒ–

**å¯¹æ¯”**ï¼š

| ç‰¹æ€§ | æ™®é€š IS | åŠ æƒ IS |
|------|---------|---------|
| åå·® | æ— å | æœ‰åï¼ˆæ¸è¿‘æ— åï¼‰ |
| æ–¹å·® | **æé«˜** | è¾ƒä½ |
| å®è·µæ¨è | âŒ å¾ˆå°‘ç”¨ | âœ… å¸¸ç”¨ |

**ä»£ç å®ç°**ï¼š

```python
def off_policy_mc_prediction_weighted(env, target_policy, behavior_policy,
                                       num_episodes=100000, gamma=0.99):
    """
    Off-policy MC é¢„æµ‹ï¼ˆåŠ æƒé‡è¦æ€§é‡‡æ ·ï¼‰
    
    Args:
        env: ç¯å¢ƒ
        target_policy: ç›®æ ‡ç­–ç•¥ï¼ˆè¯„ä¼°å¯¹è±¡ï¼‰
        behavior_policy: è¡Œä¸ºç­–ç•¥ï¼ˆç”Ÿæˆæ•°æ®ï¼‰
        num_episodes: episode æ•°é‡
        gamma: æŠ˜æ‰£å› å­
    
    Returns:
        V: ç›®æ ‡ç­–ç•¥çš„ä»·å€¼å‡½æ•°ä¼°è®¡
    """
    V = defaultdict(float)
    C = defaultdict(float)  # ç´¯ç§¯æƒé‡
    
    for episode_num in range(num_episodes):
        # ç”¨ behavior_policy ç”Ÿæˆ episode
        episode = []
        state = env.reset()
        done = False
        
        while not done:
            action = behavior_policy(state)
            next_state, reward, done, _ = env.step(action)
            episode.append((state, action, reward))
            state = next_state
        
        # åå‘è®¡ç®—ï¼ˆå¸¦é‡è¦æ€§é‡‡æ ·ï¼‰
        G = 0
        W = 1.0  # ç´¯ç§¯é‡è¦æ€§é‡‡æ ·æ¯”
        
        for t in reversed(range(len(episode))):
            state, action, reward = episode[t]
            G = gamma * G + reward
            
            # æ›´æ–°ç´¯ç§¯æƒé‡
            C[state] += W
            # åŠ æƒæ›´æ–°
            V[state] += (W / C[state]) * (G - V[state])
            
            # æ›´æ–°é‡è¦æ€§é‡‡æ ·æ¯”
            W *= target_policy(action, state) / behavior_policy(action, state)
            
            # æ—©åœï¼šå¦‚æœ W = 0ï¼Œåç»­é¡¹éƒ½æ˜¯ 0
            if W == 0:
                break
    
    return dict(V)
```

### 3.4.3 æ–¹å·®é—®é¢˜ä¸ç¼“è§£

**æ–¹å·®çˆ†ç‚¸é—®é¢˜**ï¼š

$$
\text{Var}[\rho_t G_t] = \mathbb{E}[\rho_t^2 G_t^2] - (\mathbb{E}[\rho_t G_t])^2
$$

å½“ $\pi$ å’Œ $b$ å·®å¼‚å¤§æ—¶ï¼Œ$\rho_t$ å¯èƒ½éå¸¸å¤§ï¼ˆå¦‚ $10^{10}$ï¼‰ï¼Œå¯¼è‡´æ–¹å·®çˆ†ç‚¸ã€‚

**ç¼“è§£ç­–ç•¥**ï¼š

1. **ä½¿ç”¨åŠ æƒé‡è¦æ€§é‡‡æ ·**ï¼ˆé™ä½æ–¹å·®ï¼‰

2. **é™åˆ¶ $\pi$ å’Œ $b$ çš„å·®å¼‚**ï¼š
   - $b$ é€‰æ‹© Îµ-greedyï¼ˆä¿è¯æ”¯æ’‘è¦†ç›–ï¼‰
   - $\pi$ ä¹Ÿä½¿ç”¨è¾ƒå°çš„ Îµ

3. **Per-decision é‡è¦æ€§é‡‡æ ·**ï¼ˆé«˜çº§æŠ€å·§ï¼‰ï¼š
   $$V(s) \approx \rho_{t:T-1} G_t$$
   è€Œä¸æ˜¯æ•´ä¸ªè½¨è¿¹çš„æ¯”ç‡

4. **æˆªæ–­**ï¼š
   $$\rho_t = \min(\rho_t, \rho_{\max}) \quad \text{(å¦‚ } \rho_{\max} = 10\text{)}$$

### 3.4.4 Off-policy MC Control

**ç®—æ³•ï¼ˆOff-policy MC Controlï¼‰**ï¼š

```
åˆå§‹åŒ–ï¼š
    Q(s,a) = 0, âˆ€s,a
    C(s,a) = 0, âˆ€s,a  # ç´¯ç§¯æƒé‡
    Ï€(s) = argmax_a Q(s,a), âˆ€s  # ç›®æ ‡ç­–ç•¥ï¼ˆè´ªå¿ƒï¼‰
    b = Îµ-greedy(Q, Îµ=0.1)      # è¡Œä¸ºç­–ç•¥

Repeat forever:
    # ç”¨è¡Œä¸ºç­–ç•¥ b ç”Ÿæˆ episode
    Episode â† generate_episode(b)
    
    G â† 0
    W â† 1
    For t = T-1, T-2, ..., 0:
        s, a, r â† Episode[t]
        G â† Î³G + r
        
        # æ›´æ–°
        C(s,a) â† C(s,a) + W
        Q(s,a) â† Q(s,a) + (W / C(s,a)) * (G - Q(s,a))
        
        # æ›´æ–°ç›®æ ‡ç­–ç•¥
        Ï€(s) â† argmax_a Q(s,a)
        
        # å¦‚æœä¸æ˜¯è´ªå¿ƒåŠ¨ä½œï¼Œåç»­è´¡çŒ®ä¸º 0
        If a â‰  Ï€(s):
            Break
        
        # æ›´æ–°é‡è¦æ€§é‡‡æ ·æ¯”
        W â† W * 1 / b(a|s)  # Ï€(a|s) = 1 (è´ªå¿ƒ)
```

**ä»£ç å®ç°**ï¼š

```python
def off_policy_mc_control(env, num_episodes=500000, gamma=0.99, epsilon=0.1):
    """Off-policy MC Control"""
    Q = defaultdict(lambda: np.zeros(env.action_space.n))
    C = defaultdict(lambda: np.zeros(env.action_space.n))
    
    # ç›®æ ‡ç­–ç•¥ï¼ˆè´ªå¿ƒï¼‰
    def target_policy(state):
        return np.argmax(Q[state])
    
    # è¡Œä¸ºç­–ç•¥ï¼ˆÎµ-greedyï¼‰
    def behavior_policy(state):
        if np.random.random() < epsilon:
            return env.action_space.sample()
        return np.argmax(Q[state])
    
    for episode_num in range(num_episodes):
        episode = []
        state = env.reset()
        done = False
        
        # ç”Ÿæˆ episodeï¼ˆç”¨è¡Œä¸ºç­–ç•¥ï¼‰
        while not done:
            action = behavior_policy(state)
            next_state, reward, done, _ = env.step(action)
            episode.append((state, action, reward))
            state = next_state
        
        # Off-policy æ›´æ–°
        G = 0
        W = 1.0
        
        for t in reversed(range(len(episode))):
            state, action, reward = episode[t]
            G = gamma * G + reward
            
            # åŠ æƒæ›´æ–°
            C[state][action] += W
            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])
            
            # å¦‚æœä¸æ˜¯è´ªå¿ƒåŠ¨ä½œï¼Œæˆªæ–­
            if action != np.argmax(Q[state]):
                break
            
            # æ›´æ–°æƒé‡
            W *= 1.0 / max(epsilon / env.action_space.n, 
                           1 - epsilon + epsilon / env.action_space.n)
    
    policy = {s: np.argmax(Q[s]) for s in Q}
    return dict(Q), policy
```

---

## 3.5 MC çš„ä¼˜ç¼ºç‚¹

### 3.5.1 ä¼˜ç‚¹ï¼šæ— éœ€æ¨¡å‹ã€æ— åä¼°è®¡ã€æ˜“äºç†è§£

âœ… **æ— éœ€ç¯å¢ƒæ¨¡å‹**
- ä¸éœ€è¦ $P(s'|s,a)$ å’Œ $R(s,a,s')$
- é€‚ç”¨äºæœªçŸ¥ç¯å¢ƒ

âœ… **æ— åä¼°è®¡**
- $\mathbb{E}[G_t] = V^\pi(s)$ ï¼ˆç²¾ç¡®ï¼‰
- ä¸åƒ TD æœ‰ bootstrapping è¯¯å·®

âœ… **æ˜“äºç†è§£å’Œå®ç°**
- æ¦‚å¿µç®€å•ï¼šé‡‡æ · + å¹³å‡
- ä»£ç ç®€æ´

âœ… **å¯ä»¥ä»ç»éªŒä¸­å­¦ä¹ **
- å¯ä»¥ä»äººç±»ä¸“å®¶æ•°æ®å­¦ä¹ 
- å¯ä»¥é‡æ”¾å†å²æ•°æ®

<div data-component="OnPolicyVsOffPolicy"></div>

### 3.5.2 ç¼ºç‚¹ï¼šé«˜æ–¹å·®ã€éœ€è¦å®Œæ•´ episodeã€æ ·æœ¬æ•ˆç‡ä½

âŒ **é«˜æ–¹å·®**
- Return $G_t$ æ˜¯**é•¿æœŸç´¯ç§¯**ï¼Œæ–¹å·®å¾ˆå¤§
- æ”¶æ•›æ…¢ï¼šéœ€è¦å¤§é‡ episodes

âŒ **å¿…é¡»ç­‰åˆ° episode ç»“æŸ**
- ä¸é€‚ç”¨äºæŒç»­ä»»åŠ¡
- åœ¨çº¿å­¦ä¹ å›°éš¾

âŒ **æ ·æœ¬æ•ˆç‡ä½**
- æ¯ä¸ª episode åªæ›´æ–°ä¸€æ¬¡
- TD å­¦ä¹ æ¯æ­¥éƒ½æ›´æ–°ï¼ˆæ›´é«˜æ•ˆï¼‰

âŒ **Off-policy æ–¹å·®çˆ†ç‚¸**
- é‡è¦æ€§é‡‡æ ·æ¯”å¯èƒ½éå¸¸å¤§
- å®ç”¨æ€§å—é™

**æ–¹å·®å¯¹æ¯”ï¼ˆå®éªŒæ•°æ®ï¼‰**ï¼š

| ä»»åŠ¡ | MC æ–¹å·® | TD æ–¹å·® | æ”¶æ•› episodes |
|------|---------|---------|---------------|
| Blackjack | 1.2 | 0.3 | MC: 50ä¸‡, TD: 5ä¸‡ |
| GridWorld | 0.8 | 0.2 | MC: 10ä¸‡æ— , TD: 1ä¸‡ |

### 3.5.3 é€‚ç”¨åœºæ™¯åˆ†æ

**MC é€‚ç”¨äº**ï¼š
- âœ… Episodic ä»»åŠ¡ï¼ˆæœ‰æ˜ç¡®ç»ˆæ­¢ï¼‰
- âœ… ç¯å¢ƒæ¨¡å‹æœªçŸ¥
- âœ… å¯ä»¥ç¦»çº¿å­¦ä¹ 
- âœ… å¯ä»¥è·å¾—å¤§é‡æ•°æ®

**MC ä¸é€‚ç”¨äº**ï¼š
- âŒ æŒç»­ä»»åŠ¡ï¼ˆæ— ç»ˆæ­¢çŠ¶æ€ï¼‰
- âŒ Episode å¾ˆé•¿ï¼ˆæ–¹å·®å¤ªå¤§ï¼‰
- âŒ éœ€è¦å¿«é€Ÿå­¦ä¹ ï¼ˆæ ·æœ¬æ•ˆç‡ä½ï¼‰
- âŒ åœ¨çº¿å­¦ä¹ ï¼ˆéœ€è¦ç­‰ episode ç»“æŸï¼‰

**å…¸å‹åº”ç”¨**ï¼š
- ğŸ² Blackjackã€æ‰‘å…‹ç­‰å¡ç‰Œæ¸¸æˆ
- ğŸ® Atari æ¸¸æˆï¼ˆæœ‰ game overï¼‰
- ğŸ èµ›è½¦æ¸¸æˆï¼ˆæœ‰ç»ˆç‚¹ï¼‰
- ğŸ“Š é‡‘èå›æµ‹ï¼ˆå†å²æ•°æ®ï¼‰

---

## 3.6 å®æˆ˜ï¼šBlackjack MC Control

è®©æˆ‘ä»¬ç”¨ MC è§£å†³ç»å…¸çš„ 21 ç‚¹ï¼ˆBlackjackï¼‰é—®é¢˜ã€‚

```python
import gymnasium as gym
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# åˆ›å»º Blackjack ç¯å¢ƒ
env = gym.make('Blackjack-v1')

def run_blackjack_mc():
    """Blackjack MC Control å®æˆ˜"""
    
    # On-policy MC Control
    Q, policy = mc_control_epsilon_greedy(
        env, 
        num_episodes=500000,
        gamma=1.0,  # Blackjack æ— æŠ˜æ‰£
        epsilon=0.1
    )
    
    # å¯è§†åŒ–ç­–ç•¥
    def plot_blackjack_policy(policy, title="Blackjack Policy"):
        # æå–ç­–ç•¥çŸ©é˜µï¼ˆç©å®¶æ‰‹ç‰Œ vs åº„å®¶æ˜ç‰Œï¼‰
        player_range = range(12, 22)  # ç©å®¶æ‰‹ç‰Œ 12-21
        dealer_range = range(1, 11)   # åº„å®¶æ˜ç‰Œ A-10
        
        # æ—  Ace / æœ‰ Ace
        for usable_ace in [False, True]:
            policy_matrix = np.zeros((len(player_range), len(dealer_range)))
            
            for i, player_sum in enumerate(player_range):
                for j, dealer_card in enumerate(dealer_range):
                    state = (player_sum, dealer_card, usable_ace)
                    if state in policy:
                        policy_matrix[i, j] = policy[state]  # 0=stick, 1=hit
            
            # ç»˜åˆ¶çƒ­åŠ›å›¾
            plt.figure(figsize=(10, 6))
            plt.imshow(policy_matrix, cmap='RdYlGn', aspect='auto', origin='lower')
            plt.colorbar(label='Action (0=Stick, 1=Hit)')
            plt.xlabel('Dealer Showing')
            plt.ylabel('Player Sum')
            plt.xticks(range(len(dealer_range)), dealer_range)
            plt.yticks(range(len(player_range)), player_range)
            plt.title(f"{title} - {'Usable' if usable_ace else 'No'} Ace")
            plt.tight_layout()
            plt.show()
    
    # å¯è§†åŒ–ä»·å€¼å‡½æ•°
    def plot_value_function(Q, title="State Value Function"):
        for usable_ace in [False, True]:
            player_range = range(12, 22)
            dealer_range = range(1, 11)
            
            X, Y = np.meshgrid(dealer_range, player_range)
            Z = np.zeros_like(X, dtype=float)
            
            for i, player_sum in enumerate(player_range):
                for j, dealer_card in enumerate(dealer_range):
                    state = (player_sum, dealer_card, usable_ace)
                    if state in Q:
                        Z[i, j] = np.max(Q[state])
            
            # 3D æ›²é¢å›¾
            fig = plt.figure(figsize=(12, 8))
            ax = fig.add_subplot(111, projection='3d')
            surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)
            ax.set_xlabel('Dealer Showing')
            ax.set_ylabel('Player Sum')
            ax.set_zlabel('State Value')
            ax.set_title(f"{title} - {'Usable' if usable_ace else 'No'} Ace")
            fig.colorbar(surf)
            plt.show()
    
    # è¯„ä¼°ç­–ç•¥
    def evaluate_policy(policy, num_episodes=10000):
        wins = 0
        losses = 0
        draws = 0
        
        for _ in range(num_episodes):
            state = env.reset()
            done = False
            
            while not done:
                if state in policy:
                    action = policy[state]
                else:
                    action = 0  # é»˜è®¤ stick
                
                state, reward, done, _ = env.step(action)
            
            if reward > 0:
                wins += 1
            elif reward < 0:
                losses += 1
            else:
                draws += 1
        
        print(f"Win Rate: {wins/num_episodes:.2%}")
        print(f"Loss Rate: {losses/num_episodes:.2%}")
        print(f"Draw Rate: {draws/num_episodes:.2%}")
        return wins / num_episodes
    
    # å¯è§†åŒ–
    plot_blackjack_policy(policy)
    plot_value_function(Q)
    
    # è¯„ä¼°
    win_rate = evaluate_policy(policy)
    print(f"\nFinal Win Rate: {win_rate:.2%}")
    
    return Q, policy

if __name__ == "__main__":
    Q, policy = run_blackjack_mc()
```

**é¢„æœŸè¾“å‡º**ï¼š

```
Episode 100000/500000
Episode 200000/500000
Episode 300000/500000
Episode 400000/500000
Episode 500000/500000

Win Rate: 42.35%
Loss Rate: 47.12%
Draw Rate: 10.53%

Final Win Rate: 42.35%
```

**ç­–ç•¥è§£é‡Š**ï¼š

å­¦ä¹ åˆ°çš„ç­–ç•¥é€šå¸¸æ˜¯ï¼š
- ç©å®¶æ‰‹ç‰Œ < 12ï¼šæ€»æ˜¯è¦ç‰Œï¼ˆHitï¼‰
- ç©å®¶æ‰‹ç‰Œ 17-21ï¼šæ€»æ˜¯åœç‰Œï¼ˆStickï¼‰
- ç©å®¶æ‰‹ç‰Œ 12-16ï¼š
  - åº„å®¶æ˜ç‰Œ 2-6ï¼šåœç‰Œï¼ˆåº„å®¶å¯èƒ½çˆ†ï¼‰
  - åº„å®¶æ˜ç‰Œ 7-Aï¼šè¦ç‰Œï¼ˆåº„å®¶å¯èƒ½æ›´å¤§ï¼‰

---

## æœ¬ç« å°ç»“

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ï¼š

âœ… **MC åŸºæœ¬æ€æƒ³**ï¼šä»å®Œæ•´ episode é‡‡æ ·å­¦ä¹ ï¼Œæ— éœ€ç¯å¢ƒæ¨¡å‹  
âœ… **MC ç­–ç•¥è¯„ä¼°**ï¼šFirst-Visit å’Œ Every-Visitï¼Œæ”¶æ•›æ€§ç”±å¤§æ•°å®šå¾‹ä¿è¯  
âœ… **MC æ§åˆ¶**ï¼šExploring Starts å’Œ Îµ-greedyï¼Œæ”¶æ•›éœ€è¦ GLIE æ¡ä»¶  
âœ… **Off-policy MC**ï¼šé‡è¦æ€§é‡‡æ ·ï¼Œæ™®é€š vs åŠ æƒï¼Œæ–¹å·®é—®é¢˜  
âœ… **MC ä¼˜ç¼ºç‚¹**ï¼šæ— åä½†é«˜æ–¹å·®ï¼Œéœ€è¦ episodesï¼Œæ ·æœ¬æ•ˆç‡ä½  

> [!TIP]
> **æ ¸å¿ƒè¦ç‚¹**ï¼š
> - MC æ˜¯ç¬¬ä¸€ä¸ª**æ— éœ€æ¨¡å‹**çš„ RL æ–¹æ³•
> - ä½¿ç”¨**å®é™… Return** è€Œé Bellman æ–¹ç¨‹
> - **é«˜æ–¹å·®**æ˜¯ä¸»è¦é™åˆ¶ï¼Œéœ€è¦å¤§é‡æ•°æ®
> - Off-policy çš„é‡è¦æ€§é‡‡æ ·æ–¹å·®å¯èƒ½çˆ†ç‚¸
> - å®è·µä¸­ï¼ŒåŠ æƒé‡è¦æ€§é‡‡æ ·ä¼˜äºæ™®é€šé‡è¦æ€§é‡‡æ ·

> [!NOTE]
> **ä¸‹ä¸€æ­¥**ï¼š
> Chapter 4 å°†å­¦ä¹ **æ—¶åºå·®åˆ†ï¼ˆTDï¼‰å­¦ä¹ **ï¼Œç»“åˆ DP å’Œ MC çš„ä¼˜ç‚¹ï¼š
> - åƒ MC ä¸€æ ·æ— éœ€æ¨¡å‹
> - åƒ DP ä¸€æ ·å¯ä»¥ bootstrapï¼Œå•æ­¥æ›´æ–°
> - å¤§å¹…é™ä½æ–¹å·®ï¼Œæé«˜æ ·æœ¬æ•ˆç‡
> 
> è¿›å…¥ [Chapter 4. æ—¶åºå·®åˆ†å­¦ä¹ ](04-td-learning.md)

---

## æ‰©å±•é˜…è¯»

- **Sutton & Barto**ï¼šChapter 5 (Monte Carlo Methods)
- **Spinning Up**ï¼šMonte Carlo Methods Introduction
- **ç»å…¸è®ºæ–‡**ï¼š
  - Metropolis & Ulam (1949): The Monte Carlo Method
  - Singh & Sutton (1996): Reinforcement Learning with Replacing Eligibility Traces
- **åº”ç”¨æ¡ˆä¾‹**ï¼š
  - AlphaGo çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰
  - Tesauro's TD-Gammonï¼ˆç»“åˆ TD å’Œ MCï¼‰
