---
title: "é™„å½•ï¼šæ·±åº¦å¼ºåŒ–å­¦ä¹ å‚è€ƒæ‰‹å†Œ"
description: "æ•°å­¦æ¨å¯¼ã€ç¯å¢ƒé€ŸæŸ¥è¡¨ã€é€šç”¨è®­ç»ƒæ¡†æ¶ã€è°ƒè¯•åæ¨¡å¼ä¸è®ºæ–‡å¯¼è¯»"
date: "2026-01-30"
---

# é™„å½• (Appendices)

æœ¬é™„å½•æ—¨åœ¨æˆä¸ºå¼ºåŒ–å­¦ä¹ ç ”ç©¶è€…ä¸å·¥ç¨‹å¸ˆçš„æ¡ˆå¤´å‚è€ƒæ‰‹å†Œã€‚æˆ‘ä»¬ä¸ä»…æä¾›åŸºç¡€å…¬å¼ï¼Œè¿˜æ±‡é›†äº†å·¥ç¨‹å®è·µä¸­çš„â€œæš—çŸ¥è¯†â€ä¸æ ‡å‡†æ¨¡æ¿ã€‚

---

## Appendix A: æ•°å­¦åŸºç¡€ä¸æ¨å¯¼ (Math & Derivations)

### A.1 æ¦‚ç‡ä¸ç»Ÿè®¡

**1. å¸¸ç”¨åˆ†å¸ƒä¸ KL æ•£åº¦**
*   **é«˜æ–¯åˆ†å¸ƒ (Gaussian)**: $\mathcal{N}(\mu, \sigma^2)$ï¼ŒRL ä¸­å¸¸ç”¨äºè¿ç»­åŠ¨ä½œç­–ç•¥ $\pi_\theta(a|s)$ã€‚
    *   Log Probability: $\log \pi(a|s) = -\frac{1}{2}\log(2\pi\sigma^2) - \frac{(a-\mu)^2}{2\sigma^2}$ (PPO Loss è®¡ç®—å¸¸ç”¨)
*   **KL æ•£åº¦ (Kullback-Leibler Divergence)**: è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚ã€‚
    *   $\text{KL}(P || Q) = \mathbb{E}_{x \sim P} [\log \frac{P(x)}{Q(x)}]$
    *   ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒé—´çš„ KL: $\text{KL}(\mathcal{N}_0 || \mathcal{N}_1) = \log\frac{\sigma_1}{\sigma_0} + \frac{\sigma_0^2 + (\mu_0 - \mu_1)^2}{2\sigma_1^2} - \frac{1}{2}$

**2. æ¢¯åº¦ä¼°è®¡ (Gradient Estimation)**
*   **Score Function Estimator (REINFORCE)**:
    $$ \nabla_\theta \mathbb{E}_{x \sim p_\theta}[f(x)] = \mathbb{E}_{x \sim p_\theta}[f(x) \nabla_\theta \log p_\theta(x)] $$
    *   *ç›´è§‚ç†è§£*ï¼šå¦‚æœ $f(x)$ (å¥–åŠ±) é«˜ï¼Œå°±å¢åŠ è¯¥æ ·æœ¬ $x$ çš„æ¦‚ç‡å¯†åº¦ã€‚
    *   *æ¨å¯¼*: åˆ©ç”¨ $\nabla p_\theta = p_\theta \nabla \log p_\theta$ (Log-derivative trick)ã€‚

**3. é‡è¦æ€§é‡‡æ · (Importance Sampling)**
ç”¨äº Off-policy ç­–ç•¥è¯„ä¼°ï¼Œä¿®æ­£åˆ†å¸ƒåç§»ã€‚
$$ \mathbb{E}_{x \sim \text{target}}[f(x)] = \mathbb{E}_{x \sim \text{behavior}} \left[ \rho(x) f(x) \right], \quad \rho(x) = \frac{\pi_{\text{target}}(x)}{\pi_{\text{behavior}}(x)} $$
*   **æ³¨æ„**: å¦‚æœ $\rho(x)$ æ–¹å·®è¿‡å¤§ï¼Œä¼°è®¡ä¼šæå…¶ä¸ç¨³å®šï¼ˆPPO é‡‡ç”¨ Clip æœºåˆ¶æ¥ç¼“è§£æ­¤é—®é¢˜ï¼‰ã€‚

### A.2 è´å°”æ›¼æ–¹ç¨‹ä¸æ”¶æ•›æ€§

**1. Bellman ç®—å­ (Operator)**
å®šä¹‰ç®—å­ $\mathcal{T}^\pi$:
$$ (\mathcal{T}^\pi V)(s) = R(s, \pi(s)) + \gamma \sum_{s'} P(s'|s, \pi(s)) V(s') $$
*   **æ”¶ç¼©æ˜ å°„ (Contraction Mapping)**: $\mathcal{T}^\pi$ æ˜¯ $\gamma$-contractionï¼Œå³ $ ||\mathcal{T}U - \mathcal{T}V||_\infty \le \gamma ||U - V||_\infty $ã€‚
*   **ä¸åŠ¨ç‚¹å®šç†**: æ ¹æ® Banach Fixed Point Theoremï¼Œä»·å€¼è¿­ä»£å¿…æ”¶æ•›äºå”¯ä¸€ä¸åŠ¨ç‚¹ $V^\pi$ã€‚

---

## Appendix B: ç¯å¢ƒä¸ Benchmark é€ŸæŸ¥è¡¨

### B.1 ç»å…¸æ§åˆ¶ (Classic Control)

| ç¯å¢ƒ ID | è§‚æµ‹ç©ºé—´ (Obs) | åŠ¨ä½œç©ºé—´ (Action) | å¥–åŠ±èŒƒå›´ | è§£å†³æ ‡å‡† (Solved) | ç‰¹ç‚¹ |
| :--- | :--- | :--- | :--- | :--- | :--- |
| `CartPole-v1` | Box(4): [ä½ç½®, é€Ÿ, è§’, è§’é€Ÿ] | Discrete(2): [å·¦, å³] | +1/step (max 500) | 475 | å…¥é—¨æµ‹è¯•ï¼Œè°ƒè¯•å¿…ç”¨ |
| `Pendulum-v1` | Box(3): [cos, sin, dot] | Box(1):åŠ›çŸ© [-2, 2] | -(Î¸^2 + 0.1v^2 + 0.001u^2) | Approx -150 | è¿ç»­æ§åˆ¶å…¥é—¨ |
| `MountainCar-v0` | Box(2): [ä½ç½®, é€Ÿåº¦] | Discrete(3): [æ¨å·¦, ä¸åŠ¨, æ¨å³] | -1/step | -110 | ç¨€ç–å¥–åŠ±ï¼Œéœ€è¦æ¢ç´¢ |

### B.2 MuJoCo (v4)

æ‰€æœ‰ MuJoCo ç¯å¢ƒå‡ä¸ºè¿ç»­åŠ¨ä½œç©ºé—´ `Box(k)`ï¼ŒèŒƒå›´é€šå¸¸ä¸º `[-1, 1]`ã€‚

| ç¯å¢ƒ ID | Obs Dim | Action Dim | æè¿° | SOTA åˆ†æ•° (PPO) |
| :--- | :--- | :--- | :--- | :--- |
| `HalfCheetah-v4` | 17 | 6 | äºŒç»´çŒè±¹è·‘ | ~5000-8000 |
| `Hopper-v4` | 11 | 3 | å•è„šè·³ | ~3000 |
| `Walker2d-v4` | 17 | 6 | åŒè¶³è¡Œèµ° | ~4000-5000 |
| `Ant-v4` | 27 | 8 | å››è¶³èš‚èš | ~5000-6000 |
| `Humanoid-v4` | 376 | 17 | äººå½¢æœºå™¨äºº | ~6000+ (æéš¾) |

### B.3 è‡ªå®šä¹‰ç¯å¢ƒæ¨¡æ¿

åˆ›å»ºä¸€ä¸ªå…¼å®¹ Gymnasium çš„ç¯å¢ƒæ ‡å‡†æ¨¡æ¿ï¼š

```python
import gymnasium as gym
from gymnasium import spaces
import numpy as np

class CustomEnv(gym.Env):
    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 30}

    def __init__(self, render_mode=None):
        self.observation_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.action_space = spaces.Discrete(4)
        self.render_mode = render_mode
        self.state = None

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # 1. è®¾ç½®éšæœºç§å­
        self.state = self.np_random.uniform(low=0, high=1, size=(10,)).astype(np.float32)
        info = {}
        return self.state, info

    def step(self, action):
        # 2. çŠ¶æ€è½¬ç§»é€»è¾‘
        velocity = (action - 1.5) * 0.1
        self.state = np.clip(self.state + velocity, 0, 1)
        
        # 3. è®¡ç®—å¥–åŠ±
        reward = -np.sum((self.state - 0.5)**2)  # ç›®æ ‡æ˜¯0.5
        
        # 4. ç»ˆæ­¢æ¡ä»¶
        terminated = bool(np.abs(self.state[0] - 0.5) < 0.01)
        truncated = False # è¶…æ—¶æˆªæ–­
        
        return self.state, reward, terminated, truncated, {}
```

---

## Appendix C: é€šç”¨å·¥ç¨‹æ¡†æ¶ä¸ä»£ç ç‰‡æ®µ

### C.1 ç°ä»£ RL è®­ç»ƒå™¨æ¨¡æ¿ (The "Trainer" Pattern)

ä¸ä¾èµ–ç¬¬ä¸‰æ–¹åº“çš„è½»é‡çº§æ¡†æ¶ç»“æ„å»ºè®®ï¼š

```python
class RLTrainer:
    def __init__(self, env, agent, config):
        self.env = env
        self.agent = agent
        self.buffer = ReplayBuffer(config.capacity)
        self.logger = SummaryWriter(config.log_dir)  # TensorBoard
        self.steps = 0

    def collect_rollouts(self, num_steps):
        """ä¸ç¯å¢ƒäº¤äº’å¹¶å­˜å…¥ Buffer"""
        obs, _ = self.env.reset()
        for _ in range(num_steps):
            with torch.no_grad():
                action = self.agent.select_action(obs)
            
            next_obs, reward, term, trunc, _ = self.env.step(action)
            done = term or trunc
            
            # å…³é”®ï¼šå¤„ç† Terminated æ—¶çš„ next_obs
            real_next_obs = next_obs.copy()
            if trunc: # å¦‚æœæ˜¯è¶…æ—¶ï¼Œnext_obs æ˜¯çœŸå®çš„ï¼›å¦‚æœæ˜¯ Terminatedï¼Œå¯èƒ½éœ€ç‰¹æ®Šå¤„ç†
                pass 

            self.buffer.add(obs, action, reward, real_next_obs, term)
            obs = next_obs
            if done: obs, _ = self.env.reset()

    def train_step(self):
        """ä» Buffer é‡‡æ ·å¹¶æ›´æ–°"""
        batch = self.buffer.sample(self.config.batch_size)
        loss_info = self.agent.update(batch)
        
        # Logging
        if self.steps % 100 == 0:
            for k, v in loss_info.items():
                self.logger.add_scalar(f"train/{k}", v, self.steps)
```

### C.2 GAE (Generalized Advantage Estimation) å®ç°

è¿™æ˜¯ä¸€ä¸ªææ˜“å†™é”™çš„å…³é”®å‡½æ•°ï¼š

```python
def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):
    """
    rewards: [T]
    values: [T+1]  (åŒ…å«æœ€åä¸€ä¸ªçŠ¶æ€çš„ V(s'))
    dones: [T]
    """
    gae = 0
    returns = []
    
    # é€†åºè®¡ç®—
    for step in reversed(range(len(rewards))):
        # delta = r + gamma * V(s') * (1-d) - V(s)
        delta = rewards[step] + gamma * values[step + 1] * (1 - dones[step]) - values[step]
        
        # gae = delta + gamma * lambda * (1-d) * gae_next
        gae = delta + gamma * lam * (1 - dones[step]) * gae
        
        # Return = V(s) + GAE = Q_target
        returns.insert(0, gae + values[step])
        
    return torch.tensor(returns)
```

---

## Appendix D: è°ƒè¯•ä¸å¸¸è§åæ¨¡å¼ (Anti-Patterns)

### ğŸ’€ D.1 å¸¸è§è‡´å‘½é”™è¯¯ (Deadly Bugs)

1.  **å¿˜è®° `optimizer.zero_grad()`**:
    *   *ç°è±¡*: Loss éœ‡è¡æˆ–å‘æ•£ã€‚
    *   *åŸå› *: PyTorch é»˜è®¤ç´¯åŠ æ¢¯åº¦ï¼Œå¯¼è‡´æ¢¯åº¦å€¼åœ¨å‡ ä¸ª Batch åå˜å¾—å·¨å¤§ã€‚
    
2.  **Done Flag é”™è¯¯**:
    *   *é”™è¯¯å†™æ³•*: `target = r + gamma * max_q * (1 - done)` ä¸åŒºåˆ† `TimeLimit`ã€‚
    *   *æ­£ç¡®å†™æ³•*: `truncated` (è¶…æ—¶) ä¸åº”è¯¥è¢«è§†ä¸ºçœŸæ­£çš„ç»“æŸï¼ˆçŠ¶æ€ä»·å€¼ä¸ä¸º0ï¼‰ï¼Œåªæœ‰ `terminated` (å¤±è´¥/æˆåŠŸ) æ‰æ˜¯ã€‚
    *   *ä¿®æ­£*: `mask = 1 - terminated` (å¿½ç•¥ truncated)ã€‚

3.  **Softmax ç»´åº¦é”™è¯¯**:
    *   *é”™è¯¯*: `F.softmax(logits)` (é»˜è®¤ dim=Noneï¼Œæ—§ç‰ˆå¯èƒ½è­¦å‘Š)ã€‚
    *   *æ­£ç¡®*: `F.softmax(logits, dim=-1)`ã€‚

4.  **Observation æœªå½’ä¸€åŒ–**:
    *   *ç°è±¡*: è®­ç»ƒæå…¶ç¼“æ…¢ï¼ŒLoss å¾ˆå¤§ã€‚
    *   *ä¿®æ­£*: å°†å›¾åƒé™¤ä»¥ 255.0ï¼Œæˆ–å¯¹è¿ç»­çŠ¶æ€è¿›è¡Œ Standard Scaling (å‡å‡å€¼é™¤æ–¹å·®)ã€‚

### ğŸ” D.2 æ€§èƒ½è¯Šæ–­æµç¨‹

1.  **Check 0**: éšæœºç­–ç•¥çš„è¡¨ç°æ˜¯å¤šå°‘ï¼Ÿï¼ˆä½œä¸º Baselineï¼‰ã€‚
2.  **Check 1**: èƒ½å¦è¿‡æ‹Ÿåˆä¸€ä¸ª Episodeï¼Ÿï¼ˆè®© Batch Size = Episode Lengthï¼Œé‡å¤è®­ç»ƒåŒä¸€æ•°æ®ï¼Œçœ‹ Loss æ˜¯å¦è¶‹è¿‘ 0ï¼‰ã€‚
3.  **Check 2**: è¾“å‡º Action çš„åˆ†å¸ƒã€‚æ˜¯å¦ä¸€ç›´è¾“å‡ºè¾¹ç•Œå€¼ï¼ˆSaturationï¼‰ï¼Ÿ
    *   å¦‚æœæ˜¯ `Tanh` æ¿€æ´»ï¼Œä¸€ç›´æ˜¯ 1 æˆ– -1 -> æœ€åä¸€å±‚åˆå§‹åŒ–æƒé‡è¿‡å¤§ã€‚
4.  **Check 3**: æ¢¯åº¦èŒƒæ•° (Gradient Norm)ã€‚å¦‚æœçªç„¶æš´æ¶¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é™¤ä»¥é›¶çš„æ“ä½œï¼ˆå¦‚ Standardize Advantage æ—¶ std=0ï¼‰ã€‚

---

## Appendix E: æƒå¨è®ºæ–‡å¯¼è¯» (Annotated Bibliography)

### åŸºç¡€ç®—æ³•
*   **DQN (2015)**: é¦–æ¬¡å°† CNN ä¸ Q-learning ç»“åˆã€‚
    *   *Key*: Experience Replay (æ‰“ç ´ç›¸å…³æ€§), Target Network (ç¨³å®šç›®æ ‡)ã€‚
*   **PPO (2017)**: å·¥ä¸šç•Œé»˜è®¤é¦–é€‰ã€‚
    *   *Key*: Clipped Surrogate Objective (é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦)ï¼Œç®€å•ä¸”ç¨³å¥ã€‚

### é«˜çº§ä¸»é¢˜
*   **GAE (Schulman 2015)**: *High-dimensional Continuous Control using GAE*.
    *   *Key*: Bias-Variance Tradeoffã€‚$\lambda=1$ æ˜¯ Monte Carlo (é«˜æ–¹å·®)ï¼Œ$\lambda=0$ æ˜¯ TD (é«˜åå·®)ã€‚
*   **SAC (2018)**: *Soft Actor-Critic*.
    *   *Key*: æœ€å¤§ç†µ (Max Entropy) ç›®æ ‡ $ \mathbb{E}[R + \alpha H(\pi)] $ï¼Œæå¤§æå‡äº†æ¢ç´¢èƒ½åŠ›å’Œé²æ£’æ€§ã€‚

### å¿…è¯»ç»¼è¿°
*   **Spinning Up in Deep RL**: OpenAI æ’°å†™çš„å…¥é—¨æ–‡æ¡£ï¼ŒåŒ…å«æä½³çš„ç®—æ³•ä¼ªä»£ç å’Œæ³¨æ„äº‹é¡¹ã€‚
*   **A Survey on Offline Reinforcement Learning (Levine 2020)**: ç¦»çº¿ RL çš„ç™¾ç§‘å…¨ä¹¦ã€‚

---

## Appendix F: æœ¯è¯­è¡¨ (Glossary)

*   **Episode/Rollout**: ä»åˆå§‹çŠ¶æ€åˆ°ç»ˆæ­¢çŠ¶æ€çš„ä¸€æ¬¡å®Œæ•´äº¤äº’åºåˆ—ã€‚
*   **Horizon (H)**: ä¸€ä¸ª Episode çš„æœ€å¤§æ­¥æ•°ã€‚
*   **Return (G)**: ç´¯ç§¯æŠ˜ç°å¥–åŠ± $\sum \gamma^t r_t$ã€‚
*   **On-Policy**: è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒå¿…é¡»ä¸å½“å‰ç­–ç•¥ä¸€è‡´ï¼ˆå¦‚ PPO, REINFORCEï¼‰ã€‚
*   **Off-Policy**: å¯ä»¥åˆ©ç”¨å†å²æ•°æ®ï¼ˆåˆ«äººäº§ç”Ÿçš„ç»éªŒï¼‰è¿›è¡Œè®­ç»ƒï¼ˆå¦‚ DQN, SACï¼‰ã€‚
*   **Model-Free**: ä¸å­¦ä¹  $P(s'|s,a)$ï¼Œç›´æ¥å­¦ Value æˆ– Policyã€‚
*   **Model-Based**: å­¦ä¹ ç¯å¢ƒæ¨¡å‹ï¼Œå¹¶åœ¨â€œè„‘æµ·ä¸­â€æ¨æ¼”ï¼ˆPlanningï¼‰ã€‚
*   **Sim-to-Real**: ä»¿çœŸåˆ°ç°å®çš„è¿ç§»ï¼Œä¸»è¦æŒ‘æˆ˜æ˜¯ Reality Gapã€‚
